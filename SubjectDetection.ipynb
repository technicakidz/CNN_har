{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "#create\n",
    "def plot_subject(subject,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(subject)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"user-id\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = read_data('actitracker_walk-1.txt')\n",
    "dataset = dataset.replace(\"\\t\",\",\",regex=True)#.replace(';',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x-axis</th>\n",
       "      <th>y-axis</th>\n",
       "      <th>z-axis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>4.939500e+13</td>\n",
       "      <td>-0.166767</td>\n",
       "      <td>-0.3409</td>\n",
       "      <td>0.723372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user-id activity     timestamp    x-axis  y-axis    z-axis\n",
       "0       33  Walking  4.939500e+13 -0.166767 -0.3409  0.723372"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111638, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAJ5CAYAAAD1rjIlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwk+AACADiX4AAAAOpTgAwAA6FD/Hw2GTN5jNuQWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8215f12d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAJ5CAYAAADijKLsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7kTAMRFsjjcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8215f12cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAJ5CAYAAADSALPNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8215d93c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAJ5CAYAAAD1rjIlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAABChCHwAAAAAEKEIfAAAAAAQof4/HnccOsrOunsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8215c9df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAJ5CAYAAADSALPNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZH0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8215b742b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAJ5CAYAAADSALPNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJmxEREREREQBigkbERERERFRgPp/wP027haJHTYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8214044a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAJ5CAYAAADijKLsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngMwJdgAAAJn7P/4Jhn0vurhcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81ec293d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAJ5CAYAAADSALPNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE6IsYDPGGGOMMcaYEPX/MXdDVtrQkx0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81ec188470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAJ7CAYAAACvRAPnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81e7fe4b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAJ5CAYAAADijKLsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81ec0cbef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#modify taerget(user-id)\n",
    "for subject in np.unique(dataset[\"user-id\"]):\n",
    "    subset = dataset[dataset[\"user-id\"] == subject][:40]\n",
    "    plot_subject(subject,subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saved data\n",
    "np.save('train_x.npy',train_x)\n",
    "np.save('train_y.npy',train_y)\n",
    "np.save('test_x.npy',test_x)\n",
    "np.save('test_y.npy',test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.load('train_x.npy')\n",
    "train_y = np.load('train_y.npy')\n",
    "test_x = np.load('test_x.npy')\n",
    "test_y = np.load('test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x-axis</th>\n",
       "      <th>y-axis</th>\n",
       "      <th>z-axis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>4.939500e+13</td>\n",
       "      <td>-0.166767</td>\n",
       "      <td>-0.3409</td>\n",
       "      <td>0.723372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user-id activity     timestamp    x-axis  y-axis    z-axis\n",
       "0       33  Walking  4.939500e+13 -0.166767 -0.3409  0.723372"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['timestamp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  783.345  Training Accuracy:  0.135955\n",
      "Epoch:  1  Training Loss:  416.585  Training Accuracy:  0.164607\n",
      "Epoch:  2  Training Loss:  279.622  Training Accuracy:  0.202247\n",
      "Epoch:  3  Training Loss:  196.635  Training Accuracy:  0.296067\n",
      "Epoch:  4  Training Loss:  147.199  Training Accuracy:  0.395506\n",
      "Epoch:  5  Training Loss:  118.547  Training Accuracy:  0.444382\n",
      "Epoch:  6  Training Loss:  101.19  Training Accuracy:  0.468539\n",
      "Epoch:  7  Training Loss:  88.0136  Training Accuracy:  0.494382\n",
      "Epoch:  8  Training Loss:  77.1177  Training Accuracy:  0.515169\n",
      "Epoch:  9  Training Loss:  68.254  Training Accuracy:  0.543258\n",
      "Epoch:  10  Training Loss:  61.5077  Training Accuracy:  0.564607\n",
      "Epoch:  11  Training Loss:  56.9802  Training Accuracy:  0.591573\n",
      "Epoch:  12  Training Loss:  53.8999  Training Accuracy:  0.616292\n",
      "Epoch:  13  Training Loss:  51.829  Training Accuracy:  0.647753\n",
      "Epoch:  14  Training Loss:  50.3583  Training Accuracy:  0.670225\n",
      "Epoch:  15  Training Loss:  49.1408  Training Accuracy:  0.691011\n",
      "Epoch:  16  Training Loss:  48.1426  Training Accuracy:  0.713483\n",
      "Epoch:  17  Training Loss:  47.2051  Training Accuracy:  0.735393\n",
      "Epoch:  18  Training Loss:  46.3039  Training Accuracy:  0.749438\n",
      "Epoch:  19  Training Loss:  45.4094  Training Accuracy:  0.761798\n",
      "Epoch:  20  Training Loss:  44.5245  Training Accuracy:  0.773596\n",
      "Epoch:  21  Training Loss:  43.6532  Training Accuracy:  0.792135\n",
      "Epoch:  22  Training Loss:  42.7598  Training Accuracy:  0.807303\n",
      "Epoch:  23  Training Loss:  41.8879  Training Accuracy:  0.817416\n",
      "Epoch:  24  Training Loss:  41.0852  Training Accuracy:  0.829775\n",
      "Epoch:  25  Training Loss:  40.2509  Training Accuracy:  0.836517\n",
      "Epoch:  26  Training Loss:  39.4664  Training Accuracy:  0.842135\n",
      "Epoch:  27  Training Loss:  38.6657  Training Accuracy:  0.851124\n",
      "Epoch:  28  Training Loss:  37.9367  Training Accuracy:  0.857303\n",
      "Epoch:  29  Training Loss:  37.2014  Training Accuracy:  0.863483\n",
      "Epoch:  30  Training Loss:  36.479  Training Accuracy:  0.867977\n",
      "Epoch:  31  Training Loss:  35.8288  Training Accuracy:  0.87191\n",
      "Epoch:  32  Training Loss:  35.1658  Training Accuracy:  0.876966\n",
      "Epoch:  33  Training Loss:  34.5212  Training Accuracy:  0.880899\n",
      "Epoch:  34  Training Loss:  33.8837  Training Accuracy:  0.88427\n",
      "Epoch:  35  Training Loss:  33.286  Training Accuracy:  0.885955\n",
      "Epoch:  36  Training Loss:  32.7003  Training Accuracy:  0.888764\n",
      "Epoch:  37  Training Loss:  32.1296  Training Accuracy:  0.890449\n",
      "Epoch:  38  Training Loss:  31.5894  Training Accuracy:  0.89382\n",
      "Epoch:  39  Training Loss:  31.0575  Training Accuracy:  0.897753\n",
      "Epoch:  40  Training Loss:  30.536  Training Accuracy:  0.901685\n",
      "Epoch:  41  Training Loss:  30.0399  Training Accuracy:  0.903371\n",
      "Epoch:  42  Training Loss:  29.5406  Training Accuracy:  0.905056\n",
      "Epoch:  43  Training Loss:  29.0606  Training Accuracy:  0.90618\n",
      "Epoch:  44  Training Loss:  28.5922  Training Accuracy:  0.907865\n",
      "Epoch:  45  Training Loss:  28.1402  Training Accuracy:  0.910674\n",
      "Epoch:  46  Training Loss:  27.7071  Training Accuracy:  0.912921\n",
      "Epoch:  47  Training Loss:  27.2846  Training Accuracy:  0.916292\n",
      "Epoch:  48  Training Loss:  26.8745  Training Accuracy:  0.917978\n",
      "Epoch:  49  Training Loss:  26.4775  Training Accuracy:  0.919663\n",
      "Epoch:  50  Training Loss:  26.0864  Training Accuracy:  0.920225\n",
      "Epoch:  51  Training Loss:  25.7041  Training Accuracy:  0.922472\n",
      "Epoch:  52  Training Loss:  25.3348  Training Accuracy:  0.924157\n",
      "Epoch:  53  Training Loss:  24.9718  Training Accuracy:  0.925281\n",
      "Epoch:  54  Training Loss:  24.6204  Training Accuracy:  0.926966\n",
      "Epoch:  55  Training Loss:  24.2772  Training Accuracy:  0.92809\n",
      "Epoch:  56  Training Loss:  23.9446  Training Accuracy:  0.929775\n",
      "Epoch:  57  Training Loss:  23.6195  Training Accuracy:  0.932022\n",
      "Epoch:  58  Training Loss:  23.3036  Training Accuracy:  0.933708\n",
      "Epoch:  59  Training Loss:  22.9951  Training Accuracy:  0.935393\n",
      "Epoch:  60  Training Loss:  22.6968  Training Accuracy:  0.936517\n",
      "Epoch:  61  Training Loss:  22.3955  Training Accuracy:  0.939326\n",
      "Epoch:  62  Training Loss:  22.1172  Training Accuracy:  0.940449\n",
      "Epoch:  63  Training Loss:  21.8398  Training Accuracy:  0.941011\n",
      "Epoch:  64  Training Loss:  21.5635  Training Accuracy:  0.943258\n",
      "Epoch:  65  Training Loss:  21.2887  Training Accuracy:  0.94382\n",
      "Epoch:  66  Training Loss:  21.0316  Training Accuracy:  0.944382\n",
      "Epoch:  67  Training Loss:  20.7744  Training Accuracy:  0.945506\n",
      "Epoch:  68  Training Loss:  20.5138  Training Accuracy:  0.948876\n",
      "Epoch:  69  Training Loss:  20.2751  Training Accuracy:  0.949438\n",
      "Epoch:  70  Training Loss:  20.0337  Training Accuracy:  0.95\n",
      "Epoch:  71  Training Loss:  19.7926  Training Accuracy:  0.950562\n",
      "Epoch:  72  Training Loss:  19.5675  Training Accuracy:  0.951124\n",
      "Epoch:  73  Training Loss:  19.3411  Training Accuracy:  0.951685\n",
      "Epoch:  74  Training Loss:  19.1141  Training Accuracy:  0.953371\n",
      "Epoch:  75  Training Loss:  18.9085  Training Accuracy:  0.955056\n",
      "Epoch:  76  Training Loss:  18.7002  Training Accuracy:  0.955056\n",
      "Epoch:  77  Training Loss:  18.4906  Training Accuracy:  0.955056\n",
      "Epoch:  78  Training Loss:  18.2892  Training Accuracy:  0.95618\n",
      "Epoch:  79  Training Loss:  18.0905  Training Accuracy:  0.957303\n",
      "Epoch:  80  Training Loss:  17.8904  Training Accuracy:  0.957865\n",
      "Epoch:  81  Training Loss:  17.703  Training Accuracy:  0.958427\n",
      "Epoch:  82  Training Loss:  17.5156  Training Accuracy:  0.958427\n",
      "Epoch:  83  Training Loss:  17.3276  Training Accuracy:  0.960112\n",
      "Epoch:  84  Training Loss:  17.1468  Training Accuracy:  0.960112\n",
      "Epoch:  85  Training Loss:  16.9692  Training Accuracy:  0.960112\n",
      "Epoch:  86  Training Loss:  16.7884  Training Accuracy:  0.960674\n",
      "Epoch:  87  Training Loss:  16.6198  Training Accuracy:  0.961798\n",
      "Epoch:  88  Training Loss:  16.451  Training Accuracy:  0.962359\n",
      "Epoch:  89  Training Loss:  16.2807  Training Accuracy:  0.962921\n",
      "Epoch:  90  Training Loss:  16.1206  Training Accuracy:  0.963483\n",
      "Epoch:  91  Training Loss:  15.9609  Training Accuracy:  0.964607\n",
      "Epoch:  92  Training Loss:  15.7981  Training Accuracy:  0.964607\n",
      "Epoch:  93  Training Loss:  15.6457  Training Accuracy:  0.964607\n",
      "Epoch:  94  Training Loss:  15.4934  Training Accuracy:  0.96573\n",
      "Epoch:  95  Training Loss:  15.3397  Training Accuracy:  0.966854\n",
      "Epoch:  96  Training Loss:  15.1936  Training Accuracy:  0.966854\n",
      "Epoch:  97  Training Loss:  15.0472  Training Accuracy:  0.966854\n",
      "Epoch:  98  Training Loss:  14.9038  Training Accuracy:  0.967416\n",
      "Epoch:  99  Training Loss:  14.7623  Training Accuracy:  0.967416\n",
      "Epoch:  100  Training Loss:  14.6237  Training Accuracy:  0.967416\n",
      "Epoch:  101  Training Loss:  14.4875  Training Accuracy:  0.967416\n",
      "Epoch:  102  Training Loss:  14.3533  Training Accuracy:  0.967416\n",
      "Epoch:  103  Training Loss:  14.2219  Training Accuracy:  0.967416\n",
      "Epoch:  104  Training Loss:  14.0916  Training Accuracy:  0.967977\n",
      "Epoch:  105  Training Loss:  13.963  Training Accuracy:  0.967977\n",
      "Epoch:  106  Training Loss:  13.8383  Training Accuracy:  0.967977\n",
      "Epoch:  107  Training Loss:  13.715  Training Accuracy:  0.968539\n",
      "Epoch:  108  Training Loss:  13.594  Training Accuracy:  0.968539\n",
      "Epoch:  109  Training Loss:  13.4737  Training Accuracy:  0.968539\n",
      "Epoch:  110  Training Loss:  13.3553  Training Accuracy:  0.968539\n",
      "Epoch:  111  Training Loss:  13.2382  Training Accuracy:  0.969101\n",
      "Epoch:  112  Training Loss:  13.1225  Training Accuracy:  0.970786\n",
      "Epoch:  113  Training Loss:  13.0095  Training Accuracy:  0.97191\n",
      "Epoch:  114  Training Loss:  12.8981  Training Accuracy:  0.973595\n",
      "Epoch:  115  Training Loss:  12.7882  Training Accuracy:  0.974157\n",
      "Epoch:  116  Training Loss:  12.6805  Training Accuracy:  0.974157\n",
      "Epoch:  117  Training Loss:  12.5735  Training Accuracy:  0.974157\n",
      "Epoch:  118  Training Loss:  12.4676  Training Accuracy:  0.975281\n",
      "Epoch:  119  Training Loss:  12.3629  Training Accuracy:  0.975843\n",
      "Epoch:  120  Training Loss:  12.2595  Training Accuracy:  0.976966\n",
      "Epoch:  121  Training Loss:  12.1576  Training Accuracy:  0.976966\n",
      "Epoch:  122  Training Loss:  12.0574  Training Accuracy:  0.976966\n",
      "Epoch:  123  Training Loss:  11.9581  Training Accuracy:  0.976966\n",
      "Epoch:  124  Training Loss:  11.8599  Training Accuracy:  0.976966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  11.7642  Training Accuracy:  0.977528\n",
      "Epoch:  126  Training Loss:  11.6691  Training Accuracy:  0.978652\n",
      "Epoch:  127  Training Loss:  11.5756  Training Accuracy:  0.978652\n",
      "Epoch:  128  Training Loss:  11.4827  Training Accuracy:  0.978652\n",
      "Epoch:  129  Training Loss:  11.3915  Training Accuracy:  0.979213\n",
      "Epoch:  130  Training Loss:  11.3001  Training Accuracy:  0.979213\n",
      "Epoch:  131  Training Loss:  11.2101  Training Accuracy:  0.979213\n",
      "Epoch:  132  Training Loss:  11.1209  Training Accuracy:  0.979213\n",
      "Epoch:  133  Training Loss:  11.0327  Training Accuracy:  0.979775\n",
      "Epoch:  134  Training Loss:  10.9458  Training Accuracy:  0.979775\n",
      "Epoch:  135  Training Loss:  10.8597  Training Accuracy:  0.979775\n",
      "Epoch:  136  Training Loss:  10.7755  Training Accuracy:  0.980337\n",
      "Epoch:  137  Training Loss:  10.6913  Training Accuracy:  0.980899\n",
      "Epoch:  138  Training Loss:  10.6082  Training Accuracy:  0.980899\n",
      "Epoch:  139  Training Loss:  10.5258  Training Accuracy:  0.980899\n",
      "Epoch:  140  Training Loss:  10.4447  Training Accuracy:  0.981461\n",
      "Epoch:  141  Training Loss:  10.3641  Training Accuracy:  0.981461\n",
      "Epoch:  142  Training Loss:  10.285  Training Accuracy:  0.981461\n",
      "Epoch:  143  Training Loss:  10.2059  Training Accuracy:  0.982022\n",
      "Epoch:  144  Training Loss:  10.1284  Training Accuracy:  0.982022\n",
      "Epoch:  145  Training Loss:  10.0512  Training Accuracy:  0.982022\n",
      "Epoch:  146  Training Loss:  9.97513  Training Accuracy:  0.982022\n",
      "Epoch:  147  Training Loss:  9.89892  Training Accuracy:  0.982022\n",
      "Epoch:  148  Training Loss:  9.82423  Training Accuracy:  0.982022\n",
      "Epoch:  149  Training Loss:  9.7506  Training Accuracy:  0.982584\n",
      "Epoch:  150  Training Loss:  9.67754  Training Accuracy:  0.982584\n",
      "Epoch:  151  Training Loss:  9.60556  Training Accuracy:  0.982584\n",
      "Epoch:  152  Training Loss:  9.53345  Training Accuracy:  0.982584\n",
      "Epoch:  153  Training Loss:  9.4618  Training Accuracy:  0.982584\n",
      "Epoch:  154  Training Loss:  9.39142  Training Accuracy:  0.982584\n",
      "Epoch:  155  Training Loss:  9.32141  Training Accuracy:  0.982584\n",
      "Epoch:  156  Training Loss:  9.25225  Training Accuracy:  0.982584\n",
      "Epoch:  157  Training Loss:  9.18367  Training Accuracy:  0.982584\n",
      "Epoch:  158  Training Loss:  9.11558  Training Accuracy:  0.982584\n",
      "Epoch:  159  Training Loss:  9.04819  Training Accuracy:  0.982584\n",
      "Epoch:  160  Training Loss:  8.98049  Training Accuracy:  0.982584\n",
      "Epoch:  161  Training Loss:  8.91422  Training Accuracy:  0.983146\n",
      "Epoch:  162  Training Loss:  8.848  Training Accuracy:  0.983708\n",
      "Epoch:  163  Training Loss:  8.78218  Training Accuracy:  0.983708\n",
      "Epoch:  164  Training Loss:  8.71687  Training Accuracy:  0.983708\n",
      "Epoch:  165  Training Loss:  8.65236  Training Accuracy:  0.98427\n",
      "Epoch:  166  Training Loss:  8.58871  Training Accuracy:  0.98427\n",
      "Epoch:  167  Training Loss:  8.52532  Training Accuracy:  0.98427\n",
      "Epoch:  168  Training Loss:  8.46191  Training Accuracy:  0.98427\n",
      "Epoch:  169  Training Loss:  8.39973  Training Accuracy:  0.98427\n",
      "Epoch:  170  Training Loss:  8.33756  Training Accuracy:  0.98427\n",
      "Epoch:  171  Training Loss:  8.27606  Training Accuracy:  0.98427\n",
      "Epoch:  172  Training Loss:  8.21456  Training Accuracy:  0.98427\n",
      "Epoch:  173  Training Loss:  8.15404  Training Accuracy:  0.98427\n",
      "Epoch:  174  Training Loss:  8.09396  Training Accuracy:  0.98427\n",
      "Epoch:  175  Training Loss:  8.0342  Training Accuracy:  0.98427\n",
      "Epoch:  176  Training Loss:  7.97504  Training Accuracy:  0.98427\n",
      "Epoch:  177  Training Loss:  7.9163  Training Accuracy:  0.98427\n",
      "Epoch:  178  Training Loss:  7.85813  Training Accuracy:  0.984831\n",
      "Epoch:  179  Training Loss:  7.80034  Training Accuracy:  0.984831\n",
      "Epoch:  180  Training Loss:  7.74322  Training Accuracy:  0.984831\n",
      "Epoch:  181  Training Loss:  7.68557  Training Accuracy:  0.984831\n",
      "Epoch:  182  Training Loss:  7.62933  Training Accuracy:  0.984831\n",
      "Epoch:  183  Training Loss:  7.57316  Training Accuracy:  0.984831\n",
      "Epoch:  184  Training Loss:  7.5173  Training Accuracy:  0.984831\n",
      "Epoch:  185  Training Loss:  7.46209  Training Accuracy:  0.985393\n",
      "Epoch:  186  Training Loss:  7.40735  Training Accuracy:  0.985393\n",
      "Epoch:  187  Training Loss:  7.35258  Training Accuracy:  0.985393\n",
      "Epoch:  188  Training Loss:  7.29858  Training Accuracy:  0.985393\n",
      "Epoch:  189  Training Loss:  7.24454  Training Accuracy:  0.985393\n",
      "Epoch:  190  Training Loss:  7.19116  Training Accuracy:  0.985393\n",
      "Epoch:  191  Training Loss:  7.13784  Training Accuracy:  0.985393\n",
      "Epoch:  192  Training Loss:  7.08535  Training Accuracy:  0.985393\n",
      "Epoch:  193  Training Loss:  7.03287  Training Accuracy:  0.985393\n",
      "Epoch:  194  Training Loss:  6.98041  Training Accuracy:  0.985393\n",
      "Epoch:  195  Training Loss:  6.92835  Training Accuracy:  0.985393\n",
      "Epoch:  196  Training Loss:  6.87718  Training Accuracy:  0.985393\n",
      "Epoch:  197  Training Loss:  6.82585  Training Accuracy:  0.985393\n",
      "Epoch:  198  Training Loss:  6.77541  Training Accuracy:  0.985393\n",
      "Epoch:  199  Training Loss:  6.72459  Training Accuracy:  0.985393\n",
      "Epoch:  200  Training Loss:  6.67501  Training Accuracy:  0.985393\n",
      "Epoch:  201  Training Loss:  6.62529  Training Accuracy:  0.985393\n",
      "Epoch:  202  Training Loss:  6.57602  Training Accuracy:  0.985393\n",
      "Epoch:  203  Training Loss:  6.52718  Training Accuracy:  0.985393\n",
      "Epoch:  204  Training Loss:  6.47877  Training Accuracy:  0.985393\n",
      "Epoch:  205  Training Loss:  6.43027  Training Accuracy:  0.985393\n",
      "Epoch:  206  Training Loss:  6.38277  Training Accuracy:  0.985393\n",
      "Epoch:  207  Training Loss:  6.33523  Training Accuracy:  0.985393\n",
      "Epoch:  208  Training Loss:  6.28863  Training Accuracy:  0.985393\n",
      "Epoch:  209  Training Loss:  6.24145  Training Accuracy:  0.985393\n",
      "Epoch:  210  Training Loss:  6.19531  Training Accuracy:  0.985393\n",
      "Epoch:  211  Training Loss:  6.14919  Training Accuracy:  0.985393\n",
      "Epoch:  212  Training Loss:  6.10337  Training Accuracy:  0.985393\n",
      "Epoch:  213  Training Loss:  6.05768  Training Accuracy:  0.985393\n",
      "Epoch:  214  Training Loss:  6.01234  Training Accuracy:  0.985393\n",
      "Epoch:  215  Training Loss:  5.96735  Training Accuracy:  0.985393\n",
      "Epoch:  216  Training Loss:  5.92261  Training Accuracy:  0.985393\n",
      "Epoch:  217  Training Loss:  5.87803  Training Accuracy:  0.985393\n",
      "Epoch:  218  Training Loss:  5.83417  Training Accuracy:  0.985393\n",
      "Epoch:  219  Training Loss:  5.79009  Training Accuracy:  0.985393\n",
      "Epoch:  220  Training Loss:  5.74706  Training Accuracy:  0.985393\n",
      "Epoch:  221  Training Loss:  5.70419  Training Accuracy:  0.985393\n",
      "Epoch:  222  Training Loss:  5.66169  Training Accuracy:  0.985393\n",
      "Epoch:  223  Training Loss:  5.61919  Training Accuracy:  0.985393\n",
      "Epoch:  224  Training Loss:  5.5771  Training Accuracy:  0.985393\n",
      "Epoch:  225  Training Loss:  5.53537  Training Accuracy:  0.985393\n",
      "Epoch:  226  Training Loss:  5.49387  Training Accuracy:  0.985393\n",
      "Epoch:  227  Training Loss:  5.45279  Training Accuracy:  0.985393\n",
      "Epoch:  228  Training Loss:  5.41182  Training Accuracy:  0.985393\n",
      "Epoch:  229  Training Loss:  5.37124  Training Accuracy:  0.985393\n",
      "Epoch:  230  Training Loss:  5.33112  Training Accuracy:  0.985393\n",
      "Epoch:  231  Training Loss:  5.29097  Training Accuracy:  0.985955\n",
      "Epoch:  232  Training Loss:  5.2515  Training Accuracy:  0.986517\n",
      "Epoch:  233  Training Loss:  5.21192  Training Accuracy:  0.987079\n",
      "Epoch:  234  Training Loss:  5.17296  Training Accuracy:  0.987079\n",
      "Epoch:  235  Training Loss:  5.13398  Training Accuracy:  0.987079\n",
      "Epoch:  236  Training Loss:  5.09534  Training Accuracy:  0.987079\n",
      "Epoch:  237  Training Loss:  5.05722  Training Accuracy:  0.987079\n",
      "Epoch:  238  Training Loss:  5.01909  Training Accuracy:  0.987079\n",
      "Epoch:  239  Training Loss:  4.98143  Training Accuracy:  0.987079\n",
      "Epoch:  240  Training Loss:  4.94404  Training Accuracy:  0.987079\n",
      "Epoch:  241  Training Loss:  4.90692  Training Accuracy:  0.98764\n",
      "Epoch:  242  Training Loss:  4.87032  Training Accuracy:  0.98764\n",
      "Epoch:  243  Training Loss:  4.83408  Training Accuracy:  0.98764\n",
      "Epoch:  244  Training Loss:  4.79775  Training Accuracy:  0.98764\n",
      "Epoch:  245  Training Loss:  4.76188  Training Accuracy:  0.98764\n",
      "Epoch:  246  Training Loss:  4.72621  Training Accuracy:  0.98764\n",
      "Epoch:  247  Training Loss:  4.69106  Training Accuracy:  0.98764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  4.65599  Training Accuracy:  0.98764\n",
      "Epoch:  249  Training Loss:  4.62152  Training Accuracy:  0.98764\n",
      "Epoch:  250  Training Loss:  4.5872  Training Accuracy:  0.98764\n",
      "Epoch:  251  Training Loss:  4.55304  Training Accuracy:  0.98764\n",
      "Epoch:  252  Training Loss:  4.51941  Training Accuracy:  0.98764\n",
      "Epoch:  253  Training Loss:  4.48579  Training Accuracy:  0.98764\n",
      "Epoch:  254  Training Loss:  4.45307  Training Accuracy:  0.98764\n",
      "Epoch:  255  Training Loss:  4.41995  Training Accuracy:  0.98764\n",
      "Epoch:  256  Training Loss:  4.38749  Training Accuracy:  0.98764\n",
      "Epoch:  257  Training Loss:  4.35467  Training Accuracy:  0.98764\n",
      "Epoch:  258  Training Loss:  4.32283  Training Accuracy:  0.98764\n",
      "Epoch:  259  Training Loss:  4.29058  Training Accuracy:  0.98764\n",
      "Epoch:  260  Training Loss:  4.2585  Training Accuracy:  0.98764\n",
      "Epoch:  261  Training Loss:  4.22732  Training Accuracy:  0.988202\n",
      "Epoch:  262  Training Loss:  4.19621  Training Accuracy:  0.988202\n",
      "Epoch:  263  Training Loss:  4.16564  Training Accuracy:  0.988764\n",
      "Epoch:  264  Training Loss:  4.13552  Training Accuracy:  0.988764\n",
      "Epoch:  265  Training Loss:  4.10549  Training Accuracy:  0.988764\n",
      "Epoch:  266  Training Loss:  4.07623  Training Accuracy:  0.988764\n",
      "Epoch:  267  Training Loss:  4.04721  Training Accuracy:  0.988764\n",
      "Epoch:  268  Training Loss:  4.01854  Training Accuracy:  0.988764\n",
      "Epoch:  269  Training Loss:  3.98998  Training Accuracy:  0.988764\n",
      "Epoch:  270  Training Loss:  3.96168  Training Accuracy:  0.988764\n",
      "Epoch:  271  Training Loss:  3.93368  Training Accuracy:  0.988764\n",
      "Epoch:  272  Training Loss:  3.90619  Training Accuracy:  0.988764\n",
      "Epoch:  273  Training Loss:  3.8786  Training Accuracy:  0.988764\n",
      "Epoch:  274  Training Loss:  3.85129  Training Accuracy:  0.988764\n",
      "Epoch:  275  Training Loss:  3.8242  Training Accuracy:  0.989326\n",
      "Epoch:  276  Training Loss:  3.79758  Training Accuracy:  0.989326\n",
      "Epoch:  277  Training Loss:  3.77123  Training Accuracy:  0.989326\n",
      "Epoch:  278  Training Loss:  3.74478  Training Accuracy:  0.989326\n",
      "Epoch:  279  Training Loss:  3.71891  Training Accuracy:  0.989326\n",
      "Epoch:  280  Training Loss:  3.69329  Training Accuracy:  0.989326\n",
      "Epoch:  281  Training Loss:  3.66782  Training Accuracy:  0.989326\n",
      "Epoch:  282  Training Loss:  3.64281  Training Accuracy:  0.989326\n",
      "Epoch:  283  Training Loss:  3.61775  Training Accuracy:  0.989326\n",
      "Epoch:  284  Training Loss:  3.59332  Training Accuracy:  0.989326\n",
      "Epoch:  285  Training Loss:  3.56886  Training Accuracy:  0.989326\n",
      "Epoch:  286  Training Loss:  3.54477  Training Accuracy:  0.989326\n",
      "Epoch:  287  Training Loss:  3.52097  Training Accuracy:  0.989326\n",
      "Epoch:  288  Training Loss:  3.49717  Training Accuracy:  0.989326\n",
      "Epoch:  289  Training Loss:  3.47388  Training Accuracy:  0.989888\n",
      "Epoch:  290  Training Loss:  3.45065  Training Accuracy:  0.990449\n",
      "Epoch:  291  Training Loss:  3.42764  Training Accuracy:  0.990449\n",
      "Epoch:  292  Training Loss:  3.40515  Training Accuracy:  0.990449\n",
      "Epoch:  293  Training Loss:  3.38256  Training Accuracy:  0.990449\n",
      "Epoch:  294  Training Loss:  3.36034  Training Accuracy:  0.990449\n",
      "Epoch:  295  Training Loss:  3.33826  Training Accuracy:  0.990449\n",
      "Epoch:  296  Training Loss:  3.31672  Training Accuracy:  0.990449\n",
      "Epoch:  297  Training Loss:  3.29528  Training Accuracy:  0.990449\n",
      "Epoch:  298  Training Loss:  3.2739  Training Accuracy:  0.990449\n",
      "Epoch:  299  Training Loss:  3.25301  Training Accuracy:  0.990449\n",
      "Epoch:  300  Training Loss:  3.23217  Training Accuracy:  0.990449\n",
      "Epoch:  301  Training Loss:  3.21171  Training Accuracy:  0.990449\n",
      "Epoch:  302  Training Loss:  3.19134  Training Accuracy:  0.990449\n",
      "Epoch:  303  Training Loss:  3.17107  Training Accuracy:  0.990449\n",
      "Epoch:  304  Training Loss:  3.15106  Training Accuracy:  0.990449\n",
      "Epoch:  305  Training Loss:  3.13123  Training Accuracy:  0.990449\n",
      "Epoch:  306  Training Loss:  3.11182  Training Accuracy:  0.991011\n",
      "Epoch:  307  Training Loss:  3.09259  Training Accuracy:  0.991011\n",
      "Epoch:  308  Training Loss:  3.07346  Training Accuracy:  0.991011\n",
      "Epoch:  309  Training Loss:  3.05458  Training Accuracy:  0.991011\n",
      "Epoch:  310  Training Loss:  3.03601  Training Accuracy:  0.991011\n",
      "Epoch:  311  Training Loss:  3.01748  Training Accuracy:  0.991011\n",
      "Epoch:  312  Training Loss:  2.99933  Training Accuracy:  0.991011\n",
      "Epoch:  313  Training Loss:  2.9812  Training Accuracy:  0.991573\n",
      "Epoch:  314  Training Loss:  2.9635  Training Accuracy:  0.991573\n",
      "Epoch:  315  Training Loss:  2.94591  Training Accuracy:  0.992135\n",
      "Epoch:  316  Training Loss:  2.92846  Training Accuracy:  0.992135\n",
      "Epoch:  317  Training Loss:  2.9113  Training Accuracy:  0.992135\n",
      "Epoch:  318  Training Loss:  2.89443  Training Accuracy:  0.992135\n",
      "Epoch:  319  Training Loss:  2.87755  Training Accuracy:  0.992135\n",
      "Epoch:  320  Training Loss:  2.86087  Training Accuracy:  0.992135\n",
      "Epoch:  321  Training Loss:  2.84446  Training Accuracy:  0.992135\n",
      "Epoch:  322  Training Loss:  2.82812  Training Accuracy:  0.992135\n",
      "Epoch:  323  Training Loss:  2.81215  Training Accuracy:  0.992135\n",
      "Epoch:  324  Training Loss:  2.79603  Training Accuracy:  0.992135\n",
      "Epoch:  325  Training Loss:  2.78043  Training Accuracy:  0.992135\n",
      "Epoch:  326  Training Loss:  2.76489  Training Accuracy:  0.992135\n",
      "Epoch:  327  Training Loss:  2.74949  Training Accuracy:  0.992697\n",
      "Epoch:  328  Training Loss:  2.73421  Training Accuracy:  0.993258\n",
      "Epoch:  329  Training Loss:  2.71921  Training Accuracy:  0.993258\n",
      "Epoch:  330  Training Loss:  2.70423  Training Accuracy:  0.993258\n",
      "Epoch:  331  Training Loss:  2.68946  Training Accuracy:  0.993258\n",
      "Epoch:  332  Training Loss:  2.67482  Training Accuracy:  0.993258\n",
      "Epoch:  333  Training Loss:  2.6604  Training Accuracy:  0.993258\n",
      "Epoch:  334  Training Loss:  2.64621  Training Accuracy:  0.993258\n",
      "Epoch:  335  Training Loss:  2.63202  Training Accuracy:  0.993258\n",
      "Epoch:  336  Training Loss:  2.61812  Training Accuracy:  0.99382\n",
      "Epoch:  337  Training Loss:  2.6042  Training Accuracy:  0.99382\n",
      "Epoch:  338  Training Loss:  2.59056  Training Accuracy:  0.99382\n",
      "Epoch:  339  Training Loss:  2.57699  Training Accuracy:  0.99382\n",
      "Epoch:  340  Training Loss:  2.56344  Training Accuracy:  0.99382\n",
      "Epoch:  341  Training Loss:  2.55028  Training Accuracy:  0.99382\n",
      "Epoch:  342  Training Loss:  2.53721  Training Accuracy:  0.99382\n",
      "Epoch:  343  Training Loss:  2.52432  Training Accuracy:  0.99382\n",
      "Epoch:  344  Training Loss:  2.51156  Training Accuracy:  0.99382\n",
      "Epoch:  345  Training Loss:  2.49893  Training Accuracy:  0.99382\n",
      "Epoch:  346  Training Loss:  2.48648  Training Accuracy:  0.99382\n",
      "Epoch:  347  Training Loss:  2.47424  Training Accuracy:  0.99382\n",
      "Epoch:  348  Training Loss:  2.46197  Training Accuracy:  0.99382\n",
      "Epoch:  349  Training Loss:  2.44986  Training Accuracy:  0.99382\n",
      "Epoch:  350  Training Loss:  2.43799  Training Accuracy:  0.99382\n",
      "Epoch:  351  Training Loss:  2.42605  Training Accuracy:  0.99382\n",
      "Epoch:  352  Training Loss:  2.41433  Training Accuracy:  0.99382\n",
      "Epoch:  353  Training Loss:  2.40264  Training Accuracy:  0.99382\n",
      "Epoch:  354  Training Loss:  2.39123  Training Accuracy:  0.99382\n",
      "Epoch:  355  Training Loss:  2.37988  Training Accuracy:  0.994382\n",
      "Epoch:  356  Training Loss:  2.36855  Training Accuracy:  0.994382\n",
      "Epoch:  357  Training Loss:  2.35751  Training Accuracy:  0.994382\n",
      "Epoch:  358  Training Loss:  2.34648  Training Accuracy:  0.994382\n",
      "Epoch:  359  Training Loss:  2.33559  Training Accuracy:  0.994382\n",
      "Epoch:  360  Training Loss:  2.32477  Training Accuracy:  0.994382\n",
      "Epoch:  361  Training Loss:  2.31401  Training Accuracy:  0.994382\n",
      "Epoch:  362  Training Loss:  2.30346  Training Accuracy:  0.994382\n",
      "Epoch:  363  Training Loss:  2.29295  Training Accuracy:  0.994382\n",
      "Epoch:  364  Training Loss:  2.28246  Training Accuracy:  0.994944\n",
      "Epoch:  365  Training Loss:  2.27212  Training Accuracy:  0.994944\n",
      "Epoch:  366  Training Loss:  2.26196  Training Accuracy:  0.994944\n",
      "Epoch:  367  Training Loss:  2.25171  Training Accuracy:  0.994944\n",
      "Epoch:  368  Training Loss:  2.24177  Training Accuracy:  0.994944\n",
      "Epoch:  369  Training Loss:  2.23168  Training Accuracy:  0.994944\n",
      "Epoch:  370  Training Loss:  2.22189  Training Accuracy:  0.994944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  2.21212  Training Accuracy:  0.994944\n",
      "Epoch:  372  Training Loss:  2.20237  Training Accuracy:  0.994944\n",
      "Epoch:  373  Training Loss:  2.19282  Training Accuracy:  0.994944\n",
      "Epoch:  374  Training Loss:  2.1833  Training Accuracy:  0.994944\n",
      "Epoch:  375  Training Loss:  2.17386  Training Accuracy:  0.994944\n",
      "Epoch:  376  Training Loss:  2.16451  Training Accuracy:  0.994944\n",
      "Epoch:  377  Training Loss:  2.15523  Training Accuracy:  0.994944\n",
      "Epoch:  378  Training Loss:  2.14617  Training Accuracy:  0.994944\n",
      "Epoch:  379  Training Loss:  2.1372  Training Accuracy:  0.994944\n",
      "Epoch:  380  Training Loss:  2.12817  Training Accuracy:  0.994944\n",
      "Epoch:  381  Training Loss:  2.11934  Training Accuracy:  0.994944\n",
      "Epoch:  382  Training Loss:  2.11056  Training Accuracy:  0.994944\n",
      "Epoch:  383  Training Loss:  2.10176  Training Accuracy:  0.994944\n",
      "Epoch:  384  Training Loss:  2.09315  Training Accuracy:  0.994944\n",
      "Epoch:  385  Training Loss:  2.08452  Training Accuracy:  0.994944\n",
      "Epoch:  386  Training Loss:  2.07601  Training Accuracy:  0.994944\n",
      "Epoch:  387  Training Loss:  2.06756  Training Accuracy:  0.994944\n",
      "Epoch:  388  Training Loss:  2.05916  Training Accuracy:  0.994944\n",
      "Epoch:  389  Training Loss:  2.05081  Training Accuracy:  0.994944\n",
      "Epoch:  390  Training Loss:  2.04264  Training Accuracy:  0.994944\n",
      "Epoch:  391  Training Loss:  2.03441  Training Accuracy:  0.994944\n",
      "Epoch:  392  Training Loss:  2.02637  Training Accuracy:  0.994944\n",
      "Epoch:  393  Training Loss:  2.01827  Training Accuracy:  0.994944\n",
      "Epoch:  394  Training Loss:  2.01027  Training Accuracy:  0.994944\n",
      "Epoch:  395  Training Loss:  2.00245  Training Accuracy:  0.994944\n",
      "Epoch:  396  Training Loss:  1.99458  Training Accuracy:  0.994944\n",
      "Epoch:  397  Training Loss:  1.98695  Training Accuracy:  0.994944\n",
      "Epoch:  398  Training Loss:  1.97921  Training Accuracy:  0.994944\n",
      "Epoch:  399  Training Loss:  1.97166  Training Accuracy:  0.994944\n",
      "Epoch:  400  Training Loss:  1.96418  Training Accuracy:  0.994944\n",
      "Epoch:  401  Training Loss:  1.95665  Training Accuracy:  0.994944\n",
      "Epoch:  402  Training Loss:  1.9493  Training Accuracy:  0.994944\n",
      "Epoch:  403  Training Loss:  1.94191  Training Accuracy:  0.994944\n",
      "Epoch:  404  Training Loss:  1.9346  Training Accuracy:  0.994944\n",
      "Epoch:  405  Training Loss:  1.92735  Training Accuracy:  0.994944\n",
      "Epoch:  406  Training Loss:  1.9201  Training Accuracy:  0.994944\n",
      "Epoch:  407  Training Loss:  1.91297  Training Accuracy:  0.994944\n",
      "Epoch:  408  Training Loss:  1.90584  Training Accuracy:  0.994944\n",
      "Epoch:  409  Training Loss:  1.89887  Training Accuracy:  0.994944\n",
      "Epoch:  410  Training Loss:  1.89184  Training Accuracy:  0.994944\n",
      "Epoch:  411  Training Loss:  1.88491  Training Accuracy:  0.994944\n",
      "Epoch:  412  Training Loss:  1.87787  Training Accuracy:  0.994944\n",
      "Epoch:  413  Training Loss:  1.87095  Training Accuracy:  0.994944\n",
      "Epoch:  414  Training Loss:  1.8642  Training Accuracy:  0.994944\n",
      "Epoch:  415  Training Loss:  1.85736  Training Accuracy:  0.994944\n",
      "Epoch:  416  Training Loss:  1.85069  Training Accuracy:  0.994944\n",
      "Epoch:  417  Training Loss:  1.84392  Training Accuracy:  0.994944\n",
      "Epoch:  418  Training Loss:  1.83735  Training Accuracy:  0.994944\n",
      "Epoch:  419  Training Loss:  1.83088  Training Accuracy:  0.994944\n",
      "Epoch:  420  Training Loss:  1.82429  Training Accuracy:  0.994944\n",
      "Epoch:  421  Training Loss:  1.8178  Training Accuracy:  0.994944\n",
      "Epoch:  422  Training Loss:  1.81132  Training Accuracy:  0.994944\n",
      "Epoch:  423  Training Loss:  1.805  Training Accuracy:  0.994944\n",
      "Epoch:  424  Training Loss:  1.79878  Training Accuracy:  0.994944\n",
      "Epoch:  425  Training Loss:  1.79244  Training Accuracy:  0.994944\n",
      "Epoch:  426  Training Loss:  1.78621  Training Accuracy:  0.994944\n",
      "Epoch:  427  Training Loss:  1.77999  Training Accuracy:  0.994944\n",
      "Epoch:  428  Training Loss:  1.77385  Training Accuracy:  0.994944\n",
      "Epoch:  429  Training Loss:  1.76787  Training Accuracy:  0.994944\n",
      "Epoch:  430  Training Loss:  1.76174  Training Accuracy:  0.994944\n",
      "Epoch:  431  Training Loss:  1.75571  Training Accuracy:  0.994944\n",
      "Epoch:  432  Training Loss:  1.74972  Training Accuracy:  0.994944\n",
      "Epoch:  433  Training Loss:  1.74375  Training Accuracy:  0.994944\n",
      "Epoch:  434  Training Loss:  1.73793  Training Accuracy:  0.994944\n",
      "Epoch:  435  Training Loss:  1.73198  Training Accuracy:  0.994944\n",
      "Epoch:  436  Training Loss:  1.7262  Training Accuracy:  0.994944\n",
      "Epoch:  437  Training Loss:  1.72036  Training Accuracy:  0.994944\n",
      "Epoch:  438  Training Loss:  1.71455  Training Accuracy:  0.994944\n",
      "Epoch:  439  Training Loss:  1.70892  Training Accuracy:  0.994944\n",
      "Epoch:  440  Training Loss:  1.70318  Training Accuracy:  0.994944\n",
      "Epoch:  441  Training Loss:  1.69752  Training Accuracy:  0.994944\n",
      "Epoch:  442  Training Loss:  1.69189  Training Accuracy:  0.994944\n",
      "Epoch:  443  Training Loss:  1.68629  Training Accuracy:  0.994944\n",
      "Epoch:  444  Training Loss:  1.68085  Training Accuracy:  0.994944\n",
      "Epoch:  445  Training Loss:  1.67531  Training Accuracy:  0.994944\n",
      "Epoch:  446  Training Loss:  1.66982  Training Accuracy:  0.994944\n",
      "Epoch:  447  Training Loss:  1.66438  Training Accuracy:  0.994944\n",
      "Epoch:  448  Training Loss:  1.65896  Training Accuracy:  0.994944\n",
      "Epoch:  449  Training Loss:  1.65368  Training Accuracy:  0.994944\n",
      "Epoch:  450  Training Loss:  1.64832  Training Accuracy:  0.994944\n",
      "Epoch:  451  Training Loss:  1.64295  Training Accuracy:  0.994944\n",
      "Epoch:  452  Training Loss:  1.63777  Training Accuracy:  0.994944\n",
      "Epoch:  453  Training Loss:  1.63254  Training Accuracy:  0.994944\n",
      "Epoch:  454  Training Loss:  1.62737  Training Accuracy:  0.994944\n",
      "Epoch:  455  Training Loss:  1.62225  Training Accuracy:  0.994944\n",
      "Epoch:  456  Training Loss:  1.61713  Training Accuracy:  0.994944\n",
      "Epoch:  457  Training Loss:  1.61206  Training Accuracy:  0.994944\n",
      "Epoch:  458  Training Loss:  1.60706  Training Accuracy:  0.994944\n",
      "Epoch:  459  Training Loss:  1.602  Training Accuracy:  0.994944\n",
      "Epoch:  460  Training Loss:  1.59711  Training Accuracy:  0.994944\n",
      "Epoch:  461  Training Loss:  1.59206  Training Accuracy:  0.994944\n",
      "Epoch:  462  Training Loss:  1.58718  Training Accuracy:  0.994944\n",
      "Epoch:  463  Training Loss:  1.58238  Training Accuracy:  0.994944\n",
      "Epoch:  464  Training Loss:  1.57748  Training Accuracy:  0.994944\n",
      "Epoch:  465  Training Loss:  1.57263  Training Accuracy:  0.994944\n",
      "Epoch:  466  Training Loss:  1.56783  Training Accuracy:  0.994944\n",
      "Epoch:  467  Training Loss:  1.56309  Training Accuracy:  0.994944\n",
      "Epoch:  468  Training Loss:  1.55831  Training Accuracy:  0.994944\n",
      "Epoch:  469  Training Loss:  1.55359  Training Accuracy:  0.994944\n",
      "Epoch:  470  Training Loss:  1.54888  Training Accuracy:  0.994944\n",
      "Epoch:  471  Training Loss:  1.54426  Training Accuracy:  0.994944\n",
      "Epoch:  472  Training Loss:  1.53964  Training Accuracy:  0.994944\n",
      "Epoch:  473  Training Loss:  1.53503  Training Accuracy:  0.994944\n",
      "Epoch:  474  Training Loss:  1.53041  Training Accuracy:  0.994944\n",
      "Epoch:  475  Training Loss:  1.52587  Training Accuracy:  0.994944\n",
      "Epoch:  476  Training Loss:  1.52138  Training Accuracy:  0.994944\n",
      "Epoch:  477  Training Loss:  1.51685  Training Accuracy:  0.994944\n",
      "Epoch:  478  Training Loss:  1.51238  Training Accuracy:  0.994944\n",
      "Epoch:  479  Training Loss:  1.50797  Training Accuracy:  0.994944\n",
      "Epoch:  480  Training Loss:  1.50358  Training Accuracy:  0.994944\n",
      "Epoch:  481  Training Loss:  1.49914  Training Accuracy:  0.994944\n",
      "Epoch:  482  Training Loss:  1.49479  Training Accuracy:  0.994944\n",
      "Epoch:  483  Training Loss:  1.49042  Training Accuracy:  0.994944\n",
      "Epoch:  484  Training Loss:  1.48609  Training Accuracy:  0.994944\n",
      "Epoch:  485  Training Loss:  1.48185  Training Accuracy:  0.994944\n",
      "Epoch:  486  Training Loss:  1.47759  Training Accuracy:  0.994944\n",
      "Epoch:  487  Training Loss:  1.47335  Training Accuracy:  0.994944\n",
      "Epoch:  488  Training Loss:  1.46908  Training Accuracy:  0.994944\n",
      "Epoch:  489  Training Loss:  1.46492  Training Accuracy:  0.994944\n",
      "Epoch:  490  Training Loss:  1.46076  Training Accuracy:  0.994944\n",
      "Epoch:  491  Training Loss:  1.45657  Training Accuracy:  0.994944\n",
      "Epoch:  492  Training Loss:  1.45252  Training Accuracy:  0.994944\n",
      "Epoch:  493  Training Loss:  1.44838  Training Accuracy:  0.994944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  1.44431  Training Accuracy:  0.994944\n",
      "Epoch:  495  Training Loss:  1.44025  Training Accuracy:  0.994944\n",
      "Epoch:  496  Training Loss:  1.43618  Training Accuracy:  0.994944\n",
      "Epoch:  497  Training Loss:  1.43225  Training Accuracy:  0.994944\n",
      "Epoch:  498  Training Loss:  1.42823  Training Accuracy:  0.994944\n",
      "Epoch:  499  Training Loss:  1.42422  Training Accuracy:  0.994944\n",
      "Epoch:  500  Training Loss:  1.42028  Training Accuracy:  0.994944\n",
      "Epoch:  501  Training Loss:  1.4164  Training Accuracy:  0.994944\n",
      "Epoch:  502  Training Loss:  1.41247  Training Accuracy:  0.994944\n",
      "Epoch:  503  Training Loss:  1.40853  Training Accuracy:  0.994944\n",
      "Epoch:  504  Training Loss:  1.40468  Training Accuracy:  0.994944\n",
      "Epoch:  505  Training Loss:  1.40083  Training Accuracy:  0.994944\n",
      "Epoch:  506  Training Loss:  1.39699  Training Accuracy:  0.994944\n",
      "Epoch:  507  Training Loss:  1.39318  Training Accuracy:  0.994944\n",
      "Epoch:  508  Training Loss:  1.38937  Training Accuracy:  0.994944\n",
      "Epoch:  509  Training Loss:  1.38567  Training Accuracy:  0.994944\n",
      "Epoch:  510  Training Loss:  1.3819  Training Accuracy:  0.994944\n",
      "Epoch:  511  Training Loss:  1.37818  Training Accuracy:  0.994944\n",
      "Epoch:  512  Training Loss:  1.37447  Training Accuracy:  0.994944\n",
      "Epoch:  513  Training Loss:  1.37081  Training Accuracy:  0.994944\n",
      "Epoch:  514  Training Loss:  1.36716  Training Accuracy:  0.994944\n",
      "Epoch:  515  Training Loss:  1.36355  Training Accuracy:  0.994944\n",
      "Epoch:  516  Training Loss:  1.3599  Training Accuracy:  0.994944\n",
      "Epoch:  517  Training Loss:  1.35632  Training Accuracy:  0.994944\n",
      "Epoch:  518  Training Loss:  1.35274  Training Accuracy:  0.994944\n",
      "Epoch:  519  Training Loss:  1.34915  Training Accuracy:  0.994944\n",
      "Epoch:  520  Training Loss:  1.34561  Training Accuracy:  0.994944\n",
      "Epoch:  521  Training Loss:  1.3421  Training Accuracy:  0.994944\n",
      "Epoch:  522  Training Loss:  1.33858  Training Accuracy:  0.994944\n",
      "Epoch:  523  Training Loss:  1.3351  Training Accuracy:  0.994944\n",
      "Epoch:  524  Training Loss:  1.3316  Training Accuracy:  0.994944\n",
      "Epoch:  525  Training Loss:  1.32818  Training Accuracy:  0.994944\n",
      "Epoch:  526  Training Loss:  1.32476  Training Accuracy:  0.994944\n",
      "Epoch:  527  Training Loss:  1.32131  Training Accuracy:  0.994944\n",
      "Epoch:  528  Training Loss:  1.31794  Training Accuracy:  0.994944\n",
      "Epoch:  529  Training Loss:  1.31458  Training Accuracy:  0.994944\n",
      "Epoch:  530  Training Loss:  1.31115  Training Accuracy:  0.994944\n",
      "Epoch:  531  Training Loss:  1.30784  Training Accuracy:  0.994944\n",
      "Epoch:  532  Training Loss:  1.30453  Training Accuracy:  0.994944\n",
      "Epoch:  533  Training Loss:  1.30117  Training Accuracy:  0.994944\n",
      "Epoch:  534  Training Loss:  1.29788  Training Accuracy:  0.994944\n",
      "Epoch:  535  Training Loss:  1.29457  Training Accuracy:  0.994944\n",
      "Epoch:  536  Training Loss:  1.29136  Training Accuracy:  0.994944\n",
      "Epoch:  537  Training Loss:  1.28813  Training Accuracy:  0.994944\n",
      "Epoch:  538  Training Loss:  1.28484  Training Accuracy:  0.994944\n",
      "Epoch:  539  Training Loss:  1.28161  Training Accuracy:  0.994944\n",
      "Epoch:  540  Training Loss:  1.27846  Training Accuracy:  0.994944\n",
      "Epoch:  541  Training Loss:  1.27521  Training Accuracy:  0.994944\n",
      "Epoch:  542  Training Loss:  1.27203  Training Accuracy:  0.994944\n",
      "Epoch:  543  Training Loss:  1.26891  Training Accuracy:  0.994944\n",
      "Epoch:  544  Training Loss:  1.26572  Training Accuracy:  0.994944\n",
      "Epoch:  545  Training Loss:  1.26257  Training Accuracy:  0.994944\n",
      "Epoch:  546  Training Loss:  1.25944  Training Accuracy:  0.994944\n",
      "Epoch:  547  Training Loss:  1.25635  Training Accuracy:  0.994944\n",
      "Epoch:  548  Training Loss:  1.25324  Training Accuracy:  0.994944\n",
      "Epoch:  549  Training Loss:  1.25013  Training Accuracy:  0.994944\n",
      "Epoch:  550  Training Loss:  1.2471  Training Accuracy:  0.994944\n",
      "Epoch:  551  Training Loss:  1.24405  Training Accuracy:  0.994944\n",
      "Epoch:  552  Training Loss:  1.24098  Training Accuracy:  0.994944\n",
      "Epoch:  553  Training Loss:  1.23797  Training Accuracy:  0.994944\n",
      "Epoch:  554  Training Loss:  1.23495  Training Accuracy:  0.994944\n",
      "Epoch:  555  Training Loss:  1.23196  Training Accuracy:  0.994944\n",
      "Epoch:  556  Training Loss:  1.22897  Training Accuracy:  0.994944\n",
      "Epoch:  557  Training Loss:  1.226  Training Accuracy:  0.994944\n",
      "Epoch:  558  Training Loss:  1.22307  Training Accuracy:  0.994944\n",
      "Epoch:  559  Training Loss:  1.22015  Training Accuracy:  0.994944\n",
      "Epoch:  560  Training Loss:  1.21719  Training Accuracy:  0.994944\n",
      "Epoch:  561  Training Loss:  1.21429  Training Accuracy:  0.994944\n",
      "Epoch:  562  Training Loss:  1.21136  Training Accuracy:  0.994944\n",
      "Epoch:  563  Training Loss:  1.2085  Training Accuracy:  0.994944\n",
      "Epoch:  564  Training Loss:  1.20559  Training Accuracy:  0.994944\n",
      "Epoch:  565  Training Loss:  1.20278  Training Accuracy:  0.994944\n",
      "Epoch:  566  Training Loss:  1.1999  Training Accuracy:  0.994944\n",
      "Epoch:  567  Training Loss:  1.19707  Training Accuracy:  0.994944\n",
      "Epoch:  568  Training Loss:  1.19426  Training Accuracy:  0.994944\n",
      "Epoch:  569  Training Loss:  1.19144  Training Accuracy:  0.994944\n",
      "Epoch:  570  Training Loss:  1.18861  Training Accuracy:  0.994944\n",
      "Epoch:  571  Training Loss:  1.1858  Training Accuracy:  0.994944\n",
      "Epoch:  572  Training Loss:  1.18304  Training Accuracy:  0.994944\n",
      "Epoch:  573  Training Loss:  1.18027  Training Accuracy:  0.994944\n",
      "Epoch:  574  Training Loss:  1.17753  Training Accuracy:  0.994944\n",
      "Epoch:  575  Training Loss:  1.17477  Training Accuracy:  0.994944\n",
      "Epoch:  576  Training Loss:  1.17205  Training Accuracy:  0.994944\n",
      "Epoch:  577  Training Loss:  1.16927  Training Accuracy:  0.994944\n",
      "Epoch:  578  Training Loss:  1.16657  Training Accuracy:  0.994944\n",
      "Epoch:  579  Training Loss:  1.16388  Training Accuracy:  0.994944\n",
      "Epoch:  580  Training Loss:  1.16119  Training Accuracy:  0.994944\n",
      "Epoch:  581  Training Loss:  1.15851  Training Accuracy:  0.994944\n",
      "Epoch:  582  Training Loss:  1.15588  Training Accuracy:  0.994944\n",
      "Epoch:  583  Training Loss:  1.15318  Training Accuracy:  0.994944\n",
      "Epoch:  584  Training Loss:  1.15056  Training Accuracy:  0.994944\n",
      "Epoch:  585  Training Loss:  1.1479  Training Accuracy:  0.994944\n",
      "Epoch:  586  Training Loss:  1.14532  Training Accuracy:  0.994944\n",
      "Epoch:  587  Training Loss:  1.14272  Training Accuracy:  0.994944\n",
      "Epoch:  588  Training Loss:  1.14011  Training Accuracy:  0.994944\n",
      "Epoch:  589  Training Loss:  1.13758  Training Accuracy:  0.994944\n",
      "Epoch:  590  Training Loss:  1.13501  Training Accuracy:  0.994944\n",
      "Epoch:  591  Training Loss:  1.13247  Training Accuracy:  0.994944\n",
      "Epoch:  592  Training Loss:  1.12995  Training Accuracy:  0.994944\n",
      "Epoch:  593  Training Loss:  1.12743  Training Accuracy:  0.994944\n",
      "Epoch:  594  Training Loss:  1.12491  Training Accuracy:  0.994944\n",
      "Epoch:  595  Training Loss:  1.12241  Training Accuracy:  0.994944\n",
      "Epoch:  596  Training Loss:  1.11994  Training Accuracy:  0.994944\n",
      "Epoch:  597  Training Loss:  1.11744  Training Accuracy:  0.994944\n",
      "Epoch:  598  Training Loss:  1.11497  Training Accuracy:  0.994944\n",
      "Epoch:  599  Training Loss:  1.11249  Training Accuracy:  0.994944\n",
      "Epoch:  600  Training Loss:  1.11007  Training Accuracy:  0.994944\n",
      "Epoch:  601  Training Loss:  1.10761  Training Accuracy:  0.994944\n",
      "Epoch:  602  Training Loss:  1.10519  Training Accuracy:  0.994944\n",
      "Epoch:  603  Training Loss:  1.10273  Training Accuracy:  0.994944\n",
      "Epoch:  604  Training Loss:  1.10035  Training Accuracy:  0.994944\n",
      "Epoch:  605  Training Loss:  1.09793  Training Accuracy:  0.994944\n",
      "Epoch:  606  Training Loss:  1.09555  Training Accuracy:  0.994944\n",
      "Epoch:  607  Training Loss:  1.09318  Training Accuracy:  0.994944\n",
      "Epoch:  608  Training Loss:  1.0908  Training Accuracy:  0.994944\n",
      "Epoch:  609  Training Loss:  1.08844  Training Accuracy:  0.994944\n",
      "Epoch:  610  Training Loss:  1.08609  Training Accuracy:  0.994944\n",
      "Epoch:  611  Training Loss:  1.08375  Training Accuracy:  0.994944\n",
      "Epoch:  612  Training Loss:  1.0814  Training Accuracy:  0.994944\n",
      "Epoch:  613  Training Loss:  1.0791  Training Accuracy:  0.994944\n",
      "Epoch:  614  Training Loss:  1.07679  Training Accuracy:  0.994944\n",
      "Epoch:  615  Training Loss:  1.07447  Training Accuracy:  0.994944\n",
      "Epoch:  616  Training Loss:  1.07218  Training Accuracy:  0.994944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  1.06992  Training Accuracy:  0.994944\n",
      "Epoch:  618  Training Loss:  1.06767  Training Accuracy:  0.994944\n",
      "Epoch:  619  Training Loss:  1.06535  Training Accuracy:  0.994944\n",
      "Epoch:  620  Training Loss:  1.06313  Training Accuracy:  0.994944\n",
      "Epoch:  621  Training Loss:  1.06088  Training Accuracy:  0.994944\n",
      "Epoch:  622  Training Loss:  1.05866  Training Accuracy:  0.994944\n",
      "Epoch:  623  Training Loss:  1.05642  Training Accuracy:  0.994944\n",
      "Epoch:  624  Training Loss:  1.05421  Training Accuracy:  0.994944\n",
      "Epoch:  625  Training Loss:  1.05198  Training Accuracy:  0.994944\n",
      "Epoch:  626  Training Loss:  1.0498  Training Accuracy:  0.994944\n",
      "Epoch:  627  Training Loss:  1.04758  Training Accuracy:  0.994944\n",
      "Epoch:  628  Training Loss:  1.04539  Training Accuracy:  0.994944\n",
      "Epoch:  629  Training Loss:  1.04326  Training Accuracy:  0.994944\n",
      "Epoch:  630  Training Loss:  1.04105  Training Accuracy:  0.994944\n",
      "Epoch:  631  Training Loss:  1.03891  Training Accuracy:  0.994944\n",
      "Epoch:  632  Training Loss:  1.03675  Training Accuracy:  0.994944\n",
      "Epoch:  633  Training Loss:  1.03463  Training Accuracy:  0.994944\n",
      "Epoch:  634  Training Loss:  1.03249  Training Accuracy:  0.994944\n",
      "Epoch:  635  Training Loss:  1.03036  Training Accuracy:  0.994944\n",
      "Epoch:  636  Training Loss:  1.02824  Training Accuracy:  0.994944\n",
      "Epoch:  637  Training Loss:  1.02614  Training Accuracy:  0.994944\n",
      "Epoch:  638  Training Loss:  1.02401  Training Accuracy:  0.994944\n",
      "Epoch:  639  Training Loss:  1.02196  Training Accuracy:  0.994944\n",
      "Epoch:  640  Training Loss:  1.01983  Training Accuracy:  0.994944\n",
      "Epoch:  641  Training Loss:  1.01777  Training Accuracy:  0.994944\n",
      "Epoch:  642  Training Loss:  1.0157  Training Accuracy:  0.994944\n",
      "Epoch:  643  Training Loss:  1.01365  Training Accuracy:  0.994944\n",
      "Epoch:  644  Training Loss:  1.01159  Training Accuracy:  0.994944\n",
      "Epoch:  645  Training Loss:  1.00951  Training Accuracy:  0.994944\n",
      "Epoch:  646  Training Loss:  1.00748  Training Accuracy:  0.994944\n",
      "Epoch:  647  Training Loss:  1.00546  Training Accuracy:  0.994944\n",
      "Epoch:  648  Training Loss:  1.00344  Training Accuracy:  0.994944\n",
      "Epoch:  649  Training Loss:  1.00141  Training Accuracy:  0.994944\n",
      "Epoch:  650  Training Loss:  0.999412  Training Accuracy:  0.994944\n",
      "Epoch:  651  Training Loss:  0.997424  Training Accuracy:  0.994944\n",
      "Epoch:  652  Training Loss:  0.995416  Training Accuracy:  0.994944\n",
      "Epoch:  653  Training Loss:  0.993446  Training Accuracy:  0.994944\n",
      "Epoch:  654  Training Loss:  0.991445  Training Accuracy:  0.994944\n",
      "Epoch:  655  Training Loss:  0.989476  Training Accuracy:  0.994944\n",
      "Epoch:  656  Training Loss:  0.987485  Training Accuracy:  0.994944\n",
      "Epoch:  657  Training Loss:  0.985559  Training Accuracy:  0.994944\n",
      "Epoch:  658  Training Loss:  0.983586  Training Accuracy:  0.994944\n",
      "Epoch:  659  Training Loss:  0.981645  Training Accuracy:  0.994944\n",
      "Epoch:  660  Training Loss:  0.979732  Training Accuracy:  0.994944\n",
      "Epoch:  661  Training Loss:  0.977783  Training Accuracy:  0.994944\n",
      "Epoch:  662  Training Loss:  0.975863  Training Accuracy:  0.994944\n",
      "Epoch:  663  Training Loss:  0.973962  Training Accuracy:  0.994944\n",
      "Epoch:  664  Training Loss:  0.972042  Training Accuracy:  0.994944\n",
      "Epoch:  665  Training Loss:  0.970118  Training Accuracy:  0.994944\n",
      "Epoch:  666  Training Loss:  0.968232  Training Accuracy:  0.994944\n",
      "Epoch:  667  Training Loss:  0.966314  Training Accuracy:  0.994944\n",
      "Epoch:  668  Training Loss:  0.964459  Training Accuracy:  0.994944\n",
      "Epoch:  669  Training Loss:  0.962618  Training Accuracy:  0.994944\n",
      "Epoch:  670  Training Loss:  0.960726  Training Accuracy:  0.994944\n",
      "Epoch:  671  Training Loss:  0.958876  Training Accuracy:  0.994944\n",
      "Epoch:  672  Training Loss:  0.957021  Training Accuracy:  0.994944\n",
      "Epoch:  673  Training Loss:  0.955198  Training Accuracy:  0.994944\n",
      "Epoch:  674  Training Loss:  0.953346  Training Accuracy:  0.994944\n",
      "Epoch:  675  Training Loss:  0.951516  Training Accuracy:  0.994944\n",
      "Epoch:  676  Training Loss:  0.949687  Training Accuracy:  0.994944\n",
      "Epoch:  677  Training Loss:  0.947871  Training Accuracy:  0.994944\n",
      "Epoch:  678  Training Loss:  0.946055  Training Accuracy:  0.994944\n",
      "Epoch:  679  Training Loss:  0.944265  Training Accuracy:  0.994944\n",
      "Epoch:  680  Training Loss:  0.942472  Training Accuracy:  0.994944\n",
      "Epoch:  681  Training Loss:  0.940654  Training Accuracy:  0.994944\n",
      "Epoch:  682  Training Loss:  0.938912  Training Accuracy:  0.994944\n",
      "Epoch:  683  Training Loss:  0.9371  Training Accuracy:  0.994944\n",
      "Epoch:  684  Training Loss:  0.935323  Training Accuracy:  0.994944\n",
      "Epoch:  685  Training Loss:  0.933574  Training Accuracy:  0.994944\n",
      "Epoch:  686  Training Loss:  0.931824  Training Accuracy:  0.994944\n",
      "Epoch:  687  Training Loss:  0.930028  Training Accuracy:  0.994944\n",
      "Epoch:  688  Training Loss:  0.928317  Training Accuracy:  0.994944\n",
      "Epoch:  689  Training Loss:  0.926567  Training Accuracy:  0.994944\n",
      "Epoch:  690  Training Loss:  0.924827  Training Accuracy:  0.994944\n",
      "Epoch:  691  Training Loss:  0.923117  Training Accuracy:  0.994944\n",
      "Epoch:  692  Training Loss:  0.921388  Training Accuracy:  0.994944\n",
      "Epoch:  693  Training Loss:  0.919676  Training Accuracy:  0.994944\n",
      "Epoch:  694  Training Loss:  0.917956  Training Accuracy:  0.994944\n",
      "Epoch:  695  Training Loss:  0.916239  Training Accuracy:  0.994944\n",
      "Epoch:  696  Training Loss:  0.914537  Training Accuracy:  0.994944\n",
      "Epoch:  697  Training Loss:  0.912827  Training Accuracy:  0.994944\n",
      "Epoch:  698  Training Loss:  0.911149  Training Accuracy:  0.994944\n",
      "Epoch:  699  Training Loss:  0.909445  Training Accuracy:  0.994944\n",
      "Epoch:  700  Training Loss:  0.907797  Training Accuracy:  0.994944\n",
      "Epoch:  701  Training Loss:  0.906097  Training Accuracy:  0.994944\n",
      "Epoch:  702  Training Loss:  0.904447  Training Accuracy:  0.994944\n",
      "Epoch:  703  Training Loss:  0.902785  Training Accuracy:  0.994944\n",
      "Epoch:  704  Training Loss:  0.901137  Training Accuracy:  0.994944\n",
      "Epoch:  705  Training Loss:  0.899475  Training Accuracy:  0.995506\n",
      "Epoch:  706  Training Loss:  0.897842  Training Accuracy:  0.995506\n",
      "Epoch:  707  Training Loss:  0.896195  Training Accuracy:  0.995506\n",
      "Epoch:  708  Training Loss:  0.894564  Training Accuracy:  0.995506\n",
      "Epoch:  709  Training Loss:  0.892965  Training Accuracy:  0.995506\n",
      "Epoch:  710  Training Loss:  0.891304  Training Accuracy:  0.995506\n",
      "Epoch:  711  Training Loss:  0.889699  Training Accuracy:  0.995506\n",
      "Epoch:  712  Training Loss:  0.888085  Training Accuracy:  0.995506\n",
      "Epoch:  713  Training Loss:  0.886488  Training Accuracy:  0.995506\n",
      "Epoch:  714  Training Loss:  0.884877  Training Accuracy:  0.995506\n",
      "Epoch:  715  Training Loss:  0.883291  Training Accuracy:  0.995506\n",
      "Epoch:  716  Training Loss:  0.881701  Training Accuracy:  0.995506\n",
      "Epoch:  717  Training Loss:  0.880091  Training Accuracy:  0.995506\n",
      "Epoch:  718  Training Loss:  0.878537  Training Accuracy:  0.995506\n",
      "Epoch:  719  Training Loss:  0.876927  Training Accuracy:  0.995506\n",
      "Epoch:  720  Training Loss:  0.875364  Training Accuracy:  0.995506\n",
      "Epoch:  721  Training Loss:  0.873813  Training Accuracy:  0.995506\n",
      "Epoch:  722  Training Loss:  0.872246  Training Accuracy:  0.995506\n",
      "Epoch:  723  Training Loss:  0.870694  Training Accuracy:  0.995506\n",
      "Epoch:  724  Training Loss:  0.869147  Training Accuracy:  0.995506\n",
      "Epoch:  725  Training Loss:  0.867614  Training Accuracy:  0.995506\n",
      "Epoch:  726  Training Loss:  0.866068  Training Accuracy:  0.995506\n",
      "Epoch:  727  Training Loss:  0.864547  Training Accuracy:  0.995506\n",
      "Epoch:  728  Training Loss:  0.86301  Training Accuracy:  0.995506\n",
      "Epoch:  729  Training Loss:  0.861487  Training Accuracy:  0.995506\n",
      "Epoch:  730  Training Loss:  0.859975  Training Accuracy:  0.995506\n",
      "Epoch:  731  Training Loss:  0.858462  Training Accuracy:  0.995506\n",
      "Epoch:  732  Training Loss:  0.856954  Training Accuracy:  0.995506\n",
      "Epoch:  733  Training Loss:  0.855462  Training Accuracy:  0.995506\n",
      "Epoch:  734  Training Loss:  0.853973  Training Accuracy:  0.995506\n",
      "Epoch:  735  Training Loss:  0.852473  Training Accuracy:  0.995506\n",
      "Epoch:  736  Training Loss:  0.851005  Training Accuracy:  0.995506\n",
      "Epoch:  737  Training Loss:  0.849506  Training Accuracy:  0.995506\n",
      "Epoch:  738  Training Loss:  0.848036  Training Accuracy:  0.995506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  739  Training Loss:  0.846555  Training Accuracy:  0.995506\n",
      "Epoch:  740  Training Loss:  0.845098  Training Accuracy:  0.995506\n",
      "Epoch:  741  Training Loss:  0.843637  Training Accuracy:  0.995506\n",
      "Epoch:  742  Training Loss:  0.842176  Training Accuracy:  0.995506\n",
      "Epoch:  743  Training Loss:  0.840726  Training Accuracy:  0.995506\n",
      "Epoch:  744  Training Loss:  0.839249  Training Accuracy:  0.995506\n",
      "Epoch:  745  Training Loss:  0.837829  Training Accuracy:  0.995506\n",
      "Epoch:  746  Training Loss:  0.836381  Training Accuracy:  0.995506\n",
      "Epoch:  747  Training Loss:  0.834945  Training Accuracy:  0.995506\n",
      "Epoch:  748  Training Loss:  0.833521  Training Accuracy:  0.995506\n",
      "Epoch:  749  Training Loss:  0.832103  Training Accuracy:  0.995506\n",
      "Epoch:  750  Training Loss:  0.830683  Training Accuracy:  0.995506\n",
      "Epoch:  751  Training Loss:  0.829293  Training Accuracy:  0.995506\n",
      "Epoch:  752  Training Loss:  0.827878  Training Accuracy:  0.995506\n",
      "Epoch:  753  Training Loss:  0.826477  Training Accuracy:  0.995506\n",
      "Epoch:  754  Training Loss:  0.825074  Training Accuracy:  0.995506\n",
      "Epoch:  755  Training Loss:  0.823671  Training Accuracy:  0.995506\n",
      "Epoch:  756  Training Loss:  0.822288  Training Accuracy:  0.995506\n",
      "Epoch:  757  Training Loss:  0.820898  Training Accuracy:  0.995506\n",
      "Epoch:  758  Training Loss:  0.819515  Training Accuracy:  0.995506\n",
      "Epoch:  759  Training Loss:  0.818146  Training Accuracy:  0.995506\n",
      "Epoch:  760  Training Loss:  0.816771  Training Accuracy:  0.995506\n",
      "Epoch:  761  Training Loss:  0.815389  Training Accuracy:  0.995506\n",
      "Epoch:  762  Training Loss:  0.814045  Training Accuracy:  0.995506\n",
      "Epoch:  763  Training Loss:  0.812687  Training Accuracy:  0.995506\n",
      "Epoch:  764  Training Loss:  0.811327  Training Accuracy:  0.995506\n",
      "Epoch:  765  Training Loss:  0.809978  Training Accuracy:  0.995506\n",
      "Epoch:  766  Training Loss:  0.808631  Training Accuracy:  0.995506\n",
      "Epoch:  767  Training Loss:  0.807283  Training Accuracy:  0.995506\n",
      "Epoch:  768  Training Loss:  0.805954  Training Accuracy:  0.995506\n",
      "Epoch:  769  Training Loss:  0.804614  Training Accuracy:  0.995506\n",
      "Epoch:  770  Training Loss:  0.803277  Training Accuracy:  0.995506\n",
      "Epoch:  771  Training Loss:  0.801952  Training Accuracy:  0.995506\n",
      "Epoch:  772  Training Loss:  0.800618  Training Accuracy:  0.995506\n",
      "Epoch:  773  Training Loss:  0.799304  Training Accuracy:  0.995506\n",
      "Epoch:  774  Training Loss:  0.797999  Training Accuracy:  0.995506\n",
      "Epoch:  775  Training Loss:  0.796662  Training Accuracy:  0.995506\n",
      "Epoch:  776  Training Loss:  0.79538  Training Accuracy:  0.995506\n",
      "Epoch:  777  Training Loss:  0.794059  Training Accuracy:  0.995506\n",
      "Epoch:  778  Training Loss:  0.792765  Training Accuracy:  0.995506\n",
      "Epoch:  779  Training Loss:  0.79147  Training Accuracy:  0.995506\n",
      "Epoch:  780  Training Loss:  0.79018  Training Accuracy:  0.995506\n",
      "Epoch:  781  Training Loss:  0.788897  Training Accuracy:  0.995506\n",
      "Epoch:  782  Training Loss:  0.787608  Training Accuracy:  0.995506\n",
      "Epoch:  783  Training Loss:  0.786336  Training Accuracy:  0.995506\n",
      "Epoch:  784  Training Loss:  0.785056  Training Accuracy:  0.995506\n",
      "Epoch:  785  Training Loss:  0.783786  Training Accuracy:  0.995506\n",
      "Epoch:  786  Training Loss:  0.782504  Training Accuracy:  0.995506\n",
      "Epoch:  787  Training Loss:  0.781243  Training Accuracy:  0.995506\n",
      "Epoch:  788  Training Loss:  0.779969  Training Accuracy:  0.995506\n",
      "Epoch:  789  Training Loss:  0.778723  Training Accuracy:  0.995506\n",
      "Epoch:  790  Training Loss:  0.777479  Training Accuracy:  0.995506\n",
      "Epoch:  791  Training Loss:  0.77622  Training Accuracy:  0.995506\n",
      "Epoch:  792  Training Loss:  0.77497  Training Accuracy:  0.995506\n",
      "Epoch:  793  Training Loss:  0.77373  Training Accuracy:  0.995506\n",
      "Epoch:  794  Training Loss:  0.772484  Training Accuracy:  0.995506\n",
      "Epoch:  795  Training Loss:  0.771247  Training Accuracy:  0.995506\n",
      "Epoch:  796  Training Loss:  0.770025  Training Accuracy:  0.995506\n",
      "Epoch:  797  Training Loss:  0.768803  Training Accuracy:  0.995506\n",
      "Epoch:  798  Training Loss:  0.767561  Training Accuracy:  0.995506\n",
      "Epoch:  799  Training Loss:  0.766356  Training Accuracy:  0.995506\n",
      "Epoch:  800  Training Loss:  0.765133  Training Accuracy:  0.995506\n",
      "Epoch:  801  Training Loss:  0.763942  Training Accuracy:  0.995506\n",
      "Epoch:  802  Training Loss:  0.762718  Training Accuracy:  0.995506\n",
      "Epoch:  803  Training Loss:  0.761521  Training Accuracy:  0.995506\n",
      "Epoch:  804  Training Loss:  0.760319  Training Accuracy:  0.995506\n",
      "Epoch:  805  Training Loss:  0.759132  Training Accuracy:  0.995506\n",
      "Epoch:  806  Training Loss:  0.757937  Training Accuracy:  0.995506\n",
      "Epoch:  807  Training Loss:  0.756752  Training Accuracy:  0.995506\n",
      "Epoch:  808  Training Loss:  0.755569  Training Accuracy:  0.995506\n",
      "Epoch:  809  Training Loss:  0.754369  Training Accuracy:  0.995506\n",
      "Epoch:  810  Training Loss:  0.753206  Training Accuracy:  0.995506\n",
      "Epoch:  811  Training Loss:  0.752017  Training Accuracy:  0.995506\n",
      "Epoch:  812  Training Loss:  0.75086  Training Accuracy:  0.995506\n",
      "Epoch:  813  Training Loss:  0.74969  Training Accuracy:  0.995506\n",
      "Epoch:  814  Training Loss:  0.748505  Training Accuracy:  0.995506\n",
      "Epoch:  815  Training Loss:  0.747363  Training Accuracy:  0.995506\n",
      "Epoch:  816  Training Loss:  0.746206  Training Accuracy:  0.995506\n",
      "Epoch:  817  Training Loss:  0.745062  Training Accuracy:  0.995506\n",
      "Epoch:  818  Training Loss:  0.743907  Training Accuracy:  0.995506\n",
      "Epoch:  819  Training Loss:  0.742759  Training Accuracy:  0.995506\n",
      "Epoch:  820  Training Loss:  0.741622  Training Accuracy:  0.995506\n",
      "Epoch:  821  Training Loss:  0.740495  Training Accuracy:  0.995506\n",
      "Epoch:  822  Training Loss:  0.739363  Training Accuracy:  0.995506\n",
      "Epoch:  823  Training Loss:  0.738244  Training Accuracy:  0.995506\n",
      "Epoch:  824  Training Loss:  0.737107  Training Accuracy:  0.995506\n",
      "Epoch:  825  Training Loss:  0.735986  Training Accuracy:  0.995506\n",
      "Epoch:  826  Training Loss:  0.734866  Training Accuracy:  0.995506\n",
      "Epoch:  827  Training Loss:  0.733747  Training Accuracy:  0.995506\n",
      "Epoch:  828  Training Loss:  0.73264  Training Accuracy:  0.995506\n",
      "Epoch:  829  Training Loss:  0.731536  Training Accuracy:  0.995506\n",
      "Epoch:  830  Training Loss:  0.730411  Training Accuracy:  0.995506\n",
      "Epoch:  831  Training Loss:  0.729311  Training Accuracy:  0.995506\n",
      "Epoch:  832  Training Loss:  0.7282  Training Accuracy:  0.995506\n",
      "Epoch:  833  Training Loss:  0.727102  Training Accuracy:  0.995506\n",
      "Epoch:  834  Training Loss:  0.725998  Training Accuracy:  0.995506\n",
      "Epoch:  835  Training Loss:  0.724918  Training Accuracy:  0.995506\n",
      "Epoch:  836  Training Loss:  0.723826  Training Accuracy:  0.995506\n",
      "Epoch:  837  Training Loss:  0.722742  Training Accuracy:  0.995506\n",
      "Epoch:  838  Training Loss:  0.721665  Training Accuracy:  0.995506\n",
      "Epoch:  839  Training Loss:  0.720574  Training Accuracy:  0.995506\n",
      "Epoch:  840  Training Loss:  0.719502  Training Accuracy:  0.995506\n",
      "Epoch:  841  Training Loss:  0.71842  Training Accuracy:  0.995506\n",
      "Epoch:  842  Training Loss:  0.717368  Training Accuracy:  0.995506\n",
      "Epoch:  843  Training Loss:  0.7163  Training Accuracy:  0.995506\n",
      "Epoch:  844  Training Loss:  0.71523  Training Accuracy:  0.995506\n",
      "Epoch:  845  Training Loss:  0.714172  Training Accuracy:  0.995506\n",
      "Epoch:  846  Training Loss:  0.713107  Training Accuracy:  0.995506\n",
      "Epoch:  847  Training Loss:  0.712063  Training Accuracy:  0.995506\n",
      "Epoch:  848  Training Loss:  0.711002  Training Accuracy:  0.995506\n",
      "Epoch:  849  Training Loss:  0.70995  Training Accuracy:  0.995506\n",
      "Epoch:  850  Training Loss:  0.708906  Training Accuracy:  0.995506\n",
      "Epoch:  851  Training Loss:  0.707861  Training Accuracy:  0.995506\n",
      "Epoch:  852  Training Loss:  0.706839  Training Accuracy:  0.995506\n",
      "Epoch:  853  Training Loss:  0.705781  Training Accuracy:  0.995506\n",
      "Epoch:  854  Training Loss:  0.704758  Training Accuracy:  0.995506\n",
      "Epoch:  855  Training Loss:  0.703721  Training Accuracy:  0.995506\n",
      "Epoch:  856  Training Loss:  0.702696  Training Accuracy:  0.995506\n",
      "Epoch:  857  Training Loss:  0.70168  Training Accuracy:  0.995506\n",
      "Epoch:  858  Training Loss:  0.700655  Training Accuracy:  0.995506\n",
      "Epoch:  859  Training Loss:  0.699639  Training Accuracy:  0.995506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  860  Training Loss:  0.698606  Training Accuracy:  0.995506\n",
      "Epoch:  861  Training Loss:  0.697597  Training Accuracy:  0.995506\n",
      "Epoch:  862  Training Loss:  0.696566  Training Accuracy:  0.995506\n",
      "Epoch:  863  Training Loss:  0.69557  Training Accuracy:  0.995506\n",
      "Epoch:  864  Training Loss:  0.694546  Training Accuracy:  0.995506\n",
      "Epoch:  865  Training Loss:  0.693548  Training Accuracy:  0.995506\n",
      "Epoch:  866  Training Loss:  0.692541  Training Accuracy:  0.995506\n",
      "Epoch:  867  Training Loss:  0.691535  Training Accuracy:  0.995506\n",
      "Epoch:  868  Training Loss:  0.690547  Training Accuracy:  0.995506\n",
      "Epoch:  869  Training Loss:  0.68955  Training Accuracy:  0.995506\n",
      "Epoch:  870  Training Loss:  0.688549  Training Accuracy:  0.995506\n",
      "Epoch:  871  Training Loss:  0.687571  Training Accuracy:  0.995506\n",
      "Epoch:  872  Training Loss:  0.686571  Training Accuracy:  0.995506\n",
      "Epoch:  873  Training Loss:  0.685606  Training Accuracy:  0.995506\n",
      "Epoch:  874  Training Loss:  0.684614  Training Accuracy:  0.995506\n",
      "Epoch:  875  Training Loss:  0.683628  Training Accuracy:  0.995506\n",
      "Epoch:  876  Training Loss:  0.682665  Training Accuracy:  0.995506\n",
      "Epoch:  877  Training Loss:  0.681683  Training Accuracy:  0.995506\n",
      "Epoch:  878  Training Loss:  0.680722  Training Accuracy:  0.995506\n",
      "Epoch:  879  Training Loss:  0.679745  Training Accuracy:  0.995506\n",
      "Epoch:  880  Training Loss:  0.67877  Training Accuracy:  0.995506\n",
      "Epoch:  881  Training Loss:  0.677814  Training Accuracy:  0.995506\n",
      "Epoch:  882  Training Loss:  0.676858  Training Accuracy:  0.995506\n",
      "Epoch:  883  Training Loss:  0.675907  Training Accuracy:  0.995506\n",
      "Epoch:  884  Training Loss:  0.67495  Training Accuracy:  0.995506\n",
      "Epoch:  885  Training Loss:  0.673992  Training Accuracy:  0.995506\n",
      "Epoch:  886  Training Loss:  0.673044  Training Accuracy:  0.995506\n",
      "Epoch:  887  Training Loss:  0.6721  Training Accuracy:  0.995506\n",
      "Epoch:  888  Training Loss:  0.671157  Training Accuracy:  0.995506\n",
      "Epoch:  889  Training Loss:  0.670218  Training Accuracy:  0.995506\n",
      "Epoch:  890  Training Loss:  0.669276  Training Accuracy:  0.995506\n",
      "Epoch:  891  Training Loss:  0.668335  Training Accuracy:  0.995506\n",
      "Epoch:  892  Training Loss:  0.6674  Training Accuracy:  0.995506\n",
      "Epoch:  893  Training Loss:  0.666469  Training Accuracy:  0.995506\n",
      "Epoch:  894  Training Loss:  0.665539  Training Accuracy:  0.995506\n",
      "Epoch:  895  Training Loss:  0.664616  Training Accuracy:  0.995506\n",
      "Epoch:  896  Training Loss:  0.663688  Training Accuracy:  0.995506\n",
      "Epoch:  897  Training Loss:  0.662772  Training Accuracy:  0.995506\n",
      "Epoch:  898  Training Loss:  0.661838  Training Accuracy:  0.995506\n",
      "Epoch:  899  Training Loss:  0.660927  Training Accuracy:  0.995506\n",
      "Epoch:  900  Training Loss:  0.660008  Training Accuracy:  0.995506\n",
      "Epoch:  901  Training Loss:  0.659095  Training Accuracy:  0.995506\n",
      "Epoch:  902  Training Loss:  0.658192  Training Accuracy:  0.995506\n",
      "Epoch:  903  Training Loss:  0.657274  Training Accuracy:  0.995506\n",
      "Epoch:  904  Training Loss:  0.656375  Training Accuracy:  0.995506\n",
      "Epoch:  905  Training Loss:  0.655469  Training Accuracy:  0.995506\n",
      "Epoch:  906  Training Loss:  0.654572  Training Accuracy:  0.995506\n",
      "Epoch:  907  Training Loss:  0.653685  Training Accuracy:  0.995506\n",
      "Epoch:  908  Training Loss:  0.652782  Training Accuracy:  0.995506\n",
      "Epoch:  909  Training Loss:  0.651904  Training Accuracy:  0.995506\n",
      "Epoch:  910  Training Loss:  0.651007  Training Accuracy:  0.995506\n",
      "Epoch:  911  Training Loss:  0.650135  Training Accuracy:  0.995506\n",
      "Epoch:  912  Training Loss:  0.649245  Training Accuracy:  0.995506\n",
      "Epoch:  913  Training Loss:  0.648373  Training Accuracy:  0.995506\n",
      "Epoch:  914  Training Loss:  0.647492  Training Accuracy:  0.995506\n",
      "Epoch:  915  Training Loss:  0.646618  Training Accuracy:  0.995506\n",
      "Epoch:  916  Training Loss:  0.645752  Training Accuracy:  0.995506\n",
      "Epoch:  917  Training Loss:  0.644876  Training Accuracy:  0.995506\n",
      "Epoch:  918  Training Loss:  0.644004  Training Accuracy:  0.995506\n",
      "Epoch:  919  Training Loss:  0.643148  Training Accuracy:  0.995506\n",
      "Epoch:  920  Training Loss:  0.642282  Training Accuracy:  0.995506\n",
      "Epoch:  921  Training Loss:  0.641428  Training Accuracy:  0.995506\n",
      "Epoch:  922  Training Loss:  0.640557  Training Accuracy:  0.995506\n",
      "Epoch:  923  Training Loss:  0.639702  Training Accuracy:  0.995506\n",
      "Epoch:  924  Training Loss:  0.63885  Training Accuracy:  0.995506\n",
      "Epoch:  925  Training Loss:  0.637999  Training Accuracy:  0.995506\n",
      "Epoch:  926  Training Loss:  0.637148  Training Accuracy:  0.995506\n",
      "Epoch:  927  Training Loss:  0.636297  Training Accuracy:  0.995506\n",
      "Epoch:  928  Training Loss:  0.635465  Training Accuracy:  0.995506\n",
      "Epoch:  929  Training Loss:  0.634608  Training Accuracy:  0.995506\n",
      "Epoch:  930  Training Loss:  0.633768  Training Accuracy:  0.995506\n",
      "Epoch:  931  Training Loss:  0.632936  Training Accuracy:  0.995506\n",
      "Epoch:  932  Training Loss:  0.632088  Training Accuracy:  0.995506\n",
      "Epoch:  933  Training Loss:  0.631258  Training Accuracy:  0.995506\n",
      "Epoch:  934  Training Loss:  0.630418  Training Accuracy:  0.995506\n",
      "Epoch:  935  Training Loss:  0.629596  Training Accuracy:  0.995506\n",
      "Epoch:  936  Training Loss:  0.628755  Training Accuracy:  0.995506\n",
      "Epoch:  937  Training Loss:  0.627942  Training Accuracy:  0.995506\n",
      "Epoch:  938  Training Loss:  0.627112  Training Accuracy:  0.995506\n",
      "Epoch:  939  Training Loss:  0.62629  Training Accuracy:  0.995506\n",
      "Epoch:  940  Training Loss:  0.625474  Training Accuracy:  0.995506\n",
      "Epoch:  941  Training Loss:  0.62465  Training Accuracy:  0.995506\n",
      "Epoch:  942  Training Loss:  0.623835  Training Accuracy:  0.995506\n",
      "Epoch:  943  Training Loss:  0.62302  Training Accuracy:  0.995506\n",
      "Epoch:  944  Training Loss:  0.622213  Training Accuracy:  0.995506\n",
      "Epoch:  945  Training Loss:  0.621401  Training Accuracy:  0.995506\n",
      "Epoch:  946  Training Loss:  0.620587  Training Accuracy:  0.995506\n",
      "Epoch:  947  Training Loss:  0.619783  Training Accuracy:  0.995506\n",
      "Epoch:  948  Training Loss:  0.618988  Training Accuracy:  0.995506\n",
      "Epoch:  949  Training Loss:  0.61818  Training Accuracy:  0.995506\n",
      "Epoch:  950  Training Loss:  0.61738  Training Accuracy:  0.995506\n",
      "Epoch:  951  Training Loss:  0.616577  Training Accuracy:  0.995506\n",
      "Epoch:  952  Training Loss:  0.615785  Training Accuracy:  0.995506\n",
      "Epoch:  953  Training Loss:  0.614987  Training Accuracy:  0.995506\n",
      "Epoch:  954  Training Loss:  0.614194  Training Accuracy:  0.995506\n",
      "Epoch:  955  Training Loss:  0.613405  Training Accuracy:  0.995506\n",
      "Epoch:  956  Training Loss:  0.612606  Training Accuracy:  0.995506\n",
      "Epoch:  957  Training Loss:  0.611825  Training Accuracy:  0.995506\n",
      "Epoch:  958  Training Loss:  0.611036  Training Accuracy:  0.995506\n",
      "Epoch:  959  Training Loss:  0.610253  Training Accuracy:  0.995506\n",
      "Epoch:  960  Training Loss:  0.60947  Training Accuracy:  0.995506\n",
      "Epoch:  961  Training Loss:  0.608687  Training Accuracy:  0.995506\n",
      "Epoch:  962  Training Loss:  0.607917  Training Accuracy:  0.995506\n",
      "Epoch:  963  Training Loss:  0.607136  Training Accuracy:  0.995506\n",
      "Epoch:  964  Training Loss:  0.606359  Training Accuracy:  0.995506\n",
      "Epoch:  965  Training Loss:  0.60559  Training Accuracy:  0.995506\n",
      "Epoch:  966  Training Loss:  0.604814  Training Accuracy:  0.995506\n",
      "Epoch:  967  Training Loss:  0.604046  Training Accuracy:  0.995506\n",
      "Epoch:  968  Training Loss:  0.603274  Training Accuracy:  0.995506\n",
      "Epoch:  969  Training Loss:  0.602517  Training Accuracy:  0.995506\n",
      "Epoch:  970  Training Loss:  0.601754  Training Accuracy:  0.995506\n",
      "Epoch:  971  Training Loss:  0.600988  Training Accuracy:  0.995506\n",
      "Epoch:  972  Training Loss:  0.600224  Training Accuracy:  0.995506\n",
      "Epoch:  973  Training Loss:  0.599475  Training Accuracy:  0.995506\n",
      "Epoch:  974  Training Loss:  0.598711  Training Accuracy:  0.995506\n",
      "Epoch:  975  Training Loss:  0.597967  Training Accuracy:  0.995506\n",
      "Epoch:  976  Training Loss:  0.597203  Training Accuracy:  0.995506\n",
      "Epoch:  977  Training Loss:  0.596456  Training Accuracy:  0.995506\n",
      "Epoch:  978  Training Loss:  0.59571  Training Accuracy:  0.995506\n",
      "Epoch:  979  Training Loss:  0.594954  Training Accuracy:  0.995506\n",
      "Epoch:  980  Training Loss:  0.594211  Training Accuracy:  0.995506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  981  Training Loss:  0.59347  Training Accuracy:  0.995506\n",
      "Epoch:  982  Training Loss:  0.592726  Training Accuracy:  0.995506\n",
      "Epoch:  983  Training Loss:  0.591992  Training Accuracy:  0.995506\n",
      "Epoch:  984  Training Loss:  0.591252  Training Accuracy:  0.995506\n",
      "Epoch:  985  Training Loss:  0.590507  Training Accuracy:  0.995506\n",
      "Epoch:  986  Training Loss:  0.589773  Training Accuracy:  0.995506\n",
      "Epoch:  987  Training Loss:  0.589036  Training Accuracy:  0.995506\n",
      "Epoch:  988  Training Loss:  0.588309  Training Accuracy:  0.995506\n",
      "Epoch:  989  Training Loss:  0.587573  Training Accuracy:  0.995506\n",
      "Epoch:  990  Training Loss:  0.586852  Training Accuracy:  0.995506\n",
      "Epoch:  991  Training Loss:  0.586117  Training Accuracy:  0.995506\n",
      "Epoch:  992  Training Loss:  0.585393  Training Accuracy:  0.995506\n",
      "Epoch:  993  Training Loss:  0.584665  Training Accuracy:  0.995506\n",
      "Epoch:  994  Training Loss:  0.58394  Training Accuracy:  0.995506\n",
      "Epoch:  995  Training Loss:  0.583229  Training Accuracy:  0.995506\n",
      "Epoch:  996  Training Loss:  0.582505  Training Accuracy:  0.995506\n",
      "Epoch:  997  Training Loss:  0.581786  Training Accuracy:  0.995506\n",
      "Epoch:  998  Training Loss:  0.581079  Training Accuracy:  0.995506\n",
      "Epoch:  999  Training Loss:  0.580362  Training Accuracy:  0.995506\n",
      "Epoch:  1000  Training Loss:  0.579643  Training Accuracy:  0.995506\n",
      "Epoch:  1001  Training Loss:  0.578937  Training Accuracy:  0.995506\n",
      "Epoch:  1002  Training Loss:  0.578233  Training Accuracy:  0.995506\n",
      "Epoch:  1003  Training Loss:  0.577527  Training Accuracy:  0.995506\n",
      "Epoch:  1004  Training Loss:  0.576819  Training Accuracy:  0.995506\n",
      "Epoch:  1005  Training Loss:  0.576114  Training Accuracy:  0.995506\n",
      "Epoch:  1006  Training Loss:  0.575422  Training Accuracy:  0.995506\n",
      "Epoch:  1007  Training Loss:  0.57472  Training Accuracy:  0.995506\n",
      "Epoch:  1008  Training Loss:  0.574012  Training Accuracy:  0.995506\n",
      "Epoch:  1009  Training Loss:  0.573319  Training Accuracy:  0.995506\n",
      "Epoch:  1010  Training Loss:  0.572633  Training Accuracy:  0.995506\n",
      "Epoch:  1011  Training Loss:  0.571928  Training Accuracy:  0.995506\n",
      "Epoch:  1012  Training Loss:  0.571239  Training Accuracy:  0.995506\n",
      "Epoch:  1013  Training Loss:  0.570546  Training Accuracy:  0.995506\n",
      "Epoch:  1014  Training Loss:  0.569861  Training Accuracy:  0.995506\n",
      "Epoch:  1015  Training Loss:  0.56917  Training Accuracy:  0.995506\n",
      "Epoch:  1016  Training Loss:  0.568487  Training Accuracy:  0.995506\n",
      "Epoch:  1017  Training Loss:  0.567798  Training Accuracy:  0.995506\n",
      "Epoch:  1018  Training Loss:  0.567117  Training Accuracy:  0.995506\n",
      "Epoch:  1019  Training Loss:  0.566428  Training Accuracy:  0.995506\n",
      "Epoch:  1020  Training Loss:  0.565749  Training Accuracy:  0.995506\n",
      "Epoch:  1021  Training Loss:  0.565078  Training Accuracy:  0.995506\n",
      "Epoch:  1022  Training Loss:  0.5644  Training Accuracy:  0.995506\n",
      "Epoch:  1023  Training Loss:  0.563722  Training Accuracy:  0.995506\n",
      "Epoch:  1024  Training Loss:  0.563052  Training Accuracy:  0.995506\n",
      "Epoch:  1025  Training Loss:  0.562375  Training Accuracy:  0.995506\n",
      "Epoch:  1026  Training Loss:  0.561706  Training Accuracy:  0.995506\n",
      "Epoch:  1027  Training Loss:  0.561037  Training Accuracy:  0.995506\n",
      "Epoch:  1028  Training Loss:  0.560375  Training Accuracy:  0.995506\n",
      "Epoch:  1029  Training Loss:  0.559704  Training Accuracy:  0.995506\n",
      "Epoch:  1030  Training Loss:  0.559039  Training Accuracy:  0.995506\n",
      "Epoch:  1031  Training Loss:  0.558371  Training Accuracy:  0.995506\n",
      "Epoch:  1032  Training Loss:  0.557718  Training Accuracy:  0.995506\n",
      "Epoch:  1033  Training Loss:  0.557056  Training Accuracy:  0.995506\n",
      "Epoch:  1034  Training Loss:  0.55639  Training Accuracy:  0.995506\n",
      "Epoch:  1035  Training Loss:  0.555737  Training Accuracy:  0.995506\n",
      "Epoch:  1036  Training Loss:  0.555091  Training Accuracy:  0.995506\n",
      "Epoch:  1037  Training Loss:  0.554431  Training Accuracy:  0.995506\n",
      "Epoch:  1038  Training Loss:  0.553776  Training Accuracy:  0.995506\n",
      "Epoch:  1039  Training Loss:  0.55313  Training Accuracy:  0.995506\n",
      "Epoch:  1040  Training Loss:  0.552471  Training Accuracy:  0.995506\n",
      "Epoch:  1041  Training Loss:  0.551824  Training Accuracy:  0.995506\n",
      "Epoch:  1042  Training Loss:  0.551183  Training Accuracy:  0.995506\n",
      "Epoch:  1043  Training Loss:  0.550532  Training Accuracy:  0.995506\n",
      "Epoch:  1044  Training Loss:  0.549882  Training Accuracy:  0.995506\n",
      "Epoch:  1045  Training Loss:  0.54925  Training Accuracy:  0.995506\n",
      "Epoch:  1046  Training Loss:  0.548594  Training Accuracy:  0.995506\n",
      "Epoch:  1047  Training Loss:  0.547956  Training Accuracy:  0.995506\n",
      "Epoch:  1048  Training Loss:  0.547307  Training Accuracy:  0.995506\n",
      "Epoch:  1049  Training Loss:  0.546678  Training Accuracy:  0.995506\n",
      "Epoch:  1050  Training Loss:  0.546038  Training Accuracy:  0.995506\n",
      "Epoch:  1051  Training Loss:  0.545396  Training Accuracy:  0.995506\n",
      "Epoch:  1052  Training Loss:  0.544765  Training Accuracy:  0.995506\n",
      "Epoch:  1053  Training Loss:  0.544138  Training Accuracy:  0.995506\n",
      "Epoch:  1054  Training Loss:  0.543497  Training Accuracy:  0.995506\n",
      "Epoch:  1055  Training Loss:  0.542874  Training Accuracy:  0.996067\n",
      "Epoch:  1056  Training Loss:  0.542247  Training Accuracy:  0.996067\n",
      "Epoch:  1057  Training Loss:  0.541616  Training Accuracy:  0.996067\n",
      "Epoch:  1058  Training Loss:  0.540992  Training Accuracy:  0.996067\n",
      "Epoch:  1059  Training Loss:  0.540371  Training Accuracy:  0.996067\n",
      "Epoch:  1060  Training Loss:  0.539741  Training Accuracy:  0.996067\n",
      "Epoch:  1061  Training Loss:  0.539123  Training Accuracy:  0.996067\n",
      "Epoch:  1062  Training Loss:  0.538514  Training Accuracy:  0.996067\n",
      "Epoch:  1063  Training Loss:  0.537887  Training Accuracy:  0.996067\n",
      "Epoch:  1064  Training Loss:  0.537271  Training Accuracy:  0.996067\n",
      "Epoch:  1065  Training Loss:  0.536658  Training Accuracy:  0.996067\n",
      "Epoch:  1066  Training Loss:  0.536046  Training Accuracy:  0.996067\n",
      "Epoch:  1067  Training Loss:  0.535427  Training Accuracy:  0.996067\n",
      "Epoch:  1068  Training Loss:  0.534826  Training Accuracy:  0.996067\n",
      "Epoch:  1069  Training Loss:  0.534212  Training Accuracy:  0.996067\n",
      "Epoch:  1070  Training Loss:  0.533602  Training Accuracy:  0.996067\n",
      "Epoch:  1071  Training Loss:  0.532999  Training Accuracy:  0.996067\n",
      "Epoch:  1072  Training Loss:  0.532393  Training Accuracy:  0.996067\n",
      "Epoch:  1073  Training Loss:  0.53179  Training Accuracy:  0.996067\n",
      "Epoch:  1074  Training Loss:  0.531188  Training Accuracy:  0.996067\n",
      "Epoch:  1075  Training Loss:  0.530588  Training Accuracy:  0.996067\n",
      "Epoch:  1076  Training Loss:  0.529995  Training Accuracy:  0.996067\n",
      "Epoch:  1077  Training Loss:  0.529396  Training Accuracy:  0.996067\n",
      "Epoch:  1078  Training Loss:  0.528804  Training Accuracy:  0.996067\n",
      "Epoch:  1079  Training Loss:  0.528207  Training Accuracy:  0.996067\n",
      "Epoch:  1080  Training Loss:  0.527618  Training Accuracy:  0.996067\n",
      "Epoch:  1081  Training Loss:  0.527027  Training Accuracy:  0.996067\n",
      "Epoch:  1082  Training Loss:  0.52644  Training Accuracy:  0.996067\n",
      "Epoch:  1083  Training Loss:  0.525849  Training Accuracy:  0.996067\n",
      "Epoch:  1084  Training Loss:  0.525265  Training Accuracy:  0.996067\n",
      "Epoch:  1085  Training Loss:  0.524681  Training Accuracy:  0.996067\n",
      "Epoch:  1086  Training Loss:  0.524096  Training Accuracy:  0.996067\n",
      "Epoch:  1087  Training Loss:  0.523514  Training Accuracy:  0.996067\n",
      "Epoch:  1088  Training Loss:  0.522935  Training Accuracy:  0.996067\n",
      "Epoch:  1089  Training Loss:  0.52235  Training Accuracy:  0.996067\n",
      "Epoch:  1090  Training Loss:  0.521774  Training Accuracy:  0.996067\n",
      "Epoch:  1091  Training Loss:  0.521197  Training Accuracy:  0.996067\n",
      "Epoch:  1092  Training Loss:  0.520615  Training Accuracy:  0.996067\n",
      "Epoch:  1093  Training Loss:  0.520049  Training Accuracy:  0.996067\n",
      "Epoch:  1094  Training Loss:  0.519469  Training Accuracy:  0.996067\n",
      "Epoch:  1095  Training Loss:  0.518902  Training Accuracy:  0.996067\n",
      "Epoch:  1096  Training Loss:  0.518328  Training Accuracy:  0.996067\n",
      "Epoch:  1097  Training Loss:  0.517764  Training Accuracy:  0.996067\n",
      "Epoch:  1098  Training Loss:  0.517178  Training Accuracy:  0.996067\n",
      "Epoch:  1099  Training Loss:  0.516622  Training Accuracy:  0.996067\n",
      "Epoch:  1100  Training Loss:  0.516055  Training Accuracy:  0.996067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1101  Training Loss:  0.515481  Training Accuracy:  0.996067\n",
      "Epoch:  1102  Training Loss:  0.51493  Training Accuracy:  0.996067\n",
      "Epoch:  1103  Training Loss:  0.514358  Training Accuracy:  0.996067\n",
      "Epoch:  1104  Training Loss:  0.513803  Training Accuracy:  0.996067\n",
      "Epoch:  1105  Training Loss:  0.513239  Training Accuracy:  0.996067\n",
      "Epoch:  1106  Training Loss:  0.512686  Training Accuracy:  0.996067\n",
      "Epoch:  1107  Training Loss:  0.512124  Training Accuracy:  0.996067\n",
      "Epoch:  1108  Training Loss:  0.511564  Training Accuracy:  0.996067\n",
      "Epoch:  1109  Training Loss:  0.511011  Training Accuracy:  0.996067\n",
      "Epoch:  1110  Training Loss:  0.510452  Training Accuracy:  0.996067\n",
      "Epoch:  1111  Training Loss:  0.509889  Training Accuracy:  0.996067\n",
      "Epoch:  1112  Training Loss:  0.509336  Training Accuracy:  0.996067\n",
      "Epoch:  1113  Training Loss:  0.508777  Training Accuracy:  0.996067\n",
      "Epoch:  1114  Training Loss:  0.508225  Training Accuracy:  0.996067\n",
      "Epoch:  1115  Training Loss:  0.507671  Training Accuracy:  0.996067\n",
      "Epoch:  1116  Training Loss:  0.507115  Training Accuracy:  0.996067\n",
      "Epoch:  1117  Training Loss:  0.50657  Training Accuracy:  0.996067\n",
      "Epoch:  1118  Training Loss:  0.506018  Training Accuracy:  0.996067\n",
      "Epoch:  1119  Training Loss:  0.505467  Training Accuracy:  0.996067\n",
      "Epoch:  1120  Training Loss:  0.504925  Training Accuracy:  0.996067\n",
      "Epoch:  1121  Training Loss:  0.504367  Training Accuracy:  0.996067\n",
      "Epoch:  1122  Training Loss:  0.503829  Training Accuracy:  0.996067\n",
      "Epoch:  1123  Training Loss:  0.503281  Training Accuracy:  0.996067\n",
      "Epoch:  1124  Training Loss:  0.502735  Training Accuracy:  0.996067\n",
      "Epoch:  1125  Training Loss:  0.502194  Training Accuracy:  0.996067\n",
      "Epoch:  1126  Training Loss:  0.501649  Training Accuracy:  0.996067\n",
      "Epoch:  1127  Training Loss:  0.501105  Training Accuracy:  0.996067\n",
      "Epoch:  1128  Training Loss:  0.500569  Training Accuracy:  0.996067\n",
      "Epoch:  1129  Training Loss:  0.50003  Training Accuracy:  0.996067\n",
      "Epoch:  1130  Training Loss:  0.49949  Training Accuracy:  0.996067\n",
      "Epoch:  1131  Training Loss:  0.498953  Training Accuracy:  0.996067\n",
      "Epoch:  1132  Training Loss:  0.49842  Training Accuracy:  0.996067\n",
      "Epoch:  1133  Training Loss:  0.497892  Training Accuracy:  0.996067\n",
      "Epoch:  1134  Training Loss:  0.497354  Training Accuracy:  0.996067\n",
      "Epoch:  1135  Training Loss:  0.496828  Training Accuracy:  0.996067\n",
      "Epoch:  1136  Training Loss:  0.496287  Training Accuracy:  0.996067\n",
      "Epoch:  1137  Training Loss:  0.495762  Training Accuracy:  0.996067\n",
      "Epoch:  1138  Training Loss:  0.49523  Training Accuracy:  0.996067\n",
      "Epoch:  1139  Training Loss:  0.494709  Training Accuracy:  0.996067\n",
      "Epoch:  1140  Training Loss:  0.494178  Training Accuracy:  0.996067\n",
      "Epoch:  1141  Training Loss:  0.493652  Training Accuracy:  0.996067\n",
      "Epoch:  1142  Training Loss:  0.493136  Training Accuracy:  0.996067\n",
      "Epoch:  1143  Training Loss:  0.492615  Training Accuracy:  0.996067\n",
      "Epoch:  1144  Training Loss:  0.492098  Training Accuracy:  0.996067\n",
      "Epoch:  1145  Training Loss:  0.491581  Training Accuracy:  0.996067\n",
      "Epoch:  1146  Training Loss:  0.491062  Training Accuracy:  0.996067\n",
      "Epoch:  1147  Training Loss:  0.49055  Training Accuracy:  0.996067\n",
      "Epoch:  1148  Training Loss:  0.490039  Training Accuracy:  0.996067\n",
      "Epoch:  1149  Training Loss:  0.489522  Training Accuracy:  0.996067\n",
      "Epoch:  1150  Training Loss:  0.489016  Training Accuracy:  0.996067\n",
      "Epoch:  1151  Training Loss:  0.4885  Training Accuracy:  0.996067\n",
      "Epoch:  1152  Training Loss:  0.487992  Training Accuracy:  0.996067\n",
      "Epoch:  1153  Training Loss:  0.487482  Training Accuracy:  0.996067\n",
      "Epoch:  1154  Training Loss:  0.486976  Training Accuracy:  0.996067\n",
      "Epoch:  1155  Training Loss:  0.486469  Training Accuracy:  0.996067\n",
      "Epoch:  1156  Training Loss:  0.485971  Training Accuracy:  0.996067\n",
      "Epoch:  1157  Training Loss:  0.485463  Training Accuracy:  0.996067\n",
      "Epoch:  1158  Training Loss:  0.484969  Training Accuracy:  0.996067\n",
      "Epoch:  1159  Training Loss:  0.484465  Training Accuracy:  0.996067\n",
      "Epoch:  1160  Training Loss:  0.48397  Training Accuracy:  0.996067\n",
      "Epoch:  1161  Training Loss:  0.483478  Training Accuracy:  0.996067\n",
      "Epoch:  1162  Training Loss:  0.482983  Training Accuracy:  0.996067\n",
      "Epoch:  1163  Training Loss:  0.482488  Training Accuracy:  0.996067\n",
      "Epoch:  1164  Training Loss:  0.481997  Training Accuracy:  0.996067\n",
      "Epoch:  1165  Training Loss:  0.481502  Training Accuracy:  0.996067\n",
      "Epoch:  1166  Training Loss:  0.481012  Training Accuracy:  0.996067\n",
      "Epoch:  1167  Training Loss:  0.480517  Training Accuracy:  0.996067\n",
      "Epoch:  1168  Training Loss:  0.480024  Training Accuracy:  0.996067\n",
      "Epoch:  1169  Training Loss:  0.479534  Training Accuracy:  0.996067\n",
      "Epoch:  1170  Training Loss:  0.479048  Training Accuracy:  0.996067\n",
      "Epoch:  1171  Training Loss:  0.478563  Training Accuracy:  0.996067\n",
      "Epoch:  1172  Training Loss:  0.478064  Training Accuracy:  0.996067\n",
      "Epoch:  1173  Training Loss:  0.477579  Training Accuracy:  0.996067\n",
      "Epoch:  1174  Training Loss:  0.477088  Training Accuracy:  0.996067\n",
      "Epoch:  1175  Training Loss:  0.476599  Training Accuracy:  0.996067\n",
      "Epoch:  1176  Training Loss:  0.476109  Training Accuracy:  0.996067\n",
      "Epoch:  1177  Training Loss:  0.475623  Training Accuracy:  0.996067\n",
      "Epoch:  1178  Training Loss:  0.475133  Training Accuracy:  0.996067\n",
      "Epoch:  1179  Training Loss:  0.474654  Training Accuracy:  0.996067\n",
      "Epoch:  1180  Training Loss:  0.474164  Training Accuracy:  0.996067\n",
      "Epoch:  1181  Training Loss:  0.473685  Training Accuracy:  0.996067\n",
      "Epoch:  1182  Training Loss:  0.473192  Training Accuracy:  0.996067\n",
      "Epoch:  1183  Training Loss:  0.472715  Training Accuracy:  0.996067\n",
      "Epoch:  1184  Training Loss:  0.472235  Training Accuracy:  0.996067\n",
      "Epoch:  1185  Training Loss:  0.471748  Training Accuracy:  0.996067\n",
      "Epoch:  1186  Training Loss:  0.471275  Training Accuracy:  0.996067\n",
      "Epoch:  1187  Training Loss:  0.470793  Training Accuracy:  0.996067\n",
      "Epoch:  1188  Training Loss:  0.47032  Training Accuracy:  0.996067\n",
      "Epoch:  1189  Training Loss:  0.469843  Training Accuracy:  0.996067\n",
      "Epoch:  1190  Training Loss:  0.469364  Training Accuracy:  0.996067\n",
      "Epoch:  1191  Training Loss:  0.468886  Training Accuracy:  0.996067\n",
      "Epoch:  1192  Training Loss:  0.468413  Training Accuracy:  0.996067\n",
      "Epoch:  1193  Training Loss:  0.467947  Training Accuracy:  0.996067\n",
      "Epoch:  1194  Training Loss:  0.467472  Training Accuracy:  0.996067\n",
      "Epoch:  1195  Training Loss:  0.466999  Training Accuracy:  0.996067\n",
      "Epoch:  1196  Training Loss:  0.466532  Training Accuracy:  0.996067\n",
      "Epoch:  1197  Training Loss:  0.466065  Training Accuracy:  0.996067\n",
      "Epoch:  1198  Training Loss:  0.465592  Training Accuracy:  0.996067\n",
      "Epoch:  1199  Training Loss:  0.465122  Training Accuracy:  0.996067\n",
      "Epoch:  1200  Training Loss:  0.464653  Training Accuracy:  0.996067\n",
      "Epoch:  1201  Training Loss:  0.464186  Training Accuracy:  0.996067\n",
      "Epoch:  1202  Training Loss:  0.463724  Training Accuracy:  0.996067\n",
      "Epoch:  1203  Training Loss:  0.463255  Training Accuracy:  0.996067\n",
      "Epoch:  1204  Training Loss:  0.462793  Training Accuracy:  0.996067\n",
      "Epoch:  1205  Training Loss:  0.462327  Training Accuracy:  0.996067\n",
      "Epoch:  1206  Training Loss:  0.461867  Training Accuracy:  0.996067\n",
      "Epoch:  1207  Training Loss:  0.461407  Training Accuracy:  0.996067\n",
      "Epoch:  1208  Training Loss:  0.460941  Training Accuracy:  0.996067\n",
      "Epoch:  1209  Training Loss:  0.460489  Training Accuracy:  0.996067\n",
      "Epoch:  1210  Training Loss:  0.460024  Training Accuracy:  0.996067\n",
      "Epoch:  1211  Training Loss:  0.45957  Training Accuracy:  0.996067\n",
      "Epoch:  1212  Training Loss:  0.459115  Training Accuracy:  0.996067\n",
      "Epoch:  1213  Training Loss:  0.458659  Training Accuracy:  0.996067\n",
      "Epoch:  1214  Training Loss:  0.458199  Training Accuracy:  0.996067\n",
      "Epoch:  1215  Training Loss:  0.457756  Training Accuracy:  0.996067\n",
      "Epoch:  1216  Training Loss:  0.457293  Training Accuracy:  0.996067\n",
      "Epoch:  1217  Training Loss:  0.456847  Training Accuracy:  0.996067\n",
      "Epoch:  1218  Training Loss:  0.456393  Training Accuracy:  0.996067\n",
      "Epoch:  1219  Training Loss:  0.455947  Training Accuracy:  0.996067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1220  Training Loss:  0.455497  Training Accuracy:  0.996067\n",
      "Epoch:  1221  Training Loss:  0.455049  Training Accuracy:  0.996067\n",
      "Epoch:  1222  Training Loss:  0.454607  Training Accuracy:  0.996067\n",
      "Epoch:  1223  Training Loss:  0.454156  Training Accuracy:  0.996067\n",
      "Epoch:  1224  Training Loss:  0.453713  Training Accuracy:  0.996067\n",
      "Epoch:  1225  Training Loss:  0.453269  Training Accuracy:  0.996067\n",
      "Epoch:  1226  Training Loss:  0.45282  Training Accuracy:  0.996067\n",
      "Epoch:  1227  Training Loss:  0.452379  Training Accuracy:  0.996067\n",
      "Epoch:  1228  Training Loss:  0.45194  Training Accuracy:  0.996067\n",
      "Epoch:  1229  Training Loss:  0.451493  Training Accuracy:  0.996067\n",
      "Epoch:  1230  Training Loss:  0.451051  Training Accuracy:  0.996067\n",
      "Epoch:  1231  Training Loss:  0.45061  Training Accuracy:  0.996067\n",
      "Epoch:  1232  Training Loss:  0.450171  Training Accuracy:  0.996067\n",
      "Epoch:  1233  Training Loss:  0.449733  Training Accuracy:  0.996067\n",
      "Epoch:  1234  Training Loss:  0.449294  Training Accuracy:  0.996067\n",
      "Epoch:  1235  Training Loss:  0.448854  Training Accuracy:  0.996067\n",
      "Epoch:  1236  Training Loss:  0.448417  Training Accuracy:  0.996067\n",
      "Epoch:  1237  Training Loss:  0.447982  Training Accuracy:  0.996067\n",
      "Epoch:  1238  Training Loss:  0.447549  Training Accuracy:  0.996067\n",
      "Epoch:  1239  Training Loss:  0.447116  Training Accuracy:  0.996067\n",
      "Epoch:  1240  Training Loss:  0.446685  Training Accuracy:  0.996067\n",
      "Epoch:  1241  Training Loss:  0.446248  Training Accuracy:  0.996067\n",
      "Epoch:  1242  Training Loss:  0.44582  Training Accuracy:  0.996067\n",
      "Epoch:  1243  Training Loss:  0.445388  Training Accuracy:  0.996067\n",
      "Epoch:  1244  Training Loss:  0.444952  Training Accuracy:  0.996067\n",
      "Epoch:  1245  Training Loss:  0.44453  Training Accuracy:  0.996067\n",
      "Epoch:  1246  Training Loss:  0.444098  Training Accuracy:  0.996067\n",
      "Epoch:  1247  Training Loss:  0.443672  Training Accuracy:  0.996067\n",
      "Epoch:  1248  Training Loss:  0.443244  Training Accuracy:  0.996067\n",
      "Epoch:  1249  Training Loss:  0.442819  Training Accuracy:  0.996067\n",
      "Epoch:  1250  Training Loss:  0.442391  Training Accuracy:  0.996067\n",
      "Epoch:  1251  Training Loss:  0.44197  Training Accuracy:  0.996067\n",
      "Epoch:  1252  Training Loss:  0.441542  Training Accuracy:  0.996067\n",
      "Epoch:  1253  Training Loss:  0.441122  Training Accuracy:  0.996067\n",
      "Epoch:  1254  Training Loss:  0.440697  Training Accuracy:  0.996067\n",
      "Epoch:  1255  Training Loss:  0.440274  Training Accuracy:  0.996067\n",
      "Epoch:  1256  Training Loss:  0.439856  Training Accuracy:  0.996067\n",
      "Epoch:  1257  Training Loss:  0.439429  Training Accuracy:  0.996067\n",
      "Epoch:  1258  Training Loss:  0.439018  Training Accuracy:  0.996067\n",
      "Epoch:  1259  Training Loss:  0.438596  Training Accuracy:  0.996067\n",
      "Epoch:  1260  Training Loss:  0.43818  Training Accuracy:  0.996067\n",
      "Epoch:  1261  Training Loss:  0.437767  Training Accuracy:  0.996067\n",
      "Epoch:  1262  Training Loss:  0.437346  Training Accuracy:  0.996067\n",
      "Epoch:  1263  Training Loss:  0.436935  Training Accuracy:  0.996067\n",
      "Epoch:  1264  Training Loss:  0.436524  Training Accuracy:  0.996067\n",
      "Epoch:  1265  Training Loss:  0.436109  Training Accuracy:  0.996067\n",
      "Epoch:  1266  Training Loss:  0.435697  Training Accuracy:  0.996067\n",
      "Epoch:  1267  Training Loss:  0.435286  Training Accuracy:  0.996067\n",
      "Epoch:  1268  Training Loss:  0.434874  Training Accuracy:  0.996067\n",
      "Epoch:  1269  Training Loss:  0.434463  Training Accuracy:  0.996067\n",
      "Epoch:  1270  Training Loss:  0.434057  Training Accuracy:  0.996067\n",
      "Epoch:  1271  Training Loss:  0.43365  Training Accuracy:  0.996067\n",
      "Epoch:  1272  Training Loss:  0.43324  Training Accuracy:  0.996067\n",
      "Epoch:  1273  Training Loss:  0.432834  Training Accuracy:  0.996067\n",
      "Epoch:  1274  Training Loss:  0.43243  Training Accuracy:  0.996067\n",
      "Epoch:  1275  Training Loss:  0.432024  Training Accuracy:  0.996067\n",
      "Epoch:  1276  Training Loss:  0.431622  Training Accuracy:  0.996067\n",
      "Epoch:  1277  Training Loss:  0.431217  Training Accuracy:  0.996067\n",
      "Epoch:  1278  Training Loss:  0.430812  Training Accuracy:  0.996067\n",
      "Epoch:  1279  Training Loss:  0.430409  Training Accuracy:  0.996067\n",
      "Epoch:  1280  Training Loss:  0.430007  Training Accuracy:  0.996067\n",
      "Epoch:  1281  Training Loss:  0.42961  Training Accuracy:  0.996067\n",
      "Epoch:  1282  Training Loss:  0.429206  Training Accuracy:  0.996067\n",
      "Epoch:  1283  Training Loss:  0.42881  Training Accuracy:  0.996067\n",
      "Epoch:  1284  Training Loss:  0.428406  Training Accuracy:  0.996067\n",
      "Epoch:  1285  Training Loss:  0.428006  Training Accuracy:  0.996067\n",
      "Epoch:  1286  Training Loss:  0.427612  Training Accuracy:  0.996067\n",
      "Epoch:  1287  Training Loss:  0.427212  Training Accuracy:  0.996067\n",
      "Epoch:  1288  Training Loss:  0.426812  Training Accuracy:  0.996067\n",
      "Epoch:  1289  Training Loss:  0.426416  Training Accuracy:  0.996067\n",
      "Epoch:  1290  Training Loss:  0.426024  Training Accuracy:  0.996067\n",
      "Epoch:  1291  Training Loss:  0.425627  Training Accuracy:  0.996067\n",
      "Epoch:  1292  Training Loss:  0.425236  Training Accuracy:  0.996067\n",
      "Epoch:  1293  Training Loss:  0.424842  Training Accuracy:  0.996067\n",
      "Epoch:  1294  Training Loss:  0.424452  Training Accuracy:  0.996067\n",
      "Epoch:  1295  Training Loss:  0.424061  Training Accuracy:  0.996067\n",
      "Epoch:  1296  Training Loss:  0.423669  Training Accuracy:  0.996067\n",
      "Epoch:  1297  Training Loss:  0.42328  Training Accuracy:  0.996067\n",
      "Epoch:  1298  Training Loss:  0.422887  Training Accuracy:  0.996067\n",
      "Epoch:  1299  Training Loss:  0.422501  Training Accuracy:  0.996067\n",
      "Epoch:  1300  Training Loss:  0.422116  Training Accuracy:  0.996067\n",
      "Epoch:  1301  Training Loss:  0.421732  Training Accuracy:  0.996067\n",
      "Epoch:  1302  Training Loss:  0.421343  Training Accuracy:  0.996067\n",
      "Epoch:  1303  Training Loss:  0.420954  Training Accuracy:  0.996067\n",
      "Epoch:  1304  Training Loss:  0.42057  Training Accuracy:  0.996067\n",
      "Epoch:  1305  Training Loss:  0.420182  Training Accuracy:  0.996067\n",
      "Epoch:  1306  Training Loss:  0.419796  Training Accuracy:  0.996067\n",
      "Epoch:  1307  Training Loss:  0.419414  Training Accuracy:  0.996067\n",
      "Epoch:  1308  Training Loss:  0.419024  Training Accuracy:  0.996067\n",
      "Epoch:  1309  Training Loss:  0.418644  Training Accuracy:  0.996067\n",
      "Epoch:  1310  Training Loss:  0.418264  Training Accuracy:  0.996067\n",
      "Epoch:  1311  Training Loss:  0.417879  Training Accuracy:  0.996067\n",
      "Epoch:  1312  Training Loss:  0.417498  Training Accuracy:  0.996067\n",
      "Epoch:  1313  Training Loss:  0.417123  Training Accuracy:  0.996067\n",
      "Epoch:  1314  Training Loss:  0.416737  Training Accuracy:  0.996067\n",
      "Epoch:  1315  Training Loss:  0.416363  Training Accuracy:  0.996067\n",
      "Epoch:  1316  Training Loss:  0.415981  Training Accuracy:  0.996067\n",
      "Epoch:  1317  Training Loss:  0.415604  Training Accuracy:  0.996067\n",
      "Epoch:  1318  Training Loss:  0.415231  Training Accuracy:  0.996067\n",
      "Epoch:  1319  Training Loss:  0.414853  Training Accuracy:  0.996067\n",
      "Epoch:  1320  Training Loss:  0.414478  Training Accuracy:  0.996067\n",
      "Epoch:  1321  Training Loss:  0.414103  Training Accuracy:  0.996067\n",
      "Epoch:  1322  Training Loss:  0.413727  Training Accuracy:  0.996067\n",
      "Epoch:  1323  Training Loss:  0.413355  Training Accuracy:  0.996067\n",
      "Epoch:  1324  Training Loss:  0.412984  Training Accuracy:  0.996067\n",
      "Epoch:  1325  Training Loss:  0.412611  Training Accuracy:  0.996067\n",
      "Epoch:  1326  Training Loss:  0.41224  Training Accuracy:  0.996067\n",
      "Epoch:  1327  Training Loss:  0.411869  Training Accuracy:  0.996067\n",
      "Epoch:  1328  Training Loss:  0.411503  Training Accuracy:  0.996067\n",
      "Epoch:  1329  Training Loss:  0.411128  Training Accuracy:  0.996067\n",
      "Epoch:  1330  Training Loss:  0.41076  Training Accuracy:  0.996067\n",
      "Epoch:  1331  Training Loss:  0.410392  Training Accuracy:  0.996067\n",
      "Epoch:  1332  Training Loss:  0.41002  Training Accuracy:  0.996067\n",
      "Epoch:  1333  Training Loss:  0.409654  Training Accuracy:  0.996067\n",
      "Epoch:  1334  Training Loss:  0.409284  Training Accuracy:  0.996067\n",
      "Epoch:  1335  Training Loss:  0.408922  Training Accuracy:  0.996067\n",
      "Epoch:  1336  Training Loss:  0.408555  Training Accuracy:  0.996067\n",
      "Epoch:  1337  Training Loss:  0.408188  Training Accuracy:  0.996067\n",
      "Epoch:  1338  Training Loss:  0.407822  Training Accuracy:  0.996067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1339  Training Loss:  0.407455  Training Accuracy:  0.996067\n",
      "Epoch:  1340  Training Loss:  0.407096  Training Accuracy:  0.996067\n",
      "Epoch:  1341  Training Loss:  0.406735  Training Accuracy:  0.996067\n",
      "Epoch:  1342  Training Loss:  0.406369  Training Accuracy:  0.996067\n",
      "Epoch:  1343  Training Loss:  0.406012  Training Accuracy:  0.996067\n",
      "Epoch:  1344  Training Loss:  0.405647  Training Accuracy:  0.996067\n",
      "Epoch:  1345  Training Loss:  0.405292  Training Accuracy:  0.996067\n",
      "Epoch:  1346  Training Loss:  0.404932  Training Accuracy:  0.996067\n",
      "Epoch:  1347  Training Loss:  0.404571  Training Accuracy:  0.996067\n",
      "Epoch:  1348  Training Loss:  0.404214  Training Accuracy:  0.996067\n",
      "Epoch:  1349  Training Loss:  0.403858  Training Accuracy:  0.996067\n",
      "Epoch:  1350  Training Loss:  0.4035  Training Accuracy:  0.996067\n",
      "Epoch:  1351  Training Loss:  0.403144  Training Accuracy:  0.996067\n",
      "Epoch:  1352  Training Loss:  0.402789  Training Accuracy:  0.996067\n"
     ]
    }
   ],
   "source": [
    "#walking label\n",
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 10 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 2000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "        with tf.name_scope('summary'):\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter('./logs', session.graph)\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walking label\n",
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 10 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-7\n",
    "training_epochs = 10000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "        with tf.name_scope('summary'):\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter('./logs', session.graph)\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  10  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  11  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  12  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  13  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  14  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  15  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  16  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  17  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  18  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  19  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  20  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  21  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  22  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  23  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  24  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  25  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  26  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  27  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  28  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  29  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  30  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  31  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  32  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  33  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  34  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  35  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  36  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  37  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  38  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  39  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  40  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  41  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  42  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  43  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  44  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  45  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  46  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  47  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  48  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  49  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  50  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  51  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  52  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  53  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  54  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  55  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  56  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  57  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  58  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  59  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  60  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  61  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  62  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  63  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  64  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  65  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  66  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  67  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  68  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  69  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  70  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  71  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  72  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  73  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  74  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  75  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  76  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  77  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1031  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1158  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1285  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1412  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1539  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1666  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1793  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1920  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2047  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2174  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2301  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2428  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2555  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2682  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2809  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2936  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3063  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3190  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3317  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3444  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3571  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3698  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3825  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3952  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4079  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4206  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4333  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4460  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4587  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4714  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4841  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4968  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5095  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5222  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5349  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5476  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5603  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5730  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5857  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5984  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6111  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6238  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6365  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6492  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6619  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6746  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6873  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7000  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7127  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7254  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7381  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7508  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7635  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7762  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7889  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8016  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8143  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8270  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8397  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8524  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8651  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8778  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8905  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9032  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9159  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9286  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9413  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9540  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9667  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9794  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9921  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 10000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  21.4897  Training Accuracy:  0.0442093\n",
      "Epoch:  1  Training Loss:  20.3249  Training Accuracy:  0.0463845\n",
      "Epoch:  2  Training Loss:  17.7258  Training Accuracy:  0.0452675\n",
      "Epoch:  3  Training Loss:  15.1002  Training Accuracy:  0.0462081\n",
      "Epoch:  4  Training Loss:  12.7212  Training Accuracy:  0.0466196\n",
      "Epoch:  5  Training Loss:  10.8525  Training Accuracy:  0.0481481\n",
      "Epoch:  6  Training Loss:  9.35068  Training Accuracy:  0.0505585\n",
      "Epoch:  7  Training Loss:  8.06143  Training Accuracy:  0.0537331\n",
      "Epoch:  8  Training Loss:  7.00692  Training Accuracy:  0.0574956\n",
      "Epoch:  9  Training Loss:  6.20132  Training Accuracy:  0.0620223\n",
      "Epoch:  10  Training Loss:  5.6165  Training Accuracy:  0.0660788\n",
      "Epoch:  11  Training Loss:  5.16826  Training Accuracy:  0.0721928\n",
      "Epoch:  12  Training Loss:  4.79484  Training Accuracy:  0.0783069\n",
      "Epoch:  13  Training Loss:  4.51711  Training Accuracy:  0.0850676\n",
      "Epoch:  14  Training Loss:  4.29598  Training Accuracy:  0.0905938\n",
      "Epoch:  15  Training Loss:  4.11922  Training Accuracy:  0.0974721\n",
      "Epoch:  16  Training Loss:  3.98695  Training Accuracy:  0.103762\n",
      "Epoch:  17  Training Loss:  3.8807  Training Accuracy:  0.109641\n",
      "Epoch:  18  Training Loss:  3.7939  Training Accuracy:  0.11552\n",
      "Epoch:  19  Training Loss:  3.73504  Training Accuracy:  0.120223\n",
      "Epoch:  20  Training Loss:  3.70156  Training Accuracy:  0.124985\n",
      "Epoch:  21  Training Loss:  3.66974  Training Accuracy:  0.130276\n",
      "Epoch:  22  Training Loss:  3.62243  Training Accuracy:  0.136155\n",
      "Epoch:  23  Training Loss:  3.61205  Training Accuracy:  0.140388\n",
      "Epoch:  24  Training Loss:  3.58531  Training Accuracy:  0.14662\n",
      "Epoch:  25  Training Loss:  3.57198  Training Accuracy:  0.152499\n",
      "Epoch:  26  Training Loss:  3.56382  Training Accuracy:  0.159259\n",
      "Epoch:  27  Training Loss:  3.54705  Training Accuracy:  0.164727\n",
      "Epoch:  28  Training Loss:  3.51667  Training Accuracy:  0.171487\n",
      "Epoch:  29  Training Loss:  3.49681  Training Accuracy:  0.177895\n",
      "Epoch:  30  Training Loss:  3.47311  Training Accuracy:  0.184538\n",
      "Epoch:  31  Training Loss:  3.44372  Training Accuracy:  0.189712\n",
      "Epoch:  32  Training Loss:  3.4018  Training Accuracy:  0.195826\n",
      "Epoch:  33  Training Loss:  3.37152  Training Accuracy:  0.201352\n",
      "Epoch:  34  Training Loss:  3.3317  Training Accuracy:  0.20776\n",
      "Epoch:  35  Training Loss:  3.28817  Training Accuracy:  0.213404\n",
      "Epoch:  36  Training Loss:  3.2473  Training Accuracy:  0.217989\n",
      "Epoch:  37  Training Loss:  3.19595  Training Accuracy:  0.223457\n",
      "Epoch:  38  Training Loss:  3.15151  Training Accuracy:  0.228395\n",
      "Epoch:  39  Training Loss:  3.10407  Training Accuracy:  0.232393\n",
      "Epoch:  40  Training Loss:  3.05672  Training Accuracy:  0.236625\n",
      "Epoch:  41  Training Loss:  3.02051  Training Accuracy:  0.241505\n",
      "Epoch:  42  Training Loss:  2.97131  Training Accuracy:  0.246032\n",
      "Epoch:  43  Training Loss:  2.93159  Training Accuracy:  0.250676\n",
      "Epoch:  44  Training Loss:  2.88347  Training Accuracy:  0.255144\n",
      "Epoch:  45  Training Loss:  2.84576  Training Accuracy:  0.259259\n",
      "Epoch:  46  Training Loss:  2.80293  Training Accuracy:  0.26308\n",
      "Epoch:  47  Training Loss:  2.76802  Training Accuracy:  0.266784\n",
      "Epoch:  48  Training Loss:  2.7238  Training Accuracy:  0.270782\n",
      "Epoch:  49  Training Loss:  2.69125  Training Accuracy:  0.27425\n",
      "Epoch:  50  Training Loss:  2.65347  Training Accuracy:  0.277837\n",
      "Epoch:  51  Training Loss:  2.62033  Training Accuracy:  0.282657\n",
      "Epoch:  52  Training Loss:  2.58437  Training Accuracy:  0.285714\n",
      "Epoch:  53  Training Loss:  2.5515  Training Accuracy:  0.289006\n",
      "Epoch:  54  Training Loss:  2.51911  Training Accuracy:  0.292769\n",
      "Epoch:  55  Training Loss:  2.48749  Training Accuracy:  0.297002\n",
      "Epoch:  56  Training Loss:  2.4571  Training Accuracy:  0.300647\n",
      "Epoch:  57  Training Loss:  2.42215  Training Accuracy:  0.304174\n",
      "Epoch:  58  Training Loss:  2.38884  Training Accuracy:  0.308348\n",
      "Epoch:  59  Training Loss:  2.35482  Training Accuracy:  0.311817\n",
      "Epoch:  60  Training Loss:  2.32159  Training Accuracy:  0.315873\n",
      "Epoch:  61  Training Loss:  2.28925  Training Accuracy:  0.318577\n",
      "Epoch:  62  Training Loss:  2.2575  Training Accuracy:  0.321987\n",
      "Epoch:  63  Training Loss:  2.22605  Training Accuracy:  0.325044\n",
      "Epoch:  64  Training Loss:  2.19704  Training Accuracy:  0.328748\n",
      "Epoch:  65  Training Loss:  2.17175  Training Accuracy:  0.332216\n",
      "Epoch:  66  Training Loss:  2.1471  Training Accuracy:  0.334568\n",
      "Epoch:  67  Training Loss:  2.12168  Training Accuracy:  0.337801\n",
      "Epoch:  68  Training Loss:  2.09505  Training Accuracy:  0.340976\n",
      "Epoch:  69  Training Loss:  2.07382  Training Accuracy:  0.344856\n",
      "Epoch:  70  Training Loss:  2.0513  Training Accuracy:  0.348148\n",
      "Epoch:  71  Training Loss:  1.99982  Training Accuracy:  0.351146\n",
      "Epoch:  72  Training Loss:  2.00833  Training Accuracy:  0.354027\n",
      "Epoch:  73  Training Loss:  1.98849  Training Accuracy:  0.357143\n",
      "Epoch:  74  Training Loss:  1.96237  Training Accuracy:  0.360846\n",
      "Epoch:  75  Training Loss:  1.94672  Training Accuracy:  0.364197\n",
      "Epoch:  76  Training Loss:  1.89563  Training Accuracy:  0.367548\n",
      "Epoch:  77  Training Loss:  1.89058  Training Accuracy:  0.370311\n",
      "Epoch:  78  Training Loss:  1.89923  Training Accuracy:  0.374309\n",
      "Epoch:  79  Training Loss:  1.88351  Training Accuracy:  0.378424\n",
      "Epoch:  80  Training Loss:  1.83695  Training Accuracy:  0.38201\n",
      "Epoch:  81  Training Loss:  1.82412  Training Accuracy:  0.386126\n",
      "Epoch:  82  Training Loss:  1.8468  Training Accuracy:  0.390006\n",
      "Epoch:  83  Training Loss:  1.835  Training Accuracy:  0.39318\n",
      "Epoch:  84  Training Loss:  1.82011  Training Accuracy:  0.39565\n",
      "Epoch:  85  Training Loss:  1.77848  Training Accuracy:  0.399118\n",
      "Epoch:  86  Training Loss:  1.80769  Training Accuracy:  0.402057\n",
      "Epoch:  87  Training Loss:  1.76047  Training Accuracy:  0.40535\n",
      "Epoch:  88  Training Loss:  1.79165  Training Accuracy:  0.407642\n",
      "Epoch:  89  Training Loss:  1.78158  Training Accuracy:  0.41017\n",
      "Epoch:  90  Training Loss:  1.7723  Training Accuracy:  0.412875\n",
      "Epoch:  91  Training Loss:  1.76841  Training Accuracy:  0.41505\n",
      "Epoch:  92  Training Loss:  1.76346  Training Accuracy:  0.41746\n",
      "Epoch:  93  Training Loss:  1.7571  Training Accuracy:  0.420165\n",
      "Epoch:  94  Training Loss:  1.75201  Training Accuracy:  0.422281\n",
      "Epoch:  95  Training Loss:  1.74886  Training Accuracy:  0.424103\n",
      "Epoch:  96  Training Loss:  1.7419  Training Accuracy:  0.426984\n",
      "Epoch:  97  Training Loss:  1.73957  Training Accuracy:  0.429159\n",
      "Epoch:  98  Training Loss:  1.73922  Training Accuracy:  0.43157\n",
      "Epoch:  99  Training Loss:  1.73377  Training Accuracy:  0.433392\n",
      "Epoch:  100  Training Loss:  1.73077  Training Accuracy:  0.435391\n",
      "Epoch:  101  Training Loss:  1.72936  Training Accuracy:  0.437448\n",
      "Epoch:  102  Training Loss:  1.72675  Training Accuracy:  0.440094\n",
      "Epoch:  103  Training Loss:  1.72755  Training Accuracy:  0.441622\n",
      "Epoch:  104  Training Loss:  1.72228  Training Accuracy:  0.444621\n",
      "Epoch:  105  Training Loss:  1.72183  Training Accuracy:  0.446855\n",
      "Epoch:  106  Training Loss:  1.71587  Training Accuracy:  0.44903\n",
      "Epoch:  107  Training Loss:  1.68668  Training Accuracy:  0.451029\n",
      "Epoch:  108  Training Loss:  1.68373  Training Accuracy:  0.452851\n",
      "Epoch:  109  Training Loss:  1.72199  Training Accuracy:  0.45485\n",
      "Epoch:  110  Training Loss:  1.71195  Training Accuracy:  0.456849\n",
      "Epoch:  111  Training Loss:  1.71451  Training Accuracy:  0.458612\n",
      "Epoch:  112  Training Loss:  1.70842  Training Accuracy:  0.460788\n",
      "Epoch:  113  Training Loss:  1.70445  Training Accuracy:  0.462728\n",
      "Epoch:  114  Training Loss:  1.70196  Training Accuracy:  0.464197\n",
      "Epoch:  115  Training Loss:  1.69717  Training Accuracy:  0.466608\n",
      "Epoch:  116  Training Loss:  1.6621  Training Accuracy:  0.469312\n",
      "Epoch:  117  Training Loss:  1.656  Training Accuracy:  0.471252\n",
      "Epoch:  118  Training Loss:  1.65626  Training Accuracy:  0.47331\n",
      "Epoch:  119  Training Loss:  1.68186  Training Accuracy:  0.475132\n",
      "Epoch:  120  Training Loss:  1.6439  Training Accuracy:  0.477307\n",
      "Epoch:  121  Training Loss:  1.63337  Training Accuracy:  0.478953\n",
      "Epoch:  122  Training Loss:  1.63569  Training Accuracy:  0.480893\n",
      "Epoch:  123  Training Loss:  1.66658  Training Accuracy:  0.482775\n",
      "Epoch:  124  Training Loss:  1.63414  Training Accuracy:  0.483715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  1.62605  Training Accuracy:  0.485714\n",
      "Epoch:  126  Training Loss:  1.63165  Training Accuracy:  0.487772\n",
      "Epoch:  127  Training Loss:  1.66143  Training Accuracy:  0.489771\n",
      "Epoch:  128  Training Loss:  1.62622  Training Accuracy:  0.491534\n",
      "Epoch:  129  Training Loss:  1.61967  Training Accuracy:  0.493886\n",
      "Epoch:  130  Training Loss:  1.62646  Training Accuracy:  0.495238\n",
      "Epoch:  131  Training Loss:  1.64558  Training Accuracy:  0.49706\n",
      "Epoch:  132  Training Loss:  1.61974  Training Accuracy:  0.499118\n",
      "Epoch:  133  Training Loss:  1.613  Training Accuracy:  0.500882\n",
      "Epoch:  134  Training Loss:  1.61629  Training Accuracy:  0.502881\n",
      "Epoch:  135  Training Loss:  1.62608  Training Accuracy:  0.504468\n",
      "Epoch:  136  Training Loss:  1.61109  Training Accuracy:  0.506408\n",
      "Epoch:  137  Training Loss:  1.60392  Training Accuracy:  0.509053\n",
      "Epoch:  138  Training Loss:  1.62296  Training Accuracy:  0.510112\n",
      "Epoch:  139  Training Loss:  1.62418  Training Accuracy:  0.511875\n",
      "Epoch:  140  Training Loss:  1.59915  Training Accuracy:  0.513227\n",
      "Epoch:  141  Training Loss:  1.59607  Training Accuracy:  0.514874\n",
      "Epoch:  142  Training Loss:  1.6017  Training Accuracy:  0.516755\n",
      "Epoch:  143  Training Loss:  1.60387  Training Accuracy:  0.519106\n",
      "Epoch:  144  Training Loss:  1.59659  Training Accuracy:  0.521399\n",
      "Epoch:  145  Training Loss:  1.60901  Training Accuracy:  0.524103\n",
      "Epoch:  146  Training Loss:  1.59955  Training Accuracy:  0.525279\n",
      "Epoch:  147  Training Loss:  1.594  Training Accuracy:  0.527748\n",
      "Epoch:  148  Training Loss:  1.59935  Training Accuracy:  0.529394\n",
      "Epoch:  149  Training Loss:  1.59104  Training Accuracy:  0.53104\n",
      "Epoch:  150  Training Loss:  1.5951  Training Accuracy:  0.533098\n",
      "Epoch:  151  Training Loss:  1.58677  Training Accuracy:  0.534921\n",
      "Epoch:  152  Training Loss:  1.60522  Training Accuracy:  0.536802\n",
      "Epoch:  153  Training Loss:  1.58723  Training Accuracy:  0.538918\n",
      "Epoch:  154  Training Loss:  1.58218  Training Accuracy:  0.540564\n",
      "Epoch:  155  Training Loss:  1.57779  Training Accuracy:  0.542739\n",
      "Epoch:  156  Training Loss:  1.58019  Training Accuracy:  0.544209\n",
      "Epoch:  157  Training Loss:  1.57961  Training Accuracy:  0.545385\n",
      "Epoch:  158  Training Loss:  1.57217  Training Accuracy:  0.54709\n",
      "Epoch:  159  Training Loss:  1.57721  Training Accuracy:  0.549089\n",
      "Epoch:  160  Training Loss:  1.57087  Training Accuracy:  0.550676\n",
      "Epoch:  161  Training Loss:  1.58163  Training Accuracy:  0.551793\n",
      "Epoch:  162  Training Loss:  1.58621  Training Accuracy:  0.553674\n",
      "Epoch:  163  Training Loss:  1.56094  Training Accuracy:  0.555379\n",
      "Epoch:  164  Training Loss:  1.55828  Training Accuracy:  0.557143\n",
      "Epoch:  165  Training Loss:  1.55505  Training Accuracy:  0.558554\n",
      "Epoch:  166  Training Loss:  1.56506  Training Accuracy:  0.56067\n",
      "Epoch:  167  Training Loss:  1.57554  Training Accuracy:  0.562669\n",
      "Epoch:  168  Training Loss:  1.54515  Training Accuracy:  0.56455\n",
      "Epoch:  169  Training Loss:  1.54182  Training Accuracy:  0.566137\n",
      "Epoch:  170  Training Loss:  1.53753  Training Accuracy:  0.567431\n",
      "Epoch:  171  Training Loss:  1.53198  Training Accuracy:  0.568842\n",
      "Epoch:  172  Training Loss:  1.53589  Training Accuracy:  0.570488\n",
      "Epoch:  173  Training Loss:  1.52838  Training Accuracy:  0.572134\n",
      "Epoch:  174  Training Loss:  1.52232  Training Accuracy:  0.573486\n",
      "Epoch:  175  Training Loss:  1.52584  Training Accuracy:  0.575426\n",
      "Epoch:  176  Training Loss:  1.51727  Training Accuracy:  0.577307\n",
      "Epoch:  177  Training Loss:  1.51297  Training Accuracy:  0.579424\n",
      "Epoch:  178  Training Loss:  1.51819  Training Accuracy:  0.581011\n",
      "Epoch:  179  Training Loss:  1.5114  Training Accuracy:  0.583363\n",
      "Epoch:  180  Training Loss:  1.50482  Training Accuracy:  0.58495\n",
      "Epoch:  181  Training Loss:  1.50914  Training Accuracy:  0.586537\n",
      "Epoch:  182  Training Loss:  1.50039  Training Accuracy:  0.588242\n",
      "Epoch:  183  Training Loss:  1.49475  Training Accuracy:  0.589712\n",
      "Epoch:  184  Training Loss:  1.48977  Training Accuracy:  0.591123\n",
      "Epoch:  185  Training Loss:  1.48649  Training Accuracy:  0.592887\n",
      "Epoch:  186  Training Loss:  1.48837  Training Accuracy:  0.594239\n",
      "Epoch:  187  Training Loss:  1.4808  Training Accuracy:  0.595532\n",
      "Epoch:  188  Training Loss:  1.47567  Training Accuracy:  0.597707\n",
      "Epoch:  189  Training Loss:  1.471  Training Accuracy:  0.599353\n",
      "Epoch:  190  Training Loss:  1.47408  Training Accuracy:  0.600353\n",
      "Epoch:  191  Training Loss:  1.46663  Training Accuracy:  0.602116\n",
      "Epoch:  192  Training Loss:  1.46175  Training Accuracy:  0.603351\n",
      "Epoch:  193  Training Loss:  1.46382  Training Accuracy:  0.604468\n",
      "Epoch:  194  Training Loss:  1.45556  Training Accuracy:  0.606114\n",
      "Epoch:  195  Training Loss:  1.45095  Training Accuracy:  0.607525\n",
      "Epoch:  196  Training Loss:  1.45269  Training Accuracy:  0.609112\n",
      "Epoch:  197  Training Loss:  1.44468  Training Accuracy:  0.610464\n",
      "Epoch:  198  Training Loss:  1.43929  Training Accuracy:  0.612052\n",
      "Epoch:  199  Training Loss:  1.43997  Training Accuracy:  0.613815\n",
      "Epoch:  200  Training Loss:  1.43124  Training Accuracy:  0.615873\n",
      "Epoch:  201  Training Loss:  1.43224  Training Accuracy:  0.617519\n",
      "Epoch:  202  Training Loss:  1.42337  Training Accuracy:  0.619518\n",
      "Epoch:  203  Training Loss:  1.42511  Training Accuracy:  0.620988\n",
      "Epoch:  204  Training Loss:  1.41829  Training Accuracy:  0.622751\n",
      "Epoch:  205  Training Loss:  1.41393  Training Accuracy:  0.624397\n",
      "Epoch:  206  Training Loss:  1.4141  Training Accuracy:  0.625691\n",
      "Epoch:  207  Training Loss:  1.4075  Training Accuracy:  0.626867\n",
      "Epoch:  208  Training Loss:  1.40733  Training Accuracy:  0.628689\n",
      "Epoch:  209  Training Loss:  1.40137  Training Accuracy:  0.630335\n",
      "Epoch:  210  Training Loss:  1.40105  Training Accuracy:  0.631687\n",
      "Epoch:  211  Training Loss:  1.39516  Training Accuracy:  0.632863\n",
      "Epoch:  212  Training Loss:  1.39487  Training Accuracy:  0.634333\n",
      "Epoch:  213  Training Loss:  1.38867  Training Accuracy:  0.636038\n",
      "Epoch:  214  Training Loss:  1.38868  Training Accuracy:  0.637331\n",
      "Epoch:  215  Training Loss:  1.38324  Training Accuracy:  0.638918\n",
      "Epoch:  216  Training Loss:  1.38183  Training Accuracy:  0.640094\n",
      "Epoch:  217  Training Loss:  1.37611  Training Accuracy:  0.641917\n",
      "Epoch:  218  Training Loss:  1.37589  Training Accuracy:  0.642681\n",
      "Epoch:  219  Training Loss:  1.36831  Training Accuracy:  0.644209\n",
      "Epoch:  220  Training Loss:  1.36852  Training Accuracy:  0.646149\n",
      "Epoch:  221  Training Loss:  1.36154  Training Accuracy:  0.648207\n",
      "Epoch:  222  Training Loss:  1.36109  Training Accuracy:  0.650265\n",
      "Epoch:  223  Training Loss:  1.35988  Training Accuracy:  0.651734\n",
      "Epoch:  224  Training Loss:  1.35276  Training Accuracy:  0.653439\n",
      "Epoch:  225  Training Loss:  1.35216  Training Accuracy:  0.655085\n",
      "Epoch:  226  Training Loss:  1.35029  Training Accuracy:  0.656026\n",
      "Epoch:  227  Training Loss:  1.34295  Training Accuracy:  0.65826\n",
      "Epoch:  228  Training Loss:  1.34199  Training Accuracy:  0.659847\n",
      "Epoch:  229  Training Loss:  1.3403  Training Accuracy:  0.661258\n",
      "Epoch:  230  Training Loss:  1.35196  Training Accuracy:  0.662434\n",
      "Epoch:  231  Training Loss:  1.33137  Training Accuracy:  0.663845\n",
      "Epoch:  232  Training Loss:  1.32919  Training Accuracy:  0.665197\n",
      "Epoch:  233  Training Loss:  1.32544  Training Accuracy:  0.666961\n",
      "Epoch:  234  Training Loss:  1.32571  Training Accuracy:  0.667843\n",
      "Epoch:  235  Training Loss:  1.32082  Training Accuracy:  0.66943\n",
      "Epoch:  236  Training Loss:  1.32054  Training Accuracy:  0.670547\n",
      "Epoch:  237  Training Loss:  1.31803  Training Accuracy:  0.672369\n",
      "Epoch:  238  Training Loss:  1.31483  Training Accuracy:  0.673663\n",
      "Epoch:  239  Training Loss:  1.31275  Training Accuracy:  0.675426\n",
      "Epoch:  240  Training Loss:  1.30904  Training Accuracy:  0.676896\n",
      "Epoch:  241  Training Loss:  1.30647  Training Accuracy:  0.678425\n",
      "Epoch:  242  Training Loss:  1.30435  Training Accuracy:  0.679306\n",
      "Epoch:  243  Training Loss:  1.29863  Training Accuracy:  0.681011\n",
      "Epoch:  244  Training Loss:  1.29824  Training Accuracy:  0.682775\n",
      "Epoch:  245  Training Loss:  1.29658  Training Accuracy:  0.683774\n",
      "Epoch:  246  Training Loss:  1.29429  Training Accuracy:  0.685362\n",
      "Epoch:  247  Training Loss:  1.29226  Training Accuracy:  0.68736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  1.28914  Training Accuracy:  0.688536\n",
      "Epoch:  249  Training Loss:  1.28758  Training Accuracy:  0.689947\n",
      "Epoch:  250  Training Loss:  1.28461  Training Accuracy:  0.690947\n",
      "Epoch:  251  Training Loss:  1.28242  Training Accuracy:  0.692593\n",
      "Epoch:  252  Training Loss:  1.27943  Training Accuracy:  0.694121\n",
      "Epoch:  253  Training Loss:  1.27697  Training Accuracy:  0.695121\n",
      "Epoch:  254  Training Loss:  1.2755  Training Accuracy:  0.696002\n",
      "Epoch:  255  Training Loss:  1.27272  Training Accuracy:  0.697178\n",
      "Epoch:  256  Training Loss:  1.27087  Training Accuracy:  0.698648\n",
      "Epoch:  257  Training Loss:  1.26814  Training Accuracy:  0.700176\n",
      "Epoch:  258  Training Loss:  1.26647  Training Accuracy:  0.70147\n",
      "Epoch:  259  Training Loss:  1.26443  Training Accuracy:  0.702587\n",
      "Epoch:  260  Training Loss:  1.26189  Training Accuracy:  0.70388\n",
      "Epoch:  261  Training Loss:  1.25977  Training Accuracy:  0.705585\n",
      "Epoch:  262  Training Loss:  1.25735  Training Accuracy:  0.706585\n",
      "Epoch:  263  Training Loss:  1.25496  Training Accuracy:  0.707937\n",
      "Epoch:  264  Training Loss:  1.25295  Training Accuracy:  0.708877\n",
      "Epoch:  265  Training Loss:  1.25158  Training Accuracy:  0.709759\n",
      "Epoch:  266  Training Loss:  1.24956  Training Accuracy:  0.711229\n",
      "Epoch:  267  Training Loss:  1.24735  Training Accuracy:  0.712463\n",
      "Epoch:  268  Training Loss:  1.24544  Training Accuracy:  0.713757\n",
      "Epoch:  269  Training Loss:  1.24373  Training Accuracy:  0.715168\n",
      "Epoch:  270  Training Loss:  1.24219  Training Accuracy:  0.716579\n",
      "Epoch:  271  Training Loss:  1.24043  Training Accuracy:  0.717402\n",
      "Epoch:  272  Training Loss:  1.23925  Training Accuracy:  0.718636\n",
      "Epoch:  273  Training Loss:  1.23751  Training Accuracy:  0.719636\n",
      "Epoch:  274  Training Loss:  1.23565  Training Accuracy:  0.720811\n",
      "Epoch:  275  Training Loss:  1.23408  Training Accuracy:  0.721928\n",
      "Epoch:  276  Training Loss:  1.23211  Training Accuracy:  0.722869\n",
      "Epoch:  277  Training Loss:  1.23099  Training Accuracy:  0.723868\n",
      "Epoch:  278  Training Loss:  1.22904  Training Accuracy:  0.724692\n",
      "Epoch:  279  Training Loss:  1.2291  Training Accuracy:  0.725338\n",
      "Epoch:  280  Training Loss:  1.22871  Training Accuracy:  0.726396\n",
      "Epoch:  281  Training Loss:  1.22706  Training Accuracy:  0.727572\n",
      "Epoch:  282  Training Loss:  1.22578  Training Accuracy:  0.728395\n",
      "Epoch:  283  Training Loss:  1.22394  Training Accuracy:  0.729571\n",
      "Epoch:  284  Training Loss:  1.22313  Training Accuracy:  0.730335\n",
      "Epoch:  285  Training Loss:  1.22087  Training Accuracy:  0.731394\n",
      "Epoch:  286  Training Loss:  1.21946  Training Accuracy:  0.731746\n",
      "Epoch:  287  Training Loss:  1.21878  Training Accuracy:  0.732746\n",
      "Epoch:  288  Training Loss:  1.21693  Training Accuracy:  0.73351\n",
      "Epoch:  289  Training Loss:  1.21618  Training Accuracy:  0.734568\n",
      "Epoch:  290  Training Loss:  1.21457  Training Accuracy:  0.735509\n",
      "Epoch:  291  Training Loss:  1.21282  Training Accuracy:  0.736743\n",
      "Epoch:  292  Training Loss:  1.21235  Training Accuracy:  0.737743\n",
      "Epoch:  293  Training Loss:  1.21067  Training Accuracy:  0.738566\n",
      "Epoch:  294  Training Loss:  1.20995  Training Accuracy:  0.739271\n",
      "Epoch:  295  Training Loss:  1.20901  Training Accuracy:  0.740035\n",
      "Epoch:  296  Training Loss:  1.20768  Training Accuracy:  0.740976\n",
      "Epoch:  297  Training Loss:  1.20669  Training Accuracy:  0.741917\n",
      "Epoch:  298  Training Loss:  1.20629  Training Accuracy:  0.742799\n",
      "Epoch:  299  Training Loss:  1.20422  Training Accuracy:  0.743622\n",
      "Epoch:  300  Training Loss:  1.20342  Training Accuracy:  0.745033\n",
      "Epoch:  301  Training Loss:  1.20226  Training Accuracy:  0.74615\n",
      "Epoch:  302  Training Loss:  1.20135  Training Accuracy:  0.747325\n",
      "Epoch:  303  Training Loss:  1.19969  Training Accuracy:  0.748384\n",
      "Epoch:  304  Training Loss:  1.19894  Training Accuracy:  0.749207\n",
      "Epoch:  305  Training Loss:  1.19729  Training Accuracy:  0.750206\n",
      "Epoch:  306  Training Loss:  1.19619  Training Accuracy:  0.751088\n",
      "Epoch:  307  Training Loss:  1.19479  Training Accuracy:  0.752146\n",
      "Epoch:  308  Training Loss:  1.19356  Training Accuracy:  0.753792\n",
      "Epoch:  309  Training Loss:  1.19208  Training Accuracy:  0.755144\n",
      "Epoch:  310  Training Loss:  1.19101  Training Accuracy:  0.756379\n",
      "Epoch:  311  Training Loss:  1.19032  Training Accuracy:  0.757026\n",
      "Epoch:  312  Training Loss:  1.18833  Training Accuracy:  0.758084\n",
      "Epoch:  313  Training Loss:  1.18782  Training Accuracy:  0.75926\n",
      "Epoch:  314  Training Loss:  1.18655  Training Accuracy:  0.760435\n",
      "Epoch:  315  Training Loss:  1.18531  Training Accuracy:  0.761376\n",
      "Epoch:  316  Training Loss:  1.18423  Training Accuracy:  0.76214\n",
      "Epoch:  317  Training Loss:  1.18256  Training Accuracy:  0.763257\n",
      "Epoch:  318  Training Loss:  1.18112  Training Accuracy:  0.764139\n",
      "Epoch:  319  Training Loss:  1.18016  Training Accuracy:  0.765138\n",
      "Epoch:  320  Training Loss:  1.17864  Training Accuracy:  0.765667\n",
      "Epoch:  321  Training Loss:  1.17789  Training Accuracy:  0.766197\n",
      "Epoch:  322  Training Loss:  1.17595  Training Accuracy:  0.767196\n",
      "Epoch:  323  Training Loss:  1.17516  Training Accuracy:  0.76796\n",
      "Epoch:  324  Training Loss:  1.17391  Training Accuracy:  0.768607\n",
      "Epoch:  325  Training Loss:  1.17238  Training Accuracy:  0.769371\n",
      "Epoch:  326  Training Loss:  1.17111  Training Accuracy:  0.770135\n",
      "Epoch:  327  Training Loss:  1.16955  Training Accuracy:  0.770782\n",
      "Epoch:  328  Training Loss:  1.16822  Training Accuracy:  0.771664\n",
      "Epoch:  329  Training Loss:  1.16659  Training Accuracy:  0.772722\n",
      "Epoch:  330  Training Loss:  1.16571  Training Accuracy:  0.773663\n",
      "Epoch:  331  Training Loss:  1.1641  Training Accuracy:  0.774427\n",
      "Epoch:  332  Training Loss:  1.16278  Training Accuracy:  0.775133\n",
      "Epoch:  333  Training Loss:  1.1615  Training Accuracy:  0.776073\n",
      "Epoch:  334  Training Loss:  1.15967  Training Accuracy:  0.776543\n",
      "Epoch:  335  Training Loss:  1.15805  Training Accuracy:  0.777425\n",
      "Epoch:  336  Training Loss:  1.15679  Training Accuracy:  0.778484\n",
      "Epoch:  337  Training Loss:  1.15508  Training Accuracy:  0.779542\n",
      "Epoch:  338  Training Loss:  1.15377  Training Accuracy:  0.780717\n",
      "Epoch:  339  Training Loss:  1.15172  Training Accuracy:  0.781482\n",
      "Epoch:  340  Training Loss:  1.15024  Training Accuracy:  0.782187\n",
      "Epoch:  341  Training Loss:  1.14862  Training Accuracy:  0.783187\n",
      "Epoch:  342  Training Loss:  1.14649  Training Accuracy:  0.783892\n",
      "Epoch:  343  Training Loss:  1.1447  Training Accuracy:  0.784774\n",
      "Epoch:  344  Training Loss:  1.14335  Training Accuracy:  0.785773\n",
      "Epoch:  345  Training Loss:  1.14256  Training Accuracy:  0.78642\n",
      "Epoch:  346  Training Loss:  1.1408  Training Accuracy:  0.787302\n",
      "Epoch:  347  Training Loss:  1.13995  Training Accuracy:  0.787713\n",
      "Epoch:  348  Training Loss:  1.13845  Training Accuracy:  0.788948\n",
      "Epoch:  349  Training Loss:  1.13701  Training Accuracy:  0.790183\n",
      "Epoch:  350  Training Loss:  1.13519  Training Accuracy:  0.790712\n",
      "Epoch:  351  Training Loss:  1.13433  Training Accuracy:  0.791711\n",
      "Epoch:  352  Training Loss:  1.13262  Training Accuracy:  0.792534\n",
      "Epoch:  353  Training Loss:  1.13142  Training Accuracy:  0.793416\n",
      "Epoch:  354  Training Loss:  1.12907  Training Accuracy:  0.794063\n",
      "Epoch:  355  Training Loss:  1.1279  Training Accuracy:  0.794709\n",
      "Epoch:  356  Training Loss:  1.12638  Training Accuracy:  0.795591\n",
      "Epoch:  357  Training Loss:  1.12453  Training Accuracy:  0.796238\n",
      "Epoch:  358  Training Loss:  1.12328  Training Accuracy:  0.796826\n",
      "Epoch:  359  Training Loss:  1.12178  Training Accuracy:  0.797649\n",
      "Epoch:  360  Training Loss:  1.1199  Training Accuracy:  0.798178\n",
      "Epoch:  361  Training Loss:  1.11769  Training Accuracy:  0.798707\n",
      "Epoch:  362  Training Loss:  1.11707  Training Accuracy:  0.799236\n",
      "Epoch:  363  Training Loss:  1.11549  Training Accuracy:  0.799589\n",
      "Epoch:  364  Training Loss:  1.11342  Training Accuracy:  0.800177\n",
      "Epoch:  365  Training Loss:  1.11248  Training Accuracy:  0.801117\n",
      "Epoch:  366  Training Loss:  1.1106  Training Accuracy:  0.801588\n",
      "Epoch:  367  Training Loss:  1.10912  Training Accuracy:  0.802469\n",
      "Epoch:  368  Training Loss:  1.10834  Training Accuracy:  0.802881\n",
      "Epoch:  369  Training Loss:  1.106  Training Accuracy:  0.803586\n",
      "Epoch:  370  Training Loss:  1.10526  Training Accuracy:  0.804174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  1.1036  Training Accuracy:  0.804939\n",
      "Epoch:  372  Training Loss:  1.1015  Training Accuracy:  0.805938\n",
      "Epoch:  373  Training Loss:  1.09989  Training Accuracy:  0.806467\n",
      "Epoch:  374  Training Loss:  1.09808  Training Accuracy:  0.807173\n",
      "Epoch:  375  Training Loss:  1.09689  Training Accuracy:  0.807996\n",
      "Epoch:  376  Training Loss:  1.09488  Training Accuracy:  0.80923\n",
      "Epoch:  377  Training Loss:  1.09355  Training Accuracy:  0.809936\n",
      "Epoch:  378  Training Loss:  1.09166  Training Accuracy:  0.810759\n",
      "Epoch:  379  Training Loss:  1.09036  Training Accuracy:  0.810935\n",
      "Epoch:  380  Training Loss:  1.08851  Training Accuracy:  0.811758\n",
      "Epoch:  381  Training Loss:  1.08629  Training Accuracy:  0.812464\n",
      "Epoch:  382  Training Loss:  1.08513  Training Accuracy:  0.812934\n",
      "Epoch:  383  Training Loss:  1.08261  Training Accuracy:  0.813581\n",
      "Epoch:  384  Training Loss:  1.08069  Training Accuracy:  0.814639\n",
      "Epoch:  385  Training Loss:  1.07968  Training Accuracy:  0.815815\n",
      "Epoch:  386  Training Loss:  1.07797  Training Accuracy:  0.816638\n",
      "Epoch:  387  Training Loss:  1.07588  Training Accuracy:  0.817578\n",
      "Epoch:  388  Training Loss:  1.07515  Training Accuracy:  0.818225\n",
      "Epoch:  389  Training Loss:  1.07342  Training Accuracy:  0.819048\n",
      "Epoch:  390  Training Loss:  1.07071  Training Accuracy:  0.81993\n",
      "Epoch:  391  Training Loss:  1.06972  Training Accuracy:  0.82087\n",
      "Epoch:  392  Training Loss:  1.06845  Training Accuracy:  0.822105\n",
      "Epoch:  393  Training Loss:  1.06561  Training Accuracy:  0.822752\n",
      "Epoch:  394  Training Loss:  1.06438  Training Accuracy:  0.823633\n",
      "Epoch:  395  Training Loss:  1.06325  Training Accuracy:  0.824633\n",
      "Epoch:  396  Training Loss:  1.06069  Training Accuracy:  0.825456\n",
      "Epoch:  397  Training Loss:  1.06003  Training Accuracy:  0.826691\n",
      "Epoch:  398  Training Loss:  1.05821  Training Accuracy:  0.827396\n",
      "Epoch:  399  Training Loss:  1.0564  Training Accuracy:  0.827807\n",
      "Epoch:  400  Training Loss:  1.05484  Training Accuracy:  0.828748\n",
      "Epoch:  401  Training Loss:  1.05366  Training Accuracy:  0.829865\n",
      "Epoch:  402  Training Loss:  1.05167  Training Accuracy:  0.830688\n",
      "Epoch:  403  Training Loss:  1.05071  Training Accuracy:  0.831511\n",
      "Epoch:  404  Training Loss:  1.04857  Training Accuracy:  0.832334\n",
      "Epoch:  405  Training Loss:  1.04784  Training Accuracy:  0.832922\n",
      "Epoch:  406  Training Loss:  1.04534  Training Accuracy:  0.833628\n",
      "Epoch:  407  Training Loss:  1.04429  Training Accuracy:  0.834274\n",
      "Epoch:  408  Training Loss:  1.04271  Training Accuracy:  0.834686\n",
      "Epoch:  409  Training Loss:  1.04157  Training Accuracy:  0.835509\n",
      "Epoch:  410  Training Loss:  1.03989  Training Accuracy:  0.836273\n",
      "Epoch:  411  Training Loss:  1.0385  Training Accuracy:  0.836743\n",
      "Epoch:  412  Training Loss:  1.03675  Training Accuracy:  0.837214\n",
      "Epoch:  413  Training Loss:  1.03609  Training Accuracy:  0.837802\n",
      "Epoch:  414  Training Loss:  1.03384  Training Accuracy:  0.838272\n",
      "Epoch:  415  Training Loss:  1.03283  Training Accuracy:  0.838977\n",
      "Epoch:  416  Training Loss:  1.03114  Training Accuracy:  0.83933\n",
      "Epoch:  417  Training Loss:  1.0293  Training Accuracy:  0.839977\n",
      "Epoch:  418  Training Loss:  1.02763  Training Accuracy:  0.840388\n",
      "Epoch:  419  Training Loss:  1.02668  Training Accuracy:  0.840859\n",
      "Epoch:  420  Training Loss:  1.02526  Training Accuracy:  0.84127\n",
      "Epoch:  421  Training Loss:  1.0238  Training Accuracy:  0.841623\n",
      "Epoch:  422  Training Loss:  1.02194  Training Accuracy:  0.84227\n",
      "Epoch:  423  Training Loss:  1.02083  Training Accuracy:  0.842505\n",
      "Epoch:  424  Training Loss:  1.01822  Training Accuracy:  0.843387\n",
      "Epoch:  425  Training Loss:  1.01697  Training Accuracy:  0.843916\n",
      "Epoch:  426  Training Loss:  1.01543  Training Accuracy:  0.844562\n",
      "Epoch:  427  Training Loss:  1.01425  Training Accuracy:  0.845091\n",
      "Epoch:  428  Training Loss:  1.01262  Training Accuracy:  0.845562\n",
      "Epoch:  429  Training Loss:  1.01131  Training Accuracy:  0.845973\n",
      "Epoch:  430  Training Loss:  1.00907  Training Accuracy:  0.846561\n",
      "Epoch:  431  Training Loss:  1.00831  Training Accuracy:  0.847502\n",
      "Epoch:  432  Training Loss:  1.00684  Training Accuracy:  0.848148\n",
      "Epoch:  433  Training Loss:  1.00524  Training Accuracy:  0.848501\n",
      "Epoch:  434  Training Loss:  1.00403  Training Accuracy:  0.848913\n",
      "Epoch:  435  Training Loss:  1.00271  Training Accuracy:  0.849736\n",
      "Epoch:  436  Training Loss:  1.00107  Training Accuracy:  0.850618\n",
      "Epoch:  437  Training Loss:  0.999902  Training Accuracy:  0.850912\n",
      "Epoch:  438  Training Loss:  0.998689  Training Accuracy:  0.851264\n",
      "Epoch:  439  Training Loss:  0.997261  Training Accuracy:  0.851793\n",
      "Epoch:  440  Training Loss:  0.996069  Training Accuracy:  0.852087\n",
      "Epoch:  441  Training Loss:  0.99456  Training Accuracy:  0.852558\n",
      "Epoch:  442  Training Loss:  0.99322  Training Accuracy:  0.853028\n",
      "Epoch:  443  Training Loss:  0.991801  Training Accuracy:  0.853204\n",
      "Epoch:  444  Training Loss:  0.990069  Training Accuracy:  0.853381\n",
      "Epoch:  445  Training Loss:  0.988831  Training Accuracy:  0.854263\n",
      "Epoch:  446  Training Loss:  0.986919  Training Accuracy:  0.854439\n",
      "Epoch:  447  Training Loss:  0.985596  Training Accuracy:  0.855262\n",
      "Epoch:  448  Training Loss:  0.983429  Training Accuracy:  0.855909\n",
      "Epoch:  449  Training Loss:  0.982345  Training Accuracy:  0.856203\n",
      "Epoch:  450  Training Loss:  0.980785  Training Accuracy:  0.85632\n",
      "Epoch:  451  Training Loss:  0.979025  Training Accuracy:  0.856908\n",
      "Epoch:  452  Training Loss:  0.977761  Training Accuracy:  0.857437\n",
      "Epoch:  453  Training Loss:  0.975938  Training Accuracy:  0.857907\n",
      "Epoch:  454  Training Loss:  0.974979  Training Accuracy:  0.85826\n",
      "Epoch:  455  Training Loss:  0.973197  Training Accuracy:  0.859024\n",
      "Epoch:  456  Training Loss:  0.971981  Training Accuracy:  0.859436\n",
      "Epoch:  457  Training Loss:  0.970253  Training Accuracy:  0.859906\n",
      "Epoch:  458  Training Loss:  0.968797  Training Accuracy:  0.860377\n",
      "Epoch:  459  Training Loss:  0.967465  Training Accuracy:  0.860906\n",
      "Epoch:  460  Training Loss:  0.966102  Training Accuracy:  0.861082\n",
      "Epoch:  461  Training Loss:  0.964841  Training Accuracy:  0.86167\n",
      "Epoch:  462  Training Loss:  0.963236  Training Accuracy:  0.862081\n",
      "Epoch:  463  Training Loss:  0.962034  Training Accuracy:  0.862669\n",
      "Epoch:  464  Training Loss:  0.960115  Training Accuracy:  0.86314\n",
      "Epoch:  465  Training Loss:  0.959316  Training Accuracy:  0.863316\n",
      "Epoch:  466  Training Loss:  0.957399  Training Accuracy:  0.863551\n",
      "Epoch:  467  Training Loss:  0.956808  Training Accuracy:  0.864198\n",
      "Epoch:  468  Training Loss:  0.954642  Training Accuracy:  0.864609\n",
      "Epoch:  469  Training Loss:  0.953424  Training Accuracy:  0.86508\n",
      "Epoch:  470  Training Loss:  0.95151  Training Accuracy:  0.865609\n",
      "Epoch:  471  Training Loss:  0.949833  Training Accuracy:  0.86602\n",
      "Epoch:  472  Training Loss:  0.948441  Training Accuracy:  0.866432\n",
      "Epoch:  473  Training Loss:  0.947104  Training Accuracy:  0.866843\n",
      "Epoch:  474  Training Loss:  0.945287  Training Accuracy:  0.867372\n",
      "Epoch:  475  Training Loss:  0.943275  Training Accuracy:  0.867608\n",
      "Epoch:  476  Training Loss:  0.94211  Training Accuracy:  0.867902\n",
      "Epoch:  477  Training Loss:  0.940675  Training Accuracy:  0.868313\n",
      "Epoch:  478  Training Loss:  0.938769  Training Accuracy:  0.868607\n",
      "Epoch:  479  Training Loss:  0.937783  Training Accuracy:  0.868901\n",
      "Epoch:  480  Training Loss:  0.935859  Training Accuracy:  0.869136\n",
      "Epoch:  481  Training Loss:  0.934306  Training Accuracy:  0.869548\n",
      "Epoch:  482  Training Loss:  0.933636  Training Accuracy:  0.869724\n",
      "Epoch:  483  Training Loss:  0.931791  Training Accuracy:  0.869959\n",
      "Epoch:  484  Training Loss:  0.930418  Training Accuracy:  0.870194\n",
      "Epoch:  485  Training Loss:  0.929448  Training Accuracy:  0.870724\n",
      "Epoch:  486  Training Loss:  0.927549  Training Accuracy:  0.870959\n",
      "Epoch:  487  Training Loss:  0.926048  Training Accuracy:  0.871429\n",
      "Epoch:  488  Training Loss:  0.925494  Training Accuracy:  0.871723\n",
      "Epoch:  489  Training Loss:  0.923716  Training Accuracy:  0.872076\n",
      "Epoch:  490  Training Loss:  0.922152  Training Accuracy:  0.872487\n",
      "Epoch:  491  Training Loss:  0.921339  Training Accuracy:  0.87284\n",
      "Epoch:  492  Training Loss:  0.919691  Training Accuracy:  0.873545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  493  Training Loss:  0.91895  Training Accuracy:  0.873722\n",
      "Epoch:  494  Training Loss:  0.91736  Training Accuracy:  0.874075\n",
      "Epoch:  495  Training Loss:  0.915845  Training Accuracy:  0.874486\n",
      "Epoch:  496  Training Loss:  0.914689  Training Accuracy:  0.875074\n",
      "Epoch:  497  Training Loss:  0.912481  Training Accuracy:  0.875485\n",
      "Epoch:  498  Training Loss:  0.911476  Training Accuracy:  0.875897\n",
      "Epoch:  499  Training Loss:  0.909963  Training Accuracy:  0.87672\n",
      "Epoch:  500  Training Loss:  0.908443  Training Accuracy:  0.877073\n",
      "Epoch:  501  Training Loss:  0.906542  Training Accuracy:  0.877308\n",
      "Epoch:  502  Training Loss:  0.905575  Training Accuracy:  0.877661\n",
      "Epoch:  503  Training Loss:  0.903146  Training Accuracy:  0.878072\n",
      "Epoch:  504  Training Loss:  0.901144  Training Accuracy:  0.878249\n",
      "Epoch:  505  Training Loss:  0.899161  Training Accuracy:  0.878425\n",
      "Epoch:  506  Training Loss:  0.897851  Training Accuracy:  0.878836\n",
      "Epoch:  507  Training Loss:  0.896472  Training Accuracy:  0.879013\n",
      "Epoch:  508  Training Loss:  0.893916  Training Accuracy:  0.879424\n",
      "Epoch:  509  Training Loss:  0.892871  Training Accuracy:  0.879483\n",
      "Epoch:  510  Training Loss:  0.890815  Training Accuracy:  0.880012\n",
      "Epoch:  511  Training Loss:  0.89031  Training Accuracy:  0.880306\n",
      "Epoch:  512  Training Loss:  0.888439  Training Accuracy:  0.880659\n",
      "Epoch:  513  Training Loss:  0.886592  Training Accuracy:  0.880894\n",
      "Epoch:  514  Training Loss:  0.885567  Training Accuracy:  0.881188\n",
      "Epoch:  515  Training Loss:  0.884389  Training Accuracy:  0.881599\n",
      "Epoch:  516  Training Loss:  0.882402  Training Accuracy:  0.881894\n",
      "Epoch:  517  Training Loss:  0.88111  Training Accuracy:  0.882246\n",
      "Epoch:  518  Training Loss:  0.879559  Training Accuracy:  0.882423\n",
      "Epoch:  519  Training Loss:  0.878648  Training Accuracy:  0.88254\n",
      "Epoch:  520  Training Loss:  0.876528  Training Accuracy:  0.882658\n",
      "Epoch:  521  Training Loss:  0.875256  Training Accuracy:  0.883187\n",
      "Epoch:  522  Training Loss:  0.873891  Training Accuracy:  0.883246\n",
      "Epoch:  523  Training Loss:  0.872916  Training Accuracy:  0.883539\n",
      "Epoch:  524  Training Loss:  0.871465  Training Accuracy:  0.883951\n",
      "Epoch:  525  Training Loss:  0.87021  Training Accuracy:  0.884127\n",
      "Epoch:  526  Training Loss:  0.868922  Training Accuracy:  0.884656\n",
      "Epoch:  527  Training Loss:  0.867407  Training Accuracy:  0.885303\n",
      "Epoch:  528  Training Loss:  0.865819  Training Accuracy:  0.885362\n",
      "Epoch:  529  Training Loss:  0.865196  Training Accuracy:  0.885656\n",
      "Epoch:  530  Training Loss:  0.863469  Training Accuracy:  0.885832\n",
      "Epoch:  531  Training Loss:  0.862127  Training Accuracy:  0.88595\n",
      "Epoch:  532  Training Loss:  0.861306  Training Accuracy:  0.88642\n",
      "Epoch:  533  Training Loss:  0.859963  Training Accuracy:  0.887185\n",
      "Epoch:  534  Training Loss:  0.858018  Training Accuracy:  0.887537\n",
      "Epoch:  535  Training Loss:  0.857513  Training Accuracy:  0.887831\n",
      "Epoch:  536  Training Loss:  0.855901  Training Accuracy:  0.888125\n",
      "Epoch:  537  Training Loss:  0.854429  Training Accuracy:  0.888419\n",
      "Epoch:  538  Training Loss:  0.853749  Training Accuracy:  0.889007\n",
      "Epoch:  539  Training Loss:  0.851778  Training Accuracy:  0.889477\n",
      "Epoch:  540  Training Loss:  0.850002  Training Accuracy:  0.889595\n",
      "Epoch:  541  Training Loss:  0.849313  Training Accuracy:  0.889889\n",
      "Epoch:  542  Training Loss:  0.84845  Training Accuracy:  0.890065\n",
      "Epoch:  543  Training Loss:  0.846639  Training Accuracy:  0.890418\n",
      "Epoch:  544  Training Loss:  0.846082  Training Accuracy:  0.890712\n",
      "Epoch:  545  Training Loss:  0.844261  Training Accuracy:  0.891123\n",
      "Epoch:  546  Training Loss:  0.842954  Training Accuracy:  0.891417\n",
      "Epoch:  547  Training Loss:  0.842338  Training Accuracy:  0.89177\n",
      "Epoch:  548  Training Loss:  0.840694  Training Accuracy:  0.891946\n",
      "Epoch:  549  Training Loss:  0.839347  Training Accuracy:  0.892299\n",
      "Epoch:  550  Training Loss:  0.838619  Training Accuracy:  0.892417\n",
      "Epoch:  551  Training Loss:  0.836605  Training Accuracy:  0.892534\n",
      "Epoch:  552  Training Loss:  0.836221  Training Accuracy:  0.893122\n",
      "Epoch:  553  Training Loss:  0.835082  Training Accuracy:  0.893475\n",
      "Epoch:  554  Training Loss:  0.832801  Training Accuracy:  0.893651\n",
      "Epoch:  555  Training Loss:  0.832043  Training Accuracy:  0.894239\n",
      "Epoch:  556  Training Loss:  0.830774  Training Accuracy:  0.894533\n",
      "Epoch:  557  Training Loss:  0.829001  Training Accuracy:  0.895239\n",
      "Epoch:  558  Training Loss:  0.828472  Training Accuracy:  0.895474\n",
      "Epoch:  559  Training Loss:  0.826942  Training Accuracy:  0.895826\n",
      "Epoch:  560  Training Loss:  0.826343  Training Accuracy:  0.896297\n",
      "Epoch:  561  Training Loss:  0.824874  Training Accuracy:  0.896649\n",
      "Epoch:  562  Training Loss:  0.822981  Training Accuracy:  0.896885\n",
      "Epoch:  563  Training Loss:  0.821589  Training Accuracy:  0.897296\n",
      "Epoch:  564  Training Loss:  0.820079  Training Accuracy:  0.897708\n",
      "Epoch:  565  Training Loss:  0.819565  Training Accuracy:  0.897943\n",
      "Epoch:  566  Training Loss:  0.8175  Training Accuracy:  0.898119\n",
      "Epoch:  567  Training Loss:  0.816675  Training Accuracy:  0.898237\n",
      "Epoch:  568  Training Loss:  0.815315  Training Accuracy:  0.898648\n",
      "Epoch:  569  Training Loss:  0.813764  Training Accuracy:  0.899119\n",
      "Epoch:  570  Training Loss:  0.812549  Training Accuracy:  0.899295\n",
      "Epoch:  571  Training Loss:  0.811694  Training Accuracy:  0.89953\n",
      "Epoch:  572  Training Loss:  0.809776  Training Accuracy:  0.899942\n",
      "Epoch:  573  Training Loss:  0.809152  Training Accuracy:  0.900471\n",
      "Epoch:  574  Training Loss:  0.808068  Training Accuracy:  0.900765\n",
      "Epoch:  575  Training Loss:  0.80573  Training Accuracy:  0.900823\n",
      "Epoch:  576  Training Loss:  0.80523  Training Accuracy:  0.901117\n",
      "Epoch:  577  Training Loss:  0.803657  Training Accuracy:  0.901353\n",
      "Epoch:  578  Training Loss:  0.80176  Training Accuracy:  0.901529\n",
      "Epoch:  579  Training Loss:  0.801221  Training Accuracy:  0.901999\n",
      "Epoch:  580  Training Loss:  0.799938  Training Accuracy:  0.902411\n",
      "Epoch:  581  Training Loss:  0.798514  Training Accuracy:  0.902881\n",
      "Epoch:  582  Training Loss:  0.797461  Training Accuracy:  0.903351\n",
      "Epoch:  583  Training Loss:  0.796802  Training Accuracy:  0.903587\n",
      "Epoch:  584  Training Loss:  0.79548  Training Accuracy:  0.904116\n",
      "Epoch:  585  Training Loss:  0.793622  Training Accuracy:  0.90441\n",
      "Epoch:  586  Training Loss:  0.793523  Training Accuracy:  0.904586\n",
      "Epoch:  587  Training Loss:  0.792147  Training Accuracy:  0.904762\n",
      "Epoch:  588  Training Loss:  0.790963  Training Accuracy:  0.905174\n",
      "Epoch:  589  Training Loss:  0.79038  Training Accuracy:  0.90535\n",
      "Epoch:  590  Training Loss:  0.788905  Training Accuracy:  0.905468\n",
      "Epoch:  591  Training Loss:  0.788441  Training Accuracy:  0.905644\n",
      "Epoch:  592  Training Loss:  0.787067  Training Accuracy:  0.905821\n",
      "Epoch:  593  Training Loss:  0.786717  Training Accuracy:  0.906173\n",
      "Epoch:  594  Training Loss:  0.784845  Training Accuracy:  0.906585\n",
      "Epoch:  595  Training Loss:  0.784384  Training Accuracy:  0.90682\n",
      "Epoch:  596  Training Loss:  0.782414  Training Accuracy:  0.907055\n",
      "Epoch:  597  Training Loss:  0.782286  Training Accuracy:  0.907408\n",
      "Epoch:  598  Training Loss:  0.781192  Training Accuracy:  0.907467\n",
      "Epoch:  599  Training Loss:  0.77928  Training Accuracy:  0.907761\n",
      "Epoch:  600  Training Loss:  0.779135  Training Accuracy:  0.907937\n",
      "Epoch:  601  Training Loss:  0.778045  Training Accuracy:  0.90829\n",
      "Epoch:  602  Training Loss:  0.77677  Training Accuracy:  0.908466\n",
      "Epoch:  603  Training Loss:  0.775907  Training Accuracy:  0.908701\n",
      "Epoch:  604  Training Loss:  0.775546  Training Accuracy:  0.908819\n",
      "Epoch:  605  Training Loss:  0.773849  Training Accuracy:  0.909054\n",
      "Epoch:  606  Training Loss:  0.772726  Training Accuracy:  0.909524\n",
      "Epoch:  607  Training Loss:  0.772137  Training Accuracy:  0.909818\n",
      "Epoch:  608  Training Loss:  0.770059  Training Accuracy:  0.909995\n",
      "Epoch:  609  Training Loss:  0.769725  Training Accuracy:  0.910347\n",
      "Epoch:  610  Training Loss:  0.768352  Training Accuracy:  0.910582\n",
      "Epoch:  611  Training Loss:  0.767476  Training Accuracy:  0.910994\n",
      "Epoch:  612  Training Loss:  0.76661  Training Accuracy:  0.911699\n",
      "Epoch:  613  Training Loss:  0.764939  Training Accuracy:  0.912229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  614  Training Loss:  0.764776  Training Accuracy:  0.912464\n",
      "Epoch:  615  Training Loss:  0.763001  Training Accuracy:  0.913052\n",
      "Epoch:  616  Training Loss:  0.762571  Training Accuracy:  0.913346\n",
      "Epoch:  617  Training Loss:  0.761409  Training Accuracy:  0.913639\n",
      "Epoch:  618  Training Loss:  0.759941  Training Accuracy:  0.913757\n",
      "Epoch:  619  Training Loss:  0.758699  Training Accuracy:  0.913933\n",
      "Epoch:  620  Training Loss:  0.757632  Training Accuracy:  0.914227\n",
      "Epoch:  621  Training Loss:  0.756478  Training Accuracy:  0.914404\n",
      "Epoch:  622  Training Loss:  0.75556  Training Accuracy:  0.91458\n",
      "Epoch:  623  Training Loss:  0.754462  Training Accuracy:  0.914756\n",
      "Epoch:  624  Training Loss:  0.753597  Training Accuracy:  0.915168\n",
      "Epoch:  625  Training Loss:  0.754179  Training Accuracy:  0.915521\n",
      "Epoch:  626  Training Loss:  0.752068  Training Accuracy:  0.915697\n",
      "Epoch:  627  Training Loss:  0.75123  Training Accuracy:  0.915991\n",
      "Epoch:  628  Training Loss:  0.750007  Training Accuracy:  0.916285\n",
      "Epoch:  629  Training Loss:  0.749505  Training Accuracy:  0.916932\n",
      "Epoch:  630  Training Loss:  0.74935  Training Accuracy:  0.917108\n",
      "Epoch:  631  Training Loss:  0.748126  Training Accuracy:  0.917461\n",
      "Epoch:  632  Training Loss:  0.747875  Training Accuracy:  0.917813\n",
      "Epoch:  633  Training Loss:  0.746606  Training Accuracy:  0.917931\n",
      "Epoch:  634  Training Loss:  0.745314  Training Accuracy:  0.918519\n",
      "Epoch:  635  Training Loss:  0.744967  Training Accuracy:  0.918813\n",
      "Epoch:  636  Training Loss:  0.744024  Training Accuracy:  0.919107\n",
      "Epoch:  637  Training Loss:  0.742694  Training Accuracy:  0.919401\n",
      "Epoch:  638  Training Loss:  0.740866  Training Accuracy:  0.919518\n",
      "Epoch:  639  Training Loss:  0.740284  Training Accuracy:  0.919754\n",
      "Epoch:  640  Training Loss:  0.739583  Training Accuracy:  0.920048\n",
      "Epoch:  641  Training Loss:  0.73812  Training Accuracy:  0.920048\n",
      "Epoch:  642  Training Loss:  0.737316  Training Accuracy:  0.920283\n",
      "Epoch:  643  Training Loss:  0.736328  Training Accuracy:  0.920459\n",
      "Epoch:  644  Training Loss:  0.735396  Training Accuracy:  0.920753\n",
      "Epoch:  645  Training Loss:  0.734649  Training Accuracy:  0.920871\n",
      "Epoch:  646  Training Loss:  0.733707  Training Accuracy:  0.921047\n",
      "Epoch:  647  Training Loss:  0.732606  Training Accuracy:  0.921047\n",
      "Epoch:  648  Training Loss:  0.731704  Training Accuracy:  0.921341\n",
      "Epoch:  649  Training Loss:  0.731429  Training Accuracy:  0.9214\n",
      "Epoch:  650  Training Loss:  0.729114  Training Accuracy:  0.921694\n",
      "Epoch:  651  Training Loss:  0.728964  Training Accuracy:  0.921811\n",
      "Epoch:  652  Training Loss:  0.727983  Training Accuracy:  0.922046\n",
      "Epoch:  653  Training Loss:  0.726725  Training Accuracy:  0.922517\n",
      "Epoch:  654  Training Loss:  0.725532  Training Accuracy:  0.922634\n",
      "Epoch:  655  Training Loss:  0.725166  Training Accuracy:  0.922811\n",
      "Epoch:  656  Training Loss:  0.723067  Training Accuracy:  0.922928\n",
      "Epoch:  657  Training Loss:  0.722353  Training Accuracy:  0.923163\n",
      "Epoch:  658  Training Loss:  0.721944  Training Accuracy:  0.923281\n",
      "Epoch:  659  Training Loss:  0.720021  Training Accuracy:  0.923457\n",
      "Epoch:  660  Training Loss:  0.719652  Training Accuracy:  0.923634\n",
      "Epoch:  661  Training Loss:  0.718747  Training Accuracy:  0.923869\n",
      "Epoch:  662  Training Loss:  0.71729  Training Accuracy:  0.924104\n",
      "Epoch:  663  Training Loss:  0.716458  Training Accuracy:  0.92428\n",
      "Epoch:  664  Training Loss:  0.716426  Training Accuracy:  0.924339\n",
      "Epoch:  665  Training Loss:  0.714046  Training Accuracy:  0.924633\n",
      "Epoch:  666  Training Loss:  0.713315  Training Accuracy:  0.925103\n",
      "Epoch:  667  Training Loss:  0.713062  Training Accuracy:  0.925397\n",
      "Epoch:  668  Training Loss:  0.7111  Training Accuracy:  0.925632\n",
      "Epoch:  669  Training Loss:  0.710747  Training Accuracy:  0.92575\n",
      "Epoch:  670  Training Loss:  0.70892  Training Accuracy:  0.925985\n",
      "Epoch:  671  Training Loss:  0.708219  Training Accuracy:  0.926397\n",
      "Epoch:  672  Training Loss:  0.706504  Training Accuracy:  0.926514\n",
      "Epoch:  673  Training Loss:  0.705772  Training Accuracy:  0.926573\n",
      "Epoch:  674  Training Loss:  0.705488  Training Accuracy:  0.926985\n",
      "Epoch:  675  Training Loss:  0.703446  Training Accuracy:  0.927043\n",
      "Epoch:  676  Training Loss:  0.702903  Training Accuracy:  0.927161\n",
      "Epoch:  677  Training Loss:  0.701283  Training Accuracy:  0.927279\n",
      "Epoch:  678  Training Loss:  0.700844  Training Accuracy:  0.927514\n",
      "Epoch:  679  Training Loss:  0.700009  Training Accuracy:  0.92769\n",
      "Epoch:  680  Training Loss:  0.698972  Training Accuracy:  0.927749\n",
      "Epoch:  681  Training Loss:  0.697896  Training Accuracy:  0.927925\n",
      "Epoch:  682  Training Loss:  0.696819  Training Accuracy:  0.928043\n",
      "Epoch:  683  Training Loss:  0.696159  Training Accuracy:  0.928278\n",
      "Epoch:  684  Training Loss:  0.694976  Training Accuracy:  0.928631\n",
      "Epoch:  685  Training Loss:  0.694451  Training Accuracy:  0.928983\n",
      "Epoch:  686  Training Loss:  0.692517  Training Accuracy:  0.929336\n",
      "Epoch:  687  Training Loss:  0.692454  Training Accuracy:  0.929513\n",
      "Epoch:  688  Training Loss:  0.691113  Training Accuracy:  0.929806\n",
      "Epoch:  689  Training Loss:  0.690828  Training Accuracy:  0.929865\n",
      "Epoch:  690  Training Loss:  0.689663  Training Accuracy:  0.930159\n",
      "Epoch:  691  Training Loss:  0.688435  Training Accuracy:  0.930453\n",
      "Epoch:  692  Training Loss:  0.688179  Training Accuracy:  0.930688\n",
      "Epoch:  693  Training Loss:  0.686299  Training Accuracy:  0.930982\n",
      "Epoch:  694  Training Loss:  0.685754  Training Accuracy:  0.9311\n",
      "Epoch:  695  Training Loss:  0.684264  Training Accuracy:  0.931217\n",
      "Epoch:  696  Training Loss:  0.683294  Training Accuracy:  0.93157\n",
      "Epoch:  697  Training Loss:  0.682156  Training Accuracy:  0.931629\n",
      "Epoch:  698  Training Loss:  0.681286  Training Accuracy:  0.931982\n",
      "Epoch:  699  Training Loss:  0.680221  Training Accuracy:  0.932099\n",
      "Epoch:  700  Training Loss:  0.67914  Training Accuracy:  0.932276\n",
      "Epoch:  701  Training Loss:  0.677884  Training Accuracy:  0.932393\n",
      "Epoch:  702  Training Loss:  0.677718  Training Accuracy:  0.932687\n",
      "Epoch:  703  Training Loss:  0.6762  Training Accuracy:  0.932746\n",
      "Epoch:  704  Training Loss:  0.675511  Training Accuracy:  0.932981\n",
      "Epoch:  705  Training Loss:  0.674626  Training Accuracy:  0.933275\n",
      "Epoch:  706  Training Loss:  0.673421  Training Accuracy:  0.93351\n",
      "Epoch:  707  Training Loss:  0.672797  Training Accuracy:  0.933804\n",
      "Epoch:  708  Training Loss:  0.671933  Training Accuracy:  0.934039\n",
      "Epoch:  709  Training Loss:  0.670435  Training Accuracy:  0.934274\n",
      "Epoch:  710  Training Loss:  0.670152  Training Accuracy:  0.934568\n",
      "Epoch:  711  Training Loss:  0.669076  Training Accuracy:  0.934686\n",
      "Epoch:  712  Training Loss:  0.668355  Training Accuracy:  0.934921\n",
      "Epoch:  713  Training Loss:  0.667289  Training Accuracy:  0.935215\n",
      "Epoch:  714  Training Loss:  0.665762  Training Accuracy:  0.93545\n",
      "Epoch:  715  Training Loss:  0.665284  Training Accuracy:  0.935862\n",
      "Epoch:  716  Training Loss:  0.664357  Training Accuracy:  0.936156\n",
      "Epoch:  717  Training Loss:  0.663561  Training Accuracy:  0.936156\n",
      "Epoch:  718  Training Loss:  0.662576  Training Accuracy:  0.93645\n",
      "Epoch:  719  Training Loss:  0.661643  Training Accuracy:  0.936685\n",
      "Epoch:  720  Training Loss:  0.660868  Training Accuracy:  0.936861\n",
      "Epoch:  721  Training Loss:  0.660214  Training Accuracy:  0.937155\n",
      "Epoch:  722  Training Loss:  0.659099  Training Accuracy:  0.937273\n",
      "Epoch:  723  Training Loss:  0.658324  Training Accuracy:  0.937508\n",
      "Epoch:  724  Training Loss:  0.656948  Training Accuracy:  0.937684\n",
      "Epoch:  725  Training Loss:  0.656681  Training Accuracy:  0.937861\n",
      "Epoch:  726  Training Loss:  0.655313  Training Accuracy:  0.938037\n",
      "Epoch:  727  Training Loss:  0.654944  Training Accuracy:  0.938272\n",
      "Epoch:  728  Training Loss:  0.653572  Training Accuracy:  0.938331\n",
      "Epoch:  729  Training Loss:  0.653058  Training Accuracy:  0.938507\n",
      "Epoch:  730  Training Loss:  0.652036  Training Accuracy:  0.938801\n",
      "Epoch:  731  Training Loss:  0.650508  Training Accuracy:  0.939095\n",
      "Epoch:  732  Training Loss:  0.649823  Training Accuracy:  0.939448\n",
      "Epoch:  733  Training Loss:  0.648994  Training Accuracy:  0.939565\n",
      "Epoch:  734  Training Loss:  0.648159  Training Accuracy:  0.939742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  735  Training Loss:  0.647399  Training Accuracy:  0.939977\n",
      "Epoch:  736  Training Loss:  0.646261  Training Accuracy:  0.940095\n",
      "Epoch:  737  Training Loss:  0.645666  Training Accuracy:  0.940212\n",
      "Epoch:  738  Training Loss:  0.645354  Training Accuracy:  0.94033\n",
      "Epoch:  739  Training Loss:  0.644233  Training Accuracy:  0.940389\n",
      "Epoch:  740  Training Loss:  0.643211  Training Accuracy:  0.940741\n",
      "Epoch:  741  Training Loss:  0.642424  Training Accuracy:  0.940859\n",
      "Epoch:  742  Training Loss:  0.641648  Training Accuracy:  0.940976\n",
      "Epoch:  743  Training Loss:  0.640414  Training Accuracy:  0.940976\n",
      "Epoch:  744  Training Loss:  0.640155  Training Accuracy:  0.941153\n",
      "Epoch:  745  Training Loss:  0.638689  Training Accuracy:  0.941329\n",
      "Epoch:  746  Training Loss:  0.63899  Training Accuracy:  0.941388\n",
      "Epoch:  747  Training Loss:  0.637702  Training Accuracy:  0.941447\n",
      "Epoch:  748  Training Loss:  0.636749  Training Accuracy:  0.941505\n",
      "Epoch:  749  Training Loss:  0.636023  Training Accuracy:  0.941564\n",
      "Epoch:  750  Training Loss:  0.635503  Training Accuracy:  0.941682\n",
      "Epoch:  751  Training Loss:  0.634725  Training Accuracy:  0.941858\n",
      "Epoch:  752  Training Loss:  0.633957  Training Accuracy:  0.941976\n",
      "Epoch:  753  Training Loss:  0.633022  Training Accuracy:  0.942211\n",
      "Epoch:  754  Training Loss:  0.632682  Training Accuracy:  0.942505\n",
      "Epoch:  755  Training Loss:  0.631177  Training Accuracy:  0.942858\n",
      "Epoch:  756  Training Loss:  0.63133  Training Accuracy:  0.943093\n",
      "Epoch:  757  Training Loss:  0.629734  Training Accuracy:  0.94321\n",
      "Epoch:  758  Training Loss:  0.629098  Training Accuracy:  0.943269\n",
      "Epoch:  759  Training Loss:  0.628657  Training Accuracy:  0.943446\n",
      "Epoch:  760  Training Loss:  0.62727  Training Accuracy:  0.943563\n",
      "Epoch:  761  Training Loss:  0.62656  Training Accuracy:  0.943681\n",
      "Epoch:  762  Training Loss:  0.625272  Training Accuracy:  0.943681\n",
      "Epoch:  763  Training Loss:  0.624722  Training Accuracy:  0.943798\n",
      "Epoch:  764  Training Loss:  0.623307  Training Accuracy:  0.943975\n",
      "Epoch:  765  Training Loss:  0.623199  Training Accuracy:  0.944151\n",
      "Epoch:  766  Training Loss:  0.622028  Training Accuracy:  0.944445\n",
      "Epoch:  767  Training Loss:  0.621095  Training Accuracy:  0.944621\n",
      "Epoch:  768  Training Loss:  0.620002  Training Accuracy:  0.944621\n",
      "Epoch:  769  Training Loss:  0.619467  Training Accuracy:  0.944857\n",
      "Epoch:  770  Training Loss:  0.61778  Training Accuracy:  0.944915\n",
      "Epoch:  771  Training Loss:  0.618082  Training Accuracy:  0.944974\n",
      "Epoch:  772  Training Loss:  0.616668  Training Accuracy:  0.945092\n",
      "Epoch:  773  Training Loss:  0.615816  Training Accuracy:  0.945209\n",
      "Epoch:  774  Training Loss:  0.614786  Training Accuracy:  0.945386\n",
      "Epoch:  775  Training Loss:  0.614458  Training Accuracy:  0.945621\n",
      "Epoch:  776  Training Loss:  0.613162  Training Accuracy:  0.945738\n",
      "Epoch:  777  Training Loss:  0.612746  Training Accuracy:  0.945797\n",
      "Epoch:  778  Training Loss:  0.611497  Training Accuracy:  0.945915\n",
      "Epoch:  779  Training Loss:  0.6108  Training Accuracy:  0.945973\n",
      "Epoch:  780  Training Loss:  0.609903  Training Accuracy:  0.946267\n",
      "Epoch:  781  Training Loss:  0.609308  Training Accuracy:  0.946444\n",
      "Epoch:  782  Training Loss:  0.608462  Training Accuracy:  0.946797\n",
      "Epoch:  783  Training Loss:  0.607817  Training Accuracy:  0.946973\n",
      "Epoch:  784  Training Loss:  0.607288  Training Accuracy:  0.947032\n",
      "Epoch:  785  Training Loss:  0.606398  Training Accuracy:  0.947032\n",
      "Epoch:  786  Training Loss:  0.605787  Training Accuracy:  0.947091\n",
      "Epoch:  787  Training Loss:  0.604973  Training Accuracy:  0.947091\n",
      "Epoch:  788  Training Loss:  0.604368  Training Accuracy:  0.947208\n",
      "Epoch:  789  Training Loss:  0.603502  Training Accuracy:  0.947384\n",
      "Epoch:  790  Training Loss:  0.60253  Training Accuracy:  0.947502\n",
      "Epoch:  791  Training Loss:  0.60203  Training Accuracy:  0.947737\n",
      "Epoch:  792  Training Loss:  0.600872  Training Accuracy:  0.947796\n",
      "Epoch:  793  Training Loss:  0.600658  Training Accuracy:  0.947972\n",
      "Epoch:  794  Training Loss:  0.600024  Training Accuracy:  0.948149\n",
      "Epoch:  795  Training Loss:  0.59851  Training Accuracy:  0.948325\n",
      "Epoch:  796  Training Loss:  0.598421  Training Accuracy:  0.94856\n",
      "Epoch:  797  Training Loss:  0.598042  Training Accuracy:  0.948737\n",
      "Epoch:  798  Training Loss:  0.596404  Training Accuracy:  0.948913\n",
      "Epoch:  799  Training Loss:  0.596329  Training Accuracy:  0.949089\n",
      "Epoch:  800  Training Loss:  0.595759  Training Accuracy:  0.949089\n",
      "Epoch:  801  Training Loss:  0.595364  Training Accuracy:  0.949089\n",
      "Epoch:  802  Training Loss:  0.594354  Training Accuracy:  0.949266\n",
      "Epoch:  803  Training Loss:  0.593841  Training Accuracy:  0.949324\n",
      "Epoch:  804  Training Loss:  0.59314  Training Accuracy:  0.94956\n",
      "Epoch:  805  Training Loss:  0.59198  Training Accuracy:  0.949618\n",
      "Epoch:  806  Training Loss:  0.59183  Training Accuracy:  0.949795\n",
      "Epoch:  807  Training Loss:  0.590807  Training Accuracy:  0.949912\n",
      "Epoch:  808  Training Loss:  0.590245  Training Accuracy:  0.949971\n",
      "Epoch:  809  Training Loss:  0.59064  Training Accuracy:  0.95003\n",
      "Epoch:  810  Training Loss:  0.589149  Training Accuracy:  0.950089\n",
      "Epoch:  811  Training Loss:  0.588217  Training Accuracy:  0.950089\n",
      "Epoch:  812  Training Loss:  0.587383  Training Accuracy:  0.950265\n",
      "Epoch:  813  Training Loss:  0.586821  Training Accuracy:  0.950265\n",
      "Epoch:  814  Training Loss:  0.586066  Training Accuracy:  0.950441\n",
      "Epoch:  815  Training Loss:  0.585415  Training Accuracy:  0.9505\n",
      "Epoch:  816  Training Loss:  0.584604  Training Accuracy:  0.950559\n",
      "Epoch:  817  Training Loss:  0.583687  Training Accuracy:  0.950559\n",
      "Epoch:  818  Training Loss:  0.583369  Training Accuracy:  0.950735\n",
      "Epoch:  819  Training Loss:  0.582912  Training Accuracy:  0.950853\n",
      "Epoch:  820  Training Loss:  0.582005  Training Accuracy:  0.950794\n",
      "Epoch:  821  Training Loss:  0.581369  Training Accuracy:  0.950853\n",
      "Epoch:  822  Training Loss:  0.580277  Training Accuracy:  0.951088\n",
      "Epoch:  823  Training Loss:  0.579567  Training Accuracy:  0.951441\n",
      "Epoch:  824  Training Loss:  0.579173  Training Accuracy:  0.951617\n",
      "Epoch:  825  Training Loss:  0.579133  Training Accuracy:  0.951735\n",
      "Epoch:  826  Training Loss:  0.577569  Training Accuracy:  0.951852\n",
      "Epoch:  827  Training Loss:  0.576687  Training Accuracy:  0.952146\n",
      "Epoch:  828  Training Loss:  0.576003  Training Accuracy:  0.952264\n",
      "Epoch:  829  Training Loss:  0.575476  Training Accuracy:  0.95244\n",
      "Epoch:  830  Training Loss:  0.574399  Training Accuracy:  0.952617\n",
      "Epoch:  831  Training Loss:  0.573865  Training Accuracy:  0.953087\n",
      "Epoch:  832  Training Loss:  0.573148  Training Accuracy:  0.953263\n",
      "Epoch:  833  Training Loss:  0.572035  Training Accuracy:  0.953322\n",
      "Epoch:  834  Training Loss:  0.572352  Training Accuracy:  0.953616\n",
      "Epoch:  835  Training Loss:  0.570831  Training Accuracy:  0.953616\n",
      "Epoch:  836  Training Loss:  0.569918  Training Accuracy:  0.953734\n",
      "Epoch:  837  Training Loss:  0.569048  Training Accuracy:  0.95391\n",
      "Epoch:  838  Training Loss:  0.568808  Training Accuracy:  0.95391\n",
      "Epoch:  839  Training Loss:  0.568201  Training Accuracy:  0.954145\n",
      "Epoch:  840  Training Loss:  0.567671  Training Accuracy:  0.954263\n",
      "Epoch:  841  Training Loss:  0.566625  Training Accuracy:  0.95438\n",
      "Epoch:  842  Training Loss:  0.566244  Training Accuracy:  0.954615\n",
      "Epoch:  843  Training Loss:  0.56567  Training Accuracy:  0.954733\n",
      "Epoch:  844  Training Loss:  0.564546  Training Accuracy:  0.954851\n",
      "Epoch:  845  Training Loss:  0.563735  Training Accuracy:  0.955086\n",
      "Epoch:  846  Training Loss:  0.562865  Training Accuracy:  0.955203\n",
      "Epoch:  847  Training Loss:  0.562295  Training Accuracy:  0.955262\n",
      "Epoch:  848  Training Loss:  0.5621  Training Accuracy:  0.955439\n",
      "Epoch:  849  Training Loss:  0.561288  Training Accuracy:  0.955615\n",
      "Epoch:  850  Training Loss:  0.55992  Training Accuracy:  0.955674\n",
      "Epoch:  851  Training Loss:  0.559494  Training Accuracy:  0.955791\n",
      "Epoch:  852  Training Loss:  0.55906  Training Accuracy:  0.956085\n",
      "Epoch:  853  Training Loss:  0.558004  Training Accuracy:  0.956085\n",
      "Epoch:  854  Training Loss:  0.557611  Training Accuracy:  0.956262\n",
      "Epoch:  855  Training Loss:  0.556544  Training Accuracy:  0.95632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  856  Training Loss:  0.555645  Training Accuracy:  0.956438\n",
      "Epoch:  857  Training Loss:  0.55497  Training Accuracy:  0.956673\n",
      "Epoch:  858  Training Loss:  0.554579  Training Accuracy:  0.956732\n",
      "Epoch:  859  Training Loss:  0.554351  Training Accuracy:  0.956849\n",
      "Epoch:  860  Training Loss:  0.553087  Training Accuracy:  0.956967\n",
      "Epoch:  861  Training Loss:  0.552856  Training Accuracy:  0.957143\n",
      "Epoch:  862  Training Loss:  0.551904  Training Accuracy:  0.957143\n",
      "Epoch:  863  Training Loss:  0.551374  Training Accuracy:  0.957143\n",
      "Epoch:  864  Training Loss:  0.550968  Training Accuracy:  0.957261\n",
      "Epoch:  865  Training Loss:  0.549752  Training Accuracy:  0.95732\n",
      "Epoch:  866  Training Loss:  0.550049  Training Accuracy:  0.95732\n",
      "Epoch:  867  Training Loss:  0.548865  Training Accuracy:  0.957379\n",
      "Epoch:  868  Training Loss:  0.54832  Training Accuracy:  0.957379\n",
      "Epoch:  869  Training Loss:  0.547961  Training Accuracy:  0.95779\n",
      "Epoch:  870  Training Loss:  0.547196  Training Accuracy:  0.957849\n",
      "Epoch:  871  Training Loss:  0.546537  Training Accuracy:  0.957849\n",
      "Epoch:  872  Training Loss:  0.546132  Training Accuracy:  0.957849\n",
      "Epoch:  873  Training Loss:  0.545023  Training Accuracy:  0.958084\n",
      "Epoch:  874  Training Loss:  0.544797  Training Accuracy:  0.958378\n",
      "Epoch:  875  Training Loss:  0.543836  Training Accuracy:  0.958378\n",
      "Epoch:  876  Training Loss:  0.543215  Training Accuracy:  0.958496\n",
      "Epoch:  877  Training Loss:  0.543253  Training Accuracy:  0.958613\n",
      "Epoch:  878  Training Loss:  0.542275  Training Accuracy:  0.958907\n",
      "Epoch:  879  Training Loss:  0.541207  Training Accuracy:  0.958907\n",
      "Epoch:  880  Training Loss:  0.5409  Training Accuracy:  0.958966\n",
      "Epoch:  881  Training Loss:  0.540126  Training Accuracy:  0.959025\n",
      "Epoch:  882  Training Loss:  0.539655  Training Accuracy:  0.959083\n",
      "Epoch:  883  Training Loss:  0.538745  Training Accuracy:  0.959319\n",
      "Epoch:  884  Training Loss:  0.538635  Training Accuracy:  0.959436\n",
      "Epoch:  885  Training Loss:  0.537784  Training Accuracy:  0.959671\n",
      "Epoch:  886  Training Loss:  0.537029  Training Accuracy:  0.959789\n",
      "Epoch:  887  Training Loss:  0.536544  Training Accuracy:  0.959906\n",
      "Epoch:  888  Training Loss:  0.535848  Training Accuracy:  0.960024\n",
      "Epoch:  889  Training Loss:  0.535119  Training Accuracy:  0.960142\n",
      "Epoch:  890  Training Loss:  0.534543  Training Accuracy:  0.9602\n",
      "Epoch:  891  Training Loss:  0.533813  Training Accuracy:  0.960377\n",
      "Epoch:  892  Training Loss:  0.533077  Training Accuracy:  0.960377\n",
      "Epoch:  893  Training Loss:  0.533278  Training Accuracy:  0.960612\n",
      "Epoch:  894  Training Loss:  0.532062  Training Accuracy:  0.960788\n",
      "Epoch:  895  Training Loss:  0.531398  Training Accuracy:  0.960788\n",
      "Epoch:  896  Training Loss:  0.530796  Training Accuracy:  0.960847\n",
      "Epoch:  897  Training Loss:  0.530997  Training Accuracy:  0.960965\n",
      "Epoch:  898  Training Loss:  0.530091  Training Accuracy:  0.961023\n",
      "Epoch:  899  Training Loss:  0.529264  Training Accuracy:  0.961141\n",
      "Epoch:  900  Training Loss:  0.529107  Training Accuracy:  0.961259\n",
      "Epoch:  901  Training Loss:  0.528937  Training Accuracy:  0.961435\n",
      "Epoch:  902  Training Loss:  0.527748  Training Accuracy:  0.961553\n",
      "Epoch:  903  Training Loss:  0.527209  Training Accuracy:  0.961611\n",
      "Epoch:  904  Training Loss:  0.526373  Training Accuracy:  0.961729\n",
      "Epoch:  905  Training Loss:  0.526516  Training Accuracy:  0.961847\n",
      "Epoch:  906  Training Loss:  0.525329  Training Accuracy:  0.962023\n",
      "Epoch:  907  Training Loss:  0.525162  Training Accuracy:  0.962141\n",
      "Epoch:  908  Training Loss:  0.524029  Training Accuracy:  0.962258\n",
      "Epoch:  909  Training Loss:  0.52377  Training Accuracy:  0.962376\n",
      "Epoch:  910  Training Loss:  0.523001  Training Accuracy:  0.962493\n",
      "Epoch:  911  Training Loss:  0.522052  Training Accuracy:  0.96267\n",
      "Epoch:  912  Training Loss:  0.521739  Training Accuracy:  0.962728\n",
      "Epoch:  913  Training Loss:  0.521448  Training Accuracy:  0.962787\n",
      "Epoch:  914  Training Loss:  0.520313  Training Accuracy:  0.962964\n",
      "Epoch:  915  Training Loss:  0.51979  Training Accuracy:  0.962963\n",
      "Epoch:  916  Training Loss:  0.519075  Training Accuracy:  0.962964\n",
      "Epoch:  917  Training Loss:  0.518799  Training Accuracy:  0.96314\n",
      "Epoch:  918  Training Loss:  0.518001  Training Accuracy:  0.963257\n",
      "Epoch:  919  Training Loss:  0.517365  Training Accuracy:  0.963257\n",
      "Epoch:  920  Training Loss:  0.517184  Training Accuracy:  0.963434\n",
      "Epoch:  921  Training Loss:  0.516024  Training Accuracy:  0.963434\n",
      "Epoch:  922  Training Loss:  0.51549  Training Accuracy:  0.963493\n",
      "Epoch:  923  Training Loss:  0.515034  Training Accuracy:  0.96361\n",
      "Epoch:  924  Training Loss:  0.514951  Training Accuracy:  0.963728\n",
      "Epoch:  925  Training Loss:  0.514098  Training Accuracy:  0.963728\n",
      "Epoch:  926  Training Loss:  0.51296  Training Accuracy:  0.963669\n",
      "Epoch:  927  Training Loss:  0.512822  Training Accuracy:  0.963669\n",
      "Epoch:  928  Training Loss:  0.512032  Training Accuracy:  0.963728\n",
      "Epoch:  929  Training Loss:  0.511295  Training Accuracy:  0.963786\n",
      "Epoch:  930  Training Loss:  0.511108  Training Accuracy:  0.963845\n",
      "Epoch:  931  Training Loss:  0.510465  Training Accuracy:  0.963904\n",
      "Epoch:  932  Training Loss:  0.51016  Training Accuracy:  0.963963\n",
      "Epoch:  933  Training Loss:  0.509559  Training Accuracy:  0.96408\n",
      "Epoch:  934  Training Loss:  0.509077  Training Accuracy:  0.964139\n",
      "Epoch:  935  Training Loss:  0.508618  Training Accuracy:  0.964198\n",
      "Epoch:  936  Training Loss:  0.507937  Training Accuracy:  0.964433\n",
      "Epoch:  937  Training Loss:  0.507316  Training Accuracy:  0.964551\n",
      "Epoch:  938  Training Loss:  0.506237  Training Accuracy:  0.964668\n",
      "Epoch:  939  Training Loss:  0.506067  Training Accuracy:  0.964727\n",
      "Epoch:  940  Training Loss:  0.505471  Training Accuracy:  0.964845\n",
      "Epoch:  941  Training Loss:  0.504492  Training Accuracy:  0.964904\n",
      "Epoch:  942  Training Loss:  0.504187  Training Accuracy:  0.964904\n",
      "Epoch:  943  Training Loss:  0.50325  Training Accuracy:  0.965021\n",
      "Epoch:  944  Training Loss:  0.502945  Training Accuracy:  0.96508\n",
      "Epoch:  945  Training Loss:  0.502346  Training Accuracy:  0.965198\n",
      "Epoch:  946  Training Loss:  0.501617  Training Accuracy:  0.965197\n",
      "Epoch:  947  Training Loss:  0.501365  Training Accuracy:  0.965315\n",
      "Epoch:  948  Training Loss:  0.500708  Training Accuracy:  0.965433\n",
      "Epoch:  949  Training Loss:  0.499875  Training Accuracy:  0.96555\n",
      "Epoch:  950  Training Loss:  0.499495  Training Accuracy:  0.965668\n",
      "Epoch:  951  Training Loss:  0.498454  Training Accuracy:  0.965727\n",
      "Epoch:  952  Training Loss:  0.498208  Training Accuracy:  0.965903\n",
      "Epoch:  953  Training Loss:  0.497225  Training Accuracy:  0.965903\n",
      "Epoch:  954  Training Loss:  0.497392  Training Accuracy:  0.965903\n",
      "Epoch:  955  Training Loss:  0.496483  Training Accuracy:  0.965903\n",
      "Epoch:  956  Training Loss:  0.495718  Training Accuracy:  0.965962\n",
      "Epoch:  957  Training Loss:  0.495616  Training Accuracy:  0.965962\n",
      "Epoch:  958  Training Loss:  0.494564  Training Accuracy:  0.965962\n",
      "Epoch:  959  Training Loss:  0.494394  Training Accuracy:  0.966021\n",
      "Epoch:  960  Training Loss:  0.493857  Training Accuracy:  0.966021\n",
      "Epoch:  961  Training Loss:  0.49319  Training Accuracy:  0.966197\n",
      "Epoch:  962  Training Loss:  0.492698  Training Accuracy:  0.966197\n",
      "Epoch:  963  Training Loss:  0.491832  Training Accuracy:  0.966315\n",
      "Epoch:  964  Training Loss:  0.491411  Training Accuracy:  0.966373\n",
      "Epoch:  965  Training Loss:  0.49102  Training Accuracy:  0.966432\n",
      "Epoch:  966  Training Loss:  0.490367  Training Accuracy:  0.966608\n",
      "Epoch:  967  Training Loss:  0.490235  Training Accuracy:  0.966726\n",
      "Epoch:  968  Training Loss:  0.489625  Training Accuracy:  0.966785\n",
      "Epoch:  969  Training Loss:  0.488924  Training Accuracy:  0.966844\n",
      "Epoch:  970  Training Loss:  0.488499  Training Accuracy:  0.966902\n",
      "Epoch:  971  Training Loss:  0.488252  Training Accuracy:  0.96702\n",
      "Epoch:  972  Training Loss:  0.487195  Training Accuracy:  0.96702\n",
      "Epoch:  973  Training Loss:  0.486853  Training Accuracy:  0.96702\n",
      "Epoch:  974  Training Loss:  0.486493  Training Accuracy:  0.967138\n",
      "Epoch:  975  Training Loss:  0.486059  Training Accuracy:  0.967196\n",
      "Epoch:  976  Training Loss:  0.485514  Training Accuracy:  0.967255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  977  Training Loss:  0.485162  Training Accuracy:  0.967255\n",
      "Epoch:  978  Training Loss:  0.484499  Training Accuracy:  0.967373\n",
      "Epoch:  979  Training Loss:  0.483777  Training Accuracy:  0.967549\n",
      "Epoch:  980  Training Loss:  0.483747  Training Accuracy:  0.967549\n",
      "Epoch:  981  Training Loss:  0.482974  Training Accuracy:  0.967608\n",
      "Epoch:  982  Training Loss:  0.48276  Training Accuracy:  0.967667\n",
      "Epoch:  983  Training Loss:  0.482133  Training Accuracy:  0.967725\n",
      "Epoch:  984  Training Loss:  0.481772  Training Accuracy:  0.967784\n",
      "Epoch:  985  Training Loss:  0.481217  Training Accuracy:  0.967843\n",
      "Epoch:  986  Training Loss:  0.480777  Training Accuracy:  0.967843\n",
      "Epoch:  987  Training Loss:  0.480145  Training Accuracy:  0.967843\n",
      "Epoch:  988  Training Loss:  0.479538  Training Accuracy:  0.967961\n",
      "Epoch:  989  Training Loss:  0.479233  Training Accuracy:  0.967961\n",
      "Epoch:  990  Training Loss:  0.478678  Training Accuracy:  0.967961\n",
      "Epoch:  991  Training Loss:  0.478102  Training Accuracy:  0.968078\n",
      "Epoch:  992  Training Loss:  0.477986  Training Accuracy:  0.968137\n",
      "Epoch:  993  Training Loss:  0.47727  Training Accuracy:  0.968255\n",
      "Epoch:  994  Training Loss:  0.476656  Training Accuracy:  0.968313\n",
      "Epoch:  995  Training Loss:  0.476223  Training Accuracy:  0.968372\n",
      "Epoch:  996  Training Loss:  0.475759  Training Accuracy:  0.968431\n",
      "Epoch:  997  Training Loss:  0.475493  Training Accuracy:  0.96849\n",
      "Epoch:  998  Training Loss:  0.474815  Training Accuracy:  0.96849\n",
      "Epoch:  999  Training Loss:  0.474155  Training Accuracy:  0.968548\n",
      "Epoch:  1000  Training Loss:  0.473725  Training Accuracy:  0.968666\n",
      "Epoch:  1001  Training Loss:  0.473131  Training Accuracy:  0.968666\n",
      "Epoch:  1002  Training Loss:  0.472847  Training Accuracy:  0.968725\n",
      "Epoch:  1003  Training Loss:  0.472422  Training Accuracy:  0.968842\n",
      "Epoch:  1004  Training Loss:  0.471795  Training Accuracy:  0.968901\n",
      "Epoch:  1005  Training Loss:  0.471629  Training Accuracy:  0.969019\n",
      "Epoch:  1006  Training Loss:  0.470913  Training Accuracy:  0.969077\n",
      "Epoch:  1007  Training Loss:  0.470422  Training Accuracy:  0.969195\n",
      "Epoch:  1008  Training Loss:  0.470052  Training Accuracy:  0.969313\n",
      "Epoch:  1009  Training Loss:  0.46961  Training Accuracy:  0.96943\n",
      "Epoch:  1010  Training Loss:  0.469132  Training Accuracy:  0.969548\n",
      "Epoch:  1011  Training Loss:  0.468792  Training Accuracy:  0.969548\n",
      "Epoch:  1012  Training Loss:  0.468451  Training Accuracy:  0.969548\n",
      "Epoch:  1013  Training Loss:  0.467635  Training Accuracy:  0.969607\n",
      "Epoch:  1014  Training Loss:  0.467027  Training Accuracy:  0.969607\n",
      "Epoch:  1015  Training Loss:  0.46663  Training Accuracy:  0.969665\n",
      "Epoch:  1016  Training Loss:  0.466534  Training Accuracy:  0.969724\n",
      "Epoch:  1017  Training Loss:  0.466004  Training Accuracy:  0.969724\n",
      "Epoch:  1018  Training Loss:  0.466029  Training Accuracy:  0.969724\n",
      "Epoch:  1019  Training Loss:  0.464825  Training Accuracy:  0.969724\n",
      "Epoch:  1020  Training Loss:  0.464198  Training Accuracy:  0.969724\n",
      "Epoch:  1021  Training Loss:  0.464436  Training Accuracy:  0.969842\n",
      "Epoch:  1022  Training Loss:  0.463657  Training Accuracy:  0.969901\n",
      "Epoch:  1023  Training Loss:  0.463036  Training Accuracy:  0.969901\n",
      "Epoch:  1024  Training Loss:  0.462765  Training Accuracy:  0.969959\n",
      "Epoch:  1025  Training Loss:  0.462686  Training Accuracy:  0.970077\n",
      "Epoch:  1026  Training Loss:  0.461763  Training Accuracy:  0.970077\n",
      "Epoch:  1027  Training Loss:  0.461232  Training Accuracy:  0.970195\n",
      "Epoch:  1028  Training Loss:  0.460581  Training Accuracy:  0.970195\n",
      "Epoch:  1029  Training Loss:  0.460946  Training Accuracy:  0.970253\n",
      "Epoch:  1030  Training Loss:  0.460034  Training Accuracy:  0.970312\n",
      "Epoch:  1031  Training Loss:  0.459759  Training Accuracy:  0.97043\n",
      "Epoch:  1032  Training Loss:  0.459468  Training Accuracy:  0.97043\n",
      "Epoch:  1033  Training Loss:  0.458632  Training Accuracy:  0.97043\n",
      "Epoch:  1034  Training Loss:  0.45848  Training Accuracy:  0.97043\n",
      "Epoch:  1035  Training Loss:  0.457797  Training Accuracy:  0.97043\n",
      "Epoch:  1036  Training Loss:  0.457328  Training Accuracy:  0.97043\n",
      "Epoch:  1037  Training Loss:  0.45693  Training Accuracy:  0.97043\n",
      "Epoch:  1038  Training Loss:  0.456237  Training Accuracy:  0.970488\n",
      "Epoch:  1039  Training Loss:  0.455738  Training Accuracy:  0.970606\n",
      "Epoch:  1040  Training Loss:  0.455625  Training Accuracy:  0.970782\n",
      "Epoch:  1041  Training Loss:  0.45539  Training Accuracy:  0.970782\n",
      "Epoch:  1042  Training Loss:  0.454783  Training Accuracy:  0.970841\n",
      "Epoch:  1043  Training Loss:  0.454484  Training Accuracy:  0.970959\n",
      "Epoch:  1044  Training Loss:  0.45375  Training Accuracy:  0.971076\n",
      "Epoch:  1045  Training Loss:  0.453542  Training Accuracy:  0.971135\n",
      "Epoch:  1046  Training Loss:  0.4528  Training Accuracy:  0.971194\n",
      "Epoch:  1047  Training Loss:  0.4526  Training Accuracy:  0.971194\n",
      "Epoch:  1048  Training Loss:  0.451886  Training Accuracy:  0.971312\n",
      "Epoch:  1049  Training Loss:  0.451469  Training Accuracy:  0.971312\n",
      "Epoch:  1050  Training Loss:  0.450792  Training Accuracy:  0.971312\n",
      "Epoch:  1051  Training Loss:  0.450513  Training Accuracy:  0.971488\n",
      "Epoch:  1052  Training Loss:  0.449957  Training Accuracy:  0.971547\n",
      "Epoch:  1053  Training Loss:  0.449224  Training Accuracy:  0.971605\n",
      "Epoch:  1054  Training Loss:  0.449193  Training Accuracy:  0.971723\n",
      "Epoch:  1055  Training Loss:  0.448513  Training Accuracy:  0.971723\n",
      "Epoch:  1056  Training Loss:  0.447936  Training Accuracy:  0.971782\n",
      "Epoch:  1057  Training Loss:  0.447837  Training Accuracy:  0.971782\n",
      "Epoch:  1058  Training Loss:  0.447286  Training Accuracy:  0.971782\n",
      "Epoch:  1059  Training Loss:  0.446897  Training Accuracy:  0.971841\n",
      "Epoch:  1060  Training Loss:  0.446629  Training Accuracy:  0.971958\n",
      "Epoch:  1061  Training Loss:  0.446089  Training Accuracy:  0.972017\n",
      "Epoch:  1062  Training Loss:  0.44546  Training Accuracy:  0.972135\n",
      "Epoch:  1063  Training Loss:  0.445018  Training Accuracy:  0.972311\n",
      "Epoch:  1064  Training Loss:  0.444358  Training Accuracy:  0.972311\n",
      "Epoch:  1065  Training Loss:  0.443889  Training Accuracy:  0.972311\n",
      "Epoch:  1066  Training Loss:  0.443875  Training Accuracy:  0.972311\n",
      "Epoch:  1067  Training Loss:  0.442848  Training Accuracy:  0.972311\n",
      "Epoch:  1068  Training Loss:  0.442479  Training Accuracy:  0.97237\n",
      "Epoch:  1069  Training Loss:  0.442208  Training Accuracy:  0.97237\n",
      "Epoch:  1070  Training Loss:  0.441687  Training Accuracy:  0.972487\n",
      "Epoch:  1071  Training Loss:  0.440966  Training Accuracy:  0.972605\n",
      "Epoch:  1072  Training Loss:  0.441125  Training Accuracy:  0.972664\n",
      "Epoch:  1073  Training Loss:  0.440197  Training Accuracy:  0.972722\n",
      "Epoch:  1074  Training Loss:  0.439504  Training Accuracy:  0.972722\n",
      "Epoch:  1075  Training Loss:  0.43957  Training Accuracy:  0.972722\n",
      "Epoch:  1076  Training Loss:  0.438782  Training Accuracy:  0.972722\n",
      "Epoch:  1077  Training Loss:  0.438323  Training Accuracy:  0.972722\n",
      "Epoch:  1078  Training Loss:  0.438359  Training Accuracy:  0.972722\n",
      "Epoch:  1079  Training Loss:  0.43734  Training Accuracy:  0.972781\n",
      "Epoch:  1080  Training Loss:  0.437042  Training Accuracy:  0.97284\n",
      "Epoch:  1081  Training Loss:  0.437115  Training Accuracy:  0.97284\n",
      "Epoch:  1082  Training Loss:  0.435966  Training Accuracy:  0.97284\n",
      "Epoch:  1083  Training Loss:  0.435709  Training Accuracy:  0.97284\n",
      "Epoch:  1084  Training Loss:  0.435466  Training Accuracy:  0.972899\n",
      "Epoch:  1085  Training Loss:  0.43475  Training Accuracy:  0.972899\n",
      "Epoch:  1086  Training Loss:  0.434272  Training Accuracy:  0.972958\n",
      "Epoch:  1087  Training Loss:  0.434311  Training Accuracy:  0.972958\n",
      "Epoch:  1088  Training Loss:  0.433313  Training Accuracy:  0.973075\n",
      "Epoch:  1089  Training Loss:  0.432947  Training Accuracy:  0.973134\n",
      "Epoch:  1090  Training Loss:  0.432654  Training Accuracy:  0.973193\n",
      "Epoch:  1091  Training Loss:  0.432188  Training Accuracy:  0.97331\n",
      "Epoch:  1092  Training Loss:  0.43174  Training Accuracy:  0.973369\n",
      "Epoch:  1093  Training Loss:  0.431411  Training Accuracy:  0.973369\n",
      "Epoch:  1094  Training Loss:  0.431123  Training Accuracy:  0.973428\n",
      "Epoch:  1095  Training Loss:  0.430479  Training Accuracy:  0.973428\n",
      "Epoch:  1096  Training Loss:  0.430408  Training Accuracy:  0.973487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1097  Training Loss:  0.430071  Training Accuracy:  0.973545\n",
      "Epoch:  1098  Training Loss:  0.429185  Training Accuracy:  0.973545\n",
      "Epoch:  1099  Training Loss:  0.429259  Training Accuracy:  0.973604\n",
      "Epoch:  1100  Training Loss:  0.428442  Training Accuracy:  0.973604\n",
      "Epoch:  1101  Training Loss:  0.428147  Training Accuracy:  0.973604\n",
      "Epoch:  1102  Training Loss:  0.4281  Training Accuracy:  0.973604\n",
      "Epoch:  1103  Training Loss:  0.427368  Training Accuracy:  0.973722\n",
      "Epoch:  1104  Training Loss:  0.427003  Training Accuracy:  0.973781\n",
      "Epoch:  1105  Training Loss:  0.426513  Training Accuracy:  0.97384\n",
      "Epoch:  1106  Training Loss:  0.426221  Training Accuracy:  0.97384\n",
      "Epoch:  1107  Training Loss:  0.425676  Training Accuracy:  0.973898\n",
      "Epoch:  1108  Training Loss:  0.425534  Training Accuracy:  0.973898\n",
      "Epoch:  1109  Training Loss:  0.425112  Training Accuracy:  0.974016\n",
      "Epoch:  1110  Training Loss:  0.424588  Training Accuracy:  0.974133\n",
      "Epoch:  1111  Training Loss:  0.424675  Training Accuracy:  0.974133\n",
      "Epoch:  1112  Training Loss:  0.424114  Training Accuracy:  0.974251\n",
      "Epoch:  1113  Training Loss:  0.423565  Training Accuracy:  0.974251\n",
      "Epoch:  1114  Training Loss:  0.423642  Training Accuracy:  0.974251\n",
      "Epoch:  1115  Training Loss:  0.422893  Training Accuracy:  0.974251\n",
      "Epoch:  1116  Training Loss:  0.422873  Training Accuracy:  0.974251\n",
      "Epoch:  1117  Training Loss:  0.422602  Training Accuracy:  0.974251\n",
      "Epoch:  1118  Training Loss:  0.422132  Training Accuracy:  0.974251\n",
      "Epoch:  1119  Training Loss:  0.421569  Training Accuracy:  0.974251\n",
      "Epoch:  1120  Training Loss:  0.42145  Training Accuracy:  0.974251\n",
      "Epoch:  1121  Training Loss:  0.421018  Training Accuracy:  0.974251\n",
      "Epoch:  1122  Training Loss:  0.420343  Training Accuracy:  0.97431\n",
      "Epoch:  1123  Training Loss:  0.420591  Training Accuracy:  0.97431\n",
      "Epoch:  1124  Training Loss:  0.419758  Training Accuracy:  0.974369\n",
      "Epoch:  1125  Training Loss:  0.419528  Training Accuracy:  0.974486\n",
      "Epoch:  1126  Training Loss:  0.419441  Training Accuracy:  0.974545\n",
      "Epoch:  1127  Training Loss:  0.41878  Training Accuracy:  0.974545\n",
      "Epoch:  1128  Training Loss:  0.41835  Training Accuracy:  0.974545\n",
      "Epoch:  1129  Training Loss:  0.418305  Training Accuracy:  0.974604\n",
      "Epoch:  1130  Training Loss:  0.417757  Training Accuracy:  0.974604\n",
      "Epoch:  1131  Training Loss:  0.417237  Training Accuracy:  0.974604\n",
      "Epoch:  1132  Training Loss:  0.417231  Training Accuracy:  0.974604\n",
      "Epoch:  1133  Training Loss:  0.416641  Training Accuracy:  0.974604\n",
      "Epoch:  1134  Training Loss:  0.416258  Training Accuracy:  0.974604\n",
      "Epoch:  1135  Training Loss:  0.416089  Training Accuracy:  0.974604\n",
      "Epoch:  1136  Training Loss:  0.415616  Training Accuracy:  0.974604\n",
      "Epoch:  1137  Training Loss:  0.415124  Training Accuracy:  0.974604\n",
      "Epoch:  1138  Training Loss:  0.41492  Training Accuracy:  0.974604\n",
      "Epoch:  1139  Training Loss:  0.414706  Training Accuracy:  0.974604\n",
      "Epoch:  1140  Training Loss:  0.414002  Training Accuracy:  0.974604\n",
      "Epoch:  1141  Training Loss:  0.413987  Training Accuracy:  0.974662\n",
      "Epoch:  1142  Training Loss:  0.413335  Training Accuracy:  0.974663\n",
      "Epoch:  1143  Training Loss:  0.413081  Training Accuracy:  0.974662\n",
      "Epoch:  1144  Training Loss:  0.41285  Training Accuracy:  0.974721\n",
      "Epoch:  1145  Training Loss:  0.412378  Training Accuracy:  0.974721\n",
      "Epoch:  1146  Training Loss:  0.411648  Training Accuracy:  0.97478\n",
      "Epoch:  1147  Training Loss:  0.411732  Training Accuracy:  0.974898\n",
      "Epoch:  1148  Training Loss:  0.411214  Training Accuracy:  0.974898\n",
      "Epoch:  1149  Training Loss:  0.410676  Training Accuracy:  0.974956\n",
      "Epoch:  1150  Training Loss:  0.410392  Training Accuracy:  0.975015\n",
      "Epoch:  1151  Training Loss:  0.410165  Training Accuracy:  0.975074\n",
      "Epoch:  1152  Training Loss:  0.409563  Training Accuracy:  0.975074\n",
      "Epoch:  1153  Training Loss:  0.409369  Training Accuracy:  0.975074\n",
      "Epoch:  1154  Training Loss:  0.409032  Training Accuracy:  0.975074\n",
      "Epoch:  1155  Training Loss:  0.408422  Training Accuracy:  0.975074\n",
      "Epoch:  1156  Training Loss:  0.408264  Training Accuracy:  0.975133\n",
      "Epoch:  1157  Training Loss:  0.407842  Training Accuracy:  0.975133\n",
      "Epoch:  1158  Training Loss:  0.407349  Training Accuracy:  0.975133\n",
      "Epoch:  1159  Training Loss:  0.407145  Training Accuracy:  0.975133\n",
      "Epoch:  1160  Training Loss:  0.406656  Training Accuracy:  0.975192\n",
      "Epoch:  1161  Training Loss:  0.405997  Training Accuracy:  0.97525\n",
      "Epoch:  1162  Training Loss:  0.406003  Training Accuracy:  0.97525\n",
      "Epoch:  1163  Training Loss:  0.405406  Training Accuracy:  0.97525\n",
      "Epoch:  1164  Training Loss:  0.404933  Training Accuracy:  0.97525\n",
      "Epoch:  1165  Training Loss:  0.404768  Training Accuracy:  0.97525\n",
      "Epoch:  1166  Training Loss:  0.404298  Training Accuracy:  0.975309\n",
      "Epoch:  1167  Training Loss:  0.40361  Training Accuracy:  0.975309\n",
      "Epoch:  1168  Training Loss:  0.403712  Training Accuracy:  0.975368\n",
      "Epoch:  1169  Training Loss:  0.402781  Training Accuracy:  0.975486\n",
      "Epoch:  1170  Training Loss:  0.402808  Training Accuracy:  0.975486\n",
      "Epoch:  1171  Training Loss:  0.402263  Training Accuracy:  0.975486\n",
      "Epoch:  1172  Training Loss:  0.40167  Training Accuracy:  0.975486\n",
      "Epoch:  1173  Training Loss:  0.401519  Training Accuracy:  0.975486\n",
      "Epoch:  1174  Training Loss:  0.401218  Training Accuracy:  0.975486\n",
      "Epoch:  1175  Training Loss:  0.400723  Training Accuracy:  0.975544\n",
      "Epoch:  1176  Training Loss:  0.40036  Training Accuracy:  0.975544\n",
      "Epoch:  1177  Training Loss:  0.400189  Training Accuracy:  0.975544\n",
      "Epoch:  1178  Training Loss:  0.399831  Training Accuracy:  0.975603\n",
      "Epoch:  1179  Training Loss:  0.399356  Training Accuracy:  0.975603\n",
      "Epoch:  1180  Training Loss:  0.399047  Training Accuracy:  0.975603\n",
      "Epoch:  1181  Training Loss:  0.398741  Training Accuracy:  0.975603\n",
      "Epoch:  1182  Training Loss:  0.398102  Training Accuracy:  0.975603\n",
      "Epoch:  1183  Training Loss:  0.397996  Training Accuracy:  0.975721\n",
      "Epoch:  1184  Training Loss:  0.397583  Training Accuracy:  0.975721\n",
      "Epoch:  1185  Training Loss:  0.396868  Training Accuracy:  0.975721\n",
      "Epoch:  1186  Training Loss:  0.396948  Training Accuracy:  0.975721\n",
      "Epoch:  1187  Training Loss:  0.396244  Training Accuracy:  0.975721\n",
      "Epoch:  1188  Training Loss:  0.395871  Training Accuracy:  0.975721\n",
      "Epoch:  1189  Training Loss:  0.395915  Training Accuracy:  0.975721\n",
      "Epoch:  1190  Training Loss:  0.395303  Training Accuracy:  0.975838\n",
      "Epoch:  1191  Training Loss:  0.395073  Training Accuracy:  0.975897\n",
      "Epoch:  1192  Training Loss:  0.394529  Training Accuracy:  0.975897\n",
      "Epoch:  1193  Training Loss:  0.394164  Training Accuracy:  0.975956\n",
      "Epoch:  1194  Training Loss:  0.394126  Training Accuracy:  0.975956\n",
      "Epoch:  1195  Training Loss:  0.393856  Training Accuracy:  0.976015\n",
      "Epoch:  1196  Training Loss:  0.393096  Training Accuracy:  0.976015\n",
      "Epoch:  1197  Training Loss:  0.393146  Training Accuracy:  0.976015\n",
      "Epoch:  1198  Training Loss:  0.392715  Training Accuracy:  0.976015\n",
      "Epoch:  1199  Training Loss:  0.392202  Training Accuracy:  0.976015\n",
      "Epoch:  1200  Training Loss:  0.392001  Training Accuracy:  0.976015\n",
      "Epoch:  1201  Training Loss:  0.39166  Training Accuracy:  0.976015\n",
      "Epoch:  1202  Training Loss:  0.391332  Training Accuracy:  0.976015\n",
      "Epoch:  1203  Training Loss:  0.390935  Training Accuracy:  0.976073\n",
      "Epoch:  1204  Training Loss:  0.390886  Training Accuracy:  0.976073\n",
      "Epoch:  1205  Training Loss:  0.39054  Training Accuracy:  0.976132\n",
      "Epoch:  1206  Training Loss:  0.390154  Training Accuracy:  0.976132\n",
      "Epoch:  1207  Training Loss:  0.389539  Training Accuracy:  0.976132\n",
      "Epoch:  1208  Training Loss:  0.389624  Training Accuracy:  0.976191\n",
      "Epoch:  1209  Training Loss:  0.389167  Training Accuracy:  0.976191\n",
      "Epoch:  1210  Training Loss:  0.38851  Training Accuracy:  0.976191\n",
      "Epoch:  1211  Training Loss:  0.388188  Training Accuracy:  0.976191\n",
      "Epoch:  1212  Training Loss:  0.387988  Training Accuracy:  0.976191\n",
      "Epoch:  1213  Training Loss:  0.387573  Training Accuracy:  0.976191\n",
      "Epoch:  1214  Training Loss:  0.387485  Training Accuracy:  0.976191\n",
      "Epoch:  1215  Training Loss:  0.386917  Training Accuracy:  0.976191\n",
      "Epoch:  1216  Training Loss:  0.386515  Training Accuracy:  0.976191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1217  Training Loss:  0.38625  Training Accuracy:  0.97625\n",
      "Epoch:  1218  Training Loss:  0.386001  Training Accuracy:  0.97625\n",
      "Epoch:  1219  Training Loss:  0.385595  Training Accuracy:  0.97625\n",
      "Epoch:  1220  Training Loss:  0.385268  Training Accuracy:  0.97625\n",
      "Epoch:  1221  Training Loss:  0.384964  Training Accuracy:  0.976309\n",
      "Epoch:  1222  Training Loss:  0.384592  Training Accuracy:  0.976367\n",
      "Epoch:  1223  Training Loss:  0.384069  Training Accuracy:  0.976367\n",
      "Epoch:  1224  Training Loss:  0.384141  Training Accuracy:  0.976367\n",
      "Epoch:  1225  Training Loss:  0.383264  Training Accuracy:  0.976367\n",
      "Epoch:  1226  Training Loss:  0.383352  Training Accuracy:  0.976367\n",
      "Epoch:  1227  Training Loss:  0.38291  Training Accuracy:  0.976367\n",
      "Epoch:  1228  Training Loss:  0.382491  Training Accuracy:  0.976485\n",
      "Epoch:  1229  Training Loss:  0.382064  Training Accuracy:  0.976485\n",
      "Epoch:  1230  Training Loss:  0.381859  Training Accuracy:  0.976485\n",
      "Epoch:  1231  Training Loss:  0.381416  Training Accuracy:  0.976544\n",
      "Epoch:  1232  Training Loss:  0.38125  Training Accuracy:  0.976603\n",
      "Epoch:  1233  Training Loss:  0.380749  Training Accuracy:  0.976544\n",
      "Epoch:  1234  Training Loss:  0.380566  Training Accuracy:  0.976603\n",
      "Epoch:  1235  Training Loss:  0.380229  Training Accuracy:  0.976603\n",
      "Epoch:  1236  Training Loss:  0.380127  Training Accuracy:  0.976602\n",
      "Epoch:  1237  Training Loss:  0.379554  Training Accuracy:  0.976661\n",
      "Epoch:  1238  Training Loss:  0.379251  Training Accuracy:  0.976661\n",
      "Epoch:  1239  Training Loss:  0.378932  Training Accuracy:  0.976661\n",
      "Epoch:  1240  Training Loss:  0.378555  Training Accuracy:  0.976661\n",
      "Epoch:  1241  Training Loss:  0.378174  Training Accuracy:  0.976661\n",
      "Epoch:  1242  Training Loss:  0.377973  Training Accuracy:  0.976661\n",
      "Epoch:  1243  Training Loss:  0.377569  Training Accuracy:  0.97672\n",
      "Epoch:  1244  Training Loss:  0.377252  Training Accuracy:  0.97672\n",
      "Epoch:  1245  Training Loss:  0.376851  Training Accuracy:  0.97672\n",
      "Epoch:  1246  Training Loss:  0.376761  Training Accuracy:  0.976897\n",
      "Epoch:  1247  Training Loss:  0.376285  Training Accuracy:  0.976896\n",
      "Epoch:  1248  Training Loss:  0.376156  Training Accuracy:  0.976896\n",
      "Epoch:  1249  Training Loss:  0.375667  Training Accuracy:  0.976897\n",
      "Epoch:  1250  Training Loss:  0.375287  Training Accuracy:  0.976897\n",
      "Epoch:  1251  Training Loss:  0.37485  Training Accuracy:  0.976896\n",
      "Epoch:  1252  Training Loss:  0.374753  Training Accuracy:  0.977014\n",
      "Epoch:  1253  Training Loss:  0.374299  Training Accuracy:  0.977014\n",
      "Epoch:  1254  Training Loss:  0.37416  Training Accuracy:  0.977014\n",
      "Epoch:  1255  Training Loss:  0.373656  Training Accuracy:  0.977014\n",
      "Epoch:  1256  Training Loss:  0.373209  Training Accuracy:  0.977014\n",
      "Epoch:  1257  Training Loss:  0.373066  Training Accuracy:  0.977014\n",
      "Epoch:  1258  Training Loss:  0.372723  Training Accuracy:  0.977014\n",
      "Epoch:  1259  Training Loss:  0.372466  Training Accuracy:  0.977073\n",
      "Epoch:  1260  Training Loss:  0.371955  Training Accuracy:  0.977073\n",
      "Epoch:  1261  Training Loss:  0.371648  Training Accuracy:  0.977073\n",
      "Epoch:  1262  Training Loss:  0.371374  Training Accuracy:  0.977132\n",
      "Epoch:  1263  Training Loss:  0.370969  Training Accuracy:  0.977132\n",
      "Epoch:  1264  Training Loss:  0.370773  Training Accuracy:  0.977132\n",
      "Epoch:  1265  Training Loss:  0.370269  Training Accuracy:  0.977132\n",
      "Epoch:  1266  Training Loss:  0.369943  Training Accuracy:  0.977132\n",
      "Epoch:  1267  Training Loss:  0.369738  Training Accuracy:  0.977132\n",
      "Epoch:  1268  Training Loss:  0.369195  Training Accuracy:  0.977132\n",
      "Epoch:  1269  Training Loss:  0.369038  Training Accuracy:  0.977132\n",
      "Epoch:  1270  Training Loss:  0.368742  Training Accuracy:  0.977132\n",
      "Epoch:  1271  Training Loss:  0.368729  Training Accuracy:  0.977132\n",
      "Epoch:  1272  Training Loss:  0.36817  Training Accuracy:  0.97719\n",
      "Epoch:  1273  Training Loss:  0.367926  Training Accuracy:  0.97719\n",
      "Epoch:  1274  Training Loss:  0.367588  Training Accuracy:  0.97719\n",
      "Epoch:  1275  Training Loss:  0.367159  Training Accuracy:  0.97719\n",
      "Epoch:  1276  Training Loss:  0.366821  Training Accuracy:  0.977308\n",
      "Epoch:  1277  Training Loss:  0.366556  Training Accuracy:  0.977308\n",
      "Epoch:  1278  Training Loss:  0.366259  Training Accuracy:  0.977308\n",
      "Epoch:  1279  Training Loss:  0.365604  Training Accuracy:  0.977308\n",
      "Epoch:  1280  Training Loss:  0.365544  Training Accuracy:  0.977308\n",
      "Epoch:  1281  Training Loss:  0.365201  Training Accuracy:  0.977367\n",
      "Epoch:  1282  Training Loss:  0.364736  Training Accuracy:  0.977367\n",
      "Epoch:  1283  Training Loss:  0.36432  Training Accuracy:  0.977367\n",
      "Epoch:  1284  Training Loss:  0.364037  Training Accuracy:  0.977367\n",
      "Epoch:  1285  Training Loss:  0.36376  Training Accuracy:  0.977367\n",
      "Epoch:  1286  Training Loss:  0.36346  Training Accuracy:  0.977367\n",
      "Epoch:  1287  Training Loss:  0.363309  Training Accuracy:  0.977367\n",
      "Epoch:  1288  Training Loss:  0.362612  Training Accuracy:  0.977367\n",
      "Epoch:  1289  Training Loss:  0.362552  Training Accuracy:  0.977367\n",
      "Epoch:  1290  Training Loss:  0.361946  Training Accuracy:  0.977367\n",
      "Epoch:  1291  Training Loss:  0.361702  Training Accuracy:  0.977367\n",
      "Epoch:  1292  Training Loss:  0.36163  Training Accuracy:  0.977426\n",
      "Epoch:  1293  Training Loss:  0.361068  Training Accuracy:  0.977426\n",
      "Epoch:  1294  Training Loss:  0.360827  Training Accuracy:  0.977426\n",
      "Epoch:  1295  Training Loss:  0.360489  Training Accuracy:  0.977484\n",
      "Epoch:  1296  Training Loss:  0.36027  Training Accuracy:  0.977484\n",
      "Epoch:  1297  Training Loss:  0.35986  Training Accuracy:  0.977484\n",
      "Epoch:  1298  Training Loss:  0.359809  Training Accuracy:  0.977484\n",
      "Epoch:  1299  Training Loss:  0.359194  Training Accuracy:  0.977484\n",
      "Epoch:  1300  Training Loss:  0.358933  Training Accuracy:  0.977484\n",
      "Epoch:  1301  Training Loss:  0.358694  Training Accuracy:  0.977484\n",
      "Epoch:  1302  Training Loss:  0.35822  Training Accuracy:  0.977543\n",
      "Epoch:  1303  Training Loss:  0.357867  Training Accuracy:  0.977543\n",
      "Epoch:  1304  Training Loss:  0.357867  Training Accuracy:  0.977543\n",
      "Epoch:  1305  Training Loss:  0.357497  Training Accuracy:  0.977543\n",
      "Epoch:  1306  Training Loss:  0.357065  Training Accuracy:  0.977543\n",
      "Epoch:  1307  Training Loss:  0.356835  Training Accuracy:  0.977543\n",
      "Epoch:  1308  Training Loss:  0.356664  Training Accuracy:  0.977543\n",
      "Epoch:  1309  Training Loss:  0.356111  Training Accuracy:  0.977543\n",
      "Epoch:  1310  Training Loss:  0.355913  Training Accuracy:  0.977543\n",
      "Epoch:  1311  Training Loss:  0.35562  Training Accuracy:  0.977602\n",
      "Epoch:  1312  Training Loss:  0.35526  Training Accuracy:  0.977602\n",
      "Epoch:  1313  Training Loss:  0.354889  Training Accuracy:  0.977602\n",
      "Epoch:  1314  Training Loss:  0.354816  Training Accuracy:  0.977602\n",
      "Epoch:  1315  Training Loss:  0.354372  Training Accuracy:  0.977778\n",
      "Epoch:  1316  Training Loss:  0.35422  Training Accuracy:  0.977778\n",
      "Epoch:  1317  Training Loss:  0.353844  Training Accuracy:  0.977778\n",
      "Epoch:  1318  Training Loss:  0.353559  Training Accuracy:  0.977837\n",
      "Epoch:  1319  Training Loss:  0.353295  Training Accuracy:  0.977837\n",
      "Epoch:  1320  Training Loss:  0.353065  Training Accuracy:  0.977837\n",
      "Epoch:  1321  Training Loss:  0.352846  Training Accuracy:  0.977837\n",
      "Epoch:  1322  Training Loss:  0.352373  Training Accuracy:  0.977837\n",
      "Epoch:  1323  Training Loss:  0.35252  Training Accuracy:  0.977837\n",
      "Epoch:  1324  Training Loss:  0.351922  Training Accuracy:  0.977837\n",
      "Epoch:  1325  Training Loss:  0.351628  Training Accuracy:  0.977896\n",
      "Epoch:  1326  Training Loss:  0.351528  Training Accuracy:  0.977896\n",
      "Epoch:  1327  Training Loss:  0.351252  Training Accuracy:  0.977896\n",
      "Epoch:  1328  Training Loss:  0.350924  Training Accuracy:  0.977896\n",
      "Epoch:  1329  Training Loss:  0.350716  Training Accuracy:  0.977896\n",
      "Epoch:  1330  Training Loss:  0.350348  Training Accuracy:  0.977896\n",
      "Epoch:  1331  Training Loss:  0.350149  Training Accuracy:  0.977955\n",
      "Epoch:  1332  Training Loss:  0.349799  Training Accuracy:  0.978014\n",
      "Epoch:  1333  Training Loss:  0.349666  Training Accuracy:  0.978014\n",
      "Epoch:  1334  Training Loss:  0.34922  Training Accuracy:  0.978014\n",
      "Epoch:  1335  Training Loss:  0.349339  Training Accuracy:  0.978014\n",
      "Epoch:  1336  Training Loss:  0.348755  Training Accuracy:  0.978014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1337  Training Loss:  0.348429  Training Accuracy:  0.978014\n",
      "Epoch:  1338  Training Loss:  0.348271  Training Accuracy:  0.978014\n",
      "Epoch:  1339  Training Loss:  0.348092  Training Accuracy:  0.978014\n",
      "Epoch:  1340  Training Loss:  0.347833  Training Accuracy:  0.978014\n",
      "Epoch:  1341  Training Loss:  0.347435  Training Accuracy:  0.978014\n",
      "Epoch:  1342  Training Loss:  0.347341  Training Accuracy:  0.978014\n",
      "Epoch:  1343  Training Loss:  0.347108  Training Accuracy:  0.978072\n",
      "Epoch:  1344  Training Loss:  0.34678  Training Accuracy:  0.978072\n",
      "Epoch:  1345  Training Loss:  0.34642  Training Accuracy:  0.978072\n",
      "Epoch:  1346  Training Loss:  0.346173  Training Accuracy:  0.978131\n",
      "Epoch:  1347  Training Loss:  0.345804  Training Accuracy:  0.978131\n",
      "Epoch:  1348  Training Loss:  0.345619  Training Accuracy:  0.978131\n",
      "Epoch:  1349  Training Loss:  0.345436  Training Accuracy:  0.978131\n",
      "Epoch:  1350  Training Loss:  0.345148  Training Accuracy:  0.978131\n",
      "Epoch:  1351  Training Loss:  0.34491  Training Accuracy:  0.978131\n",
      "Epoch:  1352  Training Loss:  0.344601  Training Accuracy:  0.978131\n",
      "Epoch:  1353  Training Loss:  0.344199  Training Accuracy:  0.978131\n",
      "Epoch:  1354  Training Loss:  0.34417  Training Accuracy:  0.97819\n",
      "Epoch:  1355  Training Loss:  0.343704  Training Accuracy:  0.97819\n",
      "Epoch:  1356  Training Loss:  0.343509  Training Accuracy:  0.978249\n",
      "Epoch:  1357  Training Loss:  0.343208  Training Accuracy:  0.978307\n",
      "Epoch:  1358  Training Loss:  0.343059  Training Accuracy:  0.978307\n",
      "Epoch:  1359  Training Loss:  0.342676  Training Accuracy:  0.978307\n",
      "Epoch:  1360  Training Loss:  0.342417  Training Accuracy:  0.978307\n",
      "Epoch:  1361  Training Loss:  0.342283  Training Accuracy:  0.978366\n",
      "Epoch:  1362  Training Loss:  0.341819  Training Accuracy:  0.978366\n",
      "Epoch:  1363  Training Loss:  0.341593  Training Accuracy:  0.978366\n",
      "Epoch:  1364  Training Loss:  0.341479  Training Accuracy:  0.978366\n",
      "Epoch:  1365  Training Loss:  0.340987  Training Accuracy:  0.978366\n",
      "Epoch:  1366  Training Loss:  0.340683  Training Accuracy:  0.978425\n",
      "Epoch:  1367  Training Loss:  0.340418  Training Accuracy:  0.978425\n",
      "Epoch:  1368  Training Loss:  0.340151  Training Accuracy:  0.978425\n",
      "Epoch:  1369  Training Loss:  0.339883  Training Accuracy:  0.978425\n",
      "Epoch:  1370  Training Loss:  0.339621  Training Accuracy:  0.978425\n",
      "Epoch:  1371  Training Loss:  0.339463  Training Accuracy:  0.978484\n",
      "Epoch:  1372  Training Loss:  0.339157  Training Accuracy:  0.978484\n",
      "Epoch:  1373  Training Loss:  0.338759  Training Accuracy:  0.978484\n",
      "Epoch:  1374  Training Loss:  0.338714  Training Accuracy:  0.978543\n",
      "Epoch:  1375  Training Loss:  0.338225  Training Accuracy:  0.978543\n",
      "Epoch:  1376  Training Loss:  0.338088  Training Accuracy:  0.978543\n",
      "Epoch:  1377  Training Loss:  0.337825  Training Accuracy:  0.978543\n",
      "Epoch:  1378  Training Loss:  0.337538  Training Accuracy:  0.978543\n",
      "Epoch:  1379  Training Loss:  0.33735  Training Accuracy:  0.978543\n",
      "Epoch:  1380  Training Loss:  0.337108  Training Accuracy:  0.978601\n",
      "Epoch:  1381  Training Loss:  0.336885  Training Accuracy:  0.978601\n",
      "Epoch:  1382  Training Loss:  0.336548  Training Accuracy:  0.978601\n",
      "Epoch:  1383  Training Loss:  0.336397  Training Accuracy:  0.978601\n",
      "Epoch:  1384  Training Loss:  0.336082  Training Accuracy:  0.978601\n",
      "Epoch:  1385  Training Loss:  0.335788  Training Accuracy:  0.978601\n",
      "Epoch:  1386  Training Loss:  0.335522  Training Accuracy:  0.978601\n",
      "Epoch:  1387  Training Loss:  0.335212  Training Accuracy:  0.978601\n",
      "Epoch:  1388  Training Loss:  0.3351  Training Accuracy:  0.978601\n",
      "Epoch:  1389  Training Loss:  0.334804  Training Accuracy:  0.97866\n",
      "Epoch:  1390  Training Loss:  0.334595  Training Accuracy:  0.97866\n",
      "Epoch:  1391  Training Loss:  0.334088  Training Accuracy:  0.97866\n",
      "Epoch:  1392  Training Loss:  0.334157  Training Accuracy:  0.97866\n",
      "Epoch:  1393  Training Loss:  0.333685  Training Accuracy:  0.97866\n",
      "Epoch:  1394  Training Loss:  0.333656  Training Accuracy:  0.97866\n",
      "Epoch:  1395  Training Loss:  0.33341  Training Accuracy:  0.97866\n",
      "Epoch:  1396  Training Loss:  0.333156  Training Accuracy:  0.97866\n",
      "Epoch:  1397  Training Loss:  0.332772  Training Accuracy:  0.97866\n",
      "Epoch:  1398  Training Loss:  0.332684  Training Accuracy:  0.978719\n",
      "Epoch:  1399  Training Loss:  0.332252  Training Accuracy:  0.978778\n",
      "Epoch:  1400  Training Loss:  0.332158  Training Accuracy:  0.978837\n",
      "Epoch:  1401  Training Loss:  0.331853  Training Accuracy:  0.978837\n",
      "Epoch:  1402  Training Loss:  0.331487  Training Accuracy:  0.978837\n",
      "Epoch:  1403  Training Loss:  0.331232  Training Accuracy:  0.978837\n",
      "Epoch:  1404  Training Loss:  0.331005  Training Accuracy:  0.978837\n",
      "Epoch:  1405  Training Loss:  0.330761  Training Accuracy:  0.978837\n",
      "Epoch:  1406  Training Loss:  0.33046  Training Accuracy:  0.978837\n",
      "Epoch:  1407  Training Loss:  0.330289  Training Accuracy:  0.978837\n",
      "Epoch:  1408  Training Loss:  0.330071  Training Accuracy:  0.978837\n",
      "Epoch:  1409  Training Loss:  0.329784  Training Accuracy:  0.978895\n",
      "Epoch:  1410  Training Loss:  0.329581  Training Accuracy:  0.978895\n",
      "Epoch:  1411  Training Loss:  0.329372  Training Accuracy:  0.978895\n",
      "Epoch:  1412  Training Loss:  0.329163  Training Accuracy:  0.978895\n",
      "Epoch:  1413  Training Loss:  0.328769  Training Accuracy:  0.978895\n",
      "Epoch:  1414  Training Loss:  0.328834  Training Accuracy:  0.978895\n",
      "Epoch:  1415  Training Loss:  0.32845  Training Accuracy:  0.978895\n",
      "Epoch:  1416  Training Loss:  0.328045  Training Accuracy:  0.978895\n",
      "Epoch:  1417  Training Loss:  0.328069  Training Accuracy:  0.978954\n",
      "Epoch:  1418  Training Loss:  0.327663  Training Accuracy:  0.978954\n",
      "Epoch:  1419  Training Loss:  0.327336  Training Accuracy:  0.978954\n",
      "Epoch:  1420  Training Loss:  0.327574  Training Accuracy:  0.978954\n",
      "Epoch:  1421  Training Loss:  0.327018  Training Accuracy:  0.978954\n",
      "Epoch:  1422  Training Loss:  0.326719  Training Accuracy:  0.978954\n",
      "Epoch:  1423  Training Loss:  0.326627  Training Accuracy:  0.979013\n",
      "Epoch:  1424  Training Loss:  0.326124  Training Accuracy:  0.979013\n",
      "Epoch:  1425  Training Loss:  0.325979  Training Accuracy:  0.979072\n",
      "Epoch:  1426  Training Loss:  0.325641  Training Accuracy:  0.979072\n",
      "Epoch:  1427  Training Loss:  0.325432  Training Accuracy:  0.979072\n",
      "Epoch:  1428  Training Loss:  0.325191  Training Accuracy:  0.979072\n",
      "Epoch:  1429  Training Loss:  0.325001  Training Accuracy:  0.979072\n",
      "Epoch:  1430  Training Loss:  0.324693  Training Accuracy:  0.97913\n",
      "Epoch:  1431  Training Loss:  0.324628  Training Accuracy:  0.979131\n",
      "Epoch:  1432  Training Loss:  0.324199  Training Accuracy:  0.979131\n",
      "Epoch:  1433  Training Loss:  0.324072  Training Accuracy:  0.979131\n",
      "Epoch:  1434  Training Loss:  0.323869  Training Accuracy:  0.97913\n",
      "Epoch:  1435  Training Loss:  0.323576  Training Accuracy:  0.979189\n",
      "Epoch:  1436  Training Loss:  0.323593  Training Accuracy:  0.979248\n",
      "Epoch:  1437  Training Loss:  0.323217  Training Accuracy:  0.979248\n",
      "Epoch:  1438  Training Loss:  0.32291  Training Accuracy:  0.979248\n",
      "Epoch:  1439  Training Loss:  0.322632  Training Accuracy:  0.979248\n",
      "Epoch:  1440  Training Loss:  0.322381  Training Accuracy:  0.979248\n",
      "Epoch:  1441  Training Loss:  0.322259  Training Accuracy:  0.979248\n",
      "Epoch:  1442  Training Loss:  0.322112  Training Accuracy:  0.979248\n",
      "Epoch:  1443  Training Loss:  0.321673  Training Accuracy:  0.979248\n",
      "Epoch:  1444  Training Loss:  0.321558  Training Accuracy:  0.979248\n",
      "Epoch:  1445  Training Loss:  0.321313  Training Accuracy:  0.979248\n",
      "Epoch:  1446  Training Loss:  0.321128  Training Accuracy:  0.979248\n",
      "Epoch:  1447  Training Loss:  0.320932  Training Accuracy:  0.979307\n",
      "Epoch:  1448  Training Loss:  0.320655  Training Accuracy:  0.979307\n",
      "Epoch:  1449  Training Loss:  0.320439  Training Accuracy:  0.979307\n",
      "Epoch:  1450  Training Loss:  0.32023  Training Accuracy:  0.979307\n",
      "Epoch:  1451  Training Loss:  0.320029  Training Accuracy:  0.979307\n",
      "Epoch:  1452  Training Loss:  0.319889  Training Accuracy:  0.979366\n",
      "Epoch:  1453  Training Loss:  0.319564  Training Accuracy:  0.979366\n",
      "Epoch:  1454  Training Loss:  0.319159  Training Accuracy:  0.979366\n",
      "Epoch:  1455  Training Loss:  0.319171  Training Accuracy:  0.979366\n",
      "Epoch:  1456  Training Loss:  0.318934  Training Accuracy:  0.979366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1457  Training Loss:  0.318576  Training Accuracy:  0.979366\n",
      "Epoch:  1458  Training Loss:  0.318363  Training Accuracy:  0.979366\n",
      "Epoch:  1459  Training Loss:  0.318443  Training Accuracy:  0.979366\n",
      "Epoch:  1460  Training Loss:  0.318135  Training Accuracy:  0.979366\n",
      "Epoch:  1461  Training Loss:  0.317732  Training Accuracy:  0.979366\n",
      "Epoch:  1462  Training Loss:  0.317879  Training Accuracy:  0.979366\n",
      "Epoch:  1463  Training Loss:  0.317475  Training Accuracy:  0.979366\n",
      "Epoch:  1464  Training Loss:  0.317281  Training Accuracy:  0.979366\n",
      "Epoch:  1465  Training Loss:  0.317089  Training Accuracy:  0.979366\n",
      "Epoch:  1466  Training Loss:  0.316907  Training Accuracy:  0.979366\n",
      "Epoch:  1467  Training Loss:  0.316453  Training Accuracy:  0.979366\n",
      "Epoch:  1468  Training Loss:  0.316447  Training Accuracy:  0.979366\n",
      "Epoch:  1469  Training Loss:  0.316207  Training Accuracy:  0.979424\n",
      "Epoch:  1470  Training Loss:  0.316056  Training Accuracy:  0.979424\n",
      "Epoch:  1471  Training Loss:  0.315672  Training Accuracy:  0.979424\n",
      "Epoch:  1472  Training Loss:  0.315456  Training Accuracy:  0.979424\n",
      "Epoch:  1473  Training Loss:  0.315267  Training Accuracy:  0.979424\n",
      "Epoch:  1474  Training Loss:  0.315023  Training Accuracy:  0.979424\n",
      "Epoch:  1475  Training Loss:  0.314954  Training Accuracy:  0.979483\n",
      "Epoch:  1476  Training Loss:  0.314457  Training Accuracy:  0.979424\n",
      "Epoch:  1477  Training Loss:  0.314517  Training Accuracy:  0.979483\n",
      "Epoch:  1478  Training Loss:  0.314059  Training Accuracy:  0.979483\n",
      "Epoch:  1479  Training Loss:  0.313927  Training Accuracy:  0.979483\n",
      "Epoch:  1480  Training Loss:  0.313636  Training Accuracy:  0.979483\n",
      "Epoch:  1481  Training Loss:  0.313534  Training Accuracy:  0.979483\n",
      "Epoch:  1482  Training Loss:  0.313275  Training Accuracy:  0.979483\n",
      "Epoch:  1483  Training Loss:  0.313206  Training Accuracy:  0.979483\n",
      "Epoch:  1484  Training Loss:  0.312959  Training Accuracy:  0.979483\n",
      "Epoch:  1485  Training Loss:  0.312786  Training Accuracy:  0.979483\n",
      "Epoch:  1486  Training Loss:  0.312405  Training Accuracy:  0.97966\n",
      "Epoch:  1487  Training Loss:  0.31229  Training Accuracy:  0.97966\n",
      "Epoch:  1488  Training Loss:  0.312025  Training Accuracy:  0.97966\n",
      "Epoch:  1489  Training Loss:  0.311792  Training Accuracy:  0.97966\n",
      "Epoch:  1490  Training Loss:  0.311598  Training Accuracy:  0.97966\n",
      "Epoch:  1491  Training Loss:  0.31138  Training Accuracy:  0.979718\n",
      "Epoch:  1492  Training Loss:  0.310958  Training Accuracy:  0.979777\n",
      "Epoch:  1493  Training Loss:  0.310729  Training Accuracy:  0.979836\n",
      "Epoch:  1494  Training Loss:  0.310416  Training Accuracy:  0.979895\n",
      "Epoch:  1495  Training Loss:  0.310257  Training Accuracy:  0.979954\n",
      "Epoch:  1496  Training Loss:  0.30987  Training Accuracy:  0.979954\n",
      "Epoch:  1497  Training Loss:  0.309722  Training Accuracy:  0.979954\n",
      "Epoch:  1498  Training Loss:  0.309394  Training Accuracy:  0.979954\n",
      "Epoch:  1499  Training Loss:  0.309401  Training Accuracy:  0.979954\n",
      "Epoch:  1500  Training Loss:  0.308913  Training Accuracy:  0.979954\n",
      "Epoch:  1501  Training Loss:  0.308986  Training Accuracy:  0.979954\n",
      "Epoch:  1502  Training Loss:  0.308566  Training Accuracy:  0.979954\n",
      "Epoch:  1503  Training Loss:  0.308339  Training Accuracy:  0.979954\n",
      "Epoch:  1504  Training Loss:  0.308248  Training Accuracy:  0.979954\n",
      "Epoch:  1505  Training Loss:  0.308036  Training Accuracy:  0.979954\n",
      "Epoch:  1506  Training Loss:  0.307682  Training Accuracy:  0.979954\n",
      "Epoch:  1507  Training Loss:  0.307625  Training Accuracy:  0.979954\n",
      "Epoch:  1508  Training Loss:  0.307276  Training Accuracy:  0.979954\n",
      "Epoch:  1509  Training Loss:  0.307031  Training Accuracy:  0.979954\n",
      "Epoch:  1510  Training Loss:  0.306956  Training Accuracy:  0.979954\n",
      "Epoch:  1511  Training Loss:  0.306803  Training Accuracy:  0.979954\n",
      "Epoch:  1512  Training Loss:  0.306437  Training Accuracy:  0.979954\n",
      "Epoch:  1513  Training Loss:  0.306388  Training Accuracy:  0.979954\n",
      "Epoch:  1514  Training Loss:  0.30603  Training Accuracy:  0.979954\n",
      "Epoch:  1515  Training Loss:  0.305954  Training Accuracy:  0.979954\n",
      "Epoch:  1516  Training Loss:  0.30568  Training Accuracy:  0.979954\n",
      "Epoch:  1517  Training Loss:  0.305545  Training Accuracy:  0.979954\n",
      "Epoch:  1518  Training Loss:  0.305087  Training Accuracy:  0.979954\n",
      "Epoch:  1519  Training Loss:  0.305243  Training Accuracy:  0.979954\n",
      "Epoch:  1520  Training Loss:  0.304958  Training Accuracy:  0.979954\n",
      "Epoch:  1521  Training Loss:  0.304716  Training Accuracy:  0.980012\n",
      "Epoch:  1522  Training Loss:  0.304374  Training Accuracy:  0.980012\n",
      "Epoch:  1523  Training Loss:  0.30431  Training Accuracy:  0.980012\n",
      "Epoch:  1524  Training Loss:  0.304135  Training Accuracy:  0.980012\n",
      "Epoch:  1525  Training Loss:  0.30383  Training Accuracy:  0.980012\n",
      "Epoch:  1526  Training Loss:  0.303747  Training Accuracy:  0.980012\n",
      "Epoch:  1527  Training Loss:  0.303427  Training Accuracy:  0.980012\n",
      "Epoch:  1528  Training Loss:  0.303459  Training Accuracy:  0.980012\n",
      "Epoch:  1529  Training Loss:  0.303092  Training Accuracy:  0.980012\n",
      "Epoch:  1530  Training Loss:  0.302772  Training Accuracy:  0.980012\n",
      "Epoch:  1531  Training Loss:  0.302751  Training Accuracy:  0.980012\n",
      "Epoch:  1532  Training Loss:  0.302481  Training Accuracy:  0.980012\n",
      "Epoch:  1533  Training Loss:  0.302288  Training Accuracy:  0.980012\n",
      "Epoch:  1534  Training Loss:  0.301924  Training Accuracy:  0.980012\n",
      "Epoch:  1535  Training Loss:  0.301866  Training Accuracy:  0.980012\n",
      "Epoch:  1536  Training Loss:  0.301548  Training Accuracy:  0.980012\n",
      "Epoch:  1537  Training Loss:  0.301361  Training Accuracy:  0.980012\n",
      "Epoch:  1538  Training Loss:  0.30117  Training Accuracy:  0.98013\n",
      "Epoch:  1539  Training Loss:  0.300873  Training Accuracy:  0.98013\n",
      "Epoch:  1540  Training Loss:  0.300942  Training Accuracy:  0.98013\n",
      "Epoch:  1541  Training Loss:  0.300775  Training Accuracy:  0.98013\n",
      "Epoch:  1542  Training Loss:  0.300309  Training Accuracy:  0.98013\n",
      "Epoch:  1543  Training Loss:  0.300223  Training Accuracy:  0.98013\n",
      "Epoch:  1544  Training Loss:  0.300087  Training Accuracy:  0.98013\n",
      "Epoch:  1545  Training Loss:  0.299838  Training Accuracy:  0.98013\n",
      "Epoch:  1546  Training Loss:  0.299576  Training Accuracy:  0.98013\n",
      "Epoch:  1547  Training Loss:  0.299587  Training Accuracy:  0.98013\n",
      "Epoch:  1548  Training Loss:  0.29916  Training Accuracy:  0.98013\n",
      "Epoch:  1549  Training Loss:  0.298993  Training Accuracy:  0.98013\n",
      "Epoch:  1550  Training Loss:  0.298907  Training Accuracy:  0.98013\n",
      "Epoch:  1551  Training Loss:  0.298663  Training Accuracy:  0.98013\n",
      "Epoch:  1552  Training Loss:  0.298395  Training Accuracy:  0.98013\n",
      "Epoch:  1553  Training Loss:  0.298148  Training Accuracy:  0.980189\n",
      "Epoch:  1554  Training Loss:  0.298011  Training Accuracy:  0.980189\n",
      "Epoch:  1555  Training Loss:  0.297844  Training Accuracy:  0.980189\n",
      "Epoch:  1556  Training Loss:  0.297594  Training Accuracy:  0.980189\n",
      "Epoch:  1557  Training Loss:  0.297457  Training Accuracy:  0.980247\n",
      "Epoch:  1558  Training Loss:  0.297099  Training Accuracy:  0.980247\n",
      "Epoch:  1559  Training Loss:  0.296986  Training Accuracy:  0.980247\n",
      "Epoch:  1560  Training Loss:  0.296823  Training Accuracy:  0.980247\n",
      "Epoch:  1561  Training Loss:  0.296674  Training Accuracy:  0.980247\n",
      "Epoch:  1562  Training Loss:  0.296457  Training Accuracy:  0.980247\n",
      "Epoch:  1563  Training Loss:  0.296339  Training Accuracy:  0.980247\n",
      "Epoch:  1564  Training Loss:  0.29603  Training Accuracy:  0.980247\n",
      "Epoch:  1565  Training Loss:  0.295744  Training Accuracy:  0.980247\n",
      "Epoch:  1566  Training Loss:  0.295658  Training Accuracy:  0.980306\n",
      "Epoch:  1567  Training Loss:  0.295435  Training Accuracy:  0.980365\n",
      "Epoch:  1568  Training Loss:  0.295208  Training Accuracy:  0.980365\n",
      "Epoch:  1569  Training Loss:  0.295173  Training Accuracy:  0.980365\n",
      "Epoch:  1570  Training Loss:  0.294895  Training Accuracy:  0.980365\n",
      "Epoch:  1571  Training Loss:  0.294782  Training Accuracy:  0.980365\n",
      "Epoch:  1572  Training Loss:  0.294501  Training Accuracy:  0.980424\n",
      "Epoch:  1573  Training Loss:  0.294295  Training Accuracy:  0.980424\n",
      "Epoch:  1574  Training Loss:  0.294298  Training Accuracy:  0.980424\n",
      "Epoch:  1575  Training Loss:  0.294025  Training Accuracy:  0.980424\n",
      "Epoch:  1576  Training Loss:  0.293913  Training Accuracy:  0.980424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1577  Training Loss:  0.293664  Training Accuracy:  0.980424\n",
      "Epoch:  1578  Training Loss:  0.293387  Training Accuracy:  0.980424\n",
      "Epoch:  1579  Training Loss:  0.29335  Training Accuracy:  0.980424\n",
      "Epoch:  1580  Training Loss:  0.292919  Training Accuracy:  0.980424\n",
      "Epoch:  1581  Training Loss:  0.292991  Training Accuracy:  0.980424\n",
      "Epoch:  1582  Training Loss:  0.292707  Training Accuracy:  0.980424\n",
      "Epoch:  1583  Training Loss:  0.292718  Training Accuracy:  0.980424\n",
      "Epoch:  1584  Training Loss:  0.292335  Training Accuracy:  0.980424\n",
      "Epoch:  1585  Training Loss:  0.292226  Training Accuracy:  0.980424\n",
      "Epoch:  1586  Training Loss:  0.292004  Training Accuracy:  0.980424\n",
      "Epoch:  1587  Training Loss:  0.292009  Training Accuracy:  0.980424\n",
      "Epoch:  1588  Training Loss:  0.291885  Training Accuracy:  0.980424\n",
      "Epoch:  1589  Training Loss:  0.29152  Training Accuracy:  0.980424\n",
      "Epoch:  1590  Training Loss:  0.291297  Training Accuracy:  0.980424\n",
      "Epoch:  1591  Training Loss:  0.291205  Training Accuracy:  0.980424\n",
      "Epoch:  1592  Training Loss:  0.290762  Training Accuracy:  0.980424\n",
      "Epoch:  1593  Training Loss:  0.290828  Training Accuracy:  0.980424\n",
      "Epoch:  1594  Training Loss:  0.290443  Training Accuracy:  0.980483\n",
      "Epoch:  1595  Training Loss:  0.290392  Training Accuracy:  0.980483\n",
      "Epoch:  1596  Training Loss:  0.28999  Training Accuracy:  0.980541\n",
      "Epoch:  1597  Training Loss:  0.290125  Training Accuracy:  0.980541\n",
      "Epoch:  1598  Training Loss:  0.289856  Training Accuracy:  0.980541\n",
      "Epoch:  1599  Training Loss:  0.289552  Training Accuracy:  0.980541\n",
      "Epoch:  1600  Training Loss:  0.289521  Training Accuracy:  0.980541\n",
      "Epoch:  1601  Training Loss:  0.289333  Training Accuracy:  0.980541\n",
      "Epoch:  1602  Training Loss:  0.288923  Training Accuracy:  0.980541\n",
      "Epoch:  1603  Training Loss:  0.288811  Training Accuracy:  0.980541\n",
      "Epoch:  1604  Training Loss:  0.288602  Training Accuracy:  0.9806\n",
      "Epoch:  1605  Training Loss:  0.288604  Training Accuracy:  0.9806\n",
      "Epoch:  1606  Training Loss:  0.288347  Training Accuracy:  0.9806\n",
      "Epoch:  1607  Training Loss:  0.288152  Training Accuracy:  0.9806\n",
      "Epoch:  1608  Training Loss:  0.288021  Training Accuracy:  0.9806\n",
      "Epoch:  1609  Training Loss:  0.287719  Training Accuracy:  0.9806\n",
      "Epoch:  1610  Training Loss:  0.287709  Training Accuracy:  0.9806\n",
      "Epoch:  1611  Training Loss:  0.287426  Training Accuracy:  0.9806\n",
      "Epoch:  1612  Training Loss:  0.287338  Training Accuracy:  0.9806\n",
      "Epoch:  1613  Training Loss:  0.287074  Training Accuracy:  0.9806\n",
      "Epoch:  1614  Training Loss:  0.287052  Training Accuracy:  0.9806\n",
      "Epoch:  1615  Training Loss:  0.286751  Training Accuracy:  0.9806\n",
      "Epoch:  1616  Training Loss:  0.286656  Training Accuracy:  0.9806\n",
      "Epoch:  1617  Training Loss:  0.286373  Training Accuracy:  0.9806\n",
      "Epoch:  1618  Training Loss:  0.286307  Training Accuracy:  0.9806\n",
      "Epoch:  1619  Training Loss:  0.286051  Training Accuracy:  0.9806\n",
      "Epoch:  1620  Training Loss:  0.285894  Training Accuracy:  0.9806\n",
      "Epoch:  1621  Training Loss:  0.285785  Training Accuracy:  0.9806\n",
      "Epoch:  1622  Training Loss:  0.285713  Training Accuracy:  0.9806\n",
      "Epoch:  1623  Training Loss:  0.285488  Training Accuracy:  0.980659\n",
      "Epoch:  1624  Training Loss:  0.285186  Training Accuracy:  0.980718\n",
      "Epoch:  1625  Training Loss:  0.285134  Training Accuracy:  0.980718\n",
      "Epoch:  1626  Training Loss:  0.284938  Training Accuracy:  0.980718\n",
      "Epoch:  1627  Training Loss:  0.284755  Training Accuracy:  0.980718\n",
      "Epoch:  1628  Training Loss:  0.284558  Training Accuracy:  0.980718\n",
      "Epoch:  1629  Training Loss:  0.284449  Training Accuracy:  0.980718\n",
      "Epoch:  1630  Training Loss:  0.284134  Training Accuracy:  0.980777\n",
      "Epoch:  1631  Training Loss:  0.283854  Training Accuracy:  0.980777\n",
      "Epoch:  1632  Training Loss:  0.283767  Training Accuracy:  0.980777\n",
      "Epoch:  1633  Training Loss:  0.283642  Training Accuracy:  0.980835\n",
      "Epoch:  1634  Training Loss:  0.28338  Training Accuracy:  0.980835\n",
      "Epoch:  1635  Training Loss:  0.283119  Training Accuracy:  0.980835\n",
      "Epoch:  1636  Training Loss:  0.283168  Training Accuracy:  0.980835\n",
      "Epoch:  1637  Training Loss:  0.282837  Training Accuracy:  0.980835\n",
      "Epoch:  1638  Training Loss:  0.282801  Training Accuracy:  0.980835\n",
      "Epoch:  1639  Training Loss:  0.282456  Training Accuracy:  0.980835\n",
      "Epoch:  1640  Training Loss:  0.282345  Training Accuracy:  0.980835\n",
      "Epoch:  1641  Training Loss:  0.282141  Training Accuracy:  0.980835\n",
      "Epoch:  1642  Training Loss:  0.282005  Training Accuracy:  0.980835\n",
      "Epoch:  1643  Training Loss:  0.281889  Training Accuracy:  0.980835\n",
      "Epoch:  1644  Training Loss:  0.28174  Training Accuracy:  0.980835\n",
      "Epoch:  1645  Training Loss:  0.281694  Training Accuracy:  0.980835\n",
      "Epoch:  1646  Training Loss:  0.28138  Training Accuracy:  0.980835\n",
      "Epoch:  1647  Training Loss:  0.281307  Training Accuracy:  0.980835\n",
      "Epoch:  1648  Training Loss:  0.280973  Training Accuracy:  0.980835\n",
      "Epoch:  1649  Training Loss:  0.281125  Training Accuracy:  0.980835\n",
      "Epoch:  1650  Training Loss:  0.280685  Training Accuracy:  0.980835\n",
      "Epoch:  1651  Training Loss:  0.28069  Training Accuracy:  0.980894\n",
      "Epoch:  1652  Training Loss:  0.280553  Training Accuracy:  0.980894\n",
      "Epoch:  1653  Training Loss:  0.280412  Training Accuracy:  0.980894\n",
      "Epoch:  1654  Training Loss:  0.280043  Training Accuracy:  0.980894\n",
      "Epoch:  1655  Training Loss:  0.280008  Training Accuracy:  0.980894\n",
      "Epoch:  1656  Training Loss:  0.279958  Training Accuracy:  0.980953\n",
      "Epoch:  1657  Training Loss:  0.279794  Training Accuracy:  0.980953\n",
      "Epoch:  1658  Training Loss:  0.279458  Training Accuracy:  0.980953\n",
      "Epoch:  1659  Training Loss:  0.279426  Training Accuracy:  0.980953\n",
      "Epoch:  1660  Training Loss:  0.279244  Training Accuracy:  0.980953\n",
      "Epoch:  1661  Training Loss:  0.27921  Training Accuracy:  0.980953\n",
      "Epoch:  1662  Training Loss:  0.278926  Training Accuracy:  0.980953\n",
      "Epoch:  1663  Training Loss:  0.278766  Training Accuracy:  0.981012\n",
      "Epoch:  1664  Training Loss:  0.278522  Training Accuracy:  0.981012\n",
      "Epoch:  1665  Training Loss:  0.278314  Training Accuracy:  0.981071\n",
      "Epoch:  1666  Training Loss:  0.27821  Training Accuracy:  0.981129\n",
      "Epoch:  1667  Training Loss:  0.277953  Training Accuracy:  0.981129\n",
      "Epoch:  1668  Training Loss:  0.277872  Training Accuracy:  0.981188\n",
      "Epoch:  1669  Training Loss:  0.277805  Training Accuracy:  0.981188\n",
      "Epoch:  1670  Training Loss:  0.277523  Training Accuracy:  0.981188\n",
      "Epoch:  1671  Training Loss:  0.277595  Training Accuracy:  0.981188\n",
      "Epoch:  1672  Training Loss:  0.277192  Training Accuracy:  0.981247\n",
      "Epoch:  1673  Training Loss:  0.277045  Training Accuracy:  0.981247\n",
      "Epoch:  1674  Training Loss:  0.276765  Training Accuracy:  0.981247\n",
      "Epoch:  1675  Training Loss:  0.276775  Training Accuracy:  0.981247\n",
      "Epoch:  1676  Training Loss:  0.276661  Training Accuracy:  0.981247\n",
      "Epoch:  1677  Training Loss:  0.276388  Training Accuracy:  0.981247\n",
      "Epoch:  1678  Training Loss:  0.276255  Training Accuracy:  0.981247\n",
      "Epoch:  1679  Training Loss:  0.276094  Training Accuracy:  0.981247\n",
      "Epoch:  1680  Training Loss:  0.275942  Training Accuracy:  0.981247\n",
      "Epoch:  1681  Training Loss:  0.275955  Training Accuracy:  0.981247\n",
      "Epoch:  1682  Training Loss:  0.275714  Training Accuracy:  0.981247\n",
      "Epoch:  1683  Training Loss:  0.275538  Training Accuracy:  0.981247\n",
      "Epoch:  1684  Training Loss:  0.275492  Training Accuracy:  0.981247\n",
      "Epoch:  1685  Training Loss:  0.275011  Training Accuracy:  0.981247\n",
      "Epoch:  1686  Training Loss:  0.274935  Training Accuracy:  0.981247\n",
      "Epoch:  1687  Training Loss:  0.27491  Training Accuracy:  0.981247\n",
      "Epoch:  1688  Training Loss:  0.274776  Training Accuracy:  0.981247\n",
      "Epoch:  1689  Training Loss:  0.274609  Training Accuracy:  0.981247\n",
      "Epoch:  1690  Training Loss:  0.274533  Training Accuracy:  0.981306\n",
      "Epoch:  1691  Training Loss:  0.274223  Training Accuracy:  0.981306\n",
      "Epoch:  1692  Training Loss:  0.27412  Training Accuracy:  0.981306\n",
      "Epoch:  1693  Training Loss:  0.27384  Training Accuracy:  0.981306\n",
      "Epoch:  1694  Training Loss:  0.273905  Training Accuracy:  0.981364\n",
      "Epoch:  1695  Training Loss:  0.273845  Training Accuracy:  0.981364\n",
      "Epoch:  1696  Training Loss:  0.273463  Training Accuracy:  0.981364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1697  Training Loss:  0.273365  Training Accuracy:  0.981364\n",
      "Epoch:  1698  Training Loss:  0.273162  Training Accuracy:  0.981364\n",
      "Epoch:  1699  Training Loss:  0.273043  Training Accuracy:  0.981364\n",
      "Testing Accuracy: 0.879616\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 16\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1700\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'Variable_36/Adam/Initializer/zeros', defined at:\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-53f34d6f43b5>\", line 34, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 132, in _create_slots\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 766, in _zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 146, in create_slot_with_initializer\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 66, in _create_slot_var\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 277, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 93, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1401, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-53f34d6f43b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \"\"\"\n\u001b[0;32m-> 1744\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4118\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4120\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'Variable_36/Adam/Initializer/zeros', defined at:\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-53f34d6f43b5>\", line 34, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 132, in _create_slots\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 766, in _zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 146, in create_slot_with_initializer\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 66, in _create_slot_var\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 277, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 93, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1401, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 16\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 1700\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'Variable_36/Adam/Initializer/zeros', defined at:\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-53f34d6f43b5>\", line 34, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 132, in _create_slots\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 766, in _zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 146, in create_slot_with_initializer\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 66, in _create_slot_var\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 277, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 93, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1401, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4aa1bfa4c6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \"\"\"\n\u001b[0;32m-> 1744\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4118\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4120\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'Variable_36/Adam/Initializer/zeros', defined at:\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-53f34d6f43b5>\", line 34, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 325, in minimize\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 132, in _create_slots\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 766, in _zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 146, in create_slot_with_initializer\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 66, in _create_slot_var\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 277, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 93, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1401, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/deepstation/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 1080\n  }\n  dim {\n    size: 10000\n  }\n}\nfloat_val: 0\n\n\t [[Node: Variable_36/Adam/Initializer/zeros = Const[_class=[\"loc:@Variable_36\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [1080,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 16\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 1500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  124.259  Training Accuracy:  0.0404468\n",
      "Epoch:  1  Training Loss:  135.933  Training Accuracy:  0.043739\n",
      "Epoch:  2  Training Loss:  136.487  Training Accuracy:  0.0451499\n",
      "Epoch:  3  Training Loss:  127.384  Training Accuracy:  0.0485009\n",
      "Epoch:  4  Training Loss:  116.091  Training Accuracy:  0.0547913\n",
      "Epoch:  5  Training Loss:  106.313  Training Accuracy:  0.0621987\n",
      "Epoch:  6  Training Loss:  99.0313  Training Accuracy:  0.0676661\n",
      "Epoch:  7  Training Loss:  93.637  Training Accuracy:  0.0722516\n",
      "Epoch:  8  Training Loss:  89.2988  Training Accuracy:  0.077425\n",
      "Epoch:  9  Training Loss:  85.5011  Training Accuracy:  0.086361\n",
      "Epoch:  10  Training Loss:  81.9826  Training Accuracy:  0.093886\n",
      "Epoch:  11  Training Loss:  78.6394  Training Accuracy:  0.10194\n",
      "Epoch:  12  Training Loss:  75.3232  Training Accuracy:  0.109289\n",
      "Epoch:  13  Training Loss:  71.956  Training Accuracy:  0.117049\n",
      "Epoch:  14  Training Loss:  68.5284  Training Accuracy:  0.123398\n",
      "Epoch:  15  Training Loss:  65.1356  Training Accuracy:  0.129277\n",
      "Epoch:  16  Training Loss:  61.7524  Training Accuracy:  0.135038\n",
      "Epoch:  17  Training Loss:  58.4806  Training Accuracy:  0.14127\n",
      "Epoch:  18  Training Loss:  55.2857  Training Accuracy:  0.145855\n",
      "Epoch:  19  Training Loss:  52.2719  Training Accuracy:  0.151852\n",
      "Epoch:  20  Training Loss:  49.4103  Training Accuracy:  0.158965\n",
      "Epoch:  21  Training Loss:  46.7035  Training Accuracy:  0.166549\n",
      "Epoch:  22  Training Loss:  44.1213  Training Accuracy:  0.174956\n",
      "Epoch:  23  Training Loss:  41.6709  Training Accuracy:  0.183186\n",
      "Epoch:  24  Training Loss:  39.4079  Training Accuracy:  0.191593\n",
      "Epoch:  25  Training Loss:  37.3073  Training Accuracy:  0.200529\n",
      "Epoch:  26  Training Loss:  35.3145  Training Accuracy:  0.208995\n",
      "Epoch:  27  Training Loss:  33.4437  Training Accuracy:  0.21746\n",
      "Epoch:  28  Training Loss:  31.7029  Training Accuracy:  0.224456\n",
      "Epoch:  29  Training Loss:  30.0757  Training Accuracy:  0.23351\n",
      "Epoch:  30  Training Loss:  28.5675  Training Accuracy:  0.24274\n",
      "Epoch:  31  Training Loss:  27.166  Training Accuracy:  0.250911\n",
      "Epoch:  32  Training Loss:  25.8177  Training Accuracy:  0.259318\n",
      "Epoch:  33  Training Loss:  24.5319  Training Accuracy:  0.266196\n",
      "Epoch:  34  Training Loss:  23.3023  Training Accuracy:  0.272428\n",
      "Epoch:  35  Training Loss:  22.1498  Training Accuracy:  0.277895\n",
      "Epoch:  36  Training Loss:  20.9883  Training Accuracy:  0.28348\n",
      "Epoch:  37  Training Loss:  19.8583  Training Accuracy:  0.289712\n",
      "Epoch:  38  Training Loss:  18.794  Training Accuracy:  0.295062\n",
      "Epoch:  39  Training Loss:  17.7426  Training Accuracy:  0.300353\n",
      "Epoch:  40  Training Loss:  16.7169  Training Accuracy:  0.305173\n",
      "Epoch:  41  Training Loss:  15.7717  Training Accuracy:  0.310523\n",
      "Epoch:  42  Training Loss:  14.9416  Training Accuracy:  0.316461\n",
      "Epoch:  43  Training Loss:  14.1204  Training Accuracy:  0.320341\n",
      "Epoch:  44  Training Loss:  13.2956  Training Accuracy:  0.325867\n",
      "Epoch:  45  Training Loss:  12.5667  Training Accuracy:  0.330805\n",
      "Epoch:  46  Training Loss:  11.884  Training Accuracy:  0.33545\n",
      "Epoch:  47  Training Loss:  11.2152  Training Accuracy:  0.340094\n",
      "Epoch:  48  Training Loss:  10.6202  Training Accuracy:  0.344621\n",
      "Epoch:  49  Training Loss:  10.0574  Training Accuracy:  0.348971\n",
      "Epoch:  50  Training Loss:  9.55277  Training Accuracy:  0.352616\n",
      "Epoch:  51  Training Loss:  9.02577  Training Accuracy:  0.356496\n",
      "Epoch:  52  Training Loss:  8.54755  Training Accuracy:  0.359671\n",
      "Epoch:  53  Training Loss:  8.0303  Training Accuracy:  0.363903\n",
      "Epoch:  54  Training Loss:  7.63391  Training Accuracy:  0.367196\n",
      "Epoch:  55  Training Loss:  7.17581  Training Accuracy:  0.372369\n",
      "Epoch:  56  Training Loss:  6.82096  Training Accuracy:  0.375896\n",
      "Epoch:  57  Training Loss:  6.45197  Training Accuracy:  0.378777\n",
      "Epoch:  58  Training Loss:  6.14921  Training Accuracy:  0.382069\n",
      "Epoch:  59  Training Loss:  5.80753  Training Accuracy:  0.386126\n",
      "Epoch:  60  Training Loss:  5.49191  Training Accuracy:  0.389065\n",
      "Epoch:  61  Training Loss:  5.20302  Training Accuracy:  0.391769\n",
      "Epoch:  62  Training Loss:  4.95238  Training Accuracy:  0.394885\n",
      "Epoch:  63  Training Loss:  4.71412  Training Accuracy:  0.397354\n",
      "Epoch:  64  Training Loss:  4.50001  Training Accuracy:  0.400235\n",
      "Epoch:  65  Training Loss:  4.30739  Training Accuracy:  0.403527\n",
      "Epoch:  66  Training Loss:  4.12881  Training Accuracy:  0.407349\n",
      "Epoch:  67  Training Loss:  3.97898  Training Accuracy:  0.410641\n",
      "Epoch:  68  Training Loss:  3.82131  Training Accuracy:  0.413286\n",
      "Epoch:  69  Training Loss:  3.65619  Training Accuracy:  0.418048\n",
      "Epoch:  70  Training Loss:  3.50149  Training Accuracy:  0.4204\n",
      "Epoch:  71  Training Loss:  3.36606  Training Accuracy:  0.423045\n",
      "Epoch:  72  Training Loss:  3.2416  Training Accuracy:  0.425985\n",
      "Epoch:  73  Training Loss:  3.13587  Training Accuracy:  0.429277\n",
      "Epoch:  74  Training Loss:  3.0382  Training Accuracy:  0.431864\n",
      "Epoch:  75  Training Loss:  2.91747  Training Accuracy:  0.435038\n",
      "Epoch:  76  Training Loss:  2.832  Training Accuracy:  0.437507\n",
      "Epoch:  77  Training Loss:  2.73499  Training Accuracy:  0.439859\n",
      "Epoch:  78  Training Loss:  2.65221  Training Accuracy:  0.442328\n",
      "Epoch:  79  Training Loss:  2.57013  Training Accuracy:  0.444621\n",
      "Epoch:  80  Training Loss:  2.49536  Training Accuracy:  0.446678\n",
      "Epoch:  81  Training Loss:  2.41982  Training Accuracy:  0.449559\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1028  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1155  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1282  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1409  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1536  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1663  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1790  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1917  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2044  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2171  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2298  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2425  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2552  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2679  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2806  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2933  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 9\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 3000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  140.397  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  121.771  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  122.768  Training Accuracy:  0.0437978\n",
      "Epoch:  3  Training Loss:  115.325  Training Accuracy:  0.0475603\n",
      "Epoch:  4  Training Loss:  107.437  Training Accuracy:  0.0504997\n",
      "Epoch:  5  Training Loss:  100.771  Training Accuracy:  0.0557907\n",
      "Epoch:  6  Training Loss:  95.6767  Training Accuracy:  0.0620223\n",
      "Epoch:  7  Training Loss:  91.5843  Training Accuracy:  0.0670782\n",
      "Epoch:  8  Training Loss:  88.1849  Training Accuracy:  0.0716637\n",
      "Epoch:  9  Training Loss:  85.1916  Training Accuracy:  0.0754262\n",
      "Epoch:  10  Training Loss:  82.5181  Training Accuracy:  0.0790711\n",
      "Epoch:  11  Training Loss:  80.0455  Training Accuracy:  0.0847737\n",
      "Epoch:  12  Training Loss:  77.6623  Training Accuracy:  0.0910641\n",
      "Epoch:  13  Training Loss:  75.2761  Training Accuracy:  0.0975309\n",
      "Epoch:  14  Training Loss:  72.8899  Training Accuracy:  0.104938\n",
      "Epoch:  15  Training Loss:  70.4414  Training Accuracy:  0.112287\n",
      "Epoch:  16  Training Loss:  67.9195  Training Accuracy:  0.117284\n",
      "Epoch:  17  Training Loss:  65.3736  Training Accuracy:  0.12428\n",
      "Epoch:  18  Training Loss:  62.8514  Training Accuracy:  0.131452\n",
      "Epoch:  19  Training Loss:  60.4205  Training Accuracy:  0.13639\n",
      "Epoch:  20  Training Loss:  58.1024  Training Accuracy:  0.143621\n",
      "Epoch:  21  Training Loss:  55.9197  Training Accuracy:  0.150088\n",
      "Epoch:  22  Training Loss:  53.8225  Training Accuracy:  0.156379\n",
      "Epoch:  23  Training Loss:  51.7941  Training Accuracy:  0.162787\n",
      "Epoch:  24  Training Loss:  49.8807  Training Accuracy:  0.169547\n",
      "Epoch:  25  Training Loss:  48.0581  Training Accuracy:  0.177131\n",
      "Epoch:  26  Training Loss:  46.2179  Training Accuracy:  0.184832\n",
      "Epoch:  27  Training Loss:  44.4548  Training Accuracy:  0.191476\n",
      "Epoch:  28  Training Loss:  42.7757  Training Accuracy:  0.198942\n",
      "Epoch:  29  Training Loss:  41.0777  Training Accuracy:  0.206643\n",
      "Epoch:  30  Training Loss:  39.4787  Training Accuracy:  0.214344\n",
      "Epoch:  31  Training Loss:  37.9227  Training Accuracy:  0.221869\n",
      "Epoch:  32  Training Loss:  36.4155  Training Accuracy:  0.229982\n",
      "Epoch:  33  Training Loss:  35.003  Training Accuracy:  0.236155\n",
      "Epoch:  34  Training Loss:  33.6515  Training Accuracy:  0.242034\n",
      "Epoch:  35  Training Loss:  32.2645  Training Accuracy:  0.248442\n",
      "Epoch:  36  Training Loss:  30.9498  Training Accuracy:  0.254674\n",
      "Epoch:  37  Training Loss:  29.6518  Training Accuracy:  0.261434\n",
      "Epoch:  38  Training Loss:  28.4256  Training Accuracy:  0.267725\n",
      "Epoch:  39  Training Loss:  27.1227  Training Accuracy:  0.273898\n",
      "Epoch:  40  Training Loss:  25.9961  Training Accuracy:  0.280247\n",
      "Epoch:  41  Training Loss:  24.8259  Training Accuracy:  0.286361\n",
      "Epoch:  42  Training Loss:  23.729  Training Accuracy:  0.293004\n",
      "Epoch:  43  Training Loss:  22.5991  Training Accuracy:  0.298236\n",
      "Epoch:  44  Training Loss:  21.4085  Training Accuracy:  0.30388\n",
      "Epoch:  45  Training Loss:  20.4116  Training Accuracy:  0.308818\n",
      "Epoch:  46  Training Loss:  19.3465  Training Accuracy:  0.314168\n",
      "Epoch:  47  Training Loss:  18.4249  Training Accuracy:  0.318989\n",
      "Epoch:  48  Training Loss:  17.4755  Training Accuracy:  0.323516\n",
      "Epoch:  49  Training Loss:  16.6314  Training Accuracy:  0.328983\n",
      "Epoch:  50  Training Loss:  15.8404  Training Accuracy:  0.33351\n",
      "Epoch:  51  Training Loss:  15.0241  Training Accuracy:  0.338154\n",
      "Epoch:  52  Training Loss:  14.2833  Training Accuracy:  0.343856\n",
      "Epoch:  53  Training Loss:  13.5441  Training Accuracy:  0.346913\n",
      "Epoch:  54  Training Loss:  12.8108  Training Accuracy:  0.352028\n",
      "Epoch:  55  Training Loss:  12.1444  Training Accuracy:  0.356672\n",
      "Epoch:  56  Training Loss:  11.4211  Training Accuracy:  0.361258\n",
      "Epoch:  57  Training Loss:  10.7929  Training Accuracy:  0.366137\n",
      "Epoch:  58  Training Loss:  10.2041  Training Accuracy:  0.370018\n",
      "Epoch:  59  Training Loss:  9.65922  Training Accuracy:  0.374192\n",
      "Epoch:  60  Training Loss:  9.10217  Training Accuracy:  0.377719\n",
      "Epoch:  61  Training Loss:  8.64284  Training Accuracy:  0.381658\n",
      "Epoch:  62  Training Loss:  8.17696  Training Accuracy:  0.384832\n",
      "Epoch:  63  Training Loss:  7.67898  Training Accuracy:  0.387478\n",
      "Epoch:  64  Training Loss:  7.28333  Training Accuracy:  0.389771\n",
      "Epoch:  65  Training Loss:  6.86288  Training Accuracy:  0.394239\n",
      "Epoch:  66  Training Loss:  6.52974  Training Accuracy:  0.397237\n",
      "Epoch:  67  Training Loss:  6.14238  Training Accuracy:  0.401528\n",
      "Epoch:  68  Training Loss:  5.81454  Training Accuracy:  0.405879\n",
      "Epoch:  69  Training Loss:  5.47227  Training Accuracy:  0.410758\n",
      "Epoch:  70  Training Loss:  5.27721  Training Accuracy:  0.414168\n",
      "Epoch:  71  Training Loss:  4.93679  Training Accuracy:  0.417166\n",
      "Epoch:  72  Training Loss:  4.68673  Training Accuracy:  0.421105\n",
      "Epoch:  73  Training Loss:  4.47932  Training Accuracy:  0.424809\n",
      "Epoch:  74  Training Loss:  4.30463  Training Accuracy:  0.428101\n",
      "Epoch:  75  Training Loss:  4.09616  Training Accuracy:  0.431452\n",
      "Epoch:  76  Training Loss:  3.88401  Training Accuracy:  0.434274\n",
      "Epoch:  77  Training Loss:  3.6846  Training Accuracy:  0.437566\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1027  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1154  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1281  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1408  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1535  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1662  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1789  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1916  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2043  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2170  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2297  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2424  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2551  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2678  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2805  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2932  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3059  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3186  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3313  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3440  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3567  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3694  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3821  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3948  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 9\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 4000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  10  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  11  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  12  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  13  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  14  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  15  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  16  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  17  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  18  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  19  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  20  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  21  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  22  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  23  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  24  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  25  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  26  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  27  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  28  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  29  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  30  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  31  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  32  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  33  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  34  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  35  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  36  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  37  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  38  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  39  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  40  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  41  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  42  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  43  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  44  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  45  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  46  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  47  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  48  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  49  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  50  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  51  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  52  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  53  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  54  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  55  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  56  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  57  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  58  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  59  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  60  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  61  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  62  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  63  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  64  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  65  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  66  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  67  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  68  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  69  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  70  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  71  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  72  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  73  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  74  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  75  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  76  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  77  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1031  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1158  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1285  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1412  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1539  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1666  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1793  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1920  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2047  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2174  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2301  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2428  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2555  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2682  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2809  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2936  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3063  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3190  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3317  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3444  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3571  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3698  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3825  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3952  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4079  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4206  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4333  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4460  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4587  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4714  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4841  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4968  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 5000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  10  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  11  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  12  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  13  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  14  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  15  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  16  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  17  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  18  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  19  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  20  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  21  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  22  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  23  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  24  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  25  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  26  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  27  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  28  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  29  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  30  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  31  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  32  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  33  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  34  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  35  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  36  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  37  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  38  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  39  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  40  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  41  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  42  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  43  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  44  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  45  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  46  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  47  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  48  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  49  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  50  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  51  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  52  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  53  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  54  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  55  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  56  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  57  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  58  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  59  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  60  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  61  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  62  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  63  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  64  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  65  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  66  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  67  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  68  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  69  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  70  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  71  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  72  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  73  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  74  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  75  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  76  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  77  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1031  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1158  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1285  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1412  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1539  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1666  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1793  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1920  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2047  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2174  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2301  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2428  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2555  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2682  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2809  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2936  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3063  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3190  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3317  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3444  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3571  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3698  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3825  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3952  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4079  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4206  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4333  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4460  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4587  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4714  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4841  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4968  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5095  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5222  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5349  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5476  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5603  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5730  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5857  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5984  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6111  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6238  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6365  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6492  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6619  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6746  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6873  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7000  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7127  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7254  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7381  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7508  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7635  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7762  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7889  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8016  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8143  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8270  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8397  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8524  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8651  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8778  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8905  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9032  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9159  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9286  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9413  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9540  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9667  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9794  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9921  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 10000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  3  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  4  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  5  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  6  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  7  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  8  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  9  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  10  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  11  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  12  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  13  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  14  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  15  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  16  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  17  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  18  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  19  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  20  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  21  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  22  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  23  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  24  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  25  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  26  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  27  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  28  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  29  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  30  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  31  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  32  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  33  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  34  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  35  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  36  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  37  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  38  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  39  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  40  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  41  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  42  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  43  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  44  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  45  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  46  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  47  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  48  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  49  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  50  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  51  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  52  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  53  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  54  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  55  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  56  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  57  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  58  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  59  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  60  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  61  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  62  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  63  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  64  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  65  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  66  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  67  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  68  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  69  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  70  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  71  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  72  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  73  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  74  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  75  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  76  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  77  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1031  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1047  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1158  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1285  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1412  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1539  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1666  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1793  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1920  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  1999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2000  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2001  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2002  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2003  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2004  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2005  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2006  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2007  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2008  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2009  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2010  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2011  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2012  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2013  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2014  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2015  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2016  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2017  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2018  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2019  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2020  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2021  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2022  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2023  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2024  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2025  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2026  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2027  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2028  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2029  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2030  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2031  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2032  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2033  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2034  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2035  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2036  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2037  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2038  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2039  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2040  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2041  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2042  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2043  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2044  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2045  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2046  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2047  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2048  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2049  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2050  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2051  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2052  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2053  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2054  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2055  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2056  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2057  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2058  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2059  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2060  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2061  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2062  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2063  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2064  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2065  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2066  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2067  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2068  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2069  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2070  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2071  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2072  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2073  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2074  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2075  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2076  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2077  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2078  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2079  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2080  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2081  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2082  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2083  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2084  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2085  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2086  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2087  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2088  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2089  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2090  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2091  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2092  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2093  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2094  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2095  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2096  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2097  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2098  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2099  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2174  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2301  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2428  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  2499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 2500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  175.615  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  141.838  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  127.823  Training Accuracy:  0.0462669\n",
      "Epoch:  3  Training Loss:  114.735  Training Accuracy:  0.0530864\n",
      "Epoch:  4  Training Loss:  104.378  Training Accuracy:  0.0582011\n",
      "Epoch:  5  Training Loss:  97.0706  Training Accuracy:  0.0634921\n",
      "Epoch:  6  Training Loss:  92.1192  Training Accuracy:  0.0680188\n",
      "Epoch:  7  Training Loss:  88.3668  Training Accuracy:  0.0730747\n",
      "Epoch:  8  Training Loss:  85.2273  Training Accuracy:  0.0769547\n",
      "Epoch:  9  Training Loss:  82.4338  Training Accuracy:  0.0814815\n",
      "Epoch:  10  Training Loss:  79.7954  Training Accuracy:  0.0877131\n",
      "Epoch:  11  Training Loss:  77.2156  Training Accuracy:  0.0943563\n",
      "Epoch:  12  Training Loss:  74.5958  Training Accuracy:  0.101529\n",
      "Epoch:  13  Training Loss:  71.8586  Training Accuracy:  0.109347\n",
      "Epoch:  14  Training Loss:  69.0806  Training Accuracy:  0.118812\n",
      "Epoch:  15  Training Loss:  66.2306  Training Accuracy:  0.126337\n",
      "Epoch:  16  Training Loss:  63.3231  Training Accuracy:  0.132981\n",
      "Epoch:  17  Training Loss:  60.4817  Training Accuracy:  0.139153\n",
      "Epoch:  18  Training Loss:  57.6301  Training Accuracy:  0.145797\n",
      "Epoch:  19  Training Loss:  54.7169  Training Accuracy:  0.152734\n",
      "Epoch:  20  Training Loss:  51.9101  Training Accuracy:  0.1592\n",
      "Epoch:  21  Training Loss:  49.2487  Training Accuracy:  0.167842\n",
      "Epoch:  22  Training Loss:  46.5861  Training Accuracy:  0.175426\n",
      "Epoch:  23  Training Loss:  44.0107  Training Accuracy:  0.182775\n",
      "Epoch:  24  Training Loss:  41.5013  Training Accuracy:  0.191005\n",
      "Epoch:  25  Training Loss:  39.0279  Training Accuracy:  0.199236\n",
      "Epoch:  26  Training Loss:  36.6764  Training Accuracy:  0.209112\n",
      "Epoch:  27  Training Loss:  34.4191  Training Accuracy:  0.218225\n",
      "Epoch:  28  Training Loss:  32.3597  Training Accuracy:  0.226279\n",
      "Epoch:  29  Training Loss:  30.4267  Training Accuracy:  0.233333\n",
      "Epoch:  30  Training Loss:  28.5162  Training Accuracy:  0.241387\n",
      "Epoch:  31  Training Loss:  26.8186  Training Accuracy:  0.24856\n",
      "Epoch:  32  Training Loss:  25.2061  Training Accuracy:  0.255438\n",
      "Epoch:  33  Training Loss:  23.7268  Training Accuracy:  0.264256\n",
      "Epoch:  34  Training Loss:  22.3962  Training Accuracy:  0.270488\n",
      "Epoch:  35  Training Loss:  21.0569  Training Accuracy:  0.277013\n",
      "Epoch:  36  Training Loss:  19.8395  Training Accuracy:  0.284362\n",
      "Epoch:  37  Training Loss:  18.7729  Training Accuracy:  0.290123\n",
      "Epoch:  38  Training Loss:  17.6406  Training Accuracy:  0.295708\n",
      "Epoch:  39  Training Loss:  16.6765  Training Accuracy:  0.300764\n",
      "Epoch:  40  Training Loss:  15.8434  Training Accuracy:  0.307525\n",
      "Epoch:  41  Training Loss:  15.0391  Training Accuracy:  0.312992\n",
      "Epoch:  42  Training Loss:  14.2711  Training Accuracy:  0.319283\n",
      "Epoch:  43  Training Loss:  13.538  Training Accuracy:  0.324397\n",
      "Epoch:  44  Training Loss:  12.8807  Training Accuracy:  0.328689\n",
      "Epoch:  45  Training Loss:  12.2041  Training Accuracy:  0.333392\n",
      "Epoch:  46  Training Loss:  11.5927  Training Accuracy:  0.338272\n",
      "Epoch:  47  Training Loss:  11.0398  Training Accuracy:  0.343092\n",
      "Epoch:  48  Training Loss:  10.455  Training Accuracy:  0.347501\n",
      "Epoch:  49  Training Loss:  9.93449  Training Accuracy:  0.353028\n",
      "Epoch:  50  Training Loss:  9.42705  Training Accuracy:  0.357789\n",
      "Epoch:  51  Training Loss:  9.05486  Training Accuracy:  0.362728\n",
      "Epoch:  52  Training Loss:  8.58062  Training Accuracy:  0.366784\n",
      "Epoch:  53  Training Loss:  8.20216  Training Accuracy:  0.372134\n",
      "Epoch:  54  Training Loss:  7.82149  Training Accuracy:  0.375955\n",
      "Epoch:  55  Training Loss:  7.5162  Training Accuracy:  0.38007\n",
      "Epoch:  56  Training Loss:  7.20222  Training Accuracy:  0.384538\n",
      "Epoch:  57  Training Loss:  6.97186  Training Accuracy:  0.388301\n",
      "Epoch:  58  Training Loss:  6.69191  Training Accuracy:  0.392181\n",
      "Epoch:  59  Training Loss:  6.41794  Training Accuracy:  0.39659\n",
      "Epoch:  60  Training Loss:  6.20903  Training Accuracy:  0.399941\n",
      "Epoch:  61  Training Loss:  5.95553  Training Accuracy:  0.402528\n",
      "Epoch:  62  Training Loss:  5.77545  Training Accuracy:  0.405291\n",
      "Epoch:  63  Training Loss:  5.57413  Training Accuracy:  0.408818\n",
      "Epoch:  64  Training Loss:  5.37681  Training Accuracy:  0.411229\n",
      "Epoch:  65  Training Loss:  5.18388  Training Accuracy:  0.414286\n",
      "Epoch:  66  Training Loss:  5.02898  Training Accuracy:  0.416696\n",
      "Epoch:  67  Training Loss:  4.88722  Training Accuracy:  0.419694\n",
      "Epoch:  68  Training Loss:  4.74004  Training Accuracy:  0.422046\n",
      "Epoch:  69  Training Loss:  4.64705  Training Accuracy:  0.424339\n",
      "Epoch:  70  Training Loss:  4.54412  Training Accuracy:  0.426866\n",
      "Epoch:  71  Training Loss:  4.47102  Training Accuracy:  0.429336\n",
      "Epoch:  72  Training Loss:  4.33668  Training Accuracy:  0.432569\n",
      "Epoch:  73  Training Loss:  4.17059  Training Accuracy:  0.435744\n",
      "Epoch:  74  Training Loss:  4.05004  Training Accuracy:  0.437919\n",
      "Epoch:  75  Training Loss:  3.9241  Training Accuracy:  0.440447\n",
      "Epoch:  76  Training Loss:  3.79711  Training Accuracy:  0.442445\n",
      "Epoch:  77  Training Loss:  3.65905  Training Accuracy:  0.446326\n",
      "Epoch:  78  Training Loss:  3.52553  Training Accuracy:  0.448795\n",
      "Epoch:  79  Training Loss:  3.45998  Training Accuracy:  0.450735\n",
      "Epoch:  80  Training Loss:  3.3673  Training Accuracy:  0.453263\n",
      "Epoch:  81  Training Loss:  3.3051  Training Accuracy:  0.456085\n",
      "Epoch:  82  Training Loss:  3.23287  Training Accuracy:  0.458377\n",
      "Epoch:  83  Training Loss:  3.16546  Training Accuracy:  0.461258\n",
      "Epoch:  84  Training Loss:  3.11229  Training Accuracy:  0.463492\n",
      "Epoch:  85  Training Loss:  3.05302  Training Accuracy:  0.466373\n",
      "Epoch:  86  Training Loss:  3.01315  Training Accuracy:  0.468548\n",
      "Epoch:  87  Training Loss:  2.96206  Training Accuracy:  0.470194\n",
      "Epoch:  88  Training Loss:  2.90141  Training Accuracy:  0.471664\n",
      "Epoch:  89  Training Loss:  2.8711  Training Accuracy:  0.47331\n",
      "Epoch:  90  Training Loss:  2.82875  Training Accuracy:  0.475779\n",
      "Epoch:  91  Training Loss:  2.78784  Training Accuracy:  0.477366\n",
      "Epoch:  92  Training Loss:  2.72802  Training Accuracy:  0.4796\n",
      "Epoch:  93  Training Loss:  2.71031  Training Accuracy:  0.481246\n",
      "Epoch:  94  Training Loss:  2.6703  Training Accuracy:  0.483598\n",
      "Epoch:  95  Training Loss:  2.65442  Training Accuracy:  0.485244\n",
      "Epoch:  96  Training Loss:  2.63242  Training Accuracy:  0.487243\n",
      "Epoch:  97  Training Loss:  2.6079  Training Accuracy:  0.48883\n",
      "Epoch:  98  Training Loss:  2.55533  Training Accuracy:  0.490829\n",
      "Epoch:  99  Training Loss:  2.51917  Training Accuracy:  0.493063\n",
      "Epoch:  100  Training Loss:  2.47583  Training Accuracy:  0.494826\n",
      "Epoch:  101  Training Loss:  2.4326  Training Accuracy:  0.496355\n",
      "Epoch:  102  Training Loss:  2.40664  Training Accuracy:  0.498177\n",
      "Epoch:  103  Training Loss:  2.3758  Training Accuracy:  0.499941\n",
      "Epoch:  104  Training Loss:  2.35225  Training Accuracy:  0.501999\n",
      "Epoch:  105  Training Loss:  2.31991  Training Accuracy:  0.503821\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  65.4361  Training Accuracy:  0.0450911\n",
      "Epoch:  1  Training Loss:  74.5786  Training Accuracy:  0.0451499\n",
      "Epoch:  2  Training Loss:  77.3696  Training Accuracy:  0.0477954\n",
      "Epoch:  3  Training Loss:  74.0615  Training Accuracy:  0.0507936\n",
      "Epoch:  4  Training Loss:  68.255  Training Accuracy:  0.0548501\n",
      "Epoch:  5  Training Loss:  62.7934  Training Accuracy:  0.0579659\n",
      "Epoch:  6  Training Loss:  58.4304  Training Accuracy:  0.0627278\n",
      "Epoch:  7  Training Loss:  54.8783  Training Accuracy:  0.0687831\n",
      "Epoch:  8  Training Loss:  51.2769  Training Accuracy:  0.074368\n",
      "Epoch:  9  Training Loss:  47.7494  Training Accuracy:  0.0802469\n",
      "Epoch:  10  Training Loss:  44.4792  Training Accuracy:  0.0900059\n",
      "Epoch:  11  Training Loss:  41.376  Training Accuracy:  0.0987066\n",
      "Epoch:  12  Training Loss:  38.5132  Training Accuracy:  0.107525\n",
      "Epoch:  13  Training Loss:  35.9985  Training Accuracy:  0.115344\n",
      "Epoch:  14  Training Loss:  33.8671  Training Accuracy:  0.124809\n",
      "Epoch:  15  Training Loss:  31.9693  Training Accuracy:  0.132922\n",
      "Epoch:  16  Training Loss:  30.5315  Training Accuracy:  0.141564\n",
      "Epoch:  17  Training Loss:  29.0848  Training Accuracy:  0.149383\n",
      "Epoch:  18  Training Loss:  27.7345  Training Accuracy:  0.157907\n",
      "Epoch:  19  Training Loss:  26.3122  Training Accuracy:  0.166314\n",
      "Epoch:  20  Training Loss:  25.0097  Training Accuracy:  0.175544\n",
      "Epoch:  21  Training Loss:  23.6486  Training Accuracy:  0.183774\n",
      "Epoch:  22  Training Loss:  22.1823  Training Accuracy:  0.193004\n",
      "Epoch:  23  Training Loss:  20.7498  Training Accuracy:  0.199765\n",
      "Epoch:  24  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  25  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  26  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  27  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  28  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  29  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  30  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  31  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  32  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  33  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  34  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  35  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  36  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  37  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  38  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  39  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  40  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  41  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  42  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  43  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  44  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  45  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  46  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  47  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  48  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  49  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  50  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  51  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  52  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  53  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  54  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  55  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  56  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  57  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  58  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  59  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  60  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  61  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  62  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  63  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  64  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  65  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  66  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  67  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  68  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  69  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  70  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  71  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  72  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  73  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  74  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  75  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  76  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  77  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 5\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  69.7262  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  66.9482  Training Accuracy:  0.041505\n",
      "Epoch:  2  Training Loss:  61.5993  Training Accuracy:  0.0451499\n",
      "Epoch:  3  Training Loss:  56.999  Training Accuracy:  0.0503821\n",
      "Epoch:  4  Training Loss:  52.7865  Training Accuracy:  0.0557319\n",
      "Epoch:  5  Training Loss:  48.522  Training Accuracy:  0.0599647\n",
      "Epoch:  6  Training Loss:  44.2681  Training Accuracy:  0.0644327\n",
      "Epoch:  7  Training Loss:  39.8075  Training Accuracy:  0.0712522\n",
      "Epoch:  8  Training Loss:  35.6324  Training Accuracy:  0.0789536\n",
      "Epoch:  9  Training Loss:  31.7614  Training Accuracy:  0.0838918\n",
      "Epoch:  10  Training Loss:  28.2217  Training Accuracy:  0.0897707\n",
      "Epoch:  11  Training Loss:  25.0965  Training Accuracy:  0.0946502\n",
      "Epoch:  12  Training Loss:  22.2333  Training Accuracy:  0.0981775\n",
      "Epoch:  13  Training Loss:  19.8218  Training Accuracy:  0.100764\n",
      "Epoch:  14  Training Loss:  17.7094  Training Accuracy:  0.103233\n",
      "Epoch:  15  Training Loss:  15.8418  Training Accuracy:  0.106526\n",
      "Epoch:  16  Training Loss:  14.3256  Training Accuracy:  0.110641\n",
      "Epoch:  17  Training Loss:  12.9825  Training Accuracy:  0.115638\n",
      "Epoch:  18  Training Loss:  11.7207  Training Accuracy:  0.119342\n",
      "Epoch:  19  Training Loss:  10.6015  Training Accuracy:  0.123398\n",
      "Epoch:  20  Training Loss:  9.5426  Training Accuracy:  0.127807\n",
      "Epoch:  21  Training Loss:  8.62492  Training Accuracy:  0.133098\n",
      "Epoch:  22  Training Loss:  7.77816  Training Accuracy:  0.137684\n",
      "Epoch:  23  Training Loss:  7.00481  Training Accuracy:  0.142857\n",
      "Epoch:  24  Training Loss:  6.2912  Training Accuracy:  0.146972\n",
      "Epoch:  25  Training Loss:  5.5782  Training Accuracy:  0.151734\n",
      "Epoch:  26  Training Loss:  4.93537  Training Accuracy:  0.157672\n",
      "Epoch:  27  Training Loss:  4.36606  Training Accuracy:  0.162669\n",
      "Epoch:  28  Training Loss:  3.84657  Training Accuracy:  0.167901\n",
      "Epoch:  29  Training Loss:  3.40652  Training Accuracy:  0.17331\n",
      "Epoch:  30  Training Loss:  3.01193  Training Accuracy:  0.178483\n",
      "Epoch:  31  Training Loss:  2.65813  Training Accuracy:  0.182246\n",
      "Epoch:  32  Training Loss:  2.33902  Training Accuracy:  0.187713\n",
      "Epoch:  33  Training Loss:  2.08413  Training Accuracy:  0.192828\n",
      "Epoch:  34  Training Loss:  1.87275  Training Accuracy:  0.197061\n",
      "Epoch:  35  Training Loss:  1.66974  Training Accuracy:  0.202645\n",
      "Epoch:  36  Training Loss:  1.49162  Training Accuracy:  0.208995\n",
      "Epoch:  37  Training Loss:  1.34151  Training Accuracy:  0.213051\n",
      "Epoch:  38  Training Loss:  1.19347  Training Accuracy:  0.218283\n",
      "Epoch:  39  Training Loss:  1.08401  Training Accuracy:  0.223574\n",
      "Epoch:  40  Training Loss:  0.984065  Training Accuracy:  0.22863\n",
      "Epoch:  41  Training Loss:  0.891292  Training Accuracy:  0.233862\n",
      "Epoch:  42  Training Loss:  0.815183  Training Accuracy:  0.237978\n",
      "Epoch:  43  Training Loss:  0.754218  Training Accuracy:  0.242622\n",
      "Epoch:  44  Training Loss:  0.690551  Training Accuracy:  0.246384\n",
      "Epoch:  45  Training Loss:  0.640743  Training Accuracy:  0.250441\n",
      "Epoch:  46  Training Loss:  0.601151  Training Accuracy:  0.253263\n",
      "Epoch:  47  Training Loss:  0.563896  Training Accuracy:  0.257143\n",
      "Epoch:  48  Training Loss:  0.530632  Training Accuracy:  0.260846\n",
      "Epoch:  49  Training Loss:  0.501389  Training Accuracy:  0.263727\n",
      "Epoch:  50  Training Loss:  0.47503  Training Accuracy:  0.26749\n",
      "Epoch:  51  Training Loss:  0.446536  Training Accuracy:  0.271193\n",
      "Epoch:  52  Training Loss:  0.425161  Training Accuracy:  0.274662\n",
      "Epoch:  53  Training Loss:  0.402143  Training Accuracy:  0.277543\n",
      "Epoch:  54  Training Loss:  0.381916  Training Accuracy:  0.280541\n",
      "Epoch:  55  Training Loss:  0.363631  Training Accuracy:  0.283657\n",
      "Epoch:  56  Training Loss:  0.345084  Training Accuracy:  0.286478\n",
      "Epoch:  57  Training Loss:  0.328983  Training Accuracy:  0.290241\n",
      "Epoch:  58  Training Loss:  0.308703  Training Accuracy:  0.29318\n",
      "Epoch:  59  Training Loss:  0.292971  Training Accuracy:  0.295708\n",
      "Epoch:  60  Training Loss:  0.275013  Training Accuracy:  0.297942\n",
      "Epoch:  61  Training Loss:  0.262662  Training Accuracy:  0.300999\n",
      "Epoch:  62  Training Loss:  0.249828  Training Accuracy:  0.304115\n",
      "Epoch:  63  Training Loss:  0.239827  Training Accuracy:  0.306937\n",
      "Epoch:  64  Training Loss:  0.229177  Training Accuracy:  0.3107\n",
      "Epoch:  65  Training Loss:  0.218138  Training Accuracy:  0.313521\n",
      "Epoch:  66  Training Loss:  0.21099  Training Accuracy:  0.316872\n",
      "Epoch:  67  Training Loss:  0.205731  Training Accuracy:  0.319459\n",
      "Epoch:  68  Training Loss:  0.198921  Training Accuracy:  0.322575\n",
      "Epoch:  69  Training Loss:  0.191036  Training Accuracy:  0.325044\n",
      "Epoch:  70  Training Loss:  0.185471  Training Accuracy:  0.327983\n",
      "Epoch:  71  Training Loss:  0.180497  Training Accuracy:  0.331158\n",
      "Epoch:  72  Training Loss:  0.17155  Training Accuracy:  0.333804\n",
      "Epoch:  73  Training Loss:  0.163369  Training Accuracy:  0.336214\n",
      "Epoch:  74  Training Loss:  0.156175  Training Accuracy:  0.338448\n",
      "Epoch:  75  Training Loss:  0.150524  Training Accuracy:  0.342034\n",
      "Epoch:  76  Training Loss:  0.144566  Training Accuracy:  0.34562\n",
      "Epoch:  77  Training Loss:  0.14002  Training Accuracy:  0.347443\n",
      "Epoch:  78  Training Loss:  0.134756  Training Accuracy:  0.350147\n",
      "Epoch:  79  Training Loss:  0.129788  Training Accuracy:  0.352146\n",
      "Epoch:  80  Training Loss:  0.124766  Training Accuracy:  0.354791\n",
      "Epoch:  81  Training Loss:  0.120394  Training Accuracy:  0.357731\n",
      "Epoch:  82  Training Loss:  0.117502  Training Accuracy:  0.361552\n",
      "Epoch:  83  Training Loss:  0.112864  Training Accuracy:  0.364433\n",
      "Epoch:  84  Training Loss:  0.109497  Training Accuracy:  0.367254\n",
      "Epoch:  85  Training Loss:  0.106201  Training Accuracy:  0.369488\n",
      "Epoch:  86  Training Loss:  0.102531  Training Accuracy:  0.371958\n",
      "Epoch:  87  Training Loss:  0.0981943  Training Accuracy:  0.374133\n",
      "Epoch:  88  Training Loss:  0.0951869  Training Accuracy:  0.377072\n",
      "Epoch:  89  Training Loss:  0.0925819  Training Accuracy:  0.379306\n",
      "Epoch:  90  Training Loss:  0.0884952  Training Accuracy:  0.381834\n",
      "Epoch:  91  Training Loss:  0.0855832  Training Accuracy:  0.384362\n",
      "Epoch:  92  Training Loss:  0.0830908  Training Accuracy:  0.38689\n",
      "Epoch:  93  Training Loss:  0.0808336  Training Accuracy:  0.389712\n",
      "Epoch:  94  Training Loss:  0.0798286  Training Accuracy:  0.392651\n",
      "Epoch:  95  Training Loss:  0.0775888  Training Accuracy:  0.395649\n",
      "Epoch:  96  Training Loss:  0.0754824  Training Accuracy:  0.398354\n",
      "Epoch:  97  Training Loss:  0.0734549  Training Accuracy:  0.401293\n",
      "Epoch:  98  Training Loss:  0.0711003  Training Accuracy:  0.404821\n",
      "Epoch:  99  Training Loss:  0.069198  Training Accuracy:  0.407407\n",
      "Epoch:  100  Training Loss:  0.0674405  Training Accuracy:  0.410464\n",
      "Epoch:  101  Training Loss:  0.0650344  Training Accuracy:  0.41358\n",
      "Epoch:  102  Training Loss:  0.0630999  Training Accuracy:  0.416167\n",
      "Epoch:  103  Training Loss:  0.061106  Training Accuracy:  0.418518\n",
      "Epoch:  104  Training Loss:  0.0586662  Training Accuracy:  0.421869\n",
      "Epoch:  105  Training Loss:  0.056362  Training Accuracy:  0.424691\n",
      "Epoch:  106  Training Loss:  0.0541602  Training Accuracy:  0.427337\n",
      "Epoch:  107  Training Loss:  0.0522922  Training Accuracy:  0.431393\n",
      "Epoch:  108  Training Loss:  0.0505633  Training Accuracy:  0.434568\n",
      "Epoch:  109  Training Loss:  0.0492426  Training Accuracy:  0.439624\n",
      "Epoch:  110  Training Loss:  0.0478792  Training Accuracy:  0.443269\n",
      "Epoch:  111  Training Loss:  0.0464842  Training Accuracy:  0.445679\n",
      "Epoch:  112  Training Loss:  0.0452026  Training Accuracy:  0.4495\n",
      "Epoch:  113  Training Loss:  0.0438256  Training Accuracy:  0.451558\n",
      "Epoch:  114  Training Loss:  0.0427965  Training Accuracy:  0.455438\n",
      "Epoch:  115  Training Loss:  0.0416018  Training Accuracy:  0.457378\n",
      "Epoch:  116  Training Loss:  0.0400218  Training Accuracy:  0.4602\n",
      "Epoch:  117  Training Loss:  0.038856  Training Accuracy:  0.462963\n",
      "Epoch:  118  Training Loss:  0.0377117  Training Accuracy:  0.466314\n",
      "Epoch:  119  Training Loss:  0.0361967  Training Accuracy:  0.469253\n",
      "Epoch:  120  Training Loss:  0.0351206  Training Accuracy:  0.473016\n",
      "Epoch:  121  Training Loss:  0.0338979  Training Accuracy:  0.475896\n",
      "Epoch:  122  Training Loss:  0.0328176  Training Accuracy:  0.478836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  123  Training Loss:  0.0316841  Training Accuracy:  0.481364\n",
      "Epoch:  124  Training Loss:  0.0306716  Training Accuracy:  0.483539\n",
      "Epoch:  125  Training Loss:  0.0297519  Training Accuracy:  0.486537\n",
      "Epoch:  126  Training Loss:  0.0290915  Training Accuracy:  0.488301\n",
      "Epoch:  127  Training Loss:  0.0279839  Training Accuracy:  0.491417\n",
      "Epoch:  128  Training Loss:  0.0270411  Training Accuracy:  0.494415\n",
      "Epoch:  129  Training Loss:  0.0262986  Training Accuracy:  0.497296\n",
      "Epoch:  130  Training Loss:  0.0252793  Training Accuracy:  0.499294\n",
      "Epoch:  131  Training Loss:  0.0242238  Training Accuracy:  0.501528\n",
      "Epoch:  132  Training Loss:  0.0233571  Training Accuracy:  0.503998\n",
      "Epoch:  133  Training Loss:  0.0223578  Training Accuracy:  0.506055\n",
      "Epoch:  134  Training Loss:  0.0215175  Training Accuracy:  0.509289\n",
      "Epoch:  135  Training Loss:  0.0206517  Training Accuracy:  0.512757\n",
      "Epoch:  136  Training Loss:  0.019764  Training Accuracy:  0.515755\n",
      "Epoch:  137  Training Loss:  0.0191297  Training Accuracy:  0.518577\n",
      "Epoch:  138  Training Loss:  0.0186306  Training Accuracy:  0.521634\n",
      "Epoch:  139  Training Loss:  0.017994  Training Accuracy:  0.524162\n",
      "Epoch:  140  Training Loss:  0.0174789  Training Accuracy:  0.52669\n",
      "Epoch:  141  Training Loss:  0.0170825  Training Accuracy:  0.529982\n",
      "Epoch:  142  Training Loss:  0.016545  Training Accuracy:  0.532745\n",
      "Epoch:  143  Training Loss:  0.0161802  Training Accuracy:  0.535391\n",
      "Epoch:  144  Training Loss:  0.0157682  Training Accuracy:  0.537919\n",
      "Epoch:  145  Training Loss:  0.0153775  Training Accuracy:  0.540447\n",
      "Epoch:  146  Training Loss:  0.0149295  Training Accuracy:  0.543445\n",
      "Epoch:  147  Training Loss:  0.0146071  Training Accuracy:  0.546208\n",
      "Epoch:  148  Training Loss:  0.0141091  Training Accuracy:  0.549089\n",
      "Epoch:  149  Training Loss:  0.0138296  Training Accuracy:  0.55097\n",
      "Epoch:  150  Training Loss:  0.0135531  Training Accuracy:  0.553733\n",
      "Epoch:  151  Training Loss:  0.0131853  Training Accuracy:  0.556496\n",
      "Epoch:  152  Training Loss:  0.0129086  Training Accuracy:  0.558965\n",
      "Epoch:  153  Training Loss:  0.0126387  Training Accuracy:  0.560905\n",
      "Epoch:  154  Training Loss:  0.0124376  Training Accuracy:  0.563492\n",
      "Epoch:  155  Training Loss:  0.0121529  Training Accuracy:  0.566137\n",
      "Epoch:  156  Training Loss:  0.0118508  Training Accuracy:  0.568959\n",
      "Epoch:  157  Training Loss:  0.0115645  Training Accuracy:  0.571193\n",
      "Epoch:  158  Training Loss:  0.011421  Training Accuracy:  0.573839\n",
      "Epoch:  159  Training Loss:  0.0112735  Training Accuracy:  0.575955\n",
      "Epoch:  160  Training Loss:  0.0110581  Training Accuracy:  0.578424\n",
      "Epoch:  161  Training Loss:  0.0109499  Training Accuracy:  0.580364\n",
      "Epoch:  162  Training Loss:  0.0107773  Training Accuracy:  0.582951\n",
      "Epoch:  163  Training Loss:  0.0105966  Training Accuracy:  0.585773\n",
      "Epoch:  164  Training Loss:  0.0105649  Training Accuracy:  0.588713\n",
      "Epoch:  165  Training Loss:  0.0104457  Training Accuracy:  0.591182\n",
      "Epoch:  166  Training Loss:  0.0103333  Training Accuracy:  0.594003\n",
      "Epoch:  167  Training Loss:  0.0103441  Training Accuracy:  0.596767\n",
      "Epoch:  168  Training Loss:  0.0102433  Training Accuracy:  0.599059\n",
      "Epoch:  169  Training Loss:  0.0102263  Training Accuracy:  0.601705\n",
      "Epoch:  170  Training Loss:  0.0101564  Training Accuracy:  0.603645\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 5\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  113.665  Training Accuracy:  0.0717813\n",
      "Epoch:  1  Training Loss:  66.5284  Training Accuracy:  0.0503233\n",
      "Epoch:  2  Training Loss:  70.4829  Training Accuracy:  0.0620223\n",
      "Epoch:  3  Training Loss:  72.4087  Training Accuracy:  0.0699001\n",
      "Epoch:  4  Training Loss:  73.4326  Training Accuracy:  0.0784245\n",
      "Epoch:  5  Training Loss:  73.7902  Training Accuracy:  0.0862434\n",
      "Epoch:  6  Training Loss:  73.6133  Training Accuracy:  0.0972957\n",
      "Epoch:  7  Training Loss:  73.0214  Training Accuracy:  0.104468\n",
      "Epoch:  8  Training Loss:  72.08  Training Accuracy:  0.109289\n",
      "Epoch:  9  Training Loss:  70.9796  Training Accuracy:  0.114815\n",
      "Epoch:  10  Training Loss:  69.6447  Training Accuracy:  0.120635\n",
      "Epoch:  11  Training Loss:  68.193  Training Accuracy:  0.127866\n",
      "Epoch:  12  Training Loss:  66.6143  Training Accuracy:  0.13545\n",
      "Epoch:  13  Training Loss:  64.9021  Training Accuracy:  0.14221\n",
      "Epoch:  14  Training Loss:  63.1572  Training Accuracy:  0.148854\n",
      "Epoch:  15  Training Loss:  61.386  Training Accuracy:  0.156085\n",
      "Epoch:  16  Training Loss:  59.6612  Training Accuracy:  0.162845\n",
      "Epoch:  17  Training Loss:  57.9848  Training Accuracy:  0.169195\n",
      "Epoch:  18  Training Loss:  56.3798  Training Accuracy:  0.177425\n",
      "Epoch:  19  Training Loss:  54.8957  Training Accuracy:  0.183951\n",
      "Epoch:  20  Training Loss:  53.4817  Training Accuracy:  0.19224\n",
      "Epoch:  21  Training Loss:  52.2403  Training Accuracy:  0.199824\n",
      "Epoch:  22  Training Loss:  51.0989  Training Accuracy:  0.208466\n",
      "Epoch:  23  Training Loss:  50.0557  Training Accuracy:  0.216461\n",
      "Epoch:  24  Training Loss:  49.1303  Training Accuracy:  0.22575\n",
      "Epoch:  25  Training Loss:  48.2887  Training Accuracy:  0.231452\n",
      "Epoch:  26  Training Loss:  47.5845  Training Accuracy:  0.236508\n",
      "Epoch:  27  Training Loss:  46.983  Training Accuracy:  0.242034\n",
      "Epoch:  28  Training Loss:  46.4643  Training Accuracy:  0.247795\n",
      "Epoch:  29  Training Loss:  45.9903  Training Accuracy:  0.254791\n",
      "Epoch:  30  Training Loss:  45.6403  Training Accuracy:  0.260376\n",
      "Epoch:  31  Training Loss:  45.3284  Training Accuracy:  0.266255\n",
      "Epoch:  32  Training Loss:  45.0759  Training Accuracy:  0.273369\n",
      "Epoch:  33  Training Loss:  44.8998  Training Accuracy:  0.279071\n",
      "Epoch:  34  Training Loss:  44.7772  Training Accuracy:  0.284715\n",
      "Epoch:  35  Training Loss:  44.6969  Training Accuracy:  0.289947\n",
      "Epoch:  36  Training Loss:  44.6471  Training Accuracy:  0.29512\n",
      "Epoch:  37  Training Loss:  44.6583  Training Accuracy:  0.301587\n",
      "Epoch:  38  Training Loss:  44.6892  Training Accuracy:  0.307995\n",
      "Epoch:  39  Training Loss:  44.7141  Training Accuracy:  0.314874\n",
      "Epoch:  40  Training Loss:  44.7822  Training Accuracy:  0.319929\n",
      "Epoch:  41  Training Loss:  44.8958  Training Accuracy:  0.324397\n",
      "Epoch:  42  Training Loss:  44.9879  Training Accuracy:  0.330394\n",
      "Epoch:  43  Training Loss:  45.1132  Training Accuracy:  0.335273\n",
      "Epoch:  44  Training Loss:  45.2268  Training Accuracy:  0.340741\n",
      "Epoch:  45  Training Loss:  45.3708  Training Accuracy:  0.344915\n",
      "Epoch:  46  Training Loss:  45.5135  Training Accuracy:  0.350911\n",
      "Epoch:  47  Training Loss:  45.6693  Training Accuracy:  0.356026\n",
      "Epoch:  48  Training Loss:  45.8285  Training Accuracy:  0.360905\n",
      "Epoch:  49  Training Loss:  45.9463  Training Accuracy:  0.365197\n",
      "Epoch:  50  Training Loss:  46.1452  Training Accuracy:  0.3699\n",
      "Epoch:  51  Training Loss:  46.3076  Training Accuracy:  0.373427\n",
      "Epoch:  52  Training Loss:  46.4425  Training Accuracy:  0.377366\n",
      "Epoch:  53  Training Loss:  46.5864  Training Accuracy:  0.380776\n",
      "Epoch:  54  Training Loss:  46.7222  Training Accuracy:  0.383715\n",
      "Epoch:  55  Training Loss:  46.8523  Training Accuracy:  0.387066\n",
      "Epoch:  56  Training Loss:  46.9524  Training Accuracy:  0.391064\n",
      "Epoch:  57  Training Loss:  47.0884  Training Accuracy:  0.394944\n",
      "Epoch:  58  Training Loss:  47.197  Training Accuracy:  0.398177\n",
      "Epoch:  59  Training Loss:  47.3602  Training Accuracy:  0.402058\n",
      "Epoch:  60  Training Loss:  47.4729  Training Accuracy:  0.404703\n",
      "Epoch:  61  Training Loss:  47.609  Training Accuracy:  0.407701\n",
      "Epoch:  62  Training Loss:  47.722  Training Accuracy:  0.411699\n",
      "Epoch:  63  Training Loss:  47.8605  Training Accuracy:  0.414991\n",
      "Epoch:  64  Training Loss:  47.9944  Training Accuracy:  0.418283\n",
      "Epoch:  65  Training Loss:  48.0905  Training Accuracy:  0.421693\n",
      "Epoch:  66  Training Loss:  48.2117  Training Accuracy:  0.424515\n",
      "Epoch:  67  Training Loss:  48.3272  Training Accuracy:  0.427454\n",
      "Epoch:  68  Training Loss:  48.4374  Training Accuracy:  0.430746\n",
      "Epoch:  69  Training Loss:  48.5496  Training Accuracy:  0.434097\n",
      "Epoch:  70  Training Loss:  48.6678  Training Accuracy:  0.437096\n",
      "Epoch:  71  Training Loss:  48.7561  Training Accuracy:  0.439682\n",
      "Epoch:  72  Training Loss:  48.877  Training Accuracy:  0.442269\n",
      "Epoch:  73  Training Loss:  48.9925  Training Accuracy:  0.444503\n",
      "Epoch:  74  Training Loss:  49.0543  Training Accuracy:  0.448089\n",
      "Epoch:  75  Training Loss:  49.166  Training Accuracy:  0.450676\n",
      "Epoch:  76  Training Loss:  49.2404  Training Accuracy:  0.452675\n",
      "Epoch:  77  Training Loss:  49.3434  Training Accuracy:  0.455673\n",
      "Epoch:  78  Training Loss:  49.418  Training Accuracy:  0.458906\n",
      "Epoch:  79  Training Loss:  49.5342  Training Accuracy:  0.461082\n",
      "Epoch:  80  Training Loss:  49.5802  Training Accuracy:  0.46361\n",
      "Epoch:  81  Training Loss:  49.6707  Training Accuracy:  0.466137\n",
      "Epoch:  82  Training Loss:  49.7303  Training Accuracy:  0.468842\n",
      "Epoch:  83  Training Loss:  49.8091  Training Accuracy:  0.471899\n",
      "Epoch:  84  Training Loss:  49.8438  Training Accuracy:  0.474427\n",
      "Epoch:  85  Training Loss:  49.9554  Training Accuracy:  0.476896\n",
      "Epoch:  86  Training Loss:  49.9772  Training Accuracy:  0.479659\n",
      "Epoch:  87  Training Loss:  50.0266  Training Accuracy:  0.483127\n",
      "Epoch:  88  Training Loss:  50.1078  Training Accuracy:  0.485126\n",
      "Epoch:  89  Training Loss:  50.1764  Training Accuracy:  0.487243\n",
      "Epoch:  90  Training Loss:  50.1994  Training Accuracy:  0.489359\n",
      "Epoch:  91  Training Loss:  50.268  Training Accuracy:  0.492534\n",
      "Epoch:  92  Training Loss:  50.3061  Training Accuracy:  0.494474\n",
      "Epoch:  93  Training Loss:  50.3591  Training Accuracy:  0.497354\n",
      "Epoch:  94  Training Loss:  50.4159  Training Accuracy:  0.500411\n",
      "Epoch:  95  Training Loss:  50.4764  Training Accuracy:  0.503292\n",
      "Epoch:  96  Training Loss:  50.5326  Training Accuracy:  0.50629\n",
      "Epoch:  97  Training Loss:  50.6278  Training Accuracy:  0.508407\n",
      "Epoch:  98  Training Loss:  50.6707  Training Accuracy:  0.511581\n",
      "Epoch:  99  Training Loss:  50.7412  Training Accuracy:  0.513992\n",
      "Epoch:  100  Training Loss:  50.8312  Training Accuracy:  0.516402\n",
      "Epoch:  101  Training Loss:  50.9146  Training Accuracy:  0.519165\n",
      "Epoch:  102  Training Loss:  50.9722  Training Accuracy:  0.521281\n",
      "Epoch:  103  Training Loss:  51.0405  Training Accuracy:  0.524103\n",
      "Epoch:  104  Training Loss:  51.144  Training Accuracy:  0.526514\n",
      "Epoch:  105  Training Loss:  51.185  Training Accuracy:  0.528042\n",
      "Epoch:  106  Training Loss:  51.3115  Training Accuracy:  0.530923\n",
      "Epoch:  107  Training Loss:  51.3748  Training Accuracy:  0.534391\n",
      "Epoch:  108  Training Loss:  51.4473  Training Accuracy:  0.537213\n",
      "Epoch:  109  Training Loss:  51.502  Training Accuracy:  0.540505\n",
      "Epoch:  110  Training Loss:  51.6631  Training Accuracy:  0.543621\n",
      "Epoch:  111  Training Loss:  51.7246  Training Accuracy:  0.547031\n",
      "Epoch:  112  Training Loss:  51.7597  Training Accuracy:  0.549794\n",
      "Epoch:  113  Training Loss:  51.915  Training Accuracy:  0.552851\n",
      "Epoch:  114  Training Loss:  51.9598  Training Accuracy:  0.554556\n",
      "Epoch:  115  Training Loss:  52.065  Training Accuracy:  0.555908\n",
      "Epoch:  116  Training Loss:  52.192  Training Accuracy:  0.558495\n",
      "Epoch:  117  Training Loss:  52.3506  Training Accuracy:  0.560141\n",
      "Epoch:  118  Training Loss:  52.4284  Training Accuracy:  0.562316\n",
      "Epoch:  119  Training Loss:  52.533  Training Accuracy:  0.564315\n",
      "Epoch:  120  Training Loss:  52.5846  Training Accuracy:  0.566608\n",
      "Epoch:  121  Training Loss:  52.7246  Training Accuracy:  0.568783\n",
      "Epoch:  122  Training Loss:  52.8193  Training Accuracy:  0.571017\n",
      "Epoch:  123  Training Loss:  52.9679  Training Accuracy:  0.572546\n",
      "Epoch:  124  Training Loss:  53.0131  Training Accuracy:  0.574721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  53.1867  Training Accuracy:  0.576426\n",
      "Epoch:  126  Training Loss:  53.2513  Training Accuracy:  0.578483\n",
      "Epoch:  127  Training Loss:  53.4364  Training Accuracy:  0.579483\n",
      "Epoch:  128  Training Loss:  53.5102  Training Accuracy:  0.580952\n",
      "Epoch:  129  Training Loss:  53.6514  Training Accuracy:  0.583186\n",
      "Epoch:  130  Training Loss:  53.7368  Training Accuracy:  0.585185\n",
      "Epoch:  131  Training Loss:  53.8742  Training Accuracy:  0.588007\n",
      "Epoch:  132  Training Loss:  53.9278  Training Accuracy:  0.58983\n",
      "Epoch:  133  Training Loss:  54.1115  Training Accuracy:  0.591534\n",
      "Epoch:  134  Training Loss:  54.1501  Training Accuracy:  0.593357\n",
      "Epoch:  135  Training Loss:  54.2919  Training Accuracy:  0.594768\n",
      "Epoch:  136  Training Loss:  54.3606  Training Accuracy:  0.596414\n",
      "Epoch:  137  Training Loss:  54.5238  Training Accuracy:  0.59853\n",
      "Epoch:  138  Training Loss:  54.6116  Training Accuracy:  0.599882\n",
      "Epoch:  139  Training Loss:  54.6699  Training Accuracy:  0.60147\n",
      "Epoch:  140  Training Loss:  54.7743  Training Accuracy:  0.604174\n",
      "Epoch:  141  Training Loss:  54.9269  Training Accuracy:  0.605879\n",
      "Epoch:  142  Training Loss:  54.9461  Training Accuracy:  0.607055\n",
      "Epoch:  143  Training Loss:  55.0862  Training Accuracy:  0.609053\n",
      "Epoch:  144  Training Loss:  55.1848  Training Accuracy:  0.610582\n",
      "Epoch:  145  Training Loss:  55.3426  Training Accuracy:  0.612287\n",
      "Epoch:  146  Training Loss:  55.3737  Training Accuracy:  0.613874\n",
      "Epoch:  147  Training Loss:  55.4973  Training Accuracy:  0.615755\n",
      "Epoch:  148  Training Loss:  55.6553  Training Accuracy:  0.616872\n",
      "Epoch:  149  Training Loss:  55.7062  Training Accuracy:  0.618048\n",
      "Epoch:  150  Training Loss:  55.8274  Training Accuracy:  0.619812\n",
      "Epoch:  151  Training Loss:  55.9793  Training Accuracy:  0.620752\n",
      "Epoch:  152  Training Loss:  56.0102  Training Accuracy:  0.62234\n",
      "Epoch:  153  Training Loss:  56.1283  Training Accuracy:  0.624162\n",
      "Epoch:  154  Training Loss:  56.2307  Training Accuracy:  0.625397\n",
      "Epoch:  155  Training Loss:  56.2863  Training Accuracy:  0.626984\n",
      "Epoch:  156  Training Loss:  56.4158  Training Accuracy:  0.628336\n",
      "Epoch:  157  Training Loss:  56.5474  Training Accuracy:  0.629806\n",
      "Epoch:  158  Training Loss:  56.629  Training Accuracy:  0.631628\n",
      "Epoch:  159  Training Loss:  56.6828  Training Accuracy:  0.632687\n",
      "Epoch:  160  Training Loss:  56.8229  Training Accuracy:  0.63351\n",
      "Epoch:  161  Training Loss:  56.8303  Training Accuracy:  0.634803\n",
      "Epoch:  162  Training Loss:  56.9711  Training Accuracy:  0.635744\n",
      "Epoch:  163  Training Loss:  56.9924  Training Accuracy:  0.636919\n",
      "Epoch:  164  Training Loss:  57.1832  Training Accuracy:  0.638095\n",
      "Epoch:  165  Training Loss:  57.2001  Training Accuracy:  0.639154\n",
      "Epoch:  166  Training Loss:  57.3277  Training Accuracy:  0.640035\n",
      "Epoch:  167  Training Loss:  57.323  Training Accuracy:  0.64127\n",
      "Epoch:  168  Training Loss:  57.4168  Training Accuracy:  0.642034\n",
      "Epoch:  169  Training Loss:  57.5145  Training Accuracy:  0.643504\n",
      "Epoch:  170  Training Loss:  57.5284  Training Accuracy:  0.644738\n",
      "Epoch:  171  Training Loss:  57.6013  Training Accuracy:  0.646502\n",
      "Epoch:  172  Training Loss:  57.6825  Training Accuracy:  0.64756\n",
      "Epoch:  173  Training Loss:  57.7186  Training Accuracy:  0.648736\n",
      "Epoch:  174  Training Loss:  57.7207  Training Accuracy:  0.650029\n",
      "Epoch:  175  Training Loss:  57.7844  Training Accuracy:  0.650794\n",
      "Epoch:  176  Training Loss:  57.8705  Training Accuracy:  0.651969\n",
      "Epoch:  177  Training Loss:  57.9154  Training Accuracy:  0.653263\n",
      "Epoch:  178  Training Loss:  57.9343  Training Accuracy:  0.65391\n",
      "Epoch:  179  Training Loss:  58.0478  Training Accuracy:  0.655144\n",
      "Epoch:  180  Training Loss:  58.0766  Training Accuracy:  0.656496\n",
      "Epoch:  181  Training Loss:  58.1052  Training Accuracy:  0.658142\n",
      "Epoch:  182  Training Loss:  58.1749  Training Accuracy:  0.659612\n",
      "Epoch:  183  Training Loss:  58.239  Training Accuracy:  0.660318\n",
      "Epoch:  184  Training Loss:  58.2809  Training Accuracy:  0.661317\n",
      "Epoch:  185  Training Loss:  58.3248  Training Accuracy:  0.662375\n",
      "Epoch:  186  Training Loss:  58.311  Training Accuracy:  0.663962\n",
      "Epoch:  187  Training Loss:  58.4225  Training Accuracy:  0.664786\n",
      "Epoch:  188  Training Loss:  58.4082  Training Accuracy:  0.665667\n",
      "Epoch:  189  Training Loss:  58.4637  Training Accuracy:  0.666902\n",
      "Epoch:  190  Training Loss:  58.4952  Training Accuracy:  0.667725\n",
      "Epoch:  191  Training Loss:  58.5315  Training Accuracy:  0.668724\n",
      "Epoch:  192  Training Loss:  58.5288  Training Accuracy:  0.669547\n",
      "Epoch:  193  Training Loss:  58.6169  Training Accuracy:  0.67037\n",
      "Epoch:  194  Training Loss:  58.6473  Training Accuracy:  0.671193\n",
      "Epoch:  195  Training Loss:  58.6227  Training Accuracy:  0.672604\n",
      "Epoch:  196  Training Loss:  58.6397  Training Accuracy:  0.673839\n",
      "Epoch:  197  Training Loss:  58.7211  Training Accuracy:  0.674486\n",
      "Epoch:  198  Training Loss:  58.7329  Training Accuracy:  0.675955\n",
      "Epoch:  199  Training Loss:  58.7876  Training Accuracy:  0.677131\n",
      "Epoch:  200  Training Loss:  58.8368  Training Accuracy:  0.678425\n",
      "Epoch:  201  Training Loss:  58.8137  Training Accuracy:  0.679953\n",
      "Epoch:  202  Training Loss:  58.8746  Training Accuracy:  0.680776\n",
      "Epoch:  203  Training Loss:  58.9452  Training Accuracy:  0.681776\n",
      "Epoch:  204  Training Loss:  58.9705  Training Accuracy:  0.682775\n",
      "Epoch:  205  Training Loss:  59.0071  Training Accuracy:  0.683716\n",
      "Epoch:  206  Training Loss:  59.01  Training Accuracy:  0.68495\n",
      "Epoch:  207  Training Loss:  59.0653  Training Accuracy:  0.686185\n",
      "Epoch:  208  Training Loss:  59.0943  Training Accuracy:  0.687184\n",
      "Epoch:  209  Training Loss:  59.0974  Training Accuracy:  0.688477\n",
      "Epoch:  210  Training Loss:  59.0906  Training Accuracy:  0.689653\n",
      "Epoch:  211  Training Loss:  59.159  Training Accuracy:  0.690653\n",
      "Epoch:  212  Training Loss:  59.1878  Training Accuracy:  0.691358\n",
      "Epoch:  213  Training Loss:  59.2085  Training Accuracy:  0.692534\n",
      "Epoch:  214  Training Loss:  59.2484  Training Accuracy:  0.693533\n",
      "Epoch:  215  Training Loss:  59.2551  Training Accuracy:  0.694415\n",
      "Epoch:  216  Training Loss:  59.2473  Training Accuracy:  0.69565\n",
      "Epoch:  217  Training Loss:  59.2972  Training Accuracy:  0.696414\n",
      "Epoch:  218  Training Loss:  59.2896  Training Accuracy:  0.697884\n",
      "Epoch:  219  Training Loss:  59.2936  Training Accuracy:  0.698472\n",
      "Epoch:  220  Training Loss:  59.295  Training Accuracy:  0.699706\n",
      "Epoch:  221  Training Loss:  59.3206  Training Accuracy:  0.700823\n",
      "Epoch:  222  Training Loss:  59.3309  Training Accuracy:  0.702117\n",
      "Epoch:  223  Training Loss:  59.354  Training Accuracy:  0.703175\n",
      "Epoch:  224  Training Loss:  59.3626  Training Accuracy:  0.703821\n",
      "Epoch:  225  Training Loss:  59.392  Training Accuracy:  0.70488\n",
      "Epoch:  226  Training Loss:  59.3956  Training Accuracy:  0.705879\n",
      "Epoch:  227  Training Loss:  59.4372  Training Accuracy:  0.707055\n",
      "Epoch:  228  Training Loss:  59.4817  Training Accuracy:  0.707819\n",
      "Epoch:  229  Training Loss:  59.5458  Training Accuracy:  0.70876\n",
      "Epoch:  230  Training Loss:  59.5781  Training Accuracy:  0.709406\n",
      "Epoch:  231  Training Loss:  59.6124  Training Accuracy:  0.710817\n",
      "Epoch:  232  Training Loss:  59.6849  Training Accuracy:  0.711699\n",
      "Epoch:  233  Training Loss:  59.6777  Training Accuracy:  0.712522\n",
      "Epoch:  234  Training Loss:  59.7278  Training Accuracy:  0.71358\n",
      "Epoch:  235  Training Loss:  59.794  Training Accuracy:  0.714227\n",
      "Epoch:  236  Training Loss:  59.8152  Training Accuracy:  0.715109\n",
      "Epoch:  237  Training Loss:  59.8268  Training Accuracy:  0.715697\n",
      "Epoch:  238  Training Loss:  59.9123  Training Accuracy:  0.71652\n",
      "Epoch:  239  Training Loss:  59.9163  Training Accuracy:  0.717049\n",
      "Epoch:  240  Training Loss:  59.8954  Training Accuracy:  0.718225\n",
      "Epoch:  241  Training Loss:  59.9633  Training Accuracy:  0.71893\n",
      "Epoch:  242  Training Loss:  59.9534  Training Accuracy:  0.7194\n",
      "Epoch:  243  Training Loss:  59.9713  Training Accuracy:  0.7204\n",
      "Epoch:  244  Training Loss:  59.9806  Training Accuracy:  0.721164\n",
      "Epoch:  245  Training Loss:  59.9989  Training Accuracy:  0.722164\n",
      "Epoch:  246  Training Loss:  60.0145  Training Accuracy:  0.722222\n",
      "Epoch:  247  Training Loss:  59.9818  Training Accuracy:  0.723222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  60.0481  Training Accuracy:  0.724045\n",
      "Epoch:  249  Training Loss:  60.038  Training Accuracy:  0.724633\n",
      "Epoch:  250  Training Loss:  60.0794  Training Accuracy:  0.725573\n",
      "Epoch:  251  Training Loss:  60.1391  Training Accuracy:  0.726749\n",
      "Epoch:  252  Training Loss:  60.1612  Training Accuracy:  0.728337\n",
      "Epoch:  253  Training Loss:  60.1166  Training Accuracy:  0.729453\n",
      "Epoch:  254  Training Loss:  60.1867  Training Accuracy:  0.730218\n",
      "Epoch:  255  Training Loss:  60.1858  Training Accuracy:  0.731041\n",
      "Epoch:  256  Training Loss:  60.2171  Training Accuracy:  0.731805\n",
      "Epoch:  257  Training Loss:  60.2451  Training Accuracy:  0.732687\n",
      "Epoch:  258  Training Loss:  60.2626  Training Accuracy:  0.733627\n",
      "Epoch:  259  Training Loss:  60.2416  Training Accuracy:  0.734215\n",
      "Epoch:  260  Training Loss:  60.3025  Training Accuracy:  0.734509\n",
      "Epoch:  261  Training Loss:  60.3008  Training Accuracy:  0.735685\n",
      "Epoch:  262  Training Loss:  60.3454  Training Accuracy:  0.736155\n",
      "Epoch:  263  Training Loss:  60.3409  Training Accuracy:  0.736978\n",
      "Epoch:  264  Training Loss:  60.3474  Training Accuracy:  0.73786\n",
      "Epoch:  265  Training Loss:  60.3805  Training Accuracy:  0.738801\n",
      "Epoch:  266  Training Loss:  60.4368  Training Accuracy:  0.739212\n",
      "Epoch:  267  Training Loss:  60.3638  Training Accuracy:  0.739859\n",
      "Epoch:  268  Training Loss:  60.3962  Training Accuracy:  0.740447\n",
      "Epoch:  269  Training Loss:  60.3901  Training Accuracy:  0.741152\n",
      "Epoch:  270  Training Loss:  60.4146  Training Accuracy:  0.742034\n",
      "Epoch:  271  Training Loss:  60.4452  Training Accuracy:  0.742269\n",
      "Epoch:  272  Training Loss:  60.4872  Training Accuracy:  0.743034\n",
      "Epoch:  273  Training Loss:  60.4896  Training Accuracy:  0.743798\n",
      "Epoch:  274  Training Loss:  60.5097  Training Accuracy:  0.74421\n",
      "Epoch:  275  Training Loss:  60.5181  Training Accuracy:  0.744797\n",
      "Epoch:  276  Training Loss:  60.6053  Training Accuracy:  0.745503\n",
      "Epoch:  277  Training Loss:  60.5648  Training Accuracy:  0.746149\n",
      "Epoch:  278  Training Loss:  60.6009  Training Accuracy:  0.74662\n",
      "Epoch:  279  Training Loss:  60.604  Training Accuracy:  0.747619\n",
      "Epoch:  280  Training Loss:  60.6416  Training Accuracy:  0.748383\n",
      "Epoch:  281  Training Loss:  60.6017  Training Accuracy:  0.749148\n",
      "Epoch:  282  Training Loss:  60.7035  Training Accuracy:  0.749736\n",
      "Epoch:  283  Training Loss:  60.6302  Training Accuracy:  0.750735\n",
      "Epoch:  284  Training Loss:  60.6904  Training Accuracy:  0.751264\n",
      "Epoch:  285  Training Loss:  60.6888  Training Accuracy:  0.752205\n",
      "Epoch:  286  Training Loss:  60.7248  Training Accuracy:  0.753028\n",
      "Epoch:  287  Training Loss:  60.6796  Training Accuracy:  0.753616\n",
      "Epoch:  288  Training Loss:  60.786  Training Accuracy:  0.754262\n",
      "Epoch:  289  Training Loss:  60.7581  Training Accuracy:  0.754674\n",
      "Epoch:  290  Training Loss:  60.8284  Training Accuracy:  0.755085\n",
      "Epoch:  291  Training Loss:  60.8557  Training Accuracy:  0.755497\n",
      "Epoch:  292  Training Loss:  60.8936  Training Accuracy:  0.756144\n",
      "Epoch:  293  Training Loss:  60.8976  Training Accuracy:  0.756732\n",
      "Epoch:  294  Training Loss:  61.0151  Training Accuracy:  0.757496\n",
      "Epoch:  295  Training Loss:  61.0119  Training Accuracy:  0.757966\n",
      "Epoch:  296  Training Loss:  61.1007  Training Accuracy:  0.758378\n",
      "Epoch:  297  Training Loss:  61.084  Training Accuracy:  0.759024\n",
      "Epoch:  298  Training Loss:  61.1093  Training Accuracy:  0.75973\n",
      "Epoch:  299  Training Loss:  61.1192  Training Accuracy:  0.760318\n",
      "Epoch:  300  Training Loss:  61.1498  Training Accuracy:  0.76067\n",
      "Epoch:  301  Training Loss:  61.1367  Training Accuracy:  0.761317\n",
      "Epoch:  302  Training Loss:  61.2341  Training Accuracy:  0.761729\n",
      "Epoch:  303  Training Loss:  61.2258  Training Accuracy:  0.762434\n",
      "Epoch:  304  Training Loss:  61.2605  Training Accuracy:  0.762669\n",
      "Epoch:  305  Training Loss:  61.2008  Training Accuracy:  0.763492\n",
      "Epoch:  306  Training Loss:  61.2606  Training Accuracy:  0.763963\n",
      "Epoch:  307  Training Loss:  61.2856  Training Accuracy:  0.764433\n",
      "Epoch:  308  Training Loss:  61.3321  Training Accuracy:  0.76508\n",
      "Epoch:  309  Training Loss:  61.2376  Training Accuracy:  0.766138\n",
      "Epoch:  310  Training Loss:  61.3414  Training Accuracy:  0.766608\n",
      "Epoch:  311  Training Loss:  61.3687  Training Accuracy:  0.767137\n",
      "Epoch:  312  Training Loss:  61.3093  Training Accuracy:  0.767725\n",
      "Epoch:  313  Training Loss:  61.3675  Training Accuracy:  0.768019\n",
      "Epoch:  314  Training Loss:  61.3668  Training Accuracy:  0.768842\n",
      "Epoch:  315  Training Loss:  61.3685  Training Accuracy:  0.769254\n",
      "Epoch:  316  Training Loss:  61.3456  Training Accuracy:  0.769783\n",
      "Epoch:  317  Training Loss:  61.3636  Training Accuracy:  0.770782\n",
      "Epoch:  318  Training Loss:  61.349  Training Accuracy:  0.771488\n",
      "Epoch:  319  Training Loss:  61.3877  Training Accuracy:  0.772311\n",
      "Epoch:  320  Training Loss:  61.3902  Training Accuracy:  0.772546\n",
      "Epoch:  321  Training Loss:  61.4086  Training Accuracy:  0.773134\n",
      "Epoch:  322  Training Loss:  61.3571  Training Accuracy:  0.773722\n",
      "Epoch:  323  Training Loss:  61.424  Training Accuracy:  0.773898\n",
      "Epoch:  324  Training Loss:  61.4308  Training Accuracy:  0.774251\n",
      "Epoch:  325  Training Loss:  61.4264  Training Accuracy:  0.774486\n",
      "Epoch:  326  Training Loss:  61.4244  Training Accuracy:  0.774956\n",
      "Epoch:  327  Training Loss:  61.482  Training Accuracy:  0.775368\n",
      "Epoch:  328  Training Loss:  61.4042  Training Accuracy:  0.775838\n",
      "Epoch:  329  Training Loss:  61.4688  Training Accuracy:  0.776602\n",
      "Epoch:  330  Training Loss:  61.4063  Training Accuracy:  0.777308\n",
      "Epoch:  331  Training Loss:  61.4767  Training Accuracy:  0.777719\n",
      "Epoch:  332  Training Loss:  61.4209  Training Accuracy:  0.778131\n",
      "Epoch:  333  Training Loss:  61.4553  Training Accuracy:  0.778601\n",
      "Epoch:  334  Training Loss:  61.4252  Training Accuracy:  0.779365\n",
      "Epoch:  335  Training Loss:  61.4875  Training Accuracy:  0.780247\n",
      "Epoch:  336  Training Loss:  61.4341  Training Accuracy:  0.780894\n",
      "Epoch:  337  Training Loss:  61.4905  Training Accuracy:  0.781247\n",
      "Epoch:  338  Training Loss:  61.4445  Training Accuracy:  0.781893\n",
      "Epoch:  339  Training Loss:  61.4483  Training Accuracy:  0.782422\n",
      "Epoch:  340  Training Loss:  61.4632  Training Accuracy:  0.783128\n",
      "Epoch:  341  Training Loss:  61.5009  Training Accuracy:  0.783657\n",
      "Epoch:  342  Training Loss:  61.475  Training Accuracy:  0.784127\n",
      "Epoch:  343  Training Loss:  61.4989  Training Accuracy:  0.784774\n",
      "Epoch:  344  Training Loss:  61.5086  Training Accuracy:  0.785362\n",
      "Epoch:  345  Training Loss:  61.5578  Training Accuracy:  0.786067\n",
      "Epoch:  346  Training Loss:  61.5576  Training Accuracy:  0.786596\n",
      "Epoch:  347  Training Loss:  61.5374  Training Accuracy:  0.786832\n",
      "Epoch:  348  Training Loss:  61.5196  Training Accuracy:  0.787772\n",
      "Epoch:  349  Training Loss:  61.4622  Training Accuracy:  0.788184\n",
      "Epoch:  350  Training Loss:  61.534  Training Accuracy:  0.788536\n",
      "Epoch:  351  Training Loss:  61.4561  Training Accuracy:  0.789007\n",
      "Epoch:  352  Training Loss:  61.5225  Training Accuracy:  0.789653\n",
      "Epoch:  353  Training Loss:  61.4817  Training Accuracy:  0.790359\n",
      "Epoch:  354  Training Loss:  61.5567  Training Accuracy:  0.790712\n",
      "Epoch:  355  Training Loss:  61.5326  Training Accuracy:  0.790888\n",
      "Epoch:  356  Training Loss:  61.5523  Training Accuracy:  0.791241\n",
      "Epoch:  357  Training Loss:  61.578  Training Accuracy:  0.791946\n",
      "Epoch:  358  Training Loss:  61.6113  Training Accuracy:  0.791887\n",
      "Epoch:  359  Training Loss:  61.5242  Training Accuracy:  0.792652\n",
      "Epoch:  360  Training Loss:  61.541  Training Accuracy:  0.793357\n",
      "Epoch:  361  Training Loss:  61.4951  Training Accuracy:  0.793886\n",
      "Epoch:  362  Training Loss:  61.538  Training Accuracy:  0.794827\n",
      "Epoch:  363  Training Loss:  61.5261  Training Accuracy:  0.795474\n",
      "Epoch:  364  Training Loss:  61.5535  Training Accuracy:  0.796297\n",
      "Epoch:  365  Training Loss:  61.5395  Training Accuracy:  0.797178\n",
      "Epoch:  366  Training Loss:  61.5787  Training Accuracy:  0.797649\n",
      "Epoch:  367  Training Loss:  61.5394  Training Accuracy:  0.798413\n",
      "Epoch:  368  Training Loss:  61.5687  Training Accuracy:  0.798883\n",
      "Epoch:  369  Training Loss:  61.5692  Training Accuracy:  0.799354\n",
      "Epoch:  370  Training Loss:  61.5758  Training Accuracy:  0.800177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  61.5304  Training Accuracy:  0.800647\n",
      "Epoch:  372  Training Loss:  61.5548  Training Accuracy:  0.801235\n",
      "Epoch:  373  Training Loss:  61.5657  Training Accuracy:  0.801999\n",
      "Epoch:  374  Training Loss:  61.5564  Training Accuracy:  0.802352\n",
      "Epoch:  375  Training Loss:  61.5373  Training Accuracy:  0.802999\n",
      "Epoch:  376  Training Loss:  61.5564  Training Accuracy:  0.803586\n",
      "Epoch:  377  Training Loss:  61.5779  Training Accuracy:  0.804645\n",
      "Epoch:  378  Training Loss:  61.6266  Training Accuracy:  0.805233\n",
      "Epoch:  379  Training Loss:  61.5388  Training Accuracy:  0.805879\n",
      "Epoch:  380  Training Loss:  61.5871  Training Accuracy:  0.806056\n",
      "Epoch:  381  Training Loss:  61.545  Training Accuracy:  0.806585\n",
      "Epoch:  382  Training Loss:  61.5559  Training Accuracy:  0.807055\n",
      "Epoch:  383  Training Loss:  61.595  Training Accuracy:  0.807231\n",
      "Epoch:  384  Training Loss:  61.543  Training Accuracy:  0.807819\n",
      "Epoch:  385  Training Loss:  61.5262  Training Accuracy:  0.808642\n",
      "Epoch:  386  Training Loss:  61.5556  Training Accuracy:  0.809113\n",
      "Epoch:  387  Training Loss:  61.5579  Training Accuracy:  0.8097\n",
      "Epoch:  388  Training Loss:  61.5409  Training Accuracy:  0.809994\n",
      "Epoch:  389  Training Loss:  61.4799  Training Accuracy:  0.810406\n",
      "Epoch:  390  Training Loss:  61.5467  Training Accuracy:  0.8107\n",
      "Epoch:  391  Training Loss:  61.5027  Training Accuracy:  0.810935\n",
      "Epoch:  392  Training Loss:  61.5715  Training Accuracy:  0.811464\n",
      "Epoch:  393  Training Loss:  61.5189  Training Accuracy:  0.811934\n",
      "Epoch:  394  Training Loss:  61.5956  Training Accuracy:  0.812522\n",
      "Epoch:  395  Training Loss:  61.4732  Training Accuracy:  0.813169\n",
      "Epoch:  396  Training Loss:  61.5403  Training Accuracy:  0.814168\n",
      "Epoch:  397  Training Loss:  61.4828  Training Accuracy:  0.814756\n",
      "Epoch:  398  Training Loss:  61.5016  Training Accuracy:  0.815109\n",
      "Epoch:  399  Training Loss:  61.4856  Training Accuracy:  0.815638\n",
      "Epoch:  400  Training Loss:  61.5023  Training Accuracy:  0.816108\n",
      "Epoch:  401  Training Loss:  61.43  Training Accuracy:  0.81652\n",
      "Epoch:  402  Training Loss:  61.5034  Training Accuracy:  0.817049\n",
      "Epoch:  403  Training Loss:  61.4316  Training Accuracy:  0.817519\n",
      "Epoch:  404  Training Loss:  61.4711  Training Accuracy:  0.817872\n",
      "Epoch:  405  Training Loss:  61.3603  Training Accuracy:  0.818401\n",
      "Epoch:  406  Training Loss:  61.4227  Training Accuracy:  0.819048\n",
      "Epoch:  407  Training Loss:  61.3893  Training Accuracy:  0.819459\n",
      "Epoch:  408  Training Loss:  61.4206  Training Accuracy:  0.819989\n",
      "Epoch:  409  Training Loss:  61.3672  Training Accuracy:  0.820576\n",
      "Epoch:  410  Training Loss:  61.3501  Training Accuracy:  0.820929\n",
      "Epoch:  411  Training Loss:  61.332  Training Accuracy:  0.821576\n",
      "Epoch:  412  Training Loss:  61.3639  Training Accuracy:  0.821752\n",
      "Epoch:  413  Training Loss:  61.2278  Training Accuracy:  0.822575\n",
      "Epoch:  414  Training Loss:  61.3581  Training Accuracy:  0.823104\n",
      "Epoch:  415  Training Loss:  61.2137  Training Accuracy:  0.823575\n",
      "Epoch:  416  Training Loss:  61.2806  Training Accuracy:  0.823986\n",
      "Epoch:  417  Training Loss:  61.1999  Training Accuracy:  0.824456\n",
      "Epoch:  418  Training Loss:  61.223  Training Accuracy:  0.824633\n",
      "Epoch:  419  Training Loss:  61.1276  Training Accuracy:  0.825574\n",
      "Epoch:  420  Training Loss:  61.2521  Training Accuracy:  0.825867\n",
      "Epoch:  421  Training Loss:  61.1537  Training Accuracy:  0.826632\n",
      "Epoch:  422  Training Loss:  61.149  Training Accuracy:  0.827043\n",
      "Epoch:  423  Training Loss:  61.151  Training Accuracy:  0.827337\n",
      "Epoch:  424  Training Loss:  61.1185  Training Accuracy:  0.827807\n",
      "Epoch:  425  Training Loss:  61.1644  Training Accuracy:  0.828219\n",
      "Epoch:  426  Training Loss:  61.1461  Training Accuracy:  0.828689\n",
      "Epoch:  427  Training Loss:  61.1689  Training Accuracy:  0.829454\n",
      "Epoch:  428  Training Loss:  61.1152  Training Accuracy:  0.829983\n",
      "Epoch:  429  Training Loss:  61.1688  Training Accuracy:  0.830571\n",
      "Epoch:  430  Training Loss:  61.1434  Training Accuracy:  0.831158\n",
      "Epoch:  431  Training Loss:  61.1526  Training Accuracy:  0.831805\n",
      "Epoch:  432  Training Loss:  61.1446  Training Accuracy:  0.832746\n",
      "Epoch:  433  Training Loss:  61.1545  Training Accuracy:  0.83351\n",
      "Epoch:  434  Training Loss:  61.1364  Training Accuracy:  0.833804\n",
      "Epoch:  435  Training Loss:  61.0854  Training Accuracy:  0.833922\n",
      "Epoch:  436  Training Loss:  61.0804  Training Accuracy:  0.834451\n",
      "Epoch:  437  Training Loss:  61.0933  Training Accuracy:  0.834862\n",
      "Epoch:  438  Training Loss:  61.0713  Training Accuracy:  0.835509\n",
      "Epoch:  439  Training Loss:  61.0936  Training Accuracy:  0.835979\n",
      "Epoch:  440  Training Loss:  61.036  Training Accuracy:  0.836332\n",
      "Epoch:  441  Training Loss:  61.0797  Training Accuracy:  0.837037\n",
      "Epoch:  442  Training Loss:  60.9842  Training Accuracy:  0.837214\n",
      "Epoch:  443  Training Loss:  61.0201  Training Accuracy:  0.83739\n",
      "Epoch:  444  Training Loss:  60.9698  Training Accuracy:  0.838213\n",
      "Epoch:  445  Training Loss:  60.968  Training Accuracy:  0.838625\n",
      "Epoch:  446  Training Loss:  60.9271  Training Accuracy:  0.838919\n",
      "Epoch:  447  Training Loss:  60.9061  Training Accuracy:  0.839271\n",
      "Epoch:  448  Training Loss:  60.8737  Training Accuracy:  0.8398\n",
      "Epoch:  449  Training Loss:  60.8577  Training Accuracy:  0.840388\n",
      "Epoch:  450  Training Loss:  60.8049  Training Accuracy:  0.840565\n",
      "Epoch:  451  Training Loss:  60.7725  Training Accuracy:  0.841211\n",
      "Epoch:  452  Training Loss:  60.786  Training Accuracy:  0.841623\n",
      "Epoch:  453  Training Loss:  60.7387  Training Accuracy:  0.842093\n",
      "Epoch:  454  Training Loss:  60.7042  Training Accuracy:  0.842505\n",
      "Epoch:  455  Training Loss:  60.7291  Training Accuracy:  0.843151\n",
      "Epoch:  456  Training Loss:  60.6701  Training Accuracy:  0.843328\n",
      "Epoch:  457  Training Loss:  60.5852  Training Accuracy:  0.843857\n",
      "Epoch:  458  Training Loss:  60.6413  Training Accuracy:  0.84421\n",
      "Epoch:  459  Training Loss:  60.5552  Training Accuracy:  0.844915\n",
      "Epoch:  460  Training Loss:  60.5891  Training Accuracy:  0.845385\n",
      "Epoch:  461  Training Loss:  60.5577  Training Accuracy:  0.845856\n",
      "Epoch:  462  Training Loss:  60.632  Training Accuracy:  0.846385\n",
      "Epoch:  463  Training Loss:  60.4972  Training Accuracy:  0.846973\n",
      "Epoch:  464  Training Loss:  60.5981  Training Accuracy:  0.847443\n",
      "Epoch:  465  Training Loss:  60.5396  Training Accuracy:  0.847972\n",
      "Epoch:  466  Training Loss:  60.5154  Training Accuracy:  0.848384\n",
      "Epoch:  467  Training Loss:  60.4986  Training Accuracy:  0.849148\n",
      "Epoch:  468  Training Loss:  60.5043  Training Accuracy:  0.849324\n",
      "Epoch:  469  Training Loss:  60.4274  Training Accuracy:  0.849853\n",
      "Epoch:  470  Training Loss:  60.5431  Training Accuracy:  0.850206\n",
      "Epoch:  471  Training Loss:  60.4222  Training Accuracy:  0.850618\n",
      "Epoch:  472  Training Loss:  60.3728  Training Accuracy:  0.851206\n",
      "Epoch:  473  Training Loss:  60.3817  Training Accuracy:  0.851676\n",
      "Epoch:  474  Training Loss:  60.4338  Training Accuracy:  0.852264\n",
      "Epoch:  475  Training Loss:  60.3305  Training Accuracy:  0.852381\n",
      "Epoch:  476  Training Loss:  60.3655  Training Accuracy:  0.852852\n",
      "Epoch:  477  Training Loss:  60.2669  Training Accuracy:  0.853087\n",
      "Epoch:  478  Training Loss:  60.3441  Training Accuracy:  0.853616\n",
      "Epoch:  479  Training Loss:  60.4074  Training Accuracy:  0.85391\n",
      "Epoch:  480  Training Loss:  60.341  Training Accuracy:  0.854145\n",
      "Epoch:  481  Training Loss:  60.3621  Training Accuracy:  0.854498\n",
      "Epoch:  482  Training Loss:  60.357  Training Accuracy:  0.854792\n",
      "Epoch:  483  Training Loss:  60.349  Training Accuracy:  0.855086\n",
      "Epoch:  484  Training Loss:  60.3533  Training Accuracy:  0.855556\n",
      "Epoch:  485  Training Loss:  60.3544  Training Accuracy:  0.856144\n",
      "Epoch:  486  Training Loss:  60.3215  Training Accuracy:  0.856791\n",
      "Epoch:  487  Training Loss:  60.3306  Training Accuracy:  0.85679\n",
      "Epoch:  488  Training Loss:  60.3414  Training Accuracy:  0.857143\n",
      "Epoch:  489  Training Loss:  60.354  Training Accuracy:  0.857731\n",
      "Epoch:  490  Training Loss:  60.3903  Training Accuracy:  0.858201\n",
      "Epoch:  491  Training Loss:  60.3759  Training Accuracy:  0.858554\n",
      "Epoch:  492  Training Loss:  60.384  Training Accuracy:  0.859318\n",
      "Epoch:  493  Training Loss:  60.3693  Training Accuracy:  0.859554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  60.3745  Training Accuracy:  0.860142\n",
      "Epoch:  495  Training Loss:  60.3641  Training Accuracy:  0.860671\n",
      "Epoch:  496  Training Loss:  60.3937  Training Accuracy:  0.8612\n",
      "Epoch:  497  Training Loss:  60.3861  Training Accuracy:  0.861611\n",
      "Epoch:  498  Training Loss:  60.3791  Training Accuracy:  0.861788\n",
      "Epoch:  499  Training Loss:  60.3267  Training Accuracy:  0.862199\n",
      "Epoch:  500  Training Loss:  60.3876  Training Accuracy:  0.862846\n",
      "Epoch:  501  Training Loss:  60.3723  Training Accuracy:  0.863434\n",
      "Epoch:  502  Training Loss:  60.3899  Training Accuracy:  0.863963\n",
      "Epoch:  503  Training Loss:  60.3477  Training Accuracy:  0.864257\n",
      "Epoch:  504  Training Loss:  60.3225  Training Accuracy:  0.865197\n",
      "Epoch:  505  Training Loss:  60.3666  Training Accuracy:  0.865433\n",
      "Epoch:  506  Training Loss:  60.4021  Training Accuracy:  0.86602\n",
      "Epoch:  507  Training Loss:  60.3366  Training Accuracy:  0.866138\n",
      "Epoch:  508  Training Loss:  60.3545  Training Accuracy:  0.86655\n",
      "Epoch:  509  Training Loss:  60.399  Training Accuracy:  0.866843\n",
      "Epoch:  510  Training Loss:  60.3278  Training Accuracy:  0.86702\n",
      "Epoch:  511  Training Loss:  60.2613  Training Accuracy:  0.86749\n",
      "Epoch:  512  Training Loss:  60.3263  Training Accuracy:  0.868019\n",
      "Epoch:  513  Training Loss:  60.3278  Training Accuracy:  0.868372\n",
      "Epoch:  514  Training Loss:  60.3693  Training Accuracy:  0.868842\n",
      "Epoch:  515  Training Loss:  60.2503  Training Accuracy:  0.869371\n",
      "Epoch:  516  Training Loss:  60.3299  Training Accuracy:  0.869548\n",
      "Epoch:  517  Training Loss:  60.3002  Training Accuracy:  0.869783\n",
      "Epoch:  518  Training Loss:  60.3614  Training Accuracy:  0.870018\n",
      "Epoch:  519  Training Loss:  60.2166  Training Accuracy:  0.870488\n",
      "Epoch:  520  Training Loss:  60.3009  Training Accuracy:  0.871135\n",
      "Epoch:  521  Training Loss:  60.2421  Training Accuracy:  0.871547\n",
      "Epoch:  522  Training Loss:  60.3305  Training Accuracy:  0.871782\n",
      "Epoch:  523  Training Loss:  60.2506  Training Accuracy:  0.872252\n",
      "Epoch:  524  Training Loss:  60.3295  Training Accuracy:  0.872428\n",
      "Epoch:  525  Training Loss:  60.1856  Training Accuracy:  0.873134\n",
      "Epoch:  526  Training Loss:  60.393  Training Accuracy:  0.873487\n",
      "Epoch:  527  Training Loss:  60.1607  Training Accuracy:  0.873545\n",
      "Epoch:  528  Training Loss:  60.3631  Training Accuracy:  0.873957\n",
      "Epoch:  529  Training Loss:  60.1719  Training Accuracy:  0.874251\n",
      "Epoch:  530  Training Loss:  60.3532  Training Accuracy:  0.874662\n",
      "Epoch:  531  Training Loss:  60.2178  Training Accuracy:  0.875191\n",
      "Epoch:  532  Training Loss:  60.4257  Training Accuracy:  0.875544\n",
      "Epoch:  533  Training Loss:  60.2187  Training Accuracy:  0.87625\n",
      "Epoch:  534  Training Loss:  60.411  Training Accuracy:  0.876602\n",
      "Epoch:  535  Training Loss:  60.2433  Training Accuracy:  0.876955\n",
      "Epoch:  536  Training Loss:  60.4275  Training Accuracy:  0.877249\n",
      "Epoch:  537  Training Loss:  60.2667  Training Accuracy:  0.877425\n",
      "Epoch:  538  Training Loss:  60.3478  Training Accuracy:  0.877661\n",
      "Epoch:  539  Training Loss:  60.3044  Training Accuracy:  0.877837\n",
      "Epoch:  540  Training Loss:  60.3754  Training Accuracy:  0.878307\n",
      "Epoch:  541  Training Loss:  60.2985  Training Accuracy:  0.878778\n",
      "Epoch:  542  Training Loss:  60.3705  Training Accuracy:  0.879366\n",
      "Epoch:  543  Training Loss:  60.3613  Training Accuracy:  0.879659\n",
      "Epoch:  544  Training Loss:  60.3561  Training Accuracy:  0.880189\n",
      "Epoch:  545  Training Loss:  60.2623  Training Accuracy:  0.880718\n",
      "Epoch:  546  Training Loss:  60.4212  Training Accuracy:  0.881012\n",
      "Epoch:  547  Training Loss:  60.2636  Training Accuracy:  0.881423\n",
      "Epoch:  548  Training Loss:  60.3974  Training Accuracy:  0.882011\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  104.313  Training Accuracy:  0.0436802\n",
      "Epoch:  1  Training Loss:  88.487  Training Accuracy:  0.047913\n",
      "Epoch:  2  Training Loss:  76.1234  Training Accuracy:  0.0475015\n",
      "Epoch:  3  Training Loss:  66.9187  Training Accuracy:  0.053204\n",
      "Epoch:  4  Training Loss:  60.2365  Training Accuracy:  0.0557907\n",
      "Epoch:  5  Training Loss:  54.7811  Training Accuracy:  0.0574956\n",
      "Epoch:  6  Training Loss:  49.6584  Training Accuracy:  0.0595532\n",
      "Epoch:  7  Training Loss:  45.3041  Training Accuracy:  0.0623751\n",
      "Epoch:  8  Training Loss:  41.7439  Training Accuracy:  0.065726\n",
      "Epoch:  9  Training Loss:  38.7723  Training Accuracy:  0.0701352\n",
      "Epoch:  10  Training Loss:  36.2648  Training Accuracy:  0.0756614\n",
      "Epoch:  11  Training Loss:  33.9904  Training Accuracy:  0.0824221\n",
      "Epoch:  12  Training Loss:  31.9361  Training Accuracy:  0.0905938\n",
      "Epoch:  13  Training Loss:  30.2027  Training Accuracy:  0.0984715\n",
      "Epoch:  14  Training Loss:  28.7777  Training Accuracy:  0.106937\n",
      "Epoch:  15  Training Loss:  27.6334  Training Accuracy:  0.11505\n",
      "Epoch:  16  Training Loss:  26.5631  Training Accuracy:  0.122222\n",
      "Epoch:  17  Training Loss:  25.6744  Training Accuracy:  0.129747\n",
      "Epoch:  18  Training Loss:  24.965  Training Accuracy:  0.137743\n",
      "Epoch:  19  Training Loss:  24.4718  Training Accuracy:  0.144268\n",
      "Epoch:  20  Training Loss:  24.0486  Training Accuracy:  0.151499\n",
      "Epoch:  21  Training Loss:  23.6266  Training Accuracy:  0.159318\n",
      "Epoch:  22  Training Loss:  23.2411  Training Accuracy:  0.16649\n",
      "Epoch:  23  Training Loss:  22.9222  Training Accuracy:  0.172546\n",
      "Epoch:  24  Training Loss:  22.5795  Training Accuracy:  0.179071\n",
      "Epoch:  25  Training Loss:  22.268  Training Accuracy:  0.185597\n",
      "Epoch:  26  Training Loss:  22.0129  Training Accuracy:  0.19124\n",
      "Epoch:  27  Training Loss:  21.7986  Training Accuracy:  0.196296\n",
      "Epoch:  28  Training Loss:  21.6266  Training Accuracy:  0.201999\n",
      "Epoch:  29  Training Loss:  21.4352  Training Accuracy:  0.207936\n",
      "Epoch:  30  Training Loss:  21.2675  Training Accuracy:  0.212522\n",
      "Epoch:  31  Training Loss:  21.0964  Training Accuracy:  0.217813\n",
      "Epoch:  32  Training Loss:  20.9464  Training Accuracy:  0.222634\n",
      "Epoch:  33  Training Loss:  20.8669  Training Accuracy:  0.228219\n",
      "Epoch:  34  Training Loss:  20.7563  Training Accuracy:  0.233216\n",
      "Epoch:  35  Training Loss:  20.7239  Training Accuracy:  0.238213\n",
      "Epoch:  36  Training Loss:  20.523  Training Accuracy:  0.243739\n",
      "Epoch:  37  Training Loss:  20.3994  Training Accuracy:  0.247913\n",
      "Epoch:  38  Training Loss:  20.2096  Training Accuracy:  0.252205\n",
      "Epoch:  39  Training Loss:  19.9966  Training Accuracy:  0.255849\n",
      "Epoch:  40  Training Loss:  19.8686  Training Accuracy:  0.259847\n",
      "Epoch:  41  Training Loss:  19.6956  Training Accuracy:  0.264139\n",
      "Epoch:  42  Training Loss:  19.5439  Training Accuracy:  0.268489\n",
      "Epoch:  43  Training Loss:  19.4192  Training Accuracy:  0.272428\n",
      "Epoch:  44  Training Loss:  19.3337  Training Accuracy:  0.276308\n",
      "Epoch:  45  Training Loss:  19.2259  Training Accuracy:  0.279718\n",
      "Epoch:  46  Training Loss:  19.1415  Training Accuracy:  0.282775\n",
      "Epoch:  47  Training Loss:  19.0463  Training Accuracy:  0.286714\n",
      "Epoch:  48  Training Loss:  19.0014  Training Accuracy:  0.289947\n",
      "Epoch:  49  Training Loss:  18.9391  Training Accuracy:  0.29371\n",
      "Epoch:  50  Training Loss:  18.8623  Training Accuracy:  0.297825\n",
      "Epoch:  51  Training Loss:  18.7532  Training Accuracy:  0.301822\n",
      "Epoch:  52  Training Loss:  18.662  Training Accuracy:  0.305056\n",
      "Epoch:  53  Training Loss:  18.5537  Training Accuracy:  0.30823\n",
      "Epoch:  54  Training Loss:  18.4406  Training Accuracy:  0.312052\n",
      "Epoch:  55  Training Loss:  18.3364  Training Accuracy:  0.315697\n",
      "Epoch:  56  Training Loss:  18.1652  Training Accuracy:  0.318166\n",
      "Epoch:  57  Training Loss:  18.053  Training Accuracy:  0.321634\n",
      "Epoch:  58  Training Loss:  17.9227  Training Accuracy:  0.324221\n",
      "Epoch:  59  Training Loss:  17.7842  Training Accuracy:  0.327278\n",
      "Epoch:  60  Training Loss:  17.6437  Training Accuracy:  0.330629\n",
      "Epoch:  61  Training Loss:  17.5189  Training Accuracy:  0.334333\n",
      "Epoch:  62  Training Loss:  17.3741  Training Accuracy:  0.336861\n",
      "Epoch:  63  Training Loss:  17.2397  Training Accuracy:  0.340035\n",
      "Epoch:  64  Training Loss:  17.112  Training Accuracy:  0.343445\n",
      "Epoch:  65  Training Loss:  16.9758  Training Accuracy:  0.345855\n",
      "Epoch:  66  Training Loss:  16.8391  Training Accuracy:  0.348971\n",
      "Epoch:  67  Training Loss:  16.725  Training Accuracy:  0.35144\n",
      "Epoch:  68  Training Loss:  16.6151  Training Accuracy:  0.354615\n",
      "Epoch:  69  Training Loss:  16.3704  Training Accuracy:  0.357143\n",
      "Epoch:  70  Training Loss:  16.3325  Training Accuracy:  0.359788\n",
      "Epoch:  71  Training Loss:  16.2559  Training Accuracy:  0.362199\n",
      "Epoch:  72  Training Loss:  16.1693  Training Accuracy:  0.36502\n",
      "Epoch:  73  Training Loss:  16.0913  Training Accuracy:  0.368313\n",
      "Epoch:  74  Training Loss:  15.9671  Training Accuracy:  0.371605\n",
      "Epoch:  75  Training Loss:  15.8564  Training Accuracy:  0.374015\n",
      "Epoch:  76  Training Loss:  15.7391  Training Accuracy:  0.376543\n",
      "Epoch:  77  Training Loss:  15.6358  Training Accuracy:  0.379189\n",
      "Epoch:  78  Training Loss:  15.5384  Training Accuracy:  0.382069\n",
      "Epoch:  79  Training Loss:  15.4089  Training Accuracy:  0.384303\n",
      "Epoch:  80  Training Loss:  15.3246  Training Accuracy:  0.387595\n",
      "Epoch:  81  Training Loss:  15.2837  Training Accuracy:  0.390182\n",
      "Epoch:  82  Training Loss:  15.2086  Training Accuracy:  0.392475\n",
      "Epoch:  83  Training Loss:  15.1278  Training Accuracy:  0.39512\n",
      "Epoch:  84  Training Loss:  15.0647  Training Accuracy:  0.396766\n",
      "Epoch:  85  Training Loss:  14.9939  Training Accuracy:  0.399471\n",
      "Epoch:  86  Training Loss:  14.9357  Training Accuracy:  0.402116\n",
      "Epoch:  87  Training Loss:  14.8666  Training Accuracy:  0.405056\n",
      "Epoch:  88  Training Loss:  14.7771  Training Accuracy:  0.40729\n",
      "Epoch:  89  Training Loss:  14.6891  Training Accuracy:  0.4097\n",
      "Epoch:  90  Training Loss:  14.6018  Training Accuracy:  0.411816\n",
      "Epoch:  91  Training Loss:  14.5288  Training Accuracy:  0.414638\n",
      "Epoch:  92  Training Loss:  14.4716  Training Accuracy:  0.417401\n",
      "Epoch:  93  Training Loss:  14.4124  Training Accuracy:  0.4204\n",
      "Epoch:  94  Training Loss:  14.3512  Training Accuracy:  0.422634\n",
      "Epoch:  95  Training Loss:  14.3021  Training Accuracy:  0.425691\n",
      "Epoch:  96  Training Loss:  14.2491  Training Accuracy:  0.428924\n",
      "Epoch:  97  Training Loss:  14.202  Training Accuracy:  0.431805\n",
      "Epoch:  98  Training Loss:  14.152  Training Accuracy:  0.434627\n",
      "Epoch:  99  Training Loss:  14.0796  Training Accuracy:  0.436449\n",
      "Epoch:  100  Training Loss:  14.0188  Training Accuracy:  0.438624\n",
      "Epoch:  101  Training Loss:  13.9273  Training Accuracy:  0.440623\n",
      "Epoch:  102  Training Loss:  13.8487  Training Accuracy:  0.442445\n",
      "Epoch:  103  Training Loss:  13.771  Training Accuracy:  0.445326\n",
      "Epoch:  104  Training Loss:  13.7022  Training Accuracy:  0.447325\n",
      "Epoch:  105  Training Loss:  13.6305  Training Accuracy:  0.4495\n",
      "Epoch:  106  Training Loss:  13.5867  Training Accuracy:  0.45191\n",
      "Epoch:  107  Training Loss:  13.5231  Training Accuracy:  0.453674\n",
      "Epoch:  108  Training Loss:  13.469  Training Accuracy:  0.455732\n",
      "Epoch:  109  Training Loss:  13.4063  Training Accuracy:  0.458319\n",
      "Epoch:  110  Training Loss:  13.3666  Training Accuracy:  0.460905\n",
      "Epoch:  111  Training Loss:  13.3113  Training Accuracy:  0.463668\n",
      "Epoch:  112  Training Loss:  13.2392  Training Accuracy:  0.465373\n",
      "Epoch:  113  Training Loss:  13.1908  Training Accuracy:  0.467901\n",
      "Epoch:  114  Training Loss:  13.1582  Training Accuracy:  0.470253\n",
      "Epoch:  115  Training Loss:  13.1259  Training Accuracy:  0.472722\n",
      "Epoch:  116  Training Loss:  13.0746  Training Accuracy:  0.474721\n",
      "Epoch:  117  Training Loss:  13.0475  Training Accuracy:  0.476896\n",
      "Epoch:  118  Training Loss:  13.0238  Training Accuracy:  0.479012\n",
      "Epoch:  119  Training Loss:  12.9841  Training Accuracy:  0.480776\n",
      "Epoch:  120  Training Loss:  12.9592  Training Accuracy:  0.482481\n",
      "Epoch:  121  Training Loss:  12.9308  Training Accuracy:  0.48495\n",
      "Epoch:  122  Training Loss:  12.909  Training Accuracy:  0.486949\n",
      "Epoch:  123  Training Loss:  12.8942  Training Accuracy:  0.488948\n",
      "Epoch:  124  Training Loss:  12.8661  Training Accuracy:  0.490888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  12.8617  Training Accuracy:  0.492592\n",
      "Epoch:  126  Training Loss:  12.8336  Training Accuracy:  0.494062\n",
      "Epoch:  127  Training Loss:  12.8134  Training Accuracy:  0.495179\n",
      "Epoch:  128  Training Loss:  12.794  Training Accuracy:  0.497707\n",
      "Epoch:  129  Training Loss:  12.7971  Training Accuracy:  0.499294\n",
      "Epoch:  130  Training Loss:  12.7889  Training Accuracy:  0.500823\n",
      "Epoch:  131  Training Loss:  12.7941  Training Accuracy:  0.502645\n",
      "Epoch:  132  Training Loss:  12.7875  Training Accuracy:  0.503939\n",
      "Epoch:  133  Training Loss:  12.788  Training Accuracy:  0.505526\n",
      "Epoch:  134  Training Loss:  12.7811  Training Accuracy:  0.506996\n",
      "Epoch:  135  Training Loss:  12.7623  Training Accuracy:  0.508642\n",
      "Epoch:  136  Training Loss:  12.7541  Training Accuracy:  0.509582\n",
      "Epoch:  137  Training Loss:  12.7206  Training Accuracy:  0.51117\n",
      "Epoch:  138  Training Loss:  12.722  Training Accuracy:  0.513169\n",
      "Epoch:  139  Training Loss:  12.7032  Training Accuracy:  0.514403\n",
      "Epoch:  140  Training Loss:  12.7086  Training Accuracy:  0.51599\n",
      "Epoch:  141  Training Loss:  12.6892  Training Accuracy:  0.516755\n",
      "Epoch:  142  Training Loss:  12.6941  Training Accuracy:  0.518754\n",
      "Epoch:  143  Training Loss:  12.6873  Training Accuracy:  0.520106\n",
      "Epoch:  144  Training Loss:  12.6893  Training Accuracy:  0.521399\n",
      "Epoch:  145  Training Loss:  12.6566  Training Accuracy:  0.522751\n",
      "Epoch:  146  Training Loss:  12.6486  Training Accuracy:  0.524574\n",
      "Epoch:  147  Training Loss:  12.6354  Training Accuracy:  0.526102\n",
      "Epoch:  148  Training Loss:  12.6446  Training Accuracy:  0.527337\n",
      "Epoch:  149  Training Loss:  12.6113  Training Accuracy:  0.528571\n",
      "Epoch:  150  Training Loss:  12.6279  Training Accuracy:  0.53057\n",
      "Epoch:  151  Training Loss:  12.6124  Training Accuracy:  0.532393\n",
      "Epoch:  152  Training Loss:  12.6231  Training Accuracy:  0.533745\n",
      "Epoch:  153  Training Loss:  12.6044  Training Accuracy:  0.53592\n",
      "Epoch:  154  Training Loss:  12.6039  Training Accuracy:  0.537213\n",
      "Epoch:  155  Training Loss:  12.5811  Training Accuracy:  0.538683\n",
      "Epoch:  156  Training Loss:  12.6019  Training Accuracy:  0.540917\n",
      "Epoch:  157  Training Loss:  12.6032  Training Accuracy:  0.542622\n",
      "Epoch:  158  Training Loss:  12.6205  Training Accuracy:  0.543798\n",
      "Epoch:  159  Training Loss:  12.6451  Training Accuracy:  0.544797\n",
      "Epoch:  160  Training Loss:  12.6769  Training Accuracy:  0.546208\n",
      "Epoch:  161  Training Loss:  12.6814  Training Accuracy:  0.547795\n",
      "Epoch:  162  Training Loss:  12.673  Training Accuracy:  0.548795\n",
      "Epoch:  163  Training Loss:  12.7039  Training Accuracy:  0.550264\n",
      "Epoch:  164  Training Loss:  12.6961  Training Accuracy:  0.551146\n",
      "Epoch:  165  Training Loss:  12.7014  Training Accuracy:  0.553086\n",
      "Epoch:  166  Training Loss:  12.6985  Training Accuracy:  0.554674\n",
      "Epoch:  167  Training Loss:  12.6865  Training Accuracy:  0.556085\n",
      "Epoch:  168  Training Loss:  12.6855  Training Accuracy:  0.557554\n",
      "Epoch:  169  Training Loss:  12.6453  Training Accuracy:  0.558671\n",
      "Epoch:  170  Training Loss:  12.6796  Training Accuracy:  0.560317\n",
      "Epoch:  171  Training Loss:  12.6383  Training Accuracy:  0.561552\n",
      "Epoch:  172  Training Loss:  12.6495  Training Accuracy:  0.563786\n",
      "Epoch:  173  Training Loss:  12.6312  Training Accuracy:  0.565961\n",
      "Epoch:  174  Training Loss:  12.6492  Training Accuracy:  0.568078\n",
      "Epoch:  175  Training Loss:  12.639  Training Accuracy:  0.569724\n",
      "Epoch:  176  Training Loss:  12.6483  Training Accuracy:  0.570958\n",
      "Epoch:  177  Training Loss:  12.6536  Training Accuracy:  0.573133\n",
      "Epoch:  178  Training Loss:  12.6753  Training Accuracy:  0.574603\n",
      "Epoch:  179  Training Loss:  12.6788  Training Accuracy:  0.575897\n",
      "Epoch:  180  Training Loss:  12.6914  Training Accuracy:  0.578013\n",
      "Epoch:  181  Training Loss:  12.6978  Training Accuracy:  0.579424\n",
      "Epoch:  182  Training Loss:  12.6971  Training Accuracy:  0.581246\n",
      "Epoch:  183  Training Loss:  12.6879  Training Accuracy:  0.583657\n",
      "Epoch:  184  Training Loss:  12.6962  Training Accuracy:  0.585655\n",
      "Epoch:  185  Training Loss:  12.7127  Training Accuracy:  0.587243\n",
      "Epoch:  186  Training Loss:  12.7134  Training Accuracy:  0.588948\n",
      "Epoch:  187  Training Loss:  12.7171  Training Accuracy:  0.590535\n",
      "Epoch:  188  Training Loss:  12.7142  Training Accuracy:  0.592416\n",
      "Epoch:  189  Training Loss:  12.7214  Training Accuracy:  0.592887\n",
      "Epoch:  190  Training Loss:  12.7397  Training Accuracy:  0.594885\n",
      "Epoch:  191  Training Loss:  12.7446  Training Accuracy:  0.596061\n",
      "Epoch:  192  Training Loss:  12.7595  Training Accuracy:  0.597472\n",
      "Epoch:  193  Training Loss:  12.7675  Training Accuracy:  0.598413\n",
      "Epoch:  194  Training Loss:  12.7734  Training Accuracy:  0.599765\n",
      "Epoch:  195  Training Loss:  12.7731  Training Accuracy:  0.601176\n",
      "Epoch:  196  Training Loss:  12.8096  Training Accuracy:  0.602939\n",
      "Epoch:  197  Training Loss:  12.7821  Training Accuracy:  0.604644\n",
      "Epoch:  198  Training Loss:  12.8166  Training Accuracy:  0.606173\n",
      "Epoch:  199  Training Loss:  12.7763  Training Accuracy:  0.607701\n",
      "Epoch:  200  Training Loss:  12.8228  Training Accuracy:  0.608877\n",
      "Epoch:  201  Training Loss:  12.7635  Training Accuracy:  0.609994\n",
      "Epoch:  202  Training Loss:  12.8422  Training Accuracy:  0.611346\n",
      "Epoch:  203  Training Loss:  12.7701  Training Accuracy:  0.612757\n",
      "Epoch:  204  Training Loss:  12.8213  Training Accuracy:  0.614051\n",
      "Epoch:  205  Training Loss:  12.7894  Training Accuracy:  0.615403\n",
      "Epoch:  206  Training Loss:  12.8157  Training Accuracy:  0.616755\n",
      "Epoch:  207  Training Loss:  12.7851  Training Accuracy:  0.618048\n",
      "Epoch:  208  Training Loss:  12.8478  Training Accuracy:  0.619518\n",
      "Epoch:  209  Training Loss:  12.7841  Training Accuracy:  0.621634\n",
      "Epoch:  210  Training Loss:  12.8359  Training Accuracy:  0.62328\n",
      "Epoch:  211  Training Loss:  12.8192  Training Accuracy:  0.624456\n",
      "Epoch:  212  Training Loss:  12.8465  Training Accuracy:  0.625867\n",
      "Epoch:  213  Training Loss:  12.8257  Training Accuracy:  0.627337\n",
      "Epoch:  214  Training Loss:  12.8487  Training Accuracy:  0.628513\n",
      "Epoch:  215  Training Loss:  12.8106  Training Accuracy:  0.629571\n",
      "Epoch:  216  Training Loss:  12.8442  Training Accuracy:  0.631452\n",
      "Epoch:  217  Training Loss:  12.8015  Training Accuracy:  0.632569\n",
      "Epoch:  218  Training Loss:  12.8145  Training Accuracy:  0.634333\n",
      "Epoch:  219  Training Loss:  12.7993  Training Accuracy:  0.635567\n",
      "Epoch:  220  Training Loss:  12.7908  Training Accuracy:  0.637213\n",
      "Epoch:  221  Training Loss:  12.8191  Training Accuracy:  0.638624\n",
      "Epoch:  222  Training Loss:  12.8086  Training Accuracy:  0.639153\n",
      "Epoch:  223  Training Loss:  12.7471  Training Accuracy:  0.640506\n",
      "Epoch:  224  Training Loss:  12.7639  Training Accuracy:  0.641917\n",
      "Epoch:  225  Training Loss:  12.7455  Training Accuracy:  0.643798\n",
      "Epoch:  226  Training Loss:  12.7768  Training Accuracy:  0.64515\n",
      "Epoch:  227  Training Loss:  12.7556  Training Accuracy:  0.646443\n",
      "Epoch:  228  Training Loss:  12.7564  Training Accuracy:  0.648031\n",
      "Epoch:  229  Training Loss:  12.7453  Training Accuracy:  0.649794\n",
      "Epoch:  230  Training Loss:  12.7426  Training Accuracy:  0.65144\n",
      "Epoch:  231  Training Loss:  12.7346  Training Accuracy:  0.652793\n",
      "Epoch:  232  Training Loss:  12.7247  Training Accuracy:  0.654262\n",
      "Epoch:  233  Training Loss:  12.7292  Training Accuracy:  0.656026\n",
      "Epoch:  234  Training Loss:  12.7363  Training Accuracy:  0.657202\n",
      "Epoch:  235  Training Loss:  12.7327  Training Accuracy:  0.65779\n",
      "Epoch:  236  Training Loss:  12.7491  Training Accuracy:  0.659612\n",
      "Epoch:  237  Training Loss:  12.7141  Training Accuracy:  0.661199\n",
      "Epoch:  238  Training Loss:  12.7284  Training Accuracy:  0.66261\n",
      "Epoch:  239  Training Loss:  12.7102  Training Accuracy:  0.66408\n",
      "Epoch:  240  Training Loss:  12.7201  Training Accuracy:  0.665256\n",
      "Epoch:  241  Training Loss:  12.6939  Training Accuracy:  0.666432\n",
      "Epoch:  242  Training Loss:  12.679  Training Accuracy:  0.667901\n",
      "Epoch:  243  Training Loss:  12.6709  Training Accuracy:  0.669312\n",
      "Epoch:  244  Training Loss:  12.6547  Training Accuracy:  0.670077\n",
      "Epoch:  245  Training Loss:  12.6491  Training Accuracy:  0.671135\n",
      "Epoch:  246  Training Loss:  12.6648  Training Accuracy:  0.672428\n",
      "Epoch:  247  Training Loss:  12.6399  Training Accuracy:  0.673957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  12.6369  Training Accuracy:  0.675485\n",
      "Epoch:  249  Training Loss:  12.6201  Training Accuracy:  0.676955\n",
      "Epoch:  250  Training Loss:  12.5989  Training Accuracy:  0.677778\n",
      "Epoch:  251  Training Loss:  12.5912  Training Accuracy:  0.679483\n",
      "Epoch:  252  Training Loss:  12.5819  Training Accuracy:  0.680717\n",
      "Epoch:  253  Training Loss:  12.5687  Training Accuracy:  0.682011\n",
      "Epoch:  254  Training Loss:  12.5634  Training Accuracy:  0.683186\n",
      "Epoch:  255  Training Loss:  12.5406  Training Accuracy:  0.684597\n",
      "Epoch:  256  Training Loss:  12.519  Training Accuracy:  0.68595\n",
      "Epoch:  257  Training Loss:  12.5133  Training Accuracy:  0.687066\n",
      "Epoch:  258  Training Loss:  12.4728  Training Accuracy:  0.688125\n",
      "Epoch:  259  Training Loss:  12.4643  Training Accuracy:  0.689477\n",
      "Epoch:  260  Training Loss:  12.4505  Training Accuracy:  0.690653\n",
      "Epoch:  261  Training Loss:  12.4387  Training Accuracy:  0.692358\n",
      "Epoch:  262  Training Loss:  12.4069  Training Accuracy:  0.693416\n",
      "Epoch:  263  Training Loss:  12.3851  Training Accuracy:  0.694356\n",
      "Epoch:  264  Training Loss:  12.3686  Training Accuracy:  0.695415\n",
      "Epoch:  265  Training Loss:  12.3343  Training Accuracy:  0.696649\n",
      "Epoch:  266  Training Loss:  12.3442  Training Accuracy:  0.697413\n",
      "Epoch:  267  Training Loss:  12.2896  Training Accuracy:  0.698942\n",
      "Epoch:  268  Training Loss:  12.2904  Training Accuracy:  0.700118\n",
      "Epoch:  269  Training Loss:  12.3082  Training Accuracy:  0.70147\n",
      "Epoch:  270  Training Loss:  12.2664  Training Accuracy:  0.701999\n",
      "Epoch:  271  Training Loss:  12.2494  Training Accuracy:  0.702763\n",
      "Epoch:  272  Training Loss:  12.2336  Training Accuracy:  0.704233\n",
      "Epoch:  273  Training Loss:  12.1254  Training Accuracy:  0.705585\n",
      "Epoch:  274  Training Loss:  12.149  Training Accuracy:  0.706878\n",
      "Epoch:  275  Training Loss:  12.1249  Training Accuracy:  0.707995\n",
      "Epoch:  276  Training Loss:  12.1036  Training Accuracy:  0.708936\n",
      "Epoch:  277  Training Loss:  12.1237  Training Accuracy:  0.709994\n",
      "Epoch:  278  Training Loss:  12.0931  Training Accuracy:  0.71117\n",
      "Epoch:  279  Training Loss:  12.1274  Training Accuracy:  0.711875\n",
      "Epoch:  280  Training Loss:  12.0767  Training Accuracy:  0.713639\n",
      "Epoch:  281  Training Loss:  12.0719  Training Accuracy:  0.714991\n",
      "Epoch:  282  Training Loss:  12.0326  Training Accuracy:  0.715932\n",
      "Epoch:  283  Training Loss:  12.0384  Training Accuracy:  0.717108\n",
      "Epoch:  284  Training Loss:  12.0197  Training Accuracy:  0.718519\n",
      "Epoch:  285  Training Loss:  11.9583  Training Accuracy:  0.719694\n",
      "Epoch:  286  Training Loss:  11.9495  Training Accuracy:  0.720635\n",
      "Epoch:  287  Training Loss:  11.8952  Training Accuracy:  0.721811\n",
      "Epoch:  288  Training Loss:  11.8947  Training Accuracy:  0.722458\n",
      "Epoch:  289  Training Loss:  11.8824  Training Accuracy:  0.723575\n",
      "Epoch:  290  Training Loss:  11.8967  Training Accuracy:  0.724339\n",
      "Epoch:  291  Training Loss:  11.9182  Training Accuracy:  0.72575\n",
      "Epoch:  292  Training Loss:  11.7987  Training Accuracy:  0.726926\n",
      "Epoch:  293  Training Loss:  11.8304  Training Accuracy:  0.727866\n",
      "Epoch:  294  Training Loss:  11.7788  Training Accuracy:  0.729277\n",
      "Epoch:  295  Training Loss:  11.7898  Training Accuracy:  0.730512\n",
      "Epoch:  296  Training Loss:  11.7507  Training Accuracy:  0.731393\n",
      "Epoch:  297  Training Loss:  11.8546  Training Accuracy:  0.732334\n",
      "Epoch:  298  Training Loss:  11.6522  Training Accuracy:  0.733569\n",
      "Epoch:  299  Training Loss:  11.7551  Training Accuracy:  0.734215\n",
      "Epoch:  300  Training Loss:  11.6361  Training Accuracy:  0.735509\n",
      "Epoch:  301  Training Loss:  11.6541  Training Accuracy:  0.736391\n",
      "Epoch:  302  Training Loss:  11.6027  Training Accuracy:  0.737684\n",
      "Epoch:  303  Training Loss:  11.7036  Training Accuracy:  0.738801\n",
      "Epoch:  304  Training Loss:  11.548  Training Accuracy:  0.740036\n",
      "Epoch:  305  Training Loss:  11.5682  Training Accuracy:  0.741152\n",
      "Epoch:  306  Training Loss:  11.5614  Training Accuracy:  0.742152\n",
      "Epoch:  307  Training Loss:  11.4794  Training Accuracy:  0.743504\n",
      "Epoch:  308  Training Loss:  11.59  Training Accuracy:  0.743974\n",
      "Epoch:  309  Training Loss:  11.4494  Training Accuracy:  0.745738\n",
      "Epoch:  310  Training Loss:  11.4332  Training Accuracy:  0.746855\n",
      "Epoch:  311  Training Loss:  11.4322  Training Accuracy:  0.747854\n",
      "Epoch:  312  Training Loss:  11.4391  Training Accuracy:  0.748501\n",
      "Epoch:  313  Training Loss:  11.4976  Training Accuracy:  0.749794\n",
      "Epoch:  314  Training Loss:  11.2948  Training Accuracy:  0.750794\n",
      "Epoch:  315  Training Loss:  11.3705  Training Accuracy:  0.751558\n",
      "Epoch:  316  Training Loss:  11.4409  Training Accuracy:  0.752734\n",
      "Epoch:  317  Training Loss:  11.2522  Training Accuracy:  0.753381\n",
      "Epoch:  318  Training Loss:  11.2653  Training Accuracy:  0.75438\n",
      "Epoch:  319  Training Loss:  11.3087  Training Accuracy:  0.755144\n",
      "Epoch:  320  Training Loss:  11.2023  Training Accuracy:  0.756614\n",
      "Epoch:  321  Training Loss:  11.3748  Training Accuracy:  0.757084\n",
      "Epoch:  322  Training Loss:  11.1676  Training Accuracy:  0.758554\n",
      "Epoch:  323  Training Loss:  11.1525  Training Accuracy:  0.759671\n",
      "Epoch:  324  Training Loss:  11.3707  Training Accuracy:  0.760906\n",
      "Epoch:  325  Training Loss:  11.0957  Training Accuracy:  0.76261\n",
      "Epoch:  326  Training Loss:  11.1517  Training Accuracy:  0.763198\n",
      "Epoch:  327  Training Loss:  11.0628  Training Accuracy:  0.764315\n",
      "Epoch:  328  Training Loss:  11.2806  Training Accuracy:  0.764844\n",
      "Epoch:  329  Training Loss:  11.0824  Training Accuracy:  0.765726\n",
      "Epoch:  330  Training Loss:  11.1356  Training Accuracy:  0.766608\n",
      "Epoch:  331  Training Loss:  10.9423  Training Accuracy:  0.767666\n",
      "Epoch:  332  Training Loss:  11.1817  Training Accuracy:  0.768431\n",
      "Epoch:  333  Training Loss:  11.0374  Training Accuracy:  0.768842\n",
      "Epoch:  334  Training Loss:  10.8558  Training Accuracy:  0.770371\n",
      "Epoch:  335  Training Loss:  11.1982  Training Accuracy:  0.770665\n",
      "Epoch:  336  Training Loss:  10.879  Training Accuracy:  0.771488\n",
      "Epoch:  337  Training Loss:  11.1175  Training Accuracy:  0.771958\n",
      "Epoch:  338  Training Loss:  10.7916  Training Accuracy:  0.773428\n",
      "Epoch:  339  Training Loss:  10.9779  Training Accuracy:  0.773545\n",
      "Epoch:  340  Training Loss:  10.7827  Training Accuracy:  0.774486\n",
      "Epoch:  341  Training Loss:  10.9733  Training Accuracy:  0.774897\n",
      "Epoch:  342  Training Loss:  10.8781  Training Accuracy:  0.775309\n",
      "Epoch:  343  Training Loss:  10.7464  Training Accuracy:  0.776367\n",
      "Epoch:  344  Training Loss:  10.7968  Training Accuracy:  0.776837\n",
      "Epoch:  345  Training Loss:  10.7929  Training Accuracy:  0.77766\n",
      "Epoch:  346  Training Loss:  10.6909  Training Accuracy:  0.778366\n",
      "Epoch:  347  Training Loss:  10.7565  Training Accuracy:  0.7796\n",
      "Epoch:  348  Training Loss:  10.6534  Training Accuracy:  0.78013\n",
      "Epoch:  349  Training Loss:  10.7034  Training Accuracy:  0.781893\n",
      "Epoch:  350  Training Loss:  10.6148  Training Accuracy:  0.781952\n",
      "Epoch:  351  Training Loss:  10.6011  Training Accuracy:  0.783128\n",
      "Epoch:  352  Training Loss:  10.5651  Training Accuracy:  0.783716\n",
      "Epoch:  353  Training Loss:  10.6777  Training Accuracy:  0.78448\n",
      "Epoch:  354  Training Loss:  10.4804  Training Accuracy:  0.785891\n",
      "Epoch:  355  Training Loss:  10.6601  Training Accuracy:  0.785773\n",
      "Epoch:  356  Training Loss:  10.3986  Training Accuracy:  0.787361\n",
      "Epoch:  357  Training Loss:  10.6886  Training Accuracy:  0.787243\n",
      "Epoch:  358  Training Loss:  10.411  Training Accuracy:  0.789183\n",
      "Epoch:  359  Training Loss:  10.5048  Training Accuracy:  0.789301\n",
      "Epoch:  360  Training Loss:  10.439  Training Accuracy:  0.790418\n",
      "Epoch:  361  Training Loss:  10.5635  Training Accuracy:  0.791123\n",
      "Epoch:  362  Training Loss:  10.3638  Training Accuracy:  0.791946\n",
      "Epoch:  363  Training Loss:  10.5517  Training Accuracy:  0.792593\n",
      "Epoch:  364  Training Loss:  10.3458  Training Accuracy:  0.793592\n",
      "Epoch:  365  Training Loss:  10.4961  Training Accuracy:  0.793416\n",
      "Epoch:  366  Training Loss:  10.2635  Training Accuracy:  0.794709\n",
      "Epoch:  367  Training Loss:  10.4557  Training Accuracy:  0.795356\n",
      "Epoch:  368  Training Loss:  10.2394  Training Accuracy:  0.796238\n",
      "Epoch:  369  Training Loss:  10.5522  Training Accuracy:  0.796179\n",
      "Epoch:  370  Training Loss:  10.2437  Training Accuracy:  0.797649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  10.3915  Training Accuracy:  0.798178\n",
      "Epoch:  372  Training Loss:  10.2973  Training Accuracy:  0.798237\n",
      "Epoch:  373  Training Loss:  10.229  Training Accuracy:  0.799236\n",
      "Epoch:  374  Training Loss:  10.2913  Training Accuracy:  0.799883\n",
      "Epoch:  375  Training Loss:  10.1325  Training Accuracy:  0.800823\n",
      "Epoch:  376  Training Loss:  10.401  Training Accuracy:  0.801117\n",
      "Epoch:  377  Training Loss:  10.0446  Training Accuracy:  0.802352\n",
      "Epoch:  378  Training Loss:  10.241  Training Accuracy:  0.802646\n",
      "Epoch:  379  Training Loss:  10.2537  Training Accuracy:  0.803645\n",
      "Epoch:  380  Training Loss:  10.1803  Training Accuracy:  0.803939\n",
      "Epoch:  381  Training Loss:  10.0958  Training Accuracy:  0.805644\n",
      "Epoch:  382  Training Loss:  10.0419  Training Accuracy:  0.805997\n",
      "Epoch:  383  Training Loss:  10.2988  Training Accuracy:  0.805644\n",
      "Epoch:  384  Training Loss:  9.97553  Training Accuracy:  0.807525\n",
      "Epoch:  385  Training Loss:  10.1376  Training Accuracy:  0.808466\n",
      "Epoch:  386  Training Loss:  10.1058  Training Accuracy:  0.808407\n",
      "Epoch:  387  Training Loss:  9.96048  Training Accuracy:  0.809818\n",
      "Epoch:  388  Training Loss:  10.0967  Training Accuracy:  0.810288\n",
      "Epoch:  389  Training Loss:  9.94354  Training Accuracy:  0.811111\n",
      "Epoch:  390  Training Loss:  10.0721  Training Accuracy:  0.811347\n",
      "Epoch:  391  Training Loss:  9.89449  Training Accuracy:  0.812581\n",
      "Epoch:  392  Training Loss:  10.1695  Training Accuracy:  0.812699\n",
      "Epoch:  393  Training Loss:  9.84223  Training Accuracy:  0.814462\n",
      "Epoch:  394  Training Loss:  9.94049  Training Accuracy:  0.815168\n",
      "Epoch:  395  Training Loss:  9.81697  Training Accuracy:  0.815815\n",
      "Epoch:  396  Training Loss:  9.92396  Training Accuracy:  0.817108\n",
      "Epoch:  397  Training Loss:  10.0907  Training Accuracy:  0.816285\n",
      "Epoch:  398  Training Loss:  9.7344  Training Accuracy:  0.819401\n",
      "Epoch:  399  Training Loss:  9.98108  Training Accuracy:  0.818049\n",
      "Epoch:  400  Training Loss:  9.74558  Training Accuracy:  0.820224\n",
      "Epoch:  401  Training Loss:  9.80066  Training Accuracy:  0.821106\n",
      "Epoch:  402  Training Loss:  9.80304  Training Accuracy:  0.820224\n",
      "Epoch:  403  Training Loss:  9.79242  Training Accuracy:  0.82234\n",
      "Epoch:  404  Training Loss:  9.68799  Training Accuracy:  0.822869\n",
      "Epoch:  405  Training Loss:  9.79557  Training Accuracy:  0.823869\n",
      "Epoch:  406  Training Loss:  9.75306  Training Accuracy:  0.823869\n",
      "Epoch:  407  Training Loss:  9.73785  Training Accuracy:  0.825809\n",
      "Epoch:  408  Training Loss:  9.55989  Training Accuracy:  0.826808\n",
      "Epoch:  409  Training Loss:  9.68016  Training Accuracy:  0.82722\n",
      "Epoch:  410  Training Loss:  9.66606  Training Accuracy:  0.827455\n",
      "Epoch:  411  Training Loss:  9.60289  Training Accuracy:  0.828807\n",
      "Epoch:  412  Training Loss:  9.57306  Training Accuracy:  0.829336\n",
      "Epoch:  413  Training Loss:  9.51739  Training Accuracy:  0.830335\n",
      "Epoch:  414  Training Loss:  9.63758  Training Accuracy:  0.829983\n",
      "Epoch:  415  Training Loss:  9.4631  Training Accuracy:  0.83204\n",
      "Epoch:  416  Training Loss:  9.47486  Training Accuracy:  0.832805\n",
      "Epoch:  417  Training Loss:  9.47042  Training Accuracy:  0.833569\n",
      "Epoch:  418  Training Loss:  9.6124  Training Accuracy:  0.833686\n",
      "Epoch:  419  Training Loss:  9.44569  Training Accuracy:  0.835039\n",
      "Epoch:  420  Training Loss:  9.44769  Training Accuracy:  0.835509\n",
      "Epoch:  421  Training Loss:  9.45178  Training Accuracy:  0.836273\n",
      "Epoch:  422  Training Loss:  9.5713  Training Accuracy:  0.836743\n",
      "Epoch:  423  Training Loss:  9.40851  Training Accuracy:  0.83786\n",
      "Epoch:  424  Training Loss:  9.4163  Training Accuracy:  0.838037\n",
      "Epoch:  425  Training Loss:  9.40603  Training Accuracy:  0.839036\n",
      "Epoch:  426  Training Loss:  9.53113  Training Accuracy:  0.838801\n",
      "Epoch:  427  Training Loss:  9.3588  Training Accuracy:  0.840153\n",
      "Epoch:  428  Training Loss:  9.34719  Training Accuracy:  0.840859\n",
      "Epoch:  429  Training Loss:  9.47759  Training Accuracy:  0.8408\n",
      "Epoch:  430  Training Loss:  9.31561  Training Accuracy:  0.841976\n",
      "Epoch:  431  Training Loss:  9.31086  Training Accuracy:  0.842622\n",
      "Epoch:  432  Training Loss:  9.32842  Training Accuracy:  0.842916\n",
      "Epoch:  433  Training Loss:  9.29883  Training Accuracy:  0.843563\n",
      "Epoch:  434  Training Loss:  9.32291  Training Accuracy:  0.843857\n",
      "Epoch:  435  Training Loss:  9.25174  Training Accuracy:  0.845091\n",
      "Epoch:  436  Training Loss:  9.31516  Training Accuracy:  0.845033\n",
      "Epoch:  437  Training Loss:  9.26401  Training Accuracy:  0.845562\n",
      "Epoch:  438  Training Loss:  9.23347  Training Accuracy:  0.846267\n",
      "Epoch:  439  Training Loss:  9.24162  Training Accuracy:  0.846679\n",
      "Epoch:  440  Training Loss:  9.21814  Training Accuracy:  0.847267\n",
      "Epoch:  441  Training Loss:  9.23559  Training Accuracy:  0.847443\n",
      "Epoch:  442  Training Loss:  9.18206  Training Accuracy:  0.848207\n",
      "Epoch:  443  Training Loss:  9.22182  Training Accuracy:  0.848207\n",
      "Epoch:  444  Training Loss:  9.14621  Training Accuracy:  0.849501\n",
      "Epoch:  445  Training Loss:  9.19951  Training Accuracy:  0.849324\n",
      "Epoch:  446  Training Loss:  9.13868  Training Accuracy:  0.850618\n",
      "Epoch:  447  Training Loss:  9.11668  Training Accuracy:  0.850912\n",
      "Epoch:  448  Training Loss:  9.15477  Training Accuracy:  0.850618\n",
      "Epoch:  449  Training Loss:  9.08356  Training Accuracy:  0.85197\n",
      "Epoch:  450  Training Loss:  9.08684  Training Accuracy:  0.851793\n",
      "Epoch:  451  Training Loss:  9.08338  Training Accuracy:  0.85244\n",
      "Epoch:  452  Training Loss:  9.07857  Training Accuracy:  0.852499\n",
      "Epoch:  453  Training Loss:  9.03563  Training Accuracy:  0.853028\n",
      "Epoch:  454  Training Loss:  9.06927  Training Accuracy:  0.853204\n",
      "Epoch:  455  Training Loss:  8.98271  Training Accuracy:  0.854439\n",
      "Epoch:  456  Training Loss:  8.97931  Training Accuracy:  0.854909\n",
      "Epoch:  457  Training Loss:  9.02728  Training Accuracy:  0.854733\n",
      "Epoch:  458  Training Loss:  8.95902  Training Accuracy:  0.855497\n",
      "Epoch:  459  Training Loss:  8.96355  Training Accuracy:  0.856203\n",
      "Epoch:  460  Training Loss:  8.97167  Training Accuracy:  0.856261\n",
      "Epoch:  461  Training Loss:  8.94186  Training Accuracy:  0.856967\n",
      "Epoch:  462  Training Loss:  8.94661  Training Accuracy:  0.857378\n",
      "Epoch:  463  Training Loss:  8.91065  Training Accuracy:  0.858025\n",
      "Epoch:  464  Training Loss:  8.97311  Training Accuracy:  0.858143\n",
      "Epoch:  465  Training Loss:  8.89276  Training Accuracy:  0.858848\n",
      "Epoch:  466  Training Loss:  8.87086  Training Accuracy:  0.859495\n",
      "Epoch:  467  Training Loss:  8.91438  Training Accuracy:  0.85973\n",
      "Epoch:  468  Training Loss:  8.86527  Training Accuracy:  0.860377\n",
      "Epoch:  469  Training Loss:  8.92623  Training Accuracy:  0.860729\n",
      "Epoch:  470  Training Loss:  8.83876  Training Accuracy:  0.86167\n",
      "Epoch:  471  Training Loss:  8.84735  Training Accuracy:  0.862375\n",
      "Epoch:  472  Training Loss:  8.86393  Training Accuracy:  0.862787\n",
      "Epoch:  473  Training Loss:  8.84208  Training Accuracy:  0.863375\n",
      "Epoch:  474  Training Loss:  8.84758  Training Accuracy:  0.863845\n",
      "Epoch:  475  Training Loss:  8.81763  Training Accuracy:  0.864257\n",
      "Epoch:  476  Training Loss:  8.85667  Training Accuracy:  0.864551\n",
      "Epoch:  477  Training Loss:  8.78653  Training Accuracy:  0.86508\n",
      "Epoch:  478  Training Loss:  8.77886  Training Accuracy:  0.86555\n",
      "Epoch:  479  Training Loss:  8.80343  Training Accuracy:  0.865785\n",
      "Epoch:  480  Training Loss:  8.76271  Training Accuracy:  0.866256\n",
      "Epoch:  481  Training Loss:  8.81981  Training Accuracy:  0.866667\n",
      "Epoch:  482  Training Loss:  8.73339  Training Accuracy:  0.867079\n",
      "Epoch:  483  Training Loss:  8.72365  Training Accuracy:  0.867314\n",
      "Epoch:  484  Training Loss:  8.74926  Training Accuracy:  0.867431\n",
      "Epoch:  485  Training Loss:  8.72251  Training Accuracy:  0.86796\n",
      "Epoch:  486  Training Loss:  8.76193  Training Accuracy:  0.868078\n",
      "Epoch:  487  Training Loss:  8.6968  Training Accuracy:  0.86849\n",
      "Epoch:  488  Training Loss:  8.67607  Training Accuracy:  0.868842\n",
      "Epoch:  489  Training Loss:  8.72536  Training Accuracy:  0.869136\n",
      "Epoch:  490  Training Loss:  8.66548  Training Accuracy:  0.869489\n",
      "Epoch:  491  Training Loss:  8.66165  Training Accuracy:  0.869842\n",
      "Epoch:  492  Training Loss:  8.72413  Training Accuracy:  0.869901\n",
      "Epoch:  493  Training Loss:  8.64823  Training Accuracy:  0.870371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  8.67809  Training Accuracy:  0.870665\n",
      "Epoch:  495  Training Loss:  8.62016  Training Accuracy:  0.8709\n",
      "Epoch:  496  Training Loss:  8.59817  Training Accuracy:  0.871253\n",
      "Epoch:  497  Training Loss:  8.66621  Training Accuracy:  0.871723\n",
      "Epoch:  498  Training Loss:  8.58877  Training Accuracy:  0.871841\n",
      "Epoch:  499  Training Loss:  8.58116  Training Accuracy:  0.87237\n",
      "Epoch:  500  Training Loss:  8.61217  Training Accuracy:  0.872781\n",
      "Epoch:  501  Training Loss:  8.56334  Training Accuracy:  0.873134\n",
      "Epoch:  502  Training Loss:  8.54465  Training Accuracy:  0.873251\n",
      "Epoch:  503  Training Loss:  8.66144  Training Accuracy:  0.873487\n",
      "Epoch:  504  Training Loss:  8.53645  Training Accuracy:  0.874016\n",
      "Epoch:  505  Training Loss:  8.52263  Training Accuracy:  0.874368\n",
      "Epoch:  506  Training Loss:  8.56783  Training Accuracy:  0.874604\n",
      "Epoch:  507  Training Loss:  8.50972  Training Accuracy:  0.874956\n",
      "Epoch:  508  Training Loss:  8.48791  Training Accuracy:  0.875191\n",
      "Epoch:  509  Training Loss:  8.54621  Training Accuracy:  0.875368\n",
      "Epoch:  510  Training Loss:  8.46327  Training Accuracy:  0.875779\n",
      "Epoch:  511  Training Loss:  8.46016  Training Accuracy:  0.876308\n",
      "Epoch:  512  Training Loss:  8.51082  Training Accuracy:  0.876544\n",
      "Epoch:  513  Training Loss:  8.45364  Training Accuracy:  0.876838\n",
      "Epoch:  514  Training Loss:  8.42623  Training Accuracy:  0.877132\n",
      "Epoch:  515  Training Loss:  8.48364  Training Accuracy:  0.877719\n",
      "Epoch:  516  Training Loss:  8.41492  Training Accuracy:  0.878013\n",
      "Epoch:  517  Training Loss:  8.40516  Training Accuracy:  0.878249\n",
      "Epoch:  518  Training Loss:  8.44104  Training Accuracy:  0.878895\n",
      "Epoch:  519  Training Loss:  8.3982  Training Accuracy:  0.87913\n",
      "Epoch:  520  Training Loss:  8.36139  Training Accuracy:  0.879366\n",
      "Epoch:  521  Training Loss:  8.41283  Training Accuracy:  0.879895\n",
      "Epoch:  522  Training Loss:  8.35477  Training Accuracy:  0.880247\n",
      "Epoch:  523  Training Loss:  8.34669  Training Accuracy:  0.8806\n",
      "Epoch:  524  Training Loss:  8.38797  Training Accuracy:  0.881012\n",
      "Epoch:  525  Training Loss:  8.33586  Training Accuracy:  0.881129\n",
      "Epoch:  526  Training Loss:  8.30265  Training Accuracy:  0.881423\n",
      "Epoch:  527  Training Loss:  8.35762  Training Accuracy:  0.882364\n",
      "Epoch:  528  Training Loss:  8.29592  Training Accuracy:  0.882658\n",
      "Epoch:  529  Training Loss:  8.28287  Training Accuracy:  0.882716\n",
      "Epoch:  530  Training Loss:  8.31365  Training Accuracy:  0.88301\n",
      "Epoch:  531  Training Loss:  8.27551  Training Accuracy:  0.883246\n",
      "Epoch:  532  Training Loss:  8.29948  Training Accuracy:  0.883598\n",
      "Epoch:  533  Training Loss:  8.25359  Training Accuracy:  0.88401\n",
      "Epoch:  534  Training Loss:  8.28176  Training Accuracy:  0.884127\n",
      "Epoch:  535  Training Loss:  8.23233  Training Accuracy:  0.88448\n",
      "Epoch:  536  Training Loss:  8.20632  Training Accuracy:  0.884833\n",
      "Epoch:  537  Training Loss:  8.26433  Training Accuracy:  0.885244\n",
      "Epoch:  538  Training Loss:  8.20808  Training Accuracy:  0.885362\n",
      "Epoch:  539  Training Loss:  8.19207  Training Accuracy:  0.885597\n",
      "Epoch:  540  Training Loss:  8.2276  Training Accuracy:  0.886126\n",
      "Epoch:  541  Training Loss:  8.18063  Training Accuracy:  0.886067\n",
      "Epoch:  542  Training Loss:  8.20897  Training Accuracy:  0.886597\n",
      "Epoch:  543  Training Loss:  8.16501  Training Accuracy:  0.886773\n",
      "Epoch:  544  Training Loss:  8.14439  Training Accuracy:  0.887126\n",
      "Epoch:  545  Training Loss:  8.18055  Training Accuracy:  0.887537\n",
      "Epoch:  546  Training Loss:  8.12728  Training Accuracy:  0.887537\n",
      "Epoch:  547  Training Loss:  8.13778  Training Accuracy:  0.887714\n",
      "Epoch:  548  Training Loss:  8.20919  Training Accuracy:  0.888301\n",
      "Epoch:  549  Training Loss:  8.11864  Training Accuracy:  0.888419\n",
      "Epoch:  550  Training Loss:  8.08896  Training Accuracy:  0.888654\n",
      "Epoch:  551  Training Loss:  8.10297  Training Accuracy:  0.888772\n",
      "Epoch:  552  Training Loss:  8.11119  Training Accuracy:  0.888948\n",
      "Epoch:  553  Training Loss:  8.10942  Training Accuracy:  0.889418\n",
      "Epoch:  554  Training Loss:  8.08836  Training Accuracy:  0.889595\n",
      "Epoch:  555  Training Loss:  8.08225  Training Accuracy:  0.890065\n",
      "Epoch:  556  Training Loss:  8.1368  Training Accuracy:  0.8903\n",
      "Epoch:  557  Training Loss:  8.04606  Training Accuracy:  0.890535\n",
      "Epoch:  558  Training Loss:  8.01186  Training Accuracy:  0.890829\n",
      "Epoch:  559  Training Loss:  8.04455  Training Accuracy:  0.891358\n",
      "Epoch:  560  Training Loss:  8.02454  Training Accuracy:  0.891652\n",
      "Epoch:  561  Training Loss:  8.03388  Training Accuracy:  0.892064\n",
      "Epoch:  562  Training Loss:  8.01361  Training Accuracy:  0.892123\n",
      "Epoch:  563  Training Loss:  8.06787  Training Accuracy:  0.892475\n",
      "Epoch:  564  Training Loss:  7.97009  Training Accuracy:  0.892769\n",
      "Epoch:  565  Training Loss:  7.98542  Training Accuracy:  0.893005\n",
      "Epoch:  566  Training Loss:  7.98479  Training Accuracy:  0.89324\n",
      "Epoch:  567  Training Loss:  7.98339  Training Accuracy:  0.893475\n",
      "Epoch:  568  Training Loss:  7.97073  Training Accuracy:  0.893828\n",
      "Epoch:  569  Training Loss:  8.01488  Training Accuracy:  0.894239\n",
      "Epoch:  570  Training Loss:  7.9267  Training Accuracy:  0.894592\n",
      "Epoch:  571  Training Loss:  7.94482  Training Accuracy:  0.894709\n",
      "Epoch:  572  Training Loss:  7.93353  Training Accuracy:  0.895003\n",
      "Epoch:  573  Training Loss:  7.92378  Training Accuracy:  0.895297\n",
      "Epoch:  574  Training Loss:  7.92116  Training Accuracy:  0.895532\n",
      "Epoch:  575  Training Loss:  7.91362  Training Accuracy:  0.895885\n",
      "Epoch:  576  Training Loss:  7.89613  Training Accuracy:  0.896297\n",
      "Epoch:  577  Training Loss:  7.89406  Training Accuracy:  0.896297\n",
      "Epoch:  578  Training Loss:  7.93626  Training Accuracy:  0.896708\n",
      "Epoch:  579  Training Loss:  7.854  Training Accuracy:  0.896767\n",
      "Epoch:  580  Training Loss:  7.85867  Training Accuracy:  0.897002\n",
      "Epoch:  581  Training Loss:  7.85015  Training Accuracy:  0.89712\n",
      "Epoch:  582  Training Loss:  7.84493  Training Accuracy:  0.897355\n",
      "Epoch:  583  Training Loss:  7.84632  Training Accuracy:  0.897766\n",
      "Epoch:  584  Training Loss:  7.83496  Training Accuracy:  0.897943\n",
      "Epoch:  585  Training Loss:  7.86433  Training Accuracy:  0.898237\n",
      "Epoch:  586  Training Loss:  7.82433  Training Accuracy:  0.898354\n",
      "Epoch:  587  Training Loss:  7.78352  Training Accuracy:  0.898766\n",
      "Epoch:  588  Training Loss:  7.76426  Training Accuracy:  0.898942\n",
      "Epoch:  589  Training Loss:  7.83306  Training Accuracy:  0.899177\n",
      "Epoch:  590  Training Loss:  7.80715  Training Accuracy:  0.899589\n",
      "Epoch:  591  Training Loss:  7.76138  Training Accuracy:  0.899883\n",
      "Epoch:  592  Training Loss:  7.7765  Training Accuracy:  0.900236\n",
      "Epoch:  593  Training Loss:  7.78496  Training Accuracy:  0.900236\n",
      "Epoch:  594  Training Loss:  7.77416  Training Accuracy:  0.900471\n",
      "Epoch:  595  Training Loss:  7.77413  Training Accuracy:  0.900765\n",
      "Epoch:  596  Training Loss:  7.76357  Training Accuracy:  0.901294\n",
      "Epoch:  597  Training Loss:  7.78949  Training Accuracy:  0.90147\n",
      "Epoch:  598  Training Loss:  7.7491  Training Accuracy:  0.901764\n",
      "Epoch:  599  Training Loss:  7.7101  Training Accuracy:  0.901999\n",
      "Epoch:  600  Training Loss:  7.72544  Training Accuracy:  0.902293\n",
      "Epoch:  601  Training Loss:  7.72938  Training Accuracy:  0.90247\n",
      "Epoch:  602  Training Loss:  7.72055  Training Accuracy:  0.902881\n",
      "Epoch:  603  Training Loss:  7.71813  Training Accuracy:  0.902822\n",
      "Epoch:  604  Training Loss:  7.70313  Training Accuracy:  0.903057\n",
      "Epoch:  605  Training Loss:  7.7357  Training Accuracy:  0.903116\n",
      "Epoch:  606  Training Loss:  7.69536  Training Accuracy:  0.903528\n",
      "Epoch:  607  Training Loss:  7.65249  Training Accuracy:  0.904057\n",
      "Epoch:  608  Training Loss:  7.65508  Training Accuracy:  0.904175\n",
      "Epoch:  609  Training Loss:  7.6782  Training Accuracy:  0.904233\n",
      "Epoch:  610  Training Loss:  7.63654  Training Accuracy:  0.90488\n",
      "Epoch:  611  Training Loss:  7.63404  Training Accuracy:  0.905056\n",
      "Epoch:  612  Training Loss:  7.6572  Training Accuracy:  0.905291\n",
      "Epoch:  613  Training Loss:  7.659  Training Accuracy:  0.90535\n",
      "Epoch:  614  Training Loss:  7.65437  Training Accuracy:  0.905703\n",
      "Epoch:  615  Training Loss:  7.64978  Training Accuracy:  0.905703\n",
      "Epoch:  616  Training Loss:  7.63536  Training Accuracy:  0.905821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  7.63187  Training Accuracy:  0.906232\n",
      "Epoch:  618  Training Loss:  7.62945  Training Accuracy:  0.90682\n",
      "Epoch:  619  Training Loss:  7.61442  Training Accuracy:  0.90729\n",
      "Epoch:  620  Training Loss:  7.6405  Training Accuracy:  0.907408\n",
      "Epoch:  621  Training Loss:  7.59476  Training Accuracy:  0.907996\n",
      "Epoch:  622  Training Loss:  7.59539  Training Accuracy:  0.908113\n",
      "Epoch:  623  Training Loss:  7.60094  Training Accuracy:  0.908349\n",
      "Epoch:  624  Training Loss:  7.5891  Training Accuracy:  0.908466\n",
      "Epoch:  625  Training Loss:  7.58318  Training Accuracy:  0.908878\n",
      "Epoch:  626  Training Loss:  7.57714  Training Accuracy:  0.90923\n",
      "Epoch:  627  Training Loss:  7.57636  Training Accuracy:  0.909172\n",
      "Epoch:  628  Training Loss:  7.56204  Training Accuracy:  0.909407\n",
      "Epoch:  629  Training Loss:  7.56379  Training Accuracy:  0.909524\n",
      "Epoch:  630  Training Loss:  7.5436  Training Accuracy:  0.909936\n",
      "Epoch:  631  Training Loss:  7.57347  Training Accuracy:  0.909936\n",
      "Epoch:  632  Training Loss:  7.52896  Training Accuracy:  0.910347\n",
      "Epoch:  633  Training Loss:  7.52635  Training Accuracy:  0.910582\n",
      "Epoch:  634  Training Loss:  7.52361  Training Accuracy:  0.910935\n",
      "Epoch:  635  Training Loss:  7.51936  Training Accuracy:  0.910994\n",
      "Epoch:  636  Training Loss:  7.51463  Training Accuracy:  0.911406\n",
      "Epoch:  637  Training Loss:  7.51594  Training Accuracy:  0.911582\n",
      "Epoch:  638  Training Loss:  7.50814  Training Accuracy:  0.911935\n",
      "Epoch:  639  Training Loss:  7.50795  Training Accuracy:  0.911935\n",
      "Epoch:  640  Training Loss:  7.49119  Training Accuracy:  0.912464\n",
      "Epoch:  641  Training Loss:  7.51679  Training Accuracy:  0.912522\n",
      "Epoch:  642  Training Loss:  7.47234  Training Accuracy:  0.912993\n",
      "Epoch:  643  Training Loss:  7.47162  Training Accuracy:  0.913169\n",
      "Epoch:  644  Training Loss:  7.46636  Training Accuracy:  0.913346\n",
      "Epoch:  645  Training Loss:  7.46682  Training Accuracy:  0.913639\n",
      "Epoch:  646  Training Loss:  7.45984  Training Accuracy:  0.913875\n",
      "Epoch:  647  Training Loss:  7.45931  Training Accuracy:  0.914051\n",
      "Epoch:  648  Training Loss:  7.45108  Training Accuracy:  0.914169\n",
      "Epoch:  649  Training Loss:  7.44893  Training Accuracy:  0.914463\n",
      "Epoch:  650  Training Loss:  7.44285  Training Accuracy:  0.91458\n",
      "Epoch:  651  Training Loss:  7.44315  Training Accuracy:  0.914757\n",
      "Epoch:  652  Training Loss:  7.43644  Training Accuracy:  0.914992\n",
      "Epoch:  653  Training Loss:  7.43471  Training Accuracy:  0.915109\n",
      "Epoch:  654  Training Loss:  7.42901  Training Accuracy:  0.915403\n",
      "Epoch:  655  Training Loss:  7.42221  Training Accuracy:  0.915462\n",
      "Epoch:  656  Training Loss:  7.41133  Training Accuracy:  0.915756\n",
      "Epoch:  657  Training Loss:  7.42556  Training Accuracy:  0.915815\n",
      "Epoch:  658  Training Loss:  7.3939  Training Accuracy:  0.916167\n",
      "Epoch:  659  Training Loss:  7.38527  Training Accuracy:  0.916109\n",
      "Epoch:  660  Training Loss:  7.37975  Training Accuracy:  0.916461\n",
      "Epoch:  661  Training Loss:  7.37656  Training Accuracy:  0.916638\n",
      "Epoch:  662  Training Loss:  7.37289  Training Accuracy:  0.916991\n",
      "Epoch:  663  Training Loss:  7.36734  Training Accuracy:  0.917402\n",
      "Epoch:  664  Training Loss:  7.36091  Training Accuracy:  0.917813\n",
      "Epoch:  665  Training Loss:  7.35333  Training Accuracy:  0.918049\n",
      "Epoch:  666  Training Loss:  7.35024  Training Accuracy:  0.91846\n",
      "Epoch:  667  Training Loss:  7.34164  Training Accuracy:  0.918637\n",
      "Epoch:  668  Training Loss:  7.33493  Training Accuracy:  0.919107\n",
      "Epoch:  669  Training Loss:  7.33052  Training Accuracy:  0.919577\n",
      "Epoch:  670  Training Loss:  7.32747  Training Accuracy:  0.91993\n",
      "Epoch:  671  Training Loss:  7.32058  Training Accuracy:  0.920224\n",
      "Epoch:  672  Training Loss:  7.32354  Training Accuracy:  0.9204\n",
      "Epoch:  673  Training Loss:  7.31573  Training Accuracy:  0.920812\n",
      "Epoch:  674  Training Loss:  7.31371  Training Accuracy:  0.920988\n",
      "Epoch:  675  Training Loss:  7.31029  Training Accuracy:  0.921223\n",
      "Epoch:  676  Training Loss:  7.30515  Training Accuracy:  0.9214\n",
      "Epoch:  677  Training Loss:  7.30275  Training Accuracy:  0.921458\n",
      "Epoch:  678  Training Loss:  7.29472  Training Accuracy:  0.921752\n",
      "Epoch:  679  Training Loss:  7.30128  Training Accuracy:  0.921988\n",
      "Epoch:  680  Training Loss:  7.29336  Training Accuracy:  0.922458\n",
      "Epoch:  681  Training Loss:  7.28428  Training Accuracy:  0.922693\n",
      "Epoch:  682  Training Loss:  7.27797  Training Accuracy:  0.923104\n",
      "Epoch:  683  Training Loss:  7.28219  Training Accuracy:  0.923398\n",
      "Epoch:  684  Training Loss:  7.27151  Training Accuracy:  0.923516\n",
      "Epoch:  685  Training Loss:  7.26762  Training Accuracy:  0.923751\n",
      "Epoch:  686  Training Loss:  7.26754  Training Accuracy:  0.923751\n",
      "Epoch:  687  Training Loss:  7.25824  Training Accuracy:  0.924222\n",
      "Epoch:  688  Training Loss:  7.24779  Training Accuracy:  0.92428\n",
      "Epoch:  689  Training Loss:  7.25176  Training Accuracy:  0.924457\n",
      "Epoch:  690  Training Loss:  7.24243  Training Accuracy:  0.924692\n",
      "Epoch:  691  Training Loss:  7.23687  Training Accuracy:  0.924751\n",
      "Epoch:  692  Training Loss:  7.2383  Training Accuracy:  0.924809\n",
      "Epoch:  693  Training Loss:  7.23138  Training Accuracy:  0.925103\n",
      "Epoch:  694  Training Loss:  7.22557  Training Accuracy:  0.92528\n",
      "Epoch:  695  Training Loss:  7.21717  Training Accuracy:  0.925515\n",
      "Epoch:  696  Training Loss:  7.22166  Training Accuracy:  0.925632\n",
      "Epoch:  697  Training Loss:  7.20907  Training Accuracy:  0.925868\n",
      "Epoch:  698  Training Loss:  7.21036  Training Accuracy:  0.926397\n",
      "Epoch:  699  Training Loss:  7.20141  Training Accuracy:  0.926691\n",
      "Epoch:  700  Training Loss:  7.20576  Training Accuracy:  0.926985\n",
      "Epoch:  701  Training Loss:  7.19633  Training Accuracy:  0.927102\n",
      "Epoch:  702  Training Loss:  7.20111  Training Accuracy:  0.927279\n",
      "Epoch:  703  Training Loss:  7.19044  Training Accuracy:  0.927455\n",
      "Epoch:  704  Training Loss:  7.19474  Training Accuracy:  0.927396\n",
      "Epoch:  705  Training Loss:  7.18567  Training Accuracy:  0.927808\n",
      "Epoch:  706  Training Loss:  7.18075  Training Accuracy:  0.927925\n",
      "Epoch:  707  Training Loss:  7.17606  Training Accuracy:  0.928043\n",
      "Epoch:  708  Training Loss:  7.17624  Training Accuracy:  0.92816\n",
      "Epoch:  709  Training Loss:  7.17261  Training Accuracy:  0.928631\n",
      "Epoch:  710  Training Loss:  7.16834  Training Accuracy:  0.928807\n",
      "Epoch:  711  Training Loss:  7.16531  Training Accuracy:  0.929101\n",
      "Epoch:  712  Training Loss:  7.16282  Training Accuracy:  0.929336\n",
      "Epoch:  713  Training Loss:  7.15823  Training Accuracy:  0.92963\n",
      "Epoch:  714  Training Loss:  7.15527  Training Accuracy:  0.929748\n",
      "Epoch:  715  Training Loss:  7.153  Training Accuracy:  0.9301\n",
      "Epoch:  716  Training Loss:  7.15356  Training Accuracy:  0.930336\n",
      "Epoch:  717  Training Loss:  7.14325  Training Accuracy:  0.930688\n",
      "Epoch:  718  Training Loss:  7.13916  Training Accuracy:  0.930923\n",
      "Epoch:  719  Training Loss:  7.13472  Training Accuracy:  0.931276\n",
      "Epoch:  720  Training Loss:  7.13313  Training Accuracy:  0.93157\n",
      "Epoch:  721  Training Loss:  7.12882  Training Accuracy:  0.931805\n",
      "Epoch:  722  Training Loss:  7.12398  Training Accuracy:  0.931923\n",
      "Epoch:  723  Training Loss:  7.12063  Training Accuracy:  0.93204\n",
      "Epoch:  724  Training Loss:  7.11794  Training Accuracy:  0.932276\n",
      "Epoch:  725  Training Loss:  7.11101  Training Accuracy:  0.93257\n",
      "Epoch:  726  Training Loss:  7.10889  Training Accuracy:  0.932687\n",
      "Epoch:  727  Training Loss:  7.10493  Training Accuracy:  0.93304\n",
      "Epoch:  728  Training Loss:  7.10293  Training Accuracy:  0.933216\n",
      "Epoch:  729  Training Loss:  7.09845  Training Accuracy:  0.933216\n",
      "Epoch:  730  Training Loss:  7.09589  Training Accuracy:  0.933451\n",
      "Epoch:  731  Training Loss:  7.09442  Training Accuracy:  0.933687\n",
      "Epoch:  732  Training Loss:  7.09274  Training Accuracy:  0.933863\n",
      "Epoch:  733  Training Loss:  7.08179  Training Accuracy:  0.933922\n",
      "Epoch:  734  Training Loss:  7.0766  Training Accuracy:  0.934333\n",
      "Epoch:  735  Training Loss:  7.0761  Training Accuracy:  0.934686\n",
      "Epoch:  736  Training Loss:  7.07001  Training Accuracy:  0.934862\n",
      "Epoch:  737  Training Loss:  7.06567  Training Accuracy:  0.935097\n",
      "Epoch:  738  Training Loss:  7.06237  Training Accuracy:  0.935274\n",
      "Epoch:  739  Training Loss:  7.0585  Training Accuracy:  0.935333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  740  Training Loss:  7.05498  Training Accuracy:  0.93545\n",
      "Epoch:  741  Training Loss:  7.05322  Training Accuracy:  0.935685\n",
      "Epoch:  742  Training Loss:  7.04774  Training Accuracy:  0.935862\n",
      "Epoch:  743  Training Loss:  7.04505  Training Accuracy:  0.936038\n",
      "Epoch:  744  Training Loss:  7.03927  Training Accuracy:  0.936097\n",
      "Epoch:  745  Training Loss:  7.03758  Training Accuracy:  0.936097\n",
      "Epoch:  746  Training Loss:  7.03586  Training Accuracy:  0.936273\n",
      "Epoch:  747  Training Loss:  7.03174  Training Accuracy:  0.936567\n",
      "Epoch:  748  Training Loss:  7.02883  Training Accuracy:  0.936626\n",
      "Epoch:  749  Training Loss:  7.02379  Training Accuracy:  0.936802\n",
      "Epoch:  750  Training Loss:  7.02081  Training Accuracy:  0.936861\n",
      "Epoch:  751  Training Loss:  7.01574  Training Accuracy:  0.937038\n",
      "Epoch:  752  Training Loss:  7.01181  Training Accuracy:  0.937273\n",
      "Epoch:  753  Training Loss:  7.0124  Training Accuracy:  0.937508\n",
      "Epoch:  754  Training Loss:  7.00867  Training Accuracy:  0.937684\n",
      "Epoch:  755  Training Loss:  7.00693  Training Accuracy:  0.937861\n",
      "Epoch:  756  Training Loss:  7.0025  Training Accuracy:  0.938037\n",
      "Epoch:  757  Training Loss:  6.98559  Training Accuracy:  0.938037\n",
      "Epoch:  758  Training Loss:  6.95876  Training Accuracy:  0.938448\n",
      "Epoch:  759  Training Loss:  6.99505  Training Accuracy:  0.938801\n",
      "Epoch:  760  Training Loss:  6.99371  Training Accuracy:  0.939036\n",
      "Epoch:  761  Training Loss:  6.98832  Training Accuracy:  0.93933\n",
      "Epoch:  762  Training Loss:  6.9894  Training Accuracy:  0.939389\n",
      "Epoch:  763  Training Loss:  6.98201  Training Accuracy:  0.939389\n",
      "Epoch:  764  Training Loss:  6.9822  Training Accuracy:  0.939624\n",
      "Epoch:  765  Training Loss:  6.97648  Training Accuracy:  0.939624\n",
      "Epoch:  766  Training Loss:  6.97592  Training Accuracy:  0.939801\n",
      "Epoch:  767  Training Loss:  6.96667  Training Accuracy:  0.939918\n",
      "Epoch:  768  Training Loss:  6.97029  Training Accuracy:  0.940212\n",
      "Epoch:  769  Training Loss:  6.96485  Training Accuracy:  0.94033\n",
      "Epoch:  770  Training Loss:  6.96412  Training Accuracy:  0.940506\n",
      "Epoch:  771  Training Loss:  6.95492  Training Accuracy:  0.940682\n",
      "Epoch:  772  Training Loss:  6.94094  Training Accuracy:  0.9408\n",
      "Epoch:  773  Training Loss:  6.91639  Training Accuracy:  0.94127\n",
      "Epoch:  774  Training Loss:  6.93949  Training Accuracy:  0.941447\n",
      "Epoch:  775  Training Loss:  6.94246  Training Accuracy:  0.941623\n",
      "Epoch:  776  Training Loss:  6.94195  Training Accuracy:  0.941799\n",
      "Epoch:  777  Training Loss:  6.93834  Training Accuracy:  0.941917\n",
      "Epoch:  778  Training Loss:  6.93716  Training Accuracy:  0.942035\n",
      "Epoch:  779  Training Loss:  6.93039  Training Accuracy:  0.94227\n",
      "Epoch:  780  Training Loss:  6.92917  Training Accuracy:  0.942505\n",
      "Epoch:  781  Training Loss:  6.92433  Training Accuracy:  0.942681\n",
      "Epoch:  782  Training Loss:  6.92207  Training Accuracy:  0.942799\n",
      "Epoch:  783  Training Loss:  6.91832  Training Accuracy:  0.943034\n",
      "Epoch:  784  Training Loss:  6.91913  Training Accuracy:  0.942975\n",
      "Epoch:  785  Training Loss:  6.91149  Training Accuracy:  0.943328\n",
      "Epoch:  786  Training Loss:  6.91265  Training Accuracy:  0.943446\n",
      "Epoch:  787  Training Loss:  6.9079  Training Accuracy:  0.943681\n",
      "Epoch:  788  Training Loss:  6.90628  Training Accuracy:  0.943681\n",
      "Epoch:  789  Training Loss:  6.89951  Training Accuracy:  0.943916\n",
      "Epoch:  790  Training Loss:  6.90092  Training Accuracy:  0.943975\n",
      "Epoch:  791  Training Loss:  6.89705  Training Accuracy:  0.94421\n",
      "Epoch:  792  Training Loss:  6.89308  Training Accuracy:  0.944445\n",
      "Epoch:  793  Training Loss:  6.8875  Training Accuracy:  0.944563\n",
      "Epoch:  794  Training Loss:  6.87924  Training Accuracy:  0.94468\n",
      "Epoch:  795  Training Loss:  6.85117  Training Accuracy:  0.94468\n",
      "Epoch:  796  Training Loss:  6.8819  Training Accuracy:  0.944915\n",
      "Epoch:  797  Training Loss:  6.88163  Training Accuracy:  0.945209\n",
      "Epoch:  798  Training Loss:  6.8872  Training Accuracy:  0.945444\n",
      "Epoch:  799  Training Loss:  6.87634  Training Accuracy:  0.945503\n",
      "Epoch:  800  Training Loss:  6.8692  Training Accuracy:  0.945562\n",
      "Epoch:  801  Training Loss:  6.84607  Training Accuracy:  0.945621\n",
      "Epoch:  802  Training Loss:  6.86886  Training Accuracy:  0.94568\n",
      "Epoch:  803  Training Loss:  6.86925  Training Accuracy:  0.945738\n",
      "Epoch:  804  Training Loss:  6.87135  Training Accuracy:  0.945797\n",
      "Epoch:  805  Training Loss:  6.86814  Training Accuracy:  0.946032\n",
      "Epoch:  806  Training Loss:  6.86226  Training Accuracy:  0.94615\n",
      "Epoch:  807  Training Loss:  6.86135  Training Accuracy:  0.946326\n",
      "Epoch:  808  Training Loss:  6.85904  Training Accuracy:  0.946503\n",
      "Epoch:  809  Training Loss:  6.85749  Training Accuracy:  0.946738\n",
      "Epoch:  810  Training Loss:  6.85501  Training Accuracy:  0.947032\n",
      "Epoch:  811  Training Loss:  6.85284  Training Accuracy:  0.947502\n",
      "Epoch:  812  Training Loss:  6.8472  Training Accuracy:  0.94762\n",
      "Epoch:  813  Training Loss:  6.84778  Training Accuracy:  0.947796\n",
      "Epoch:  814  Training Loss:  6.83283  Training Accuracy:  0.947796\n",
      "Epoch:  815  Training Loss:  6.81082  Training Accuracy:  0.947914\n",
      "Epoch:  816  Training Loss:  6.83836  Training Accuracy:  0.94809\n",
      "Epoch:  817  Training Loss:  6.84047  Training Accuracy:  0.948384\n",
      "Epoch:  818  Training Loss:  6.8331  Training Accuracy:  0.948325\n",
      "Epoch:  819  Training Loss:  6.80855  Training Accuracy:  0.948443\n",
      "Epoch:  820  Training Loss:  6.83554  Training Accuracy:  0.948619\n",
      "Epoch:  821  Training Loss:  6.83582  Training Accuracy:  0.948913\n",
      "Epoch:  822  Training Loss:  6.83257  Training Accuracy:  0.94903\n",
      "Epoch:  823  Training Loss:  6.83003  Training Accuracy:  0.949207\n",
      "Epoch:  824  Training Loss:  6.82911  Training Accuracy:  0.949324\n",
      "Epoch:  825  Training Loss:  6.8272  Training Accuracy:  0.949383\n",
      "Epoch:  826  Training Loss:  6.82713  Training Accuracy:  0.949442\n",
      "Epoch:  827  Training Loss:  6.82363  Training Accuracy:  0.949736\n",
      "Epoch:  828  Training Loss:  6.82413  Training Accuracy:  0.949971\n",
      "Epoch:  829  Training Loss:  6.81989  Training Accuracy:  0.950089\n",
      "Epoch:  830  Training Loss:  6.82126  Training Accuracy:  0.950265\n",
      "Epoch:  831  Training Loss:  6.81993  Training Accuracy:  0.950265\n",
      "Epoch:  832  Training Loss:  6.81983  Training Accuracy:  0.950441\n",
      "Epoch:  833  Training Loss:  6.81569  Training Accuracy:  0.950559\n",
      "Epoch:  834  Training Loss:  6.81214  Training Accuracy:  0.950618\n",
      "Epoch:  835  Training Loss:  6.80357  Training Accuracy:  0.950677\n",
      "Epoch:  836  Training Loss:  6.78379  Training Accuracy:  0.951029\n",
      "Epoch:  837  Training Loss:  6.81321  Training Accuracy:  0.951147\n",
      "Epoch:  838  Training Loss:  6.81147  Training Accuracy:  0.951265\n",
      "Epoch:  839  Training Loss:  6.81108  Training Accuracy:  0.951558\n",
      "Epoch:  840  Training Loss:  6.81252  Training Accuracy:  0.951676\n",
      "Epoch:  841  Training Loss:  6.8084  Training Accuracy:  0.951911\n",
      "Epoch:  842  Training Loss:  6.80879  Training Accuracy:  0.952088\n",
      "Epoch:  843  Training Loss:  6.80744  Training Accuracy:  0.952323\n",
      "Epoch:  844  Training Loss:  6.80822  Training Accuracy:  0.95244\n",
      "Epoch:  845  Training Loss:  6.8028  Training Accuracy:  0.952558\n",
      "Epoch:  846  Training Loss:  6.79653  Training Accuracy:  0.952617\n",
      "Epoch:  847  Training Loss:  6.76899  Training Accuracy:  0.952675\n",
      "Epoch:  848  Training Loss:  6.80162  Training Accuracy:  0.952852\n",
      "Epoch:  849  Training Loss:  6.80008  Training Accuracy:  0.952852\n",
      "Epoch:  850  Training Loss:  6.7994  Training Accuracy:  0.952911\n",
      "Epoch:  851  Training Loss:  6.80036  Training Accuracy:  0.952969\n",
      "Epoch:  852  Training Loss:  6.80137  Training Accuracy:  0.952969\n",
      "Epoch:  853  Training Loss:  6.79667  Training Accuracy:  0.953087\n",
      "Epoch:  854  Training Loss:  6.78927  Training Accuracy:  0.953205\n",
      "Epoch:  855  Training Loss:  6.76318  Training Accuracy:  0.953381\n",
      "Epoch:  856  Training Loss:  6.79054  Training Accuracy:  0.953381\n",
      "Epoch:  857  Training Loss:  6.79032  Training Accuracy:  0.95344\n",
      "Epoch:  858  Training Loss:  6.79048  Training Accuracy:  0.953557\n",
      "Epoch:  859  Training Loss:  6.78663  Training Accuracy:  0.953675\n",
      "Epoch:  860  Training Loss:  6.7848  Training Accuracy:  0.953675\n",
      "Epoch:  861  Training Loss:  6.78348  Training Accuracy:  0.953851\n",
      "Epoch:  862  Training Loss:  6.77445  Training Accuracy:  0.953851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  863  Training Loss:  6.75424  Training Accuracy:  0.954086\n",
      "Epoch:  864  Training Loss:  6.77484  Training Accuracy:  0.954145\n",
      "Epoch:  865  Training Loss:  6.78054  Training Accuracy:  0.954204\n",
      "Epoch:  866  Training Loss:  6.77555  Training Accuracy:  0.954439\n",
      "Epoch:  867  Training Loss:  6.75269  Training Accuracy:  0.954733\n",
      "Epoch:  868  Training Loss:  6.78055  Training Accuracy:  0.954674\n",
      "Epoch:  869  Training Loss:  6.78491  Training Accuracy:  0.954792\n",
      "Epoch:  870  Training Loss:  6.78574  Training Accuracy:  0.954733\n",
      "Epoch:  871  Training Loss:  6.77935  Training Accuracy:  0.954968\n",
      "Epoch:  872  Training Loss:  6.78037  Training Accuracy:  0.955086\n",
      "Epoch:  873  Training Loss:  6.77539  Training Accuracy:  0.955321\n",
      "Epoch:  874  Training Loss:  6.77743  Training Accuracy:  0.955321\n",
      "Epoch:  875  Training Loss:  6.77647  Training Accuracy:  0.955615\n",
      "Epoch:  876  Training Loss:  6.76826  Training Accuracy:  0.955615\n",
      "Epoch:  877  Training Loss:  6.74744  Training Accuracy:  0.955909\n",
      "Epoch:  878  Training Loss:  6.77771  Training Accuracy:  0.955968\n",
      "Epoch:  879  Training Loss:  6.77995  Training Accuracy:  0.956203\n",
      "Epoch:  880  Training Loss:  6.77854  Training Accuracy:  0.956203\n",
      "Epoch:  881  Training Loss:  6.77446  Training Accuracy:  0.956438\n",
      "Epoch:  882  Training Loss:  6.77307  Training Accuracy:  0.956497\n",
      "Epoch:  883  Training Loss:  6.77284  Training Accuracy:  0.956732\n",
      "Epoch:  884  Training Loss:  6.77304  Training Accuracy:  0.956791\n",
      "Epoch:  885  Training Loss:  6.77169  Training Accuracy:  0.956908\n",
      "Epoch:  886  Training Loss:  6.774  Training Accuracy:  0.957085\n",
      "Epoch:  887  Training Loss:  6.76979  Training Accuracy:  0.957379\n",
      "Epoch:  888  Training Loss:  6.77066  Training Accuracy:  0.957437\n",
      "Epoch:  889  Training Loss:  6.76721  Training Accuracy:  0.957614\n",
      "Epoch:  890  Training Loss:  6.76865  Training Accuracy:  0.957614\n",
      "Epoch:  891  Training Loss:  6.76646  Training Accuracy:  0.957731\n",
      "Epoch:  892  Training Loss:  6.76678  Training Accuracy:  0.957672\n",
      "Epoch:  893  Training Loss:  6.76575  Training Accuracy:  0.95779\n",
      "Epoch:  894  Training Loss:  6.76512  Training Accuracy:  0.957849\n",
      "Epoch:  895  Training Loss:  6.76363  Training Accuracy:  0.957849\n",
      "Epoch:  896  Training Loss:  6.76475  Training Accuracy:  0.957966\n",
      "Epoch:  897  Training Loss:  6.76328  Training Accuracy:  0.958084\n",
      "Epoch:  898  Training Loss:  6.76507  Training Accuracy:  0.958319\n",
      "Epoch:  899  Training Loss:  6.75969  Training Accuracy:  0.958496\n",
      "Epoch:  900  Training Loss:  6.76332  Training Accuracy:  0.958437\n",
      "Epoch:  901  Training Loss:  6.76098  Training Accuracy:  0.958613\n",
      "Epoch:  902  Training Loss:  6.76272  Training Accuracy:  0.958613\n",
      "Epoch:  903  Training Loss:  6.76096  Training Accuracy:  0.958789\n",
      "Epoch:  904  Training Loss:  6.75934  Training Accuracy:  0.958848\n",
      "Epoch:  905  Training Loss:  6.75923  Training Accuracy:  0.958966\n",
      "Epoch:  906  Training Loss:  6.76101  Training Accuracy:  0.959025\n",
      "Epoch:  907  Training Loss:  6.75811  Training Accuracy:  0.959201\n",
      "Epoch:  908  Training Loss:  6.75698  Training Accuracy:  0.959142\n",
      "Epoch:  909  Training Loss:  6.74741  Training Accuracy:  0.959201\n",
      "Epoch:  910  Training Loss:  6.73334  Training Accuracy:  0.95926\n",
      "Epoch:  911  Training Loss:  6.75371  Training Accuracy:  0.95926\n",
      "Epoch:  912  Training Loss:  6.75532  Training Accuracy:  0.95926\n",
      "Epoch:  913  Training Loss:  6.74775  Training Accuracy:  0.959436\n",
      "Epoch:  914  Training Loss:  6.73672  Training Accuracy:  0.95973\n",
      "Epoch:  915  Training Loss:  6.72896  Training Accuracy:  0.959965\n",
      "Epoch:  916  Training Loss:  6.76131  Training Accuracy:  0.960024\n",
      "Epoch:  917  Training Loss:  6.76165  Training Accuracy:  0.9602\n",
      "Epoch:  918  Training Loss:  6.76288  Training Accuracy:  0.960377\n",
      "Epoch:  919  Training Loss:  6.75053  Training Accuracy:  0.960494\n",
      "Epoch:  920  Training Loss:  6.73533  Training Accuracy:  0.960436\n",
      "Epoch:  921  Training Loss:  6.74951  Training Accuracy:  0.960553\n",
      "Epoch:  922  Training Loss:  6.7505  Training Accuracy:  0.960553\n",
      "Epoch:  923  Training Loss:  6.73462  Training Accuracy:  0.960671\n",
      "Epoch:  924  Training Loss:  6.72976  Training Accuracy:  0.960788\n",
      "Epoch:  925  Training Loss:  6.75646  Training Accuracy:  0.960965\n",
      "Epoch:  926  Training Loss:  6.76013  Training Accuracy:  0.960965\n",
      "Epoch:  927  Training Loss:  6.75578  Training Accuracy:  0.9612\n",
      "Epoch:  928  Training Loss:  6.7356  Training Accuracy:  0.961141\n",
      "Epoch:  929  Training Loss:  6.75787  Training Accuracy:  0.9612\n",
      "Epoch:  930  Training Loss:  6.75988  Training Accuracy:  0.9612\n",
      "Epoch:  931  Training Loss:  6.75338  Training Accuracy:  0.961435\n",
      "Epoch:  932  Training Loss:  6.73652  Training Accuracy:  0.961435\n",
      "Epoch:  933  Training Loss:  6.74644  Training Accuracy:  0.96167\n",
      "Epoch:  934  Training Loss:  6.73352  Training Accuracy:  0.961964\n",
      "Epoch:  935  Training Loss:  6.73884  Training Accuracy:  0.962023\n",
      "Epoch:  936  Training Loss:  6.74806  Training Accuracy:  0.962023\n",
      "Epoch:  937  Training Loss:  6.7389  Training Accuracy:  0.962258\n",
      "Epoch:  938  Training Loss:  6.73531  Training Accuracy:  0.962199\n",
      "Epoch:  939  Training Loss:  6.75602  Training Accuracy:  0.962317\n",
      "Epoch:  940  Training Loss:  6.74683  Training Accuracy:  0.962258\n",
      "Epoch:  941  Training Loss:  6.7642  Training Accuracy:  0.962434\n",
      "Epoch:  942  Training Loss:  6.76779  Training Accuracy:  0.96267\n",
      "Epoch:  943  Training Loss:  6.75075  Training Accuracy:  0.962905\n",
      "Epoch:  944  Training Loss:  6.74232  Training Accuracy:  0.962963\n",
      "Epoch:  945  Training Loss:  6.75976  Training Accuracy:  0.963081\n",
      "Epoch:  946  Training Loss:  6.76656  Training Accuracy:  0.96314\n",
      "Epoch:  947  Training Loss:  6.75911  Training Accuracy:  0.963316\n",
      "Epoch:  948  Training Loss:  6.74229  Training Accuracy:  0.963316\n",
      "Epoch:  949  Training Loss:  6.74106  Training Accuracy:  0.963493\n",
      "Epoch:  950  Training Loss:  6.76217  Training Accuracy:  0.963375\n",
      "Epoch:  951  Training Loss:  6.76863  Training Accuracy:  0.963786\n",
      "Epoch:  952  Training Loss:  6.7567  Training Accuracy:  0.963728\n",
      "Epoch:  953  Training Loss:  6.74367  Training Accuracy:  0.963845\n",
      "Epoch:  954  Training Loss:  6.76509  Training Accuracy:  0.963787\n",
      "Epoch:  955  Training Loss:  6.77183  Training Accuracy:  0.963904\n",
      "Epoch:  956  Training Loss:  6.77048  Training Accuracy:  0.963904\n",
      "Epoch:  957  Training Loss:  6.75053  Training Accuracy:  0.964022\n",
      "Epoch:  958  Training Loss:  6.77132  Training Accuracy:  0.96408\n",
      "Epoch:  959  Training Loss:  6.77068  Training Accuracy:  0.964316\n",
      "Epoch:  960  Training Loss:  6.76414  Training Accuracy:  0.964374\n",
      "Epoch:  961  Training Loss:  6.74884  Training Accuracy:  0.964433\n",
      "Epoch:  962  Training Loss:  6.746  Training Accuracy:  0.964433\n",
      "Epoch:  963  Training Loss:  6.76602  Training Accuracy:  0.964551\n",
      "Epoch:  964  Training Loss:  6.75019  Training Accuracy:  0.964845\n",
      "Epoch:  965  Training Loss:  6.77549  Training Accuracy:  0.964727\n",
      "Epoch:  966  Training Loss:  6.779  Training Accuracy:  0.964904\n",
      "Epoch:  967  Training Loss:  6.77689  Training Accuracy:  0.964962\n",
      "Epoch:  968  Training Loss:  6.75576  Training Accuracy:  0.96508\n",
      "Epoch:  969  Training Loss:  6.77558  Training Accuracy:  0.96508\n",
      "Epoch:  970  Training Loss:  6.77671  Training Accuracy:  0.965256\n",
      "Epoch:  971  Training Loss:  6.77795  Training Accuracy:  0.965433\n",
      "Epoch:  972  Training Loss:  6.76086  Training Accuracy:  0.965433\n",
      "Epoch:  973  Training Loss:  6.77692  Training Accuracy:  0.965491\n",
      "Epoch:  974  Training Loss:  6.78128  Training Accuracy:  0.965491\n",
      "Epoch:  975  Training Loss:  6.76052  Training Accuracy:  0.965668\n",
      "Epoch:  976  Training Loss:  6.78  Training Accuracy:  0.965668\n",
      "Epoch:  977  Training Loss:  6.78317  Training Accuracy:  0.965785\n",
      "Epoch:  978  Training Loss:  6.78497  Training Accuracy:  0.965844\n",
      "Epoch:  979  Training Loss:  6.76504  Training Accuracy:  0.966138\n",
      "Epoch:  980  Training Loss:  6.78564  Training Accuracy:  0.966021\n",
      "Epoch:  981  Training Loss:  6.7833  Training Accuracy:  0.966197\n",
      "Epoch:  982  Training Loss:  6.78489  Training Accuracy:  0.966197\n",
      "Epoch:  983  Training Loss:  6.76299  Training Accuracy:  0.966491\n",
      "Epoch:  984  Training Loss:  6.78789  Training Accuracy:  0.966256\n",
      "Epoch:  985  Training Loss:  6.78552  Training Accuracy:  0.966491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  986  Training Loss:  6.78843  Training Accuracy:  0.96655\n",
      "Epoch:  987  Training Loss:  6.76816  Training Accuracy:  0.966785\n",
      "Epoch:  988  Training Loss:  6.79121  Training Accuracy:  0.966785\n",
      "Epoch:  989  Training Loss:  6.78658  Training Accuracy:  0.966844\n",
      "Epoch:  990  Training Loss:  6.77548  Training Accuracy:  0.966902\n",
      "Epoch:  991  Training Loss:  6.78957  Training Accuracy:  0.966961\n",
      "Epoch:  992  Training Loss:  6.79432  Training Accuracy:  0.966902\n",
      "Epoch:  993  Training Loss:  6.79037  Training Accuracy:  0.966961\n",
      "Epoch:  994  Training Loss:  6.7775  Training Accuracy:  0.967079\n",
      "Epoch:  995  Training Loss:  6.79459  Training Accuracy:  0.967138\n",
      "Epoch:  996  Training Loss:  6.80014  Training Accuracy:  0.967196\n",
      "Epoch:  997  Training Loss:  6.79529  Training Accuracy:  0.967255\n",
      "Epoch:  998  Training Loss:  6.77956  Training Accuracy:  0.967549\n",
      "Epoch:  999  Training Loss:  6.79999  Training Accuracy:  0.967608\n",
      "Testing Accuracy: 0.863114\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  59.1449  Training Accuracy:  0.0330394\n",
      "Epoch:  1  Training Loss:  59.1437  Training Accuracy:  0.0330394\n",
      "Epoch:  2  Training Loss:  59.1426  Training Accuracy:  0.0330394\n",
      "Epoch:  3  Training Loss:  59.1414  Training Accuracy:  0.0330394\n",
      "Epoch:  4  Training Loss:  59.1402  Training Accuracy:  0.0330394\n",
      "Epoch:  5  Training Loss:  59.1391  Training Accuracy:  0.0330394\n",
      "Epoch:  6  Training Loss:  59.1379  Training Accuracy:  0.0330394\n",
      "Epoch:  7  Training Loss:  59.1367  Training Accuracy:  0.0330394\n",
      "Epoch:  8  Training Loss:  59.1356  Training Accuracy:  0.0330394\n",
      "Epoch:  9  Training Loss:  59.1344  Training Accuracy:  0.0330394\n",
      "Epoch:  10  Training Loss:  59.1333  Training Accuracy:  0.0330394\n",
      "Epoch:  11  Training Loss:  59.1321  Training Accuracy:  0.0330394\n",
      "Epoch:  12  Training Loss:  59.1309  Training Accuracy:  0.0330394\n",
      "Epoch:  13  Training Loss:  59.1298  Training Accuracy:  0.0330394\n",
      "Epoch:  14  Training Loss:  59.1286  Training Accuracy:  0.0330394\n",
      "Epoch:  15  Training Loss:  59.1275  Training Accuracy:  0.0330394\n",
      "Epoch:  16  Training Loss:  59.1263  Training Accuracy:  0.0329806\n",
      "Epoch:  17  Training Loss:  59.1251  Training Accuracy:  0.0329806\n",
      "Epoch:  18  Training Loss:  59.124  Training Accuracy:  0.0329806\n",
      "Epoch:  19  Training Loss:  59.1228  Training Accuracy:  0.0329218\n",
      "Epoch:  20  Training Loss:  59.1217  Training Accuracy:  0.0329218\n",
      "Epoch:  21  Training Loss:  59.1205  Training Accuracy:  0.0329218\n",
      "Epoch:  22  Training Loss:  59.1193  Training Accuracy:  0.0329218\n",
      "Epoch:  23  Training Loss:  59.1182  Training Accuracy:  0.0329218\n",
      "Epoch:  24  Training Loss:  59.117  Training Accuracy:  0.0329218\n",
      "Epoch:  25  Training Loss:  59.1159  Training Accuracy:  0.0329218\n",
      "Epoch:  26  Training Loss:  59.1147  Training Accuracy:  0.0329218\n",
      "Epoch:  27  Training Loss:  59.1135  Training Accuracy:  0.0329218\n",
      "Epoch:  28  Training Loss:  59.1124  Training Accuracy:  0.0329218\n",
      "Epoch:  29  Training Loss:  59.1112  Training Accuracy:  0.0329218\n",
      "Epoch:  30  Training Loss:  59.1101  Training Accuracy:  0.0329218\n",
      "Epoch:  31  Training Loss:  59.1089  Training Accuracy:  0.0329218\n",
      "Epoch:  32  Training Loss:  59.1077  Training Accuracy:  0.032863\n",
      "Epoch:  33  Training Loss:  59.1066  Training Accuracy:  0.032863\n",
      "Epoch:  34  Training Loss:  59.1054  Training Accuracy:  0.032863\n",
      "Epoch:  35  Training Loss:  59.1043  Training Accuracy:  0.032863\n",
      "Epoch:  36  Training Loss:  59.1031  Training Accuracy:  0.032863\n",
      "Epoch:  37  Training Loss:  59.102  Training Accuracy:  0.032863\n",
      "Epoch:  38  Training Loss:  59.1008  Training Accuracy:  0.032863\n",
      "Epoch:  39  Training Loss:  59.0997  Training Accuracy:  0.032863\n",
      "Epoch:  40  Training Loss:  59.0985  Training Accuracy:  0.032863\n",
      "Epoch:  41  Training Loss:  59.0973  Training Accuracy:  0.032863\n",
      "Epoch:  42  Training Loss:  59.0962  Training Accuracy:  0.032863\n",
      "Epoch:  43  Training Loss:  59.095  Training Accuracy:  0.032863\n",
      "Epoch:  44  Training Loss:  59.0939  Training Accuracy:  0.032863\n",
      "Epoch:  45  Training Loss:  59.0927  Training Accuracy:  0.032863\n",
      "Epoch:  46  Training Loss:  59.0915  Training Accuracy:  0.032863\n",
      "Epoch:  47  Training Loss:  59.0904  Training Accuracy:  0.032863\n",
      "Epoch:  48  Training Loss:  59.0892  Training Accuracy:  0.032863\n",
      "Epoch:  49  Training Loss:  59.0881  Training Accuracy:  0.0329218\n",
      "Epoch:  50  Training Loss:  59.0869  Training Accuracy:  0.0329218\n",
      "Epoch:  51  Training Loss:  59.0858  Training Accuracy:  0.0329218\n",
      "Epoch:  52  Training Loss:  59.0846  Training Accuracy:  0.0329218\n",
      "Epoch:  53  Training Loss:  59.0834  Training Accuracy:  0.0329218\n",
      "Epoch:  54  Training Loss:  59.0823  Training Accuracy:  0.0329218\n",
      "Epoch:  55  Training Loss:  59.0811  Training Accuracy:  0.0329218\n",
      "Epoch:  56  Training Loss:  59.08  Training Accuracy:  0.0329218\n",
      "Epoch:  57  Training Loss:  59.0788  Training Accuracy:  0.0329218\n",
      "Epoch:  58  Training Loss:  59.0777  Training Accuracy:  0.0329218\n",
      "Epoch:  59  Training Loss:  59.0765  Training Accuracy:  0.0329218\n",
      "Epoch:  60  Training Loss:  59.0753  Training Accuracy:  0.0329218\n",
      "Epoch:  61  Training Loss:  59.0742  Training Accuracy:  0.0329218\n",
      "Epoch:  62  Training Loss:  59.073  Training Accuracy:  0.0329218\n",
      "Epoch:  63  Training Loss:  59.0719  Training Accuracy:  0.0329218\n",
      "Epoch:  64  Training Loss:  59.0707  Training Accuracy:  0.0329218\n",
      "Epoch:  65  Training Loss:  59.0696  Training Accuracy:  0.0329218\n",
      "Epoch:  66  Training Loss:  59.0684  Training Accuracy:  0.0329218\n",
      "Epoch:  67  Training Loss:  59.0673  Training Accuracy:  0.032863\n",
      "Epoch:  68  Training Loss:  59.0661  Training Accuracy:  0.032863\n",
      "Epoch:  69  Training Loss:  59.0649  Training Accuracy:  0.032863\n",
      "Epoch:  70  Training Loss:  59.0638  Training Accuracy:  0.032863\n",
      "Epoch:  71  Training Loss:  59.0626  Training Accuracy:  0.032863\n",
      "Epoch:  72  Training Loss:  59.0615  Training Accuracy:  0.032863\n",
      "Epoch:  73  Training Loss:  59.0603  Training Accuracy:  0.032863\n",
      "Epoch:  74  Training Loss:  59.0592  Training Accuracy:  0.0328042\n",
      "Epoch:  75  Training Loss:  59.058  Training Accuracy:  0.0328042\n",
      "Epoch:  76  Training Loss:  59.0569  Training Accuracy:  0.0328042\n",
      "Epoch:  77  Training Loss:  59.0557  Training Accuracy:  0.0328042\n",
      "Epoch:  78  Training Loss:  59.0546  Training Accuracy:  0.0328042\n",
      "Epoch:  79  Training Loss:  59.0534  Training Accuracy:  0.0328042\n",
      "Epoch:  80  Training Loss:  59.0522  Training Accuracy:  0.0328042\n",
      "Epoch:  81  Training Loss:  59.0511  Training Accuracy:  0.0328042\n",
      "Epoch:  82  Training Loss:  59.0499  Training Accuracy:  0.0328042\n",
      "Epoch:  83  Training Loss:  59.0488  Training Accuracy:  0.0328042\n",
      "Epoch:  84  Training Loss:  59.0476  Training Accuracy:  0.0328042\n",
      "Epoch:  85  Training Loss:  59.0465  Training Accuracy:  0.0328042\n",
      "Epoch:  86  Training Loss:  59.0453  Training Accuracy:  0.0328042\n",
      "Epoch:  87  Training Loss:  59.0442  Training Accuracy:  0.0328042\n",
      "Epoch:  88  Training Loss:  59.043  Training Accuracy:  0.0328042\n",
      "Epoch:  89  Training Loss:  59.0419  Training Accuracy:  0.0328042\n",
      "Epoch:  90  Training Loss:  59.0407  Training Accuracy:  0.0328042\n",
      "Epoch:  91  Training Loss:  59.0396  Training Accuracy:  0.0328042\n",
      "Epoch:  92  Training Loss:  59.0384  Training Accuracy:  0.0328042\n",
      "Epoch:  93  Training Loss:  59.0372  Training Accuracy:  0.0328042\n",
      "Epoch:  94  Training Loss:  59.0361  Training Accuracy:  0.0328042\n",
      "Epoch:  95  Training Loss:  59.0349  Training Accuracy:  0.0328042\n",
      "Epoch:  96  Training Loss:  59.0338  Training Accuracy:  0.0328042\n",
      "Epoch:  97  Training Loss:  59.0326  Training Accuracy:  0.0327454\n",
      "Epoch:  98  Training Loss:  59.0315  Training Accuracy:  0.0327454\n",
      "Epoch:  99  Training Loss:  59.0303  Training Accuracy:  0.0327454\n",
      "Epoch:  100  Training Loss:  59.0292  Training Accuracy:  0.0327454\n",
      "Epoch:  101  Training Loss:  59.028  Training Accuracy:  0.0327454\n",
      "Epoch:  102  Training Loss:  59.0269  Training Accuracy:  0.0327454\n",
      "Epoch:  103  Training Loss:  59.0257  Training Accuracy:  0.0327454\n",
      "Epoch:  104  Training Loss:  59.0246  Training Accuracy:  0.0327454\n",
      "Epoch:  105  Training Loss:  59.0234  Training Accuracy:  0.0327454\n",
      "Epoch:  106  Training Loss:  59.0223  Training Accuracy:  0.0327454\n",
      "Epoch:  107  Training Loss:  59.0211  Training Accuracy:  0.0327454\n",
      "Epoch:  108  Training Loss:  59.02  Training Accuracy:  0.0327454\n",
      "Epoch:  109  Training Loss:  59.0188  Training Accuracy:  0.0327454\n",
      "Epoch:  110  Training Loss:  59.0177  Training Accuracy:  0.0327454\n",
      "Epoch:  111  Training Loss:  59.0165  Training Accuracy:  0.0327454\n",
      "Epoch:  112  Training Loss:  59.0154  Training Accuracy:  0.0327454\n",
      "Epoch:  113  Training Loss:  59.0142  Training Accuracy:  0.0327454\n",
      "Epoch:  114  Training Loss:  59.013  Training Accuracy:  0.0327454\n",
      "Epoch:  115  Training Loss:  59.0119  Training Accuracy:  0.0327454\n",
      "Epoch:  116  Training Loss:  59.0107  Training Accuracy:  0.0327454\n",
      "Epoch:  117  Training Loss:  59.0096  Training Accuracy:  0.0328042\n",
      "Epoch:  118  Training Loss:  59.0084  Training Accuracy:  0.0328042\n",
      "Epoch:  119  Training Loss:  59.0073  Training Accuracy:  0.0328042\n",
      "Epoch:  120  Training Loss:  59.0061  Training Accuracy:  0.0328042\n",
      "Epoch:  121  Training Loss:  59.005  Training Accuracy:  0.0328042\n",
      "Epoch:  122  Training Loss:  59.0038  Training Accuracy:  0.0328042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  123  Training Loss:  59.0027  Training Accuracy:  0.0328042\n",
      "Epoch:  124  Training Loss:  59.0015  Training Accuracy:  0.0328042\n",
      "Epoch:  125  Training Loss:  59.0004  Training Accuracy:  0.0328042\n",
      "Epoch:  126  Training Loss:  58.9992  Training Accuracy:  0.0328042\n",
      "Epoch:  127  Training Loss:  58.9981  Training Accuracy:  0.0328042\n",
      "Epoch:  128  Training Loss:  58.9969  Training Accuracy:  0.0328042\n",
      "Epoch:  129  Training Loss:  58.9958  Training Accuracy:  0.0328042\n",
      "Epoch:  130  Training Loss:  58.9946  Training Accuracy:  0.0328042\n",
      "Epoch:  131  Training Loss:  58.9935  Training Accuracy:  0.0328042\n",
      "Epoch:  132  Training Loss:  58.9923  Training Accuracy:  0.0328042\n",
      "Epoch:  133  Training Loss:  58.9912  Training Accuracy:  0.0328042\n",
      "Epoch:  134  Training Loss:  58.99  Training Accuracy:  0.0327454\n",
      "Epoch:  135  Training Loss:  58.9889  Training Accuracy:  0.0327454\n",
      "Epoch:  136  Training Loss:  58.9877  Training Accuracy:  0.0327454\n",
      "Epoch:  137  Training Loss:  58.9866  Training Accuracy:  0.0327454\n",
      "Epoch:  138  Training Loss:  58.9854  Training Accuracy:  0.0327454\n",
      "Epoch:  139  Training Loss:  58.9843  Training Accuracy:  0.0327454\n",
      "Epoch:  140  Training Loss:  58.9831  Training Accuracy:  0.0327454\n",
      "Epoch:  141  Training Loss:  58.982  Training Accuracy:  0.0327454\n",
      "Epoch:  142  Training Loss:  58.9808  Training Accuracy:  0.0327454\n",
      "Epoch:  143  Training Loss:  58.9797  Training Accuracy:  0.0327454\n",
      "Epoch:  144  Training Loss:  58.9785  Training Accuracy:  0.0327454\n",
      "Epoch:  145  Training Loss:  58.9774  Training Accuracy:  0.0327454\n",
      "Epoch:  146  Training Loss:  58.9762  Training Accuracy:  0.0327454\n",
      "Epoch:  147  Training Loss:  58.9751  Training Accuracy:  0.0328042\n",
      "Epoch:  148  Training Loss:  58.974  Training Accuracy:  0.0328042\n",
      "Epoch:  149  Training Loss:  58.9728  Training Accuracy:  0.0328042\n",
      "Epoch:  150  Training Loss:  58.9717  Training Accuracy:  0.0328042\n",
      "Epoch:  151  Training Loss:  58.9705  Training Accuracy:  0.0328042\n",
      "Epoch:  152  Training Loss:  58.9694  Training Accuracy:  0.0328042\n",
      "Epoch:  153  Training Loss:  58.9682  Training Accuracy:  0.0328042\n",
      "Epoch:  154  Training Loss:  58.9671  Training Accuracy:  0.0328042\n",
      "Epoch:  155  Training Loss:  58.9659  Training Accuracy:  0.0328042\n",
      "Epoch:  156  Training Loss:  58.9648  Training Accuracy:  0.0328042\n",
      "Epoch:  157  Training Loss:  58.9636  Training Accuracy:  0.0328042\n",
      "Epoch:  158  Training Loss:  58.9625  Training Accuracy:  0.0328042\n",
      "Epoch:  159  Training Loss:  58.9613  Training Accuracy:  0.0328042\n",
      "Epoch:  160  Training Loss:  58.9602  Training Accuracy:  0.0327454\n",
      "Epoch:  161  Training Loss:  58.959  Training Accuracy:  0.0327454\n",
      "Epoch:  162  Training Loss:  58.9579  Training Accuracy:  0.0327454\n",
      "Epoch:  163  Training Loss:  58.9568  Training Accuracy:  0.0327454\n",
      "Epoch:  164  Training Loss:  58.9556  Training Accuracy:  0.0327454\n",
      "Epoch:  165  Training Loss:  58.9545  Training Accuracy:  0.0327454\n",
      "Epoch:  166  Training Loss:  58.9533  Training Accuracy:  0.0327454\n",
      "Epoch:  167  Training Loss:  58.9522  Training Accuracy:  0.0327454\n",
      "Epoch:  168  Training Loss:  58.951  Training Accuracy:  0.0327454\n",
      "Epoch:  169  Training Loss:  58.9499  Training Accuracy:  0.0328042\n",
      "Epoch:  170  Training Loss:  58.9487  Training Accuracy:  0.0328042\n",
      "Epoch:  171  Training Loss:  58.9476  Training Accuracy:  0.0328042\n",
      "Epoch:  172  Training Loss:  58.9464  Training Accuracy:  0.0328042\n",
      "Epoch:  173  Training Loss:  58.9453  Training Accuracy:  0.0328042\n",
      "Epoch:  174  Training Loss:  58.9441  Training Accuracy:  0.0328042\n",
      "Epoch:  175  Training Loss:  58.943  Training Accuracy:  0.0328042\n",
      "Epoch:  176  Training Loss:  58.9418  Training Accuracy:  0.0328042\n",
      "Epoch:  177  Training Loss:  58.9407  Training Accuracy:  0.0328042\n",
      "Epoch:  178  Training Loss:  58.9396  Training Accuracy:  0.0328042\n",
      "Epoch:  179  Training Loss:  58.9384  Training Accuracy:  0.0328042\n",
      "Epoch:  180  Training Loss:  58.9373  Training Accuracy:  0.0328042\n",
      "Epoch:  181  Training Loss:  58.9361  Training Accuracy:  0.0328042\n",
      "Epoch:  182  Training Loss:  58.935  Training Accuracy:  0.0328042\n",
      "Epoch:  183  Training Loss:  58.9338  Training Accuracy:  0.0328042\n",
      "Epoch:  184  Training Loss:  58.9327  Training Accuracy:  0.0328042\n",
      "Epoch:  185  Training Loss:  58.9315  Training Accuracy:  0.0328042\n",
      "Epoch:  186  Training Loss:  58.9304  Training Accuracy:  0.0328042\n",
      "Epoch:  187  Training Loss:  58.9292  Training Accuracy:  0.0328042\n",
      "Epoch:  188  Training Loss:  58.9281  Training Accuracy:  0.0328042\n",
      "Epoch:  189  Training Loss:  58.927  Training Accuracy:  0.0328042\n",
      "Epoch:  190  Training Loss:  58.9258  Training Accuracy:  0.0328042\n",
      "Epoch:  191  Training Loss:  58.9247  Training Accuracy:  0.0327454\n",
      "Epoch:  192  Training Loss:  58.9235  Training Accuracy:  0.0327454\n",
      "Epoch:  193  Training Loss:  58.9224  Training Accuracy:  0.0327454\n",
      "Epoch:  194  Training Loss:  58.9212  Training Accuracy:  0.0327454\n",
      "Epoch:  195  Training Loss:  58.9201  Training Accuracy:  0.0327454\n",
      "Epoch:  196  Training Loss:  58.919  Training Accuracy:  0.0327454\n",
      "Epoch:  197  Training Loss:  58.9178  Training Accuracy:  0.0327454\n",
      "Epoch:  198  Training Loss:  58.9167  Training Accuracy:  0.0327454\n",
      "Epoch:  199  Training Loss:  58.9155  Training Accuracy:  0.0327454\n",
      "Epoch:  200  Training Loss:  58.9144  Training Accuracy:  0.0327454\n",
      "Epoch:  201  Training Loss:  58.9132  Training Accuracy:  0.0327454\n",
      "Epoch:  202  Training Loss:  58.9121  Training Accuracy:  0.0327454\n",
      "Epoch:  203  Training Loss:  58.9109  Training Accuracy:  0.0327454\n",
      "Epoch:  204  Training Loss:  58.9098  Training Accuracy:  0.0327454\n",
      "Epoch:  205  Training Loss:  58.9087  Training Accuracy:  0.0327454\n",
      "Epoch:  206  Training Loss:  58.9075  Training Accuracy:  0.0327454\n",
      "Epoch:  207  Training Loss:  58.9064  Training Accuracy:  0.0327454\n",
      "Epoch:  208  Training Loss:  58.9052  Training Accuracy:  0.0327454\n",
      "Epoch:  209  Training Loss:  58.9041  Training Accuracy:  0.0327454\n",
      "Epoch:  210  Training Loss:  58.9029  Training Accuracy:  0.0327454\n",
      "Epoch:  211  Training Loss:  58.9018  Training Accuracy:  0.0327454\n",
      "Epoch:  212  Training Loss:  58.9007  Training Accuracy:  0.0327454\n",
      "Epoch:  213  Training Loss:  58.8995  Training Accuracy:  0.0327454\n",
      "Epoch:  214  Training Loss:  58.8984  Training Accuracy:  0.0327454\n",
      "Epoch:  215  Training Loss:  58.8972  Training Accuracy:  0.0327454\n",
      "Epoch:  216  Training Loss:  58.8961  Training Accuracy:  0.0327454\n",
      "Epoch:  217  Training Loss:  58.895  Training Accuracy:  0.0327454\n",
      "Epoch:  218  Training Loss:  58.8938  Training Accuracy:  0.0327454\n",
      "Epoch:  219  Training Loss:  58.8927  Training Accuracy:  0.0327454\n",
      "Epoch:  220  Training Loss:  58.8915  Training Accuracy:  0.0327454\n",
      "Epoch:  221  Training Loss:  58.8904  Training Accuracy:  0.0327454\n",
      "Epoch:  222  Training Loss:  58.8893  Training Accuracy:  0.0327454\n",
      "Epoch:  223  Training Loss:  58.8881  Training Accuracy:  0.0327454\n",
      "Epoch:  224  Training Loss:  58.887  Training Accuracy:  0.0327454\n",
      "Epoch:  225  Training Loss:  58.8858  Training Accuracy:  0.0327454\n",
      "Epoch:  226  Training Loss:  58.8847  Training Accuracy:  0.0327454\n",
      "Epoch:  227  Training Loss:  58.8835  Training Accuracy:  0.0327454\n",
      "Epoch:  228  Training Loss:  58.8824  Training Accuracy:  0.0327454\n",
      "Epoch:  229  Training Loss:  58.8813  Training Accuracy:  0.0327454\n",
      "Epoch:  230  Training Loss:  58.8801  Training Accuracy:  0.0327454\n",
      "Epoch:  231  Training Loss:  58.879  Training Accuracy:  0.0327454\n",
      "Epoch:  232  Training Loss:  58.8778  Training Accuracy:  0.0327454\n",
      "Epoch:  233  Training Loss:  58.8767  Training Accuracy:  0.0327454\n",
      "Epoch:  234  Training Loss:  58.8756  Training Accuracy:  0.0327454\n",
      "Epoch:  235  Training Loss:  58.8744  Training Accuracy:  0.0327454\n",
      "Epoch:  236  Training Loss:  58.8733  Training Accuracy:  0.0327454\n",
      "Epoch:  237  Training Loss:  58.8721  Training Accuracy:  0.0327454\n",
      "Epoch:  238  Training Loss:  58.871  Training Accuracy:  0.0327454\n",
      "Epoch:  239  Training Loss:  58.8699  Training Accuracy:  0.0327454\n",
      "Epoch:  240  Training Loss:  58.8687  Training Accuracy:  0.0327454\n",
      "Epoch:  241  Training Loss:  58.8676  Training Accuracy:  0.0327454\n",
      "Epoch:  242  Training Loss:  58.8664  Training Accuracy:  0.0327454\n",
      "Epoch:  243  Training Loss:  58.8653  Training Accuracy:  0.0327454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  244  Training Loss:  58.8642  Training Accuracy:  0.0327454\n",
      "Epoch:  245  Training Loss:  58.863  Training Accuracy:  0.0327454\n",
      "Epoch:  246  Training Loss:  58.8619  Training Accuracy:  0.0326867\n",
      "Epoch:  247  Training Loss:  58.8607  Training Accuracy:  0.0326867\n",
      "Epoch:  248  Training Loss:  58.8596  Training Accuracy:  0.0326867\n",
      "Epoch:  249  Training Loss:  58.8585  Training Accuracy:  0.0326867\n",
      "Epoch:  250  Training Loss:  58.8573  Training Accuracy:  0.0326867\n",
      "Epoch:  251  Training Loss:  58.8562  Training Accuracy:  0.0326867\n",
      "Epoch:  252  Training Loss:  58.855  Training Accuracy:  0.0326867\n",
      "Epoch:  253  Training Loss:  58.8539  Training Accuracy:  0.0326867\n",
      "Epoch:  254  Training Loss:  58.8528  Training Accuracy:  0.0326867\n",
      "Epoch:  255  Training Loss:  58.8516  Training Accuracy:  0.0326867\n",
      "Epoch:  256  Training Loss:  58.8505  Training Accuracy:  0.0326279\n",
      "Epoch:  257  Training Loss:  58.8494  Training Accuracy:  0.0326279\n",
      "Epoch:  258  Training Loss:  58.8482  Training Accuracy:  0.0326279\n",
      "Epoch:  259  Training Loss:  58.8471  Training Accuracy:  0.0326279\n",
      "Epoch:  260  Training Loss:  58.8459  Training Accuracy:  0.0326867\n",
      "Epoch:  261  Training Loss:  58.8448  Training Accuracy:  0.0326867\n",
      "Epoch:  262  Training Loss:  58.8437  Training Accuracy:  0.0326867\n",
      "Epoch:  263  Training Loss:  58.8425  Training Accuracy:  0.0326867\n",
      "Epoch:  264  Training Loss:  58.8414  Training Accuracy:  0.0326867\n",
      "Epoch:  265  Training Loss:  58.8403  Training Accuracy:  0.0326867\n",
      "Epoch:  266  Training Loss:  58.8391  Training Accuracy:  0.0326867\n",
      "Epoch:  267  Training Loss:  58.838  Training Accuracy:  0.0326867\n",
      "Epoch:  268  Training Loss:  58.8368  Training Accuracy:  0.0326867\n",
      "Epoch:  269  Training Loss:  58.8357  Training Accuracy:  0.0326867\n",
      "Epoch:  270  Training Loss:  58.8346  Training Accuracy:  0.0326867\n",
      "Epoch:  271  Training Loss:  58.8334  Training Accuracy:  0.0326867\n",
      "Epoch:  272  Training Loss:  58.8323  Training Accuracy:  0.0326279\n",
      "Epoch:  273  Training Loss:  58.8311  Training Accuracy:  0.0326279\n",
      "Epoch:  274  Training Loss:  58.83  Training Accuracy:  0.0326279\n",
      "Epoch:  275  Training Loss:  58.8289  Training Accuracy:  0.0326279\n",
      "Epoch:  276  Training Loss:  58.8277  Training Accuracy:  0.0326279\n",
      "Epoch:  277  Training Loss:  58.8266  Training Accuracy:  0.0326279\n",
      "Epoch:  278  Training Loss:  58.8255  Training Accuracy:  0.0326279\n",
      "Epoch:  279  Training Loss:  58.8243  Training Accuracy:  0.0326279\n",
      "Epoch:  280  Training Loss:  58.8232  Training Accuracy:  0.0326279\n",
      "Epoch:  281  Training Loss:  58.8221  Training Accuracy:  0.0326279\n",
      "Epoch:  282  Training Loss:  58.8209  Training Accuracy:  0.0326279\n",
      "Epoch:  283  Training Loss:  58.8198  Training Accuracy:  0.0326279\n",
      "Epoch:  284  Training Loss:  58.8186  Training Accuracy:  0.0326279\n",
      "Epoch:  285  Training Loss:  58.8175  Training Accuracy:  0.0326279\n",
      "Epoch:  286  Training Loss:  58.8164  Training Accuracy:  0.0326279\n",
      "Epoch:  287  Training Loss:  58.8152  Training Accuracy:  0.0326279\n",
      "Epoch:  288  Training Loss:  58.8141  Training Accuracy:  0.0326279\n",
      "Epoch:  289  Training Loss:  58.813  Training Accuracy:  0.0326279\n",
      "Epoch:  290  Training Loss:  58.8118  Training Accuracy:  0.0326279\n",
      "Epoch:  291  Training Loss:  58.8107  Training Accuracy:  0.0326279\n",
      "Epoch:  292  Training Loss:  58.8096  Training Accuracy:  0.0326279\n",
      "Epoch:  293  Training Loss:  58.8084  Training Accuracy:  0.0326279\n",
      "Epoch:  294  Training Loss:  58.8073  Training Accuracy:  0.0326279\n",
      "Epoch:  295  Training Loss:  58.8062  Training Accuracy:  0.0326279\n",
      "Epoch:  296  Training Loss:  58.805  Training Accuracy:  0.0326279\n",
      "Epoch:  297  Training Loss:  58.8039  Training Accuracy:  0.0326279\n",
      "Epoch:  298  Training Loss:  58.8028  Training Accuracy:  0.0326279\n",
      "Epoch:  299  Training Loss:  58.8016  Training Accuracy:  0.0326279\n",
      "Epoch:  300  Training Loss:  58.8005  Training Accuracy:  0.0326279\n",
      "Epoch:  301  Training Loss:  58.7994  Training Accuracy:  0.0326279\n",
      "Epoch:  302  Training Loss:  58.7982  Training Accuracy:  0.0326279\n",
      "Epoch:  303  Training Loss:  58.7971  Training Accuracy:  0.0326279\n",
      "Epoch:  304  Training Loss:  58.7959  Training Accuracy:  0.0326279\n",
      "Epoch:  305  Training Loss:  58.7948  Training Accuracy:  0.0326279\n",
      "Epoch:  306  Training Loss:  58.7937  Training Accuracy:  0.0326279\n",
      "Epoch:  307  Training Loss:  58.7925  Training Accuracy:  0.0326279\n",
      "Epoch:  308  Training Loss:  58.7914  Training Accuracy:  0.0326279\n",
      "Epoch:  309  Training Loss:  58.7903  Training Accuracy:  0.0326279\n",
      "Epoch:  310  Training Loss:  58.7891  Training Accuracy:  0.0326867\n",
      "Epoch:  311  Training Loss:  58.788  Training Accuracy:  0.0326867\n",
      "Epoch:  312  Training Loss:  58.7869  Training Accuracy:  0.0326867\n",
      "Epoch:  313  Training Loss:  58.7857  Training Accuracy:  0.0326867\n",
      "Epoch:  314  Training Loss:  58.7846  Training Accuracy:  0.0326867\n",
      "Epoch:  315  Training Loss:  58.7835  Training Accuracy:  0.0326867\n",
      "Epoch:  316  Training Loss:  58.7823  Training Accuracy:  0.0326279\n",
      "Epoch:  317  Training Loss:  58.7812  Training Accuracy:  0.0326279\n",
      "Epoch:  318  Training Loss:  58.7801  Training Accuracy:  0.0326279\n",
      "Epoch:  319  Training Loss:  58.7789  Training Accuracy:  0.0326279\n",
      "Epoch:  320  Training Loss:  58.7778  Training Accuracy:  0.0326279\n",
      "Epoch:  321  Training Loss:  58.7767  Training Accuracy:  0.0326279\n",
      "Epoch:  322  Training Loss:  58.7756  Training Accuracy:  0.0326867\n",
      "Epoch:  323  Training Loss:  58.7744  Training Accuracy:  0.0326867\n",
      "Epoch:  324  Training Loss:  58.7733  Training Accuracy:  0.0326867\n",
      "Epoch:  325  Training Loss:  58.7721  Training Accuracy:  0.0326867\n",
      "Epoch:  326  Training Loss:  58.771  Training Accuracy:  0.0326867\n",
      "Epoch:  327  Training Loss:  58.7699  Training Accuracy:  0.0326867\n",
      "Epoch:  328  Training Loss:  58.7688  Training Accuracy:  0.0326867\n",
      "Epoch:  329  Training Loss:  58.7676  Training Accuracy:  0.0326867\n",
      "Epoch:  330  Training Loss:  58.7665  Training Accuracy:  0.0326867\n",
      "Epoch:  331  Training Loss:  58.7654  Training Accuracy:  0.0326867\n",
      "Epoch:  332  Training Loss:  58.7642  Training Accuracy:  0.0326867\n",
      "Epoch:  333  Training Loss:  58.7631  Training Accuracy:  0.0326867\n",
      "Epoch:  334  Training Loss:  58.762  Training Accuracy:  0.0326867\n",
      "Epoch:  335  Training Loss:  58.7608  Training Accuracy:  0.0326279\n",
      "Epoch:  336  Training Loss:  58.7597  Training Accuracy:  0.0326279\n",
      "Epoch:  337  Training Loss:  58.7586  Training Accuracy:  0.0326279\n",
      "Epoch:  338  Training Loss:  58.7574  Training Accuracy:  0.0326279\n",
      "Epoch:  339  Training Loss:  58.7563  Training Accuracy:  0.0326279\n",
      "Epoch:  340  Training Loss:  58.7552  Training Accuracy:  0.0326279\n",
      "Epoch:  341  Training Loss:  58.754  Training Accuracy:  0.0326279\n",
      "Epoch:  342  Training Loss:  58.7529  Training Accuracy:  0.0326279\n",
      "Epoch:  343  Training Loss:  58.7518  Training Accuracy:  0.0326279\n",
      "Epoch:  344  Training Loss:  58.7506  Training Accuracy:  0.0326279\n",
      "Epoch:  345  Training Loss:  58.7495  Training Accuracy:  0.0326279\n",
      "Epoch:  346  Training Loss:  58.7484  Training Accuracy:  0.0326279\n",
      "Epoch:  347  Training Loss:  58.7473  Training Accuracy:  0.0326279\n",
      "Epoch:  348  Training Loss:  58.7461  Training Accuracy:  0.0326279\n",
      "Epoch:  349  Training Loss:  58.745  Training Accuracy:  0.0326279\n",
      "Epoch:  350  Training Loss:  58.7439  Training Accuracy:  0.0326279\n",
      "Epoch:  351  Training Loss:  58.7427  Training Accuracy:  0.0326279\n",
      "Epoch:  352  Training Loss:  58.7416  Training Accuracy:  0.0326279\n",
      "Epoch:  353  Training Loss:  58.7405  Training Accuracy:  0.0326279\n",
      "Epoch:  354  Training Loss:  58.7393  Training Accuracy:  0.0326279\n",
      "Epoch:  355  Training Loss:  58.7382  Training Accuracy:  0.0326279\n",
      "Epoch:  356  Training Loss:  58.7371  Training Accuracy:  0.0326279\n",
      "Epoch:  357  Training Loss:  58.736  Training Accuracy:  0.0326279\n",
      "Epoch:  358  Training Loss:  58.7348  Training Accuracy:  0.0326279\n",
      "Epoch:  359  Training Loss:  58.7337  Training Accuracy:  0.0326279\n",
      "Epoch:  360  Training Loss:  58.7326  Training Accuracy:  0.0326279\n",
      "Epoch:  361  Training Loss:  58.7314  Training Accuracy:  0.0326279\n",
      "Epoch:  362  Training Loss:  58.7303  Training Accuracy:  0.0326279\n",
      "Epoch:  363  Training Loss:  58.7292  Training Accuracy:  0.0326279\n",
      "Epoch:  364  Training Loss:  58.728  Training Accuracy:  0.0326279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  365  Training Loss:  58.7269  Training Accuracy:  0.0326279\n",
      "Epoch:  366  Training Loss:  58.7258  Training Accuracy:  0.0326279\n",
      "Epoch:  367  Training Loss:  58.7247  Training Accuracy:  0.0326279\n",
      "Epoch:  368  Training Loss:  58.7235  Training Accuracy:  0.0326279\n",
      "Epoch:  369  Training Loss:  58.7224  Training Accuracy:  0.0326279\n",
      "Epoch:  370  Training Loss:  58.7213  Training Accuracy:  0.0326279\n",
      "Epoch:  371  Training Loss:  58.7201  Training Accuracy:  0.0326279\n",
      "Epoch:  372  Training Loss:  58.719  Training Accuracy:  0.0326279\n",
      "Epoch:  373  Training Loss:  58.7179  Training Accuracy:  0.0326279\n",
      "Epoch:  374  Training Loss:  58.7168  Training Accuracy:  0.0326279\n",
      "Epoch:  375  Training Loss:  58.7156  Training Accuracy:  0.0326279\n",
      "Epoch:  376  Training Loss:  58.7145  Training Accuracy:  0.0326279\n",
      "Epoch:  377  Training Loss:  58.7134  Training Accuracy:  0.0326279\n",
      "Epoch:  378  Training Loss:  58.7122  Training Accuracy:  0.0326279\n",
      "Epoch:  379  Training Loss:  58.7111  Training Accuracy:  0.0326279\n",
      "Epoch:  380  Training Loss:  58.71  Training Accuracy:  0.0326279\n",
      "Epoch:  381  Training Loss:  58.7089  Training Accuracy:  0.0326279\n",
      "Epoch:  382  Training Loss:  58.7077  Training Accuracy:  0.0326279\n",
      "Epoch:  383  Training Loss:  58.7066  Training Accuracy:  0.0326279\n",
      "Epoch:  384  Training Loss:  58.7055  Training Accuracy:  0.0326279\n",
      "Epoch:  385  Training Loss:  58.7044  Training Accuracy:  0.0326867\n",
      "Epoch:  386  Training Loss:  58.7032  Training Accuracy:  0.0326867\n",
      "Epoch:  387  Training Loss:  58.7021  Training Accuracy:  0.0326867\n",
      "Epoch:  388  Training Loss:  58.701  Training Accuracy:  0.0326867\n",
      "Epoch:  389  Training Loss:  58.6998  Training Accuracy:  0.0326867\n",
      "Epoch:  390  Training Loss:  58.6987  Training Accuracy:  0.0326867\n",
      "Epoch:  391  Training Loss:  58.6976  Training Accuracy:  0.0326867\n",
      "Epoch:  392  Training Loss:  58.6965  Training Accuracy:  0.0326867\n",
      "Epoch:  393  Training Loss:  58.6953  Training Accuracy:  0.0326867\n",
      "Epoch:  394  Training Loss:  58.6942  Training Accuracy:  0.0326867\n",
      "Epoch:  395  Training Loss:  58.6931  Training Accuracy:  0.0326867\n",
      "Epoch:  396  Training Loss:  58.692  Training Accuracy:  0.0326867\n",
      "Epoch:  397  Training Loss:  58.6908  Training Accuracy:  0.0326867\n",
      "Epoch:  398  Training Loss:  58.6897  Training Accuracy:  0.0326867\n",
      "Epoch:  399  Training Loss:  58.6886  Training Accuracy:  0.0326867\n",
      "Epoch:  400  Training Loss:  58.6875  Training Accuracy:  0.0326867\n",
      "Epoch:  401  Training Loss:  58.6863  Training Accuracy:  0.0326867\n",
      "Epoch:  402  Training Loss:  58.6852  Training Accuracy:  0.0326867\n",
      "Epoch:  403  Training Loss:  58.6841  Training Accuracy:  0.0326867\n",
      "Epoch:  404  Training Loss:  58.683  Training Accuracy:  0.0326279\n",
      "Epoch:  405  Training Loss:  58.6818  Training Accuracy:  0.0326279\n",
      "Epoch:  406  Training Loss:  58.6807  Training Accuracy:  0.0325691\n",
      "Epoch:  407  Training Loss:  58.6796  Training Accuracy:  0.0325691\n",
      "Epoch:  408  Training Loss:  58.6784  Training Accuracy:  0.0326279\n",
      "Epoch:  409  Training Loss:  58.6773  Training Accuracy:  0.0326279\n",
      "Epoch:  410  Training Loss:  58.6762  Training Accuracy:  0.0326279\n",
      "Epoch:  411  Training Loss:  58.6751  Training Accuracy:  0.0326279\n",
      "Epoch:  412  Training Loss:  58.674  Training Accuracy:  0.0326279\n",
      "Epoch:  413  Training Loss:  58.6728  Training Accuracy:  0.0326279\n",
      "Epoch:  414  Training Loss:  58.6717  Training Accuracy:  0.0326279\n",
      "Epoch:  415  Training Loss:  58.6706  Training Accuracy:  0.0326279\n",
      "Epoch:  416  Training Loss:  58.6695  Training Accuracy:  0.0326279\n",
      "Epoch:  417  Training Loss:  58.6683  Training Accuracy:  0.0326279\n",
      "Epoch:  418  Training Loss:  58.6672  Training Accuracy:  0.0326279\n",
      "Epoch:  419  Training Loss:  58.6661  Training Accuracy:  0.0326279\n",
      "Epoch:  420  Training Loss:  58.665  Training Accuracy:  0.0326279\n",
      "Epoch:  421  Training Loss:  58.6638  Training Accuracy:  0.0326279\n",
      "Epoch:  422  Training Loss:  58.6627  Training Accuracy:  0.0326279\n",
      "Epoch:  423  Training Loss:  58.6616  Training Accuracy:  0.0326279\n",
      "Epoch:  424  Training Loss:  58.6605  Training Accuracy:  0.0325691\n",
      "Epoch:  425  Training Loss:  58.6593  Training Accuracy:  0.0325691\n",
      "Epoch:  426  Training Loss:  58.6582  Training Accuracy:  0.0325691\n",
      "Epoch:  427  Training Loss:  58.6571  Training Accuracy:  0.0325691\n",
      "Epoch:  428  Training Loss:  58.656  Training Accuracy:  0.0325691\n",
      "Epoch:  429  Training Loss:  58.6548  Training Accuracy:  0.0325691\n",
      "Epoch:  430  Training Loss:  58.6537  Training Accuracy:  0.0325691\n",
      "Epoch:  431  Training Loss:  58.6526  Training Accuracy:  0.0325103\n",
      "Epoch:  432  Training Loss:  58.6515  Training Accuracy:  0.0325103\n",
      "Epoch:  433  Training Loss:  58.6504  Training Accuracy:  0.0325103\n",
      "Epoch:  434  Training Loss:  58.6492  Training Accuracy:  0.0325103\n",
      "Epoch:  435  Training Loss:  58.6481  Training Accuracy:  0.0325103\n",
      "Epoch:  436  Training Loss:  58.647  Training Accuracy:  0.0325103\n",
      "Epoch:  437  Training Loss:  58.6459  Training Accuracy:  0.0325103\n",
      "Epoch:  438  Training Loss:  58.6447  Training Accuracy:  0.0325103\n",
      "Epoch:  439  Training Loss:  58.6436  Training Accuracy:  0.0325103\n",
      "Epoch:  440  Training Loss:  58.6425  Training Accuracy:  0.0325103\n",
      "Epoch:  441  Training Loss:  58.6414  Training Accuracy:  0.0325103\n",
      "Epoch:  442  Training Loss:  58.6403  Training Accuracy:  0.0325103\n",
      "Epoch:  443  Training Loss:  58.6391  Training Accuracy:  0.0325103\n",
      "Epoch:  444  Training Loss:  58.638  Training Accuracy:  0.0325103\n",
      "Epoch:  445  Training Loss:  58.6369  Training Accuracy:  0.0325691\n",
      "Epoch:  446  Training Loss:  58.6358  Training Accuracy:  0.0325691\n",
      "Epoch:  447  Training Loss:  58.6346  Training Accuracy:  0.0325691\n",
      "Epoch:  448  Training Loss:  58.6335  Training Accuracy:  0.0325691\n",
      "Epoch:  449  Training Loss:  58.6324  Training Accuracy:  0.0325691\n",
      "Epoch:  450  Training Loss:  58.6313  Training Accuracy:  0.0325691\n",
      "Epoch:  451  Training Loss:  58.6301  Training Accuracy:  0.0325691\n",
      "Epoch:  452  Training Loss:  58.629  Training Accuracy:  0.0325691\n",
      "Epoch:  453  Training Loss:  58.6279  Training Accuracy:  0.0325691\n",
      "Epoch:  454  Training Loss:  58.6268  Training Accuracy:  0.0325691\n",
      "Epoch:  455  Training Loss:  58.6257  Training Accuracy:  0.0325691\n",
      "Epoch:  456  Training Loss:  58.6245  Training Accuracy:  0.0325691\n",
      "Epoch:  457  Training Loss:  58.6234  Training Accuracy:  0.0325691\n",
      "Epoch:  458  Training Loss:  58.6223  Training Accuracy:  0.0325691\n",
      "Epoch:  459  Training Loss:  58.6212  Training Accuracy:  0.0325691\n",
      "Epoch:  460  Training Loss:  58.6201  Training Accuracy:  0.0325691\n",
      "Epoch:  461  Training Loss:  58.6189  Training Accuracy:  0.0325691\n",
      "Epoch:  462  Training Loss:  58.6178  Training Accuracy:  0.0325691\n",
      "Epoch:  463  Training Loss:  58.6167  Training Accuracy:  0.0325691\n",
      "Epoch:  464  Training Loss:  58.6156  Training Accuracy:  0.0325691\n",
      "Epoch:  465  Training Loss:  58.6145  Training Accuracy:  0.0325691\n",
      "Epoch:  466  Training Loss:  58.6134  Training Accuracy:  0.0325691\n",
      "Epoch:  467  Training Loss:  58.6122  Training Accuracy:  0.0325691\n",
      "Epoch:  468  Training Loss:  58.6111  Training Accuracy:  0.0325691\n",
      "Epoch:  469  Training Loss:  58.61  Training Accuracy:  0.0325691\n",
      "Epoch:  470  Training Loss:  58.6089  Training Accuracy:  0.0325691\n",
      "Epoch:  471  Training Loss:  58.6077  Training Accuracy:  0.0325691\n",
      "Epoch:  472  Training Loss:  58.6066  Training Accuracy:  0.0325691\n",
      "Epoch:  473  Training Loss:  58.6055  Training Accuracy:  0.0325691\n",
      "Epoch:  474  Training Loss:  58.6044  Training Accuracy:  0.0325691\n",
      "Epoch:  475  Training Loss:  58.6033  Training Accuracy:  0.0325691\n",
      "Epoch:  476  Training Loss:  58.6022  Training Accuracy:  0.0325691\n",
      "Epoch:  477  Training Loss:  58.601  Training Accuracy:  0.0325691\n",
      "Epoch:  478  Training Loss:  58.5999  Training Accuracy:  0.0325691\n",
      "Epoch:  479  Training Loss:  58.5988  Training Accuracy:  0.0325691\n",
      "Epoch:  480  Training Loss:  58.5977  Training Accuracy:  0.0325691\n",
      "Epoch:  481  Training Loss:  58.5966  Training Accuracy:  0.0325103\n",
      "Epoch:  482  Training Loss:  58.5954  Training Accuracy:  0.0325103\n",
      "Epoch:  483  Training Loss:  58.5943  Training Accuracy:  0.0325103\n",
      "Epoch:  484  Training Loss:  58.5932  Training Accuracy:  0.0325103\n",
      "Epoch:  485  Training Loss:  58.5921  Training Accuracy:  0.0325103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  486  Training Loss:  58.591  Training Accuracy:  0.0325103\n",
      "Epoch:  487  Training Loss:  58.5899  Training Accuracy:  0.0325103\n",
      "Epoch:  488  Training Loss:  58.5887  Training Accuracy:  0.0325103\n",
      "Epoch:  489  Training Loss:  58.5876  Training Accuracy:  0.0325103\n",
      "Epoch:  490  Training Loss:  58.5865  Training Accuracy:  0.0325103\n",
      "Epoch:  491  Training Loss:  58.5854  Training Accuracy:  0.0325103\n",
      "Epoch:  492  Training Loss:  58.5843  Training Accuracy:  0.0325103\n",
      "Epoch:  493  Training Loss:  58.5831  Training Accuracy:  0.0325103\n",
      "Epoch:  494  Training Loss:  58.582  Training Accuracy:  0.0325103\n",
      "Epoch:  495  Training Loss:  58.5809  Training Accuracy:  0.0325103\n",
      "Epoch:  496  Training Loss:  58.5798  Training Accuracy:  0.0325103\n",
      "Epoch:  497  Training Loss:  58.5787  Training Accuracy:  0.0325103\n",
      "Epoch:  498  Training Loss:  58.5776  Training Accuracy:  0.0325103\n",
      "Epoch:  499  Training Loss:  58.5764  Training Accuracy:  0.0325103\n",
      "Epoch:  500  Training Loss:  58.5753  Training Accuracy:  0.0325103\n",
      "Epoch:  501  Training Loss:  58.5742  Training Accuracy:  0.0325103\n",
      "Epoch:  502  Training Loss:  58.5731  Training Accuracy:  0.0325103\n",
      "Epoch:  503  Training Loss:  58.572  Training Accuracy:  0.0325103\n",
      "Epoch:  504  Training Loss:  58.5709  Training Accuracy:  0.0325103\n",
      "Epoch:  505  Training Loss:  58.5697  Training Accuracy:  0.0325103\n",
      "Epoch:  506  Training Loss:  58.5686  Training Accuracy:  0.0325103\n",
      "Epoch:  507  Training Loss:  58.5675  Training Accuracy:  0.0325103\n",
      "Epoch:  508  Training Loss:  58.5664  Training Accuracy:  0.0325103\n",
      "Epoch:  509  Training Loss:  58.5653  Training Accuracy:  0.0325103\n",
      "Epoch:  510  Training Loss:  58.5642  Training Accuracy:  0.0325103\n",
      "Epoch:  511  Training Loss:  58.563  Training Accuracy:  0.0325103\n",
      "Epoch:  512  Training Loss:  58.5619  Training Accuracy:  0.0325103\n",
      "Epoch:  513  Training Loss:  58.5608  Training Accuracy:  0.0325103\n",
      "Epoch:  514  Training Loss:  58.5597  Training Accuracy:  0.0325103\n",
      "Epoch:  515  Training Loss:  58.5586  Training Accuracy:  0.0325103\n",
      "Epoch:  516  Training Loss:  58.5575  Training Accuracy:  0.0325103\n",
      "Epoch:  517  Training Loss:  58.5563  Training Accuracy:  0.0325103\n",
      "Epoch:  518  Training Loss:  58.5552  Training Accuracy:  0.0325103\n",
      "Epoch:  519  Training Loss:  58.5541  Training Accuracy:  0.0325103\n",
      "Epoch:  520  Training Loss:  58.553  Training Accuracy:  0.0325103\n",
      "Epoch:  521  Training Loss:  58.5519  Training Accuracy:  0.0325103\n",
      "Epoch:  522  Training Loss:  58.5508  Training Accuracy:  0.0325103\n",
      "Epoch:  523  Training Loss:  58.5496  Training Accuracy:  0.0325103\n",
      "Epoch:  524  Training Loss:  58.5485  Training Accuracy:  0.0325103\n",
      "Epoch:  525  Training Loss:  58.5474  Training Accuracy:  0.0325103\n",
      "Epoch:  526  Training Loss:  58.5463  Training Accuracy:  0.0325103\n",
      "Epoch:  527  Training Loss:  58.5452  Training Accuracy:  0.0325103\n",
      "Epoch:  528  Training Loss:  58.5441  Training Accuracy:  0.0324515\n",
      "Epoch:  529  Training Loss:  58.543  Training Accuracy:  0.0324515\n",
      "Epoch:  530  Training Loss:  58.5419  Training Accuracy:  0.0324515\n",
      "Epoch:  531  Training Loss:  58.5407  Training Accuracy:  0.0324515\n",
      "Epoch:  532  Training Loss:  58.5396  Training Accuracy:  0.0324515\n",
      "Epoch:  533  Training Loss:  58.5385  Training Accuracy:  0.0324515\n",
      "Epoch:  534  Training Loss:  58.5374  Training Accuracy:  0.0324515\n",
      "Epoch:  535  Training Loss:  58.5363  Training Accuracy:  0.0324515\n",
      "Epoch:  536  Training Loss:  58.5352  Training Accuracy:  0.0324515\n",
      "Epoch:  537  Training Loss:  58.5341  Training Accuracy:  0.0324515\n",
      "Epoch:  538  Training Loss:  58.5329  Training Accuracy:  0.0324515\n",
      "Epoch:  539  Training Loss:  58.5318  Training Accuracy:  0.0324515\n",
      "Epoch:  540  Training Loss:  58.5307  Training Accuracy:  0.0324515\n",
      "Epoch:  541  Training Loss:  58.5296  Training Accuracy:  0.0323927\n",
      "Epoch:  542  Training Loss:  58.5285  Training Accuracy:  0.0323927\n",
      "Epoch:  543  Training Loss:  58.5274  Training Accuracy:  0.0323927\n",
      "Epoch:  544  Training Loss:  58.5263  Training Accuracy:  0.0323927\n",
      "Epoch:  545  Training Loss:  58.5252  Training Accuracy:  0.0323339\n",
      "Epoch:  546  Training Loss:  58.524  Training Accuracy:  0.0323339\n",
      "Epoch:  547  Training Loss:  58.5229  Training Accuracy:  0.0323339\n",
      "Epoch:  548  Training Loss:  58.5218  Training Accuracy:  0.0323339\n",
      "Epoch:  549  Training Loss:  58.5207  Training Accuracy:  0.0323339\n",
      "Epoch:  550  Training Loss:  58.5196  Training Accuracy:  0.0323339\n",
      "Epoch:  551  Training Loss:  58.5185  Training Accuracy:  0.0323339\n",
      "Epoch:  552  Training Loss:  58.5174  Training Accuracy:  0.0323339\n",
      "Epoch:  553  Training Loss:  58.5162  Training Accuracy:  0.0323339\n",
      "Epoch:  554  Training Loss:  58.5151  Training Accuracy:  0.0323339\n",
      "Epoch:  555  Training Loss:  58.514  Training Accuracy:  0.0323339\n",
      "Epoch:  556  Training Loss:  58.5129  Training Accuracy:  0.0323339\n",
      "Epoch:  557  Training Loss:  58.5118  Training Accuracy:  0.0323339\n",
      "Epoch:  558  Training Loss:  58.5107  Training Accuracy:  0.0323339\n",
      "Epoch:  559  Training Loss:  58.5096  Training Accuracy:  0.0323339\n",
      "Epoch:  560  Training Loss:  58.5085  Training Accuracy:  0.0323339\n",
      "Epoch:  561  Training Loss:  58.5073  Training Accuracy:  0.0323339\n",
      "Epoch:  562  Training Loss:  58.5062  Training Accuracy:  0.0323339\n",
      "Epoch:  563  Training Loss:  58.5051  Training Accuracy:  0.0323339\n",
      "Epoch:  564  Training Loss:  58.504  Training Accuracy:  0.0323339\n",
      "Epoch:  565  Training Loss:  58.5029  Training Accuracy:  0.0323339\n",
      "Epoch:  566  Training Loss:  58.5018  Training Accuracy:  0.0323339\n",
      "Epoch:  567  Training Loss:  58.5007  Training Accuracy:  0.0323339\n",
      "Epoch:  568  Training Loss:  58.4996  Training Accuracy:  0.0323339\n",
      "Epoch:  569  Training Loss:  58.4985  Training Accuracy:  0.0323339\n",
      "Epoch:  570  Training Loss:  58.4974  Training Accuracy:  0.0323339\n",
      "Epoch:  571  Training Loss:  58.4962  Training Accuracy:  0.0323927\n",
      "Epoch:  572  Training Loss:  58.4951  Training Accuracy:  0.0323927\n",
      "Epoch:  573  Training Loss:  58.494  Training Accuracy:  0.0323927\n",
      "Epoch:  574  Training Loss:  58.4929  Training Accuracy:  0.0323927\n",
      "Epoch:  575  Training Loss:  58.4918  Training Accuracy:  0.0323927\n",
      "Epoch:  576  Training Loss:  58.4907  Training Accuracy:  0.0323927\n",
      "Epoch:  577  Training Loss:  58.4896  Training Accuracy:  0.0323927\n",
      "Epoch:  578  Training Loss:  58.4885  Training Accuracy:  0.0323927\n",
      "Epoch:  579  Training Loss:  58.4874  Training Accuracy:  0.0323927\n",
      "Epoch:  580  Training Loss:  58.4863  Training Accuracy:  0.0323927\n",
      "Epoch:  581  Training Loss:  58.4851  Training Accuracy:  0.0323927\n",
      "Epoch:  582  Training Loss:  58.484  Training Accuracy:  0.0323927\n",
      "Epoch:  583  Training Loss:  58.4829  Training Accuracy:  0.0323927\n",
      "Epoch:  584  Training Loss:  58.4818  Training Accuracy:  0.0323927\n",
      "Epoch:  585  Training Loss:  58.4807  Training Accuracy:  0.0323339\n",
      "Epoch:  586  Training Loss:  58.4796  Training Accuracy:  0.0323339\n",
      "Epoch:  587  Training Loss:  58.4785  Training Accuracy:  0.0323339\n",
      "Epoch:  588  Training Loss:  58.4774  Training Accuracy:  0.0323339\n",
      "Epoch:  589  Training Loss:  58.4763  Training Accuracy:  0.0323339\n",
      "Epoch:  590  Training Loss:  58.4751  Training Accuracy:  0.0323339\n",
      "Epoch:  591  Training Loss:  58.474  Training Accuracy:  0.0323339\n",
      "Epoch:  592  Training Loss:  58.4729  Training Accuracy:  0.0323339\n",
      "Epoch:  593  Training Loss:  58.4718  Training Accuracy:  0.0323339\n",
      "Epoch:  594  Training Loss:  58.4707  Training Accuracy:  0.0323339\n",
      "Epoch:  595  Training Loss:  58.4696  Training Accuracy:  0.0323339\n",
      "Epoch:  596  Training Loss:  58.4685  Training Accuracy:  0.0323339\n",
      "Epoch:  597  Training Loss:  58.4674  Training Accuracy:  0.0323339\n",
      "Epoch:  598  Training Loss:  58.4663  Training Accuracy:  0.0323339\n",
      "Epoch:  599  Training Loss:  58.4652  Training Accuracy:  0.0323339\n",
      "Epoch:  600  Training Loss:  58.4641  Training Accuracy:  0.0323339\n",
      "Epoch:  601  Training Loss:  58.463  Training Accuracy:  0.0323927\n",
      "Epoch:  602  Training Loss:  58.4618  Training Accuracy:  0.0323927\n",
      "Epoch:  603  Training Loss:  58.4607  Training Accuracy:  0.0323927\n",
      "Epoch:  604  Training Loss:  58.4596  Training Accuracy:  0.0323927\n",
      "Epoch:  605  Training Loss:  58.4585  Training Accuracy:  0.0323927\n",
      "Epoch:  606  Training Loss:  58.4574  Training Accuracy:  0.0323927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  607  Training Loss:  58.4563  Training Accuracy:  0.0323927\n",
      "Epoch:  608  Training Loss:  58.4552  Training Accuracy:  0.0323927\n",
      "Epoch:  609  Training Loss:  58.4541  Training Accuracy:  0.0323927\n",
      "Epoch:  610  Training Loss:  58.453  Training Accuracy:  0.0323927\n",
      "Epoch:  611  Training Loss:  58.4519  Training Accuracy:  0.0323927\n",
      "Epoch:  612  Training Loss:  58.4508  Training Accuracy:  0.0323927\n",
      "Epoch:  613  Training Loss:  58.4497  Training Accuracy:  0.0323927\n",
      "Epoch:  614  Training Loss:  58.4486  Training Accuracy:  0.0323927\n",
      "Epoch:  615  Training Loss:  58.4474  Training Accuracy:  0.0323927\n",
      "Epoch:  616  Training Loss:  58.4463  Training Accuracy:  0.0323927\n",
      "Epoch:  617  Training Loss:  58.4452  Training Accuracy:  0.0323927\n",
      "Epoch:  618  Training Loss:  58.4441  Training Accuracy:  0.0323927\n",
      "Epoch:  619  Training Loss:  58.443  Training Accuracy:  0.0323927\n",
      "Epoch:  620  Training Loss:  58.4419  Training Accuracy:  0.0323927\n",
      "Epoch:  621  Training Loss:  58.4408  Training Accuracy:  0.0323927\n",
      "Epoch:  622  Training Loss:  58.4397  Training Accuracy:  0.0323927\n",
      "Epoch:  623  Training Loss:  58.4386  Training Accuracy:  0.0323927\n",
      "Epoch:  624  Training Loss:  58.4375  Training Accuracy:  0.0323927\n",
      "Epoch:  625  Training Loss:  58.4364  Training Accuracy:  0.0323927\n",
      "Epoch:  626  Training Loss:  58.4353  Training Accuracy:  0.0323927\n",
      "Epoch:  627  Training Loss:  58.4342  Training Accuracy:  0.0323927\n",
      "Epoch:  628  Training Loss:  58.4331  Training Accuracy:  0.0323927\n",
      "Epoch:  629  Training Loss:  58.432  Training Accuracy:  0.0323927\n",
      "Epoch:  630  Training Loss:  58.4309  Training Accuracy:  0.0323927\n",
      "Epoch:  631  Training Loss:  58.4298  Training Accuracy:  0.0323927\n",
      "Epoch:  632  Training Loss:  58.4286  Training Accuracy:  0.0323927\n",
      "Epoch:  633  Training Loss:  58.4275  Training Accuracy:  0.0323927\n",
      "Epoch:  634  Training Loss:  58.4264  Training Accuracy:  0.0323927\n",
      "Epoch:  635  Training Loss:  58.4253  Training Accuracy:  0.0323927\n",
      "Epoch:  636  Training Loss:  58.4242  Training Accuracy:  0.0323927\n",
      "Epoch:  637  Training Loss:  58.4231  Training Accuracy:  0.0323927\n",
      "Epoch:  638  Training Loss:  58.422  Training Accuracy:  0.0323927\n",
      "Epoch:  639  Training Loss:  58.4209  Training Accuracy:  0.0323927\n",
      "Epoch:  640  Training Loss:  58.4198  Training Accuracy:  0.0323927\n",
      "Epoch:  641  Training Loss:  58.4187  Training Accuracy:  0.0323927\n",
      "Epoch:  642  Training Loss:  58.4176  Training Accuracy:  0.0323927\n",
      "Epoch:  643  Training Loss:  58.4165  Training Accuracy:  0.0323927\n",
      "Epoch:  644  Training Loss:  58.4154  Training Accuracy:  0.0323927\n",
      "Epoch:  645  Training Loss:  58.4143  Training Accuracy:  0.0324515\n",
      "Epoch:  646  Training Loss:  58.4132  Training Accuracy:  0.0324515\n",
      "Epoch:  647  Training Loss:  58.4121  Training Accuracy:  0.0324515\n",
      "Epoch:  648  Training Loss:  58.411  Training Accuracy:  0.0323927\n",
      "Epoch:  649  Training Loss:  58.4099  Training Accuracy:  0.0323927\n",
      "Epoch:  650  Training Loss:  58.4088  Training Accuracy:  0.0323927\n",
      "Epoch:  651  Training Loss:  58.4077  Training Accuracy:  0.0323927\n",
      "Epoch:  652  Training Loss:  58.4066  Training Accuracy:  0.0323927\n",
      "Epoch:  653  Training Loss:  58.4054  Training Accuracy:  0.0323927\n",
      "Epoch:  654  Training Loss:  58.4043  Training Accuracy:  0.0323927\n",
      "Epoch:  655  Training Loss:  58.4032  Training Accuracy:  0.0323927\n",
      "Epoch:  656  Training Loss:  58.4021  Training Accuracy:  0.0323927\n",
      "Epoch:  657  Training Loss:  58.401  Training Accuracy:  0.0323927\n",
      "Epoch:  658  Training Loss:  58.3999  Training Accuracy:  0.0323927\n",
      "Epoch:  659  Training Loss:  58.3988  Training Accuracy:  0.0323927\n",
      "Epoch:  660  Training Loss:  58.3977  Training Accuracy:  0.0323927\n",
      "Epoch:  661  Training Loss:  58.3966  Training Accuracy:  0.0324515\n",
      "Epoch:  662  Training Loss:  58.3955  Training Accuracy:  0.0324515\n",
      "Epoch:  663  Training Loss:  58.3944  Training Accuracy:  0.0324515\n",
      "Epoch:  664  Training Loss:  58.3933  Training Accuracy:  0.0324515\n",
      "Epoch:  665  Training Loss:  58.3922  Training Accuracy:  0.0324515\n",
      "Epoch:  666  Training Loss:  58.3911  Training Accuracy:  0.0324515\n",
      "Epoch:  667  Training Loss:  58.39  Training Accuracy:  0.0324515\n",
      "Epoch:  668  Training Loss:  58.3889  Training Accuracy:  0.0324515\n",
      "Epoch:  669  Training Loss:  58.3878  Training Accuracy:  0.0324515\n",
      "Epoch:  670  Training Loss:  58.3867  Training Accuracy:  0.0324515\n",
      "Epoch:  671  Training Loss:  58.3856  Training Accuracy:  0.0324515\n",
      "Epoch:  672  Training Loss:  58.3845  Training Accuracy:  0.0324515\n",
      "Epoch:  673  Training Loss:  58.3834  Training Accuracy:  0.0324515\n",
      "Epoch:  674  Training Loss:  58.3823  Training Accuracy:  0.0324515\n",
      "Epoch:  675  Training Loss:  58.3812  Training Accuracy:  0.0324515\n",
      "Epoch:  676  Training Loss:  58.3801  Training Accuracy:  0.0324515\n",
      "Epoch:  677  Training Loss:  58.379  Training Accuracy:  0.0324515\n",
      "Epoch:  678  Training Loss:  58.3779  Training Accuracy:  0.0324515\n",
      "Epoch:  679  Training Loss:  58.3768  Training Accuracy:  0.0324515\n",
      "Epoch:  680  Training Loss:  58.3757  Training Accuracy:  0.0324515\n",
      "Epoch:  681  Training Loss:  58.3746  Training Accuracy:  0.0324515\n",
      "Epoch:  682  Training Loss:  58.3735  Training Accuracy:  0.0324515\n",
      "Epoch:  683  Training Loss:  58.3724  Training Accuracy:  0.0324515\n",
      "Epoch:  684  Training Loss:  58.3713  Training Accuracy:  0.0324515\n",
      "Epoch:  685  Training Loss:  58.3702  Training Accuracy:  0.0324515\n",
      "Epoch:  686  Training Loss:  58.3691  Training Accuracy:  0.0324515\n",
      "Epoch:  687  Training Loss:  58.368  Training Accuracy:  0.0324515\n",
      "Epoch:  688  Training Loss:  58.3669  Training Accuracy:  0.0324515\n",
      "Epoch:  689  Training Loss:  58.3658  Training Accuracy:  0.0324515\n",
      "Epoch:  690  Training Loss:  58.3647  Training Accuracy:  0.0324515\n",
      "Epoch:  691  Training Loss:  58.3636  Training Accuracy:  0.0324515\n",
      "Epoch:  692  Training Loss:  58.3625  Training Accuracy:  0.0324515\n",
      "Epoch:  693  Training Loss:  58.3614  Training Accuracy:  0.0324515\n",
      "Epoch:  694  Training Loss:  58.3603  Training Accuracy:  0.0324515\n",
      "Epoch:  695  Training Loss:  58.3592  Training Accuracy:  0.0324515\n",
      "Epoch:  696  Training Loss:  58.3581  Training Accuracy:  0.0324515\n",
      "Epoch:  697  Training Loss:  58.357  Training Accuracy:  0.0325103\n",
      "Epoch:  698  Training Loss:  58.3559  Training Accuracy:  0.0325103\n",
      "Epoch:  699  Training Loss:  58.3548  Training Accuracy:  0.0325103\n",
      "Epoch:  700  Training Loss:  58.3537  Training Accuracy:  0.0325103\n",
      "Epoch:  701  Training Loss:  58.3526  Training Accuracy:  0.0325103\n",
      "Epoch:  702  Training Loss:  58.3515  Training Accuracy:  0.0325103\n",
      "Epoch:  703  Training Loss:  58.3504  Training Accuracy:  0.0325103\n",
      "Epoch:  704  Training Loss:  58.3493  Training Accuracy:  0.0325103\n",
      "Epoch:  705  Training Loss:  58.3482  Training Accuracy:  0.0325103\n",
      "Epoch:  706  Training Loss:  58.3471  Training Accuracy:  0.0325103\n",
      "Epoch:  707  Training Loss:  58.346  Training Accuracy:  0.0325103\n",
      "Epoch:  708  Training Loss:  58.3449  Training Accuracy:  0.0325103\n",
      "Epoch:  709  Training Loss:  58.3438  Training Accuracy:  0.0325103\n",
      "Epoch:  710  Training Loss:  58.3427  Training Accuracy:  0.0325103\n",
      "Epoch:  711  Training Loss:  58.3416  Training Accuracy:  0.0325103\n",
      "Epoch:  712  Training Loss:  58.3405  Training Accuracy:  0.0325103\n",
      "Epoch:  713  Training Loss:  58.3394  Training Accuracy:  0.0325103\n",
      "Epoch:  714  Training Loss:  58.3383  Training Accuracy:  0.0325103\n",
      "Epoch:  715  Training Loss:  58.3372  Training Accuracy:  0.0325103\n",
      "Epoch:  716  Training Loss:  58.3361  Training Accuracy:  0.0325103\n",
      "Epoch:  717  Training Loss:  58.335  Training Accuracy:  0.0325103\n",
      "Epoch:  718  Training Loss:  58.3339  Training Accuracy:  0.0325103\n",
      "Epoch:  719  Training Loss:  58.3328  Training Accuracy:  0.0325103\n",
      "Epoch:  720  Training Loss:  58.3317  Training Accuracy:  0.0325103\n",
      "Epoch:  721  Training Loss:  58.3306  Training Accuracy:  0.0325103\n",
      "Epoch:  722  Training Loss:  58.3295  Training Accuracy:  0.0325103\n",
      "Epoch:  723  Training Loss:  58.3284  Training Accuracy:  0.0325103\n",
      "Epoch:  724  Training Loss:  58.3273  Training Accuracy:  0.0325103\n",
      "Epoch:  725  Training Loss:  58.3262  Training Accuracy:  0.0325103\n",
      "Epoch:  726  Training Loss:  58.3251  Training Accuracy:  0.0325103\n",
      "Epoch:  727  Training Loss:  58.324  Training Accuracy:  0.0325103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  728  Training Loss:  58.3229  Training Accuracy:  0.0325103\n",
      "Epoch:  729  Training Loss:  58.3218  Training Accuracy:  0.0325103\n",
      "Epoch:  730  Training Loss:  58.3207  Training Accuracy:  0.0325103\n",
      "Epoch:  731  Training Loss:  58.3196  Training Accuracy:  0.0325691\n",
      "Epoch:  732  Training Loss:  58.3185  Training Accuracy:  0.0325691\n",
      "Epoch:  733  Training Loss:  58.3175  Training Accuracy:  0.0325691\n",
      "Epoch:  734  Training Loss:  58.3164  Training Accuracy:  0.0325691\n",
      "Epoch:  735  Training Loss:  58.3153  Training Accuracy:  0.0325691\n",
      "Epoch:  736  Training Loss:  58.3142  Training Accuracy:  0.0325691\n",
      "Epoch:  737  Training Loss:  58.3131  Training Accuracy:  0.0325691\n",
      "Epoch:  738  Training Loss:  58.312  Training Accuracy:  0.0325691\n",
      "Epoch:  739  Training Loss:  58.3109  Training Accuracy:  0.0325691\n",
      "Epoch:  740  Training Loss:  58.3098  Training Accuracy:  0.0325691\n",
      "Epoch:  741  Training Loss:  58.3087  Training Accuracy:  0.0325691\n",
      "Epoch:  742  Training Loss:  58.3076  Training Accuracy:  0.0325691\n",
      "Epoch:  743  Training Loss:  58.3065  Training Accuracy:  0.0325691\n",
      "Epoch:  744  Training Loss:  58.3054  Training Accuracy:  0.0325691\n",
      "Epoch:  745  Training Loss:  58.3043  Training Accuracy:  0.0325691\n",
      "Epoch:  746  Training Loss:  58.3032  Training Accuracy:  0.0325691\n",
      "Epoch:  747  Training Loss:  58.3021  Training Accuracy:  0.0325691\n",
      "Epoch:  748  Training Loss:  58.301  Training Accuracy:  0.0325691\n",
      "Epoch:  749  Training Loss:  58.2999  Training Accuracy:  0.0325691\n",
      "Epoch:  750  Training Loss:  58.2988  Training Accuracy:  0.0325691\n",
      "Epoch:  751  Training Loss:  58.2977  Training Accuracy:  0.0325691\n",
      "Epoch:  752  Training Loss:  58.2966  Training Accuracy:  0.0325691\n",
      "Epoch:  753  Training Loss:  58.2955  Training Accuracy:  0.0325691\n",
      "Epoch:  754  Training Loss:  58.2944  Training Accuracy:  0.0325103\n",
      "Epoch:  755  Training Loss:  58.2933  Training Accuracy:  0.0325103\n",
      "Epoch:  756  Training Loss:  58.2923  Training Accuracy:  0.0325103\n",
      "Epoch:  757  Training Loss:  58.2912  Training Accuracy:  0.0325103\n",
      "Epoch:  758  Training Loss:  58.2901  Training Accuracy:  0.0325103\n",
      "Epoch:  759  Training Loss:  58.289  Training Accuracy:  0.0325103\n",
      "Epoch:  760  Training Loss:  58.2879  Training Accuracy:  0.0325103\n",
      "Epoch:  761  Training Loss:  58.2868  Training Accuracy:  0.0325103\n",
      "Epoch:  762  Training Loss:  58.2857  Training Accuracy:  0.0325103\n",
      "Epoch:  763  Training Loss:  58.2846  Training Accuracy:  0.0325103\n",
      "Epoch:  764  Training Loss:  58.2835  Training Accuracy:  0.0325103\n",
      "Epoch:  765  Training Loss:  58.2824  Training Accuracy:  0.0325103\n",
      "Epoch:  766  Training Loss:  58.2813  Training Accuracy:  0.0325103\n",
      "Epoch:  767  Training Loss:  58.2802  Training Accuracy:  0.0325103\n",
      "Epoch:  768  Training Loss:  58.2791  Training Accuracy:  0.0325103\n",
      "Epoch:  769  Training Loss:  58.278  Training Accuracy:  0.0325103\n",
      "Epoch:  770  Training Loss:  58.2769  Training Accuracy:  0.0325103\n",
      "Epoch:  771  Training Loss:  58.2758  Training Accuracy:  0.0325103\n",
      "Epoch:  772  Training Loss:  58.2747  Training Accuracy:  0.0325103\n",
      "Epoch:  773  Training Loss:  58.2736  Training Accuracy:  0.0325103\n",
      "Epoch:  774  Training Loss:  58.2726  Training Accuracy:  0.0325103\n",
      "Epoch:  775  Training Loss:  58.2715  Training Accuracy:  0.0325103\n",
      "Epoch:  776  Training Loss:  58.2704  Training Accuracy:  0.0325103\n",
      "Epoch:  777  Training Loss:  58.2693  Training Accuracy:  0.0325103\n",
      "Epoch:  778  Training Loss:  58.2682  Training Accuracy:  0.0325103\n",
      "Epoch:  779  Training Loss:  58.2671  Training Accuracy:  0.0325103\n",
      "Epoch:  780  Training Loss:  58.266  Training Accuracy:  0.0325103\n",
      "Epoch:  781  Training Loss:  58.2649  Training Accuracy:  0.0325103\n",
      "Epoch:  782  Training Loss:  58.2638  Training Accuracy:  0.0325691\n",
      "Epoch:  783  Training Loss:  58.2627  Training Accuracy:  0.0325691\n",
      "Epoch:  784  Training Loss:  58.2616  Training Accuracy:  0.0325691\n",
      "Epoch:  785  Training Loss:  58.2605  Training Accuracy:  0.0325691\n",
      "Epoch:  786  Training Loss:  58.2594  Training Accuracy:  0.0325691\n",
      "Epoch:  787  Training Loss:  58.2583  Training Accuracy:  0.0325691\n",
      "Epoch:  788  Training Loss:  58.2573  Training Accuracy:  0.0325691\n",
      "Epoch:  789  Training Loss:  58.2562  Training Accuracy:  0.0325691\n",
      "Epoch:  790  Training Loss:  58.2551  Training Accuracy:  0.0325691\n",
      "Epoch:  791  Training Loss:  58.254  Training Accuracy:  0.0325691\n",
      "Epoch:  792  Training Loss:  58.2529  Training Accuracy:  0.0325691\n",
      "Epoch:  793  Training Loss:  58.2518  Training Accuracy:  0.0325691\n",
      "Epoch:  794  Training Loss:  58.2507  Training Accuracy:  0.0325691\n",
      "Epoch:  795  Training Loss:  58.2496  Training Accuracy:  0.0325691\n",
      "Epoch:  796  Training Loss:  58.2485  Training Accuracy:  0.0325691\n",
      "Epoch:  797  Training Loss:  58.2474  Training Accuracy:  0.0325691\n",
      "Epoch:  798  Training Loss:  58.2463  Training Accuracy:  0.0325691\n",
      "Epoch:  799  Training Loss:  58.2453  Training Accuracy:  0.0325691\n",
      "Epoch:  800  Training Loss:  58.2442  Training Accuracy:  0.0325691\n",
      "Epoch:  801  Training Loss:  58.2431  Training Accuracy:  0.0325691\n",
      "Epoch:  802  Training Loss:  58.242  Training Accuracy:  0.0325691\n",
      "Epoch:  803  Training Loss:  58.2409  Training Accuracy:  0.0325691\n",
      "Epoch:  804  Training Loss:  58.2398  Training Accuracy:  0.0325691\n",
      "Epoch:  805  Training Loss:  58.2387  Training Accuracy:  0.0325691\n",
      "Epoch:  806  Training Loss:  58.2376  Training Accuracy:  0.0325691\n",
      "Epoch:  807  Training Loss:  58.2365  Training Accuracy:  0.0325691\n",
      "Epoch:  808  Training Loss:  58.2354  Training Accuracy:  0.0325691\n",
      "Epoch:  809  Training Loss:  58.2343  Training Accuracy:  0.0325691\n",
      "Epoch:  810  Training Loss:  58.2333  Training Accuracy:  0.0325691\n",
      "Epoch:  811  Training Loss:  58.2322  Training Accuracy:  0.0325103\n",
      "Epoch:  812  Training Loss:  58.2311  Training Accuracy:  0.0325103\n",
      "Epoch:  813  Training Loss:  58.23  Training Accuracy:  0.0325691\n",
      "Epoch:  814  Training Loss:  58.2289  Training Accuracy:  0.0325691\n",
      "Epoch:  815  Training Loss:  58.2278  Training Accuracy:  0.0325691\n",
      "Epoch:  816  Training Loss:  58.2267  Training Accuracy:  0.0325691\n",
      "Epoch:  817  Training Loss:  58.2256  Training Accuracy:  0.0325691\n",
      "Epoch:  818  Training Loss:  58.2245  Training Accuracy:  0.0325691\n",
      "Epoch:  819  Training Loss:  58.2234  Training Accuracy:  0.0325691\n",
      "Epoch:  820  Training Loss:  58.2224  Training Accuracy:  0.0325691\n",
      "Epoch:  821  Training Loss:  58.2213  Training Accuracy:  0.0325691\n",
      "Epoch:  822  Training Loss:  58.2202  Training Accuracy:  0.0325691\n",
      "Epoch:  823  Training Loss:  58.2191  Training Accuracy:  0.0325691\n",
      "Epoch:  824  Training Loss:  58.218  Training Accuracy:  0.0325691\n",
      "Epoch:  825  Training Loss:  58.2169  Training Accuracy:  0.0325691\n",
      "Epoch:  826  Training Loss:  58.2158  Training Accuracy:  0.0325691\n",
      "Epoch:  827  Training Loss:  58.2147  Training Accuracy:  0.0325691\n",
      "Epoch:  828  Training Loss:  58.2136  Training Accuracy:  0.0325691\n",
      "Epoch:  829  Training Loss:  58.2126  Training Accuracy:  0.0325691\n",
      "Epoch:  830  Training Loss:  58.2115  Training Accuracy:  0.0325691\n",
      "Epoch:  831  Training Loss:  58.2104  Training Accuracy:  0.0325691\n",
      "Epoch:  832  Training Loss:  58.2093  Training Accuracy:  0.0325691\n",
      "Epoch:  833  Training Loss:  58.2082  Training Accuracy:  0.0325691\n",
      "Epoch:  834  Training Loss:  58.2071  Training Accuracy:  0.0325691\n",
      "Epoch:  835  Training Loss:  58.206  Training Accuracy:  0.0325691\n",
      "Epoch:  836  Training Loss:  58.2049  Training Accuracy:  0.0325691\n",
      "Epoch:  837  Training Loss:  58.2038  Training Accuracy:  0.0325691\n",
      "Epoch:  838  Training Loss:  58.2028  Training Accuracy:  0.0325691\n",
      "Epoch:  839  Training Loss:  58.2017  Training Accuracy:  0.0325691\n",
      "Epoch:  840  Training Loss:  58.2006  Training Accuracy:  0.0325691\n",
      "Epoch:  841  Training Loss:  58.1995  Training Accuracy:  0.0325691\n",
      "Epoch:  842  Training Loss:  58.1984  Training Accuracy:  0.0325691\n",
      "Epoch:  843  Training Loss:  58.1973  Training Accuracy:  0.0325691\n",
      "Epoch:  844  Training Loss:  58.1962  Training Accuracy:  0.0325691\n",
      "Epoch:  845  Training Loss:  58.1951  Training Accuracy:  0.0325691\n",
      "Epoch:  846  Training Loss:  58.1941  Training Accuracy:  0.0325691\n",
      "Epoch:  847  Training Loss:  58.193  Training Accuracy:  0.0325691\n",
      "Epoch:  848  Training Loss:  58.1919  Training Accuracy:  0.0325691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  849  Training Loss:  58.1908  Training Accuracy:  0.0325691\n",
      "Epoch:  850  Training Loss:  58.1897  Training Accuracy:  0.0325691\n",
      "Epoch:  851  Training Loss:  58.1886  Training Accuracy:  0.0325691\n",
      "Epoch:  852  Training Loss:  58.1875  Training Accuracy:  0.0325691\n",
      "Epoch:  853  Training Loss:  58.1864  Training Accuracy:  0.0325691\n",
      "Epoch:  854  Training Loss:  58.1854  Training Accuracy:  0.0325691\n",
      "Epoch:  855  Training Loss:  58.1843  Training Accuracy:  0.0325691\n",
      "Epoch:  856  Training Loss:  58.1832  Training Accuracy:  0.0325691\n",
      "Epoch:  857  Training Loss:  58.1821  Training Accuracy:  0.0325691\n",
      "Epoch:  858  Training Loss:  58.181  Training Accuracy:  0.0325691\n",
      "Epoch:  859  Training Loss:  58.1799  Training Accuracy:  0.0325103\n",
      "Epoch:  860  Training Loss:  58.1788  Training Accuracy:  0.0325103\n",
      "Epoch:  861  Training Loss:  58.1778  Training Accuracy:  0.0325103\n",
      "Epoch:  862  Training Loss:  58.1767  Training Accuracy:  0.0325103\n",
      "Epoch:  863  Training Loss:  58.1756  Training Accuracy:  0.0325103\n",
      "Epoch:  864  Training Loss:  58.1745  Training Accuracy:  0.0325103\n",
      "Epoch:  865  Training Loss:  58.1734  Training Accuracy:  0.0325103\n",
      "Epoch:  866  Training Loss:  58.1723  Training Accuracy:  0.0325103\n",
      "Epoch:  867  Training Loss:  58.1712  Training Accuracy:  0.0325103\n",
      "Epoch:  868  Training Loss:  58.1702  Training Accuracy:  0.0325103\n",
      "Epoch:  869  Training Loss:  58.1691  Training Accuracy:  0.0325103\n",
      "Epoch:  870  Training Loss:  58.168  Training Accuracy:  0.0325103\n",
      "Epoch:  871  Training Loss:  58.1669  Training Accuracy:  0.0325103\n",
      "Epoch:  872  Training Loss:  58.1658  Training Accuracy:  0.0325103\n",
      "Epoch:  873  Training Loss:  58.1647  Training Accuracy:  0.0325103\n",
      "Epoch:  874  Training Loss:  58.1636  Training Accuracy:  0.0325103\n",
      "Epoch:  875  Training Loss:  58.1625  Training Accuracy:  0.0325691\n",
      "Epoch:  876  Training Loss:  58.1615  Training Accuracy:  0.0325691\n",
      "Epoch:  877  Training Loss:  58.1604  Training Accuracy:  0.0325691\n",
      "Epoch:  878  Training Loss:  58.1593  Training Accuracy:  0.0325691\n",
      "Epoch:  879  Training Loss:  58.1582  Training Accuracy:  0.0325691\n",
      "Epoch:  880  Training Loss:  58.1571  Training Accuracy:  0.0325691\n",
      "Epoch:  881  Training Loss:  58.156  Training Accuracy:  0.0325691\n",
      "Epoch:  882  Training Loss:  58.1549  Training Accuracy:  0.0325691\n",
      "Epoch:  883  Training Loss:  58.1539  Training Accuracy:  0.0325691\n",
      "Epoch:  884  Training Loss:  58.1528  Training Accuracy:  0.0325691\n",
      "Epoch:  885  Training Loss:  58.1517  Training Accuracy:  0.0325691\n",
      "Epoch:  886  Training Loss:  58.1506  Training Accuracy:  0.0325691\n",
      "Epoch:  887  Training Loss:  58.1495  Training Accuracy:  0.0325691\n",
      "Epoch:  888  Training Loss:  58.1484  Training Accuracy:  0.0325691\n",
      "Epoch:  889  Training Loss:  58.1474  Training Accuracy:  0.0325691\n",
      "Epoch:  890  Training Loss:  58.1463  Training Accuracy:  0.0325691\n",
      "Epoch:  891  Training Loss:  58.1452  Training Accuracy:  0.0325691\n",
      "Epoch:  892  Training Loss:  58.1441  Training Accuracy:  0.0326279\n",
      "Epoch:  893  Training Loss:  58.143  Training Accuracy:  0.0326279\n",
      "Epoch:  894  Training Loss:  58.1419  Training Accuracy:  0.0326279\n",
      "Epoch:  895  Training Loss:  58.1409  Training Accuracy:  0.0326279\n",
      "Epoch:  896  Training Loss:  58.1398  Training Accuracy:  0.0326279\n",
      "Epoch:  897  Training Loss:  58.1387  Training Accuracy:  0.0326279\n",
      "Epoch:  898  Training Loss:  58.1376  Training Accuracy:  0.0326279\n",
      "Epoch:  899  Training Loss:  58.1365  Training Accuracy:  0.0326279\n",
      "Epoch:  900  Training Loss:  58.1354  Training Accuracy:  0.0326279\n",
      "Epoch:  901  Training Loss:  58.1344  Training Accuracy:  0.0326279\n",
      "Epoch:  902  Training Loss:  58.1333  Training Accuracy:  0.0326279\n",
      "Epoch:  903  Training Loss:  58.1322  Training Accuracy:  0.0326279\n",
      "Epoch:  904  Training Loss:  58.1311  Training Accuracy:  0.0326279\n",
      "Epoch:  905  Training Loss:  58.13  Training Accuracy:  0.0326279\n",
      "Epoch:  906  Training Loss:  58.129  Training Accuracy:  0.0326279\n",
      "Epoch:  907  Training Loss:  58.1279  Training Accuracy:  0.0326279\n",
      "Epoch:  908  Training Loss:  58.1268  Training Accuracy:  0.0326279\n",
      "Epoch:  909  Training Loss:  58.1257  Training Accuracy:  0.0326279\n",
      "Epoch:  910  Training Loss:  58.1246  Training Accuracy:  0.0326279\n",
      "Epoch:  911  Training Loss:  58.1235  Training Accuracy:  0.0325691\n",
      "Epoch:  912  Training Loss:  58.1225  Training Accuracy:  0.0325691\n",
      "Epoch:  913  Training Loss:  58.1214  Training Accuracy:  0.0325103\n",
      "Epoch:  914  Training Loss:  58.1203  Training Accuracy:  0.0325103\n",
      "Epoch:  915  Training Loss:  58.1192  Training Accuracy:  0.0325103\n",
      "Epoch:  916  Training Loss:  58.1181  Training Accuracy:  0.0325103\n",
      "Epoch:  917  Training Loss:  58.117  Training Accuracy:  0.0325103\n",
      "Epoch:  918  Training Loss:  58.116  Training Accuracy:  0.0325103\n",
      "Epoch:  919  Training Loss:  58.1149  Training Accuracy:  0.0325103\n",
      "Epoch:  920  Training Loss:  58.1138  Training Accuracy:  0.0325103\n",
      "Epoch:  921  Training Loss:  58.1127  Training Accuracy:  0.0325103\n",
      "Epoch:  922  Training Loss:  58.1116  Training Accuracy:  0.0325103\n",
      "Epoch:  923  Training Loss:  58.1106  Training Accuracy:  0.0325103\n",
      "Epoch:  924  Training Loss:  58.1095  Training Accuracy:  0.0325103\n",
      "Epoch:  925  Training Loss:  58.1084  Training Accuracy:  0.0325103\n",
      "Epoch:  926  Training Loss:  58.1073  Training Accuracy:  0.0325691\n",
      "Epoch:  927  Training Loss:  58.1062  Training Accuracy:  0.0325691\n",
      "Epoch:  928  Training Loss:  58.1052  Training Accuracy:  0.0325691\n",
      "Epoch:  929  Training Loss:  58.1041  Training Accuracy:  0.0325691\n",
      "Epoch:  930  Training Loss:  58.103  Training Accuracy:  0.0325691\n",
      "Epoch:  931  Training Loss:  58.1019  Training Accuracy:  0.0325691\n",
      "Epoch:  932  Training Loss:  58.1008  Training Accuracy:  0.0325691\n",
      "Epoch:  933  Training Loss:  58.0998  Training Accuracy:  0.0325691\n",
      "Epoch:  934  Training Loss:  58.0987  Training Accuracy:  0.0325691\n",
      "Epoch:  935  Training Loss:  58.0976  Training Accuracy:  0.0325691\n",
      "Epoch:  936  Training Loss:  58.0965  Training Accuracy:  0.0325691\n",
      "Epoch:  937  Training Loss:  58.0954  Training Accuracy:  0.0325691\n",
      "Epoch:  938  Training Loss:  58.0943  Training Accuracy:  0.0325691\n",
      "Epoch:  939  Training Loss:  58.0933  Training Accuracy:  0.0325691\n",
      "Epoch:  940  Training Loss:  58.0922  Training Accuracy:  0.0325691\n",
      "Epoch:  941  Training Loss:  58.0911  Training Accuracy:  0.0325691\n",
      "Epoch:  942  Training Loss:  58.09  Training Accuracy:  0.0325691\n",
      "Epoch:  943  Training Loss:  58.089  Training Accuracy:  0.0325103\n",
      "Epoch:  944  Training Loss:  58.0879  Training Accuracy:  0.0325103\n",
      "Epoch:  945  Training Loss:  58.0868  Training Accuracy:  0.0325103\n",
      "Epoch:  946  Training Loss:  58.0857  Training Accuracy:  0.0325103\n",
      "Epoch:  947  Training Loss:  58.0846  Training Accuracy:  0.0325103\n",
      "Epoch:  948  Training Loss:  58.0836  Training Accuracy:  0.0325103\n",
      "Epoch:  949  Training Loss:  58.0825  Training Accuracy:  0.0325103\n",
      "Epoch:  950  Training Loss:  58.0814  Training Accuracy:  0.0325103\n",
      "Epoch:  951  Training Loss:  58.0803  Training Accuracy:  0.0325103\n",
      "Epoch:  952  Training Loss:  58.0792  Training Accuracy:  0.0325103\n",
      "Epoch:  953  Training Loss:  58.0782  Training Accuracy:  0.0325103\n",
      "Epoch:  954  Training Loss:  58.0771  Training Accuracy:  0.0325103\n",
      "Epoch:  955  Training Loss:  58.076  Training Accuracy:  0.0325103\n",
      "Epoch:  956  Training Loss:  58.0749  Training Accuracy:  0.0325103\n",
      "Epoch:  957  Training Loss:  58.0738  Training Accuracy:  0.0325103\n",
      "Epoch:  958  Training Loss:  58.0728  Training Accuracy:  0.0325103\n",
      "Epoch:  959  Training Loss:  58.0717  Training Accuracy:  0.0325103\n",
      "Epoch:  960  Training Loss:  58.0706  Training Accuracy:  0.0325103\n",
      "Epoch:  961  Training Loss:  58.0695  Training Accuracy:  0.0325103\n",
      "Epoch:  962  Training Loss:  58.0685  Training Accuracy:  0.0325103\n",
      "Epoch:  963  Training Loss:  58.0674  Training Accuracy:  0.0325103\n",
      "Epoch:  964  Training Loss:  58.0663  Training Accuracy:  0.0325103\n",
      "Epoch:  965  Training Loss:  58.0652  Training Accuracy:  0.0325103\n",
      "Epoch:  966  Training Loss:  58.0641  Training Accuracy:  0.0325103\n",
      "Epoch:  967  Training Loss:  58.0631  Training Accuracy:  0.0325103\n",
      "Epoch:  968  Training Loss:  58.062  Training Accuracy:  0.0325103\n",
      "Epoch:  969  Training Loss:  58.0609  Training Accuracy:  0.0325103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  970  Training Loss:  58.0598  Training Accuracy:  0.0325103\n",
      "Epoch:  971  Training Loss:  58.0588  Training Accuracy:  0.0325103\n",
      "Epoch:  972  Training Loss:  58.0577  Training Accuracy:  0.0325103\n",
      "Epoch:  973  Training Loss:  58.0566  Training Accuracy:  0.0325103\n",
      "Epoch:  974  Training Loss:  58.0555  Training Accuracy:  0.0325103\n",
      "Epoch:  975  Training Loss:  58.0544  Training Accuracy:  0.0325691\n",
      "Epoch:  976  Training Loss:  58.0534  Training Accuracy:  0.0325103\n",
      "Epoch:  977  Training Loss:  58.0523  Training Accuracy:  0.0325103\n",
      "Epoch:  978  Training Loss:  58.0512  Training Accuracy:  0.0325103\n",
      "Epoch:  979  Training Loss:  58.0501  Training Accuracy:  0.0325103\n",
      "Epoch:  980  Training Loss:  58.0491  Training Accuracy:  0.0325103\n",
      "Epoch:  981  Training Loss:  58.048  Training Accuracy:  0.0325103\n",
      "Epoch:  982  Training Loss:  58.0469  Training Accuracy:  0.0325103\n",
      "Epoch:  983  Training Loss:  58.0458  Training Accuracy:  0.0325103\n",
      "Epoch:  984  Training Loss:  58.0448  Training Accuracy:  0.0325103\n",
      "Epoch:  985  Training Loss:  58.0437  Training Accuracy:  0.0325103\n",
      "Epoch:  986  Training Loss:  58.0426  Training Accuracy:  0.0325103\n",
      "Epoch:  987  Training Loss:  58.0415  Training Accuracy:  0.0325103\n",
      "Epoch:  988  Training Loss:  58.0405  Training Accuracy:  0.0325103\n",
      "Epoch:  989  Training Loss:  58.0394  Training Accuracy:  0.0325103\n",
      "Epoch:  990  Training Loss:  58.0383  Training Accuracy:  0.0325103\n",
      "Epoch:  991  Training Loss:  58.0372  Training Accuracy:  0.0325103\n",
      "Epoch:  992  Training Loss:  58.0361  Training Accuracy:  0.0325103\n",
      "Epoch:  993  Training Loss:  58.0351  Training Accuracy:  0.0325103\n",
      "Epoch:  994  Training Loss:  58.034  Training Accuracy:  0.0325103\n",
      "Epoch:  995  Training Loss:  58.0329  Training Accuracy:  0.0325103\n",
      "Epoch:  996  Training Loss:  58.0318  Training Accuracy:  0.0325103\n",
      "Epoch:  997  Training Loss:  58.0308  Training Accuracy:  0.0325103\n",
      "Epoch:  998  Training Loss:  58.0297  Training Accuracy:  0.0325103\n",
      "Epoch:  999  Training Loss:  58.0286  Training Accuracy:  0.0325103\n",
      "Testing Accuracy: 0.0346273\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 6\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-10\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
