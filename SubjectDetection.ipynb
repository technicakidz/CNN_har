{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "#create\n",
    "def plot_subject(subject,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(subject)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"user-id\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = read_data('actitracker_raw2.txt')\n",
    "dataset = dataset.replace(\";\",\"\",regex=True)#.replace(';',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-223c9458af1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-57cc2f865b90>\u001b[0m in \u001b[0;36mfeature_normalize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6340\u001b[0m                                       skipna=skipna)\n\u001b[1;32m   6341\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m-> 6342\u001b[0;31m                             numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   6343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   2379\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x-axis</th>\n",
       "      <th>y-axis</th>\n",
       "      <th>z-axis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.198203</td>\n",
       "      <td>0.804142</td>\n",
       "      <td>0.50395286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>0.635039</td>\n",
       "      <td>0.594170</td>\n",
       "      <td>0.95342433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.537639</td>\n",
       "      <td>-0.08172209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.186271</td>\n",
       "      <td>1.666240</td>\n",
       "      <td>3.0237172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-0.269795</td>\n",
       "      <td>0.719346</td>\n",
       "      <td>7.205164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106442306000</td>\n",
       "      <td>0.104071</td>\n",
       "      <td>-1.444986</td>\n",
       "      <td>-6.510526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106542312000</td>\n",
       "      <td>-0.186271</td>\n",
       "      <td>0.491202</td>\n",
       "      <td>5.706926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106652389000</td>\n",
       "      <td>-0.170362</td>\n",
       "      <td>0.991906</td>\n",
       "      <td>7.0553403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106762313000</td>\n",
       "      <td>-1.327754</td>\n",
       "      <td>0.616378</td>\n",
       "      <td>5.134871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106872299000</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>-0.871599</td>\n",
       "      <td>1.6480621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106982315000</td>\n",
       "      <td>-1.293948</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>2.7240696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107092330000</td>\n",
       "      <td>0.110037</td>\n",
       "      <td>-0.217454</td>\n",
       "      <td>2.982856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107202316000</td>\n",
       "      <td>-0.371216</td>\n",
       "      <td>-1.517668</td>\n",
       "      <td>-0.29964766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107312332000</td>\n",
       "      <td>-0.991673</td>\n",
       "      <td>-0.059975</td>\n",
       "      <td>-8.158588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107422348000</td>\n",
       "      <td>0.754358</td>\n",
       "      <td>1.593558</td>\n",
       "      <td>8.539958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107522293000</td>\n",
       "      <td>0.819984</td>\n",
       "      <td>-0.633361</td>\n",
       "      <td>2.9147544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107632339000</td>\n",
       "      <td>-0.325477</td>\n",
       "      <td>0.156054</td>\n",
       "      <td>-1.4573772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107742355000</td>\n",
       "      <td>0.418277</td>\n",
       "      <td>0.939413</td>\n",
       "      <td>9.425281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107852340000</td>\n",
       "      <td>-0.393091</td>\n",
       "      <td>-1.921461</td>\n",
       "      <td>-10.18802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107962326000</td>\n",
       "      <td>0.306913</td>\n",
       "      <td>0.456880</td>\n",
       "      <td>-9.724928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108062271000</td>\n",
       "      <td>0.424243</td>\n",
       "      <td>0.951527</td>\n",
       "      <td>1.5390993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108172348000</td>\n",
       "      <td>-0.170362</td>\n",
       "      <td>-0.502129</td>\n",
       "      <td>3.718355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108272262000</td>\n",
       "      <td>-0.432864</td>\n",
       "      <td>-0.825163</td>\n",
       "      <td>0.08172209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108382370000</td>\n",
       "      <td>-0.617808</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>6.510526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108492294000</td>\n",
       "      <td>-0.214113</td>\n",
       "      <td>-1.564104</td>\n",
       "      <td>-4.630918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108602371000</td>\n",
       "      <td>-0.023202</td>\n",
       "      <td>0.531582</td>\n",
       "      <td>13.525005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108702285000</td>\n",
       "      <td>0.736460</td>\n",
       "      <td>1.236201</td>\n",
       "      <td>6.1700177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108812332000</td>\n",
       "      <td>-1.361561</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>4.0180025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108922378000</td>\n",
       "      <td>-0.291670</td>\n",
       "      <td>-0.893808</td>\n",
       "      <td>2.3699405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49109022293000</td>\n",
       "      <td>-0.766956</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>4.7126403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098174</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622091524000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.263769</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098175</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622131471000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098176</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622171541000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098177</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622211580000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.286004</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098178</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622291475000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098179</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622331483000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098180</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622371522000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.244499</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098181</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622451479000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098182</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622491487000</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098183</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622531465000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098184</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622571443000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.244499</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098185</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622611635000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098186</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622691469000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098187</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622731477000</td>\n",
       "      <td>1.183686</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098188</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622771486000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098189</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622851472000</td>\n",
       "      <td>1.144265</td>\n",
       "      <td>-1.297862</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098190</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622891511000</td>\n",
       "      <td>1.110683</td>\n",
       "      <td>-1.320097</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098191</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622931490000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098192</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622971498000</td>\n",
       "      <td>1.250849</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098193</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623051485000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098194</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623091524000</td>\n",
       "      <td>1.150105</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098195</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623131471000</td>\n",
       "      <td>1.167626</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098196</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623172578000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.263769</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098197</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623251466000</td>\n",
       "      <td>1.233328</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098198</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623291475000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.297862</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098199</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623331483000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.308238</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098200</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623371431000</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098201</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623411592000</td>\n",
       "      <td>1.228948</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098202</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623491487000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098203</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623531465000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1098204 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user-id activity        timestamp    x-axis    y-axis       z-axis\n",
       "0             33  Jogging   49105962326000 -0.198203  0.804142   0.50395286\n",
       "1             33  Jogging   49106062271000  0.635039  0.594170   0.95342433\n",
       "2             33  Jogging   49106112167000  0.619130  0.537639  -0.08172209\n",
       "3             33  Jogging   49106222305000 -0.186271  1.666240    3.0237172\n",
       "4             33  Jogging   49106332290000 -0.269795  0.719346     7.205164\n",
       "5             33  Jogging   49106442306000  0.104071 -1.444986    -6.510526\n",
       "6             33  Jogging   49106542312000 -0.186271  0.491202     5.706926\n",
       "7             33  Jogging   49106652389000 -0.170362  0.991906    7.0553403\n",
       "8             33  Jogging   49106762313000 -1.327754  0.616378     5.134871\n",
       "9             33  Jogging   49106872299000  0.042423 -0.871599    1.6480621\n",
       "10            33  Jogging   49106982315000 -1.293948  1.825739    2.7240696\n",
       "11            33  Jogging   49107092330000  0.110037 -0.217454     2.982856\n",
       "12            33  Jogging   49107202316000 -0.371216 -1.517668  -0.29964766\n",
       "13            33  Jogging   49107312332000 -0.991673 -0.059975    -8.158588\n",
       "14            33  Jogging   49107422348000  0.754358  1.593558     8.539958\n",
       "15            33  Jogging   49107522293000  0.819984 -0.633361    2.9147544\n",
       "16            33  Jogging   49107632339000 -0.325477  0.156054   -1.4573772\n",
       "17            33  Jogging   49107742355000  0.418277  0.939413     9.425281\n",
       "18            33  Jogging   49107852340000 -0.393091 -1.921461    -10.18802\n",
       "19            33  Jogging   49107962326000  0.306913  0.456880    -9.724928\n",
       "20            33  Jogging   49108062271000  0.424243  0.951527    1.5390993\n",
       "21            33  Jogging   49108172348000 -0.170362 -0.502129     3.718355\n",
       "22            33  Jogging   49108272262000 -0.432864 -0.825163   0.08172209\n",
       "23            33  Jogging   49108382370000 -0.617808  1.825739     6.510526\n",
       "24            33  Jogging   49108492294000 -0.214113 -1.564104    -4.630918\n",
       "25            33  Jogging   49108602371000 -0.023202  0.531582    13.525005\n",
       "26            33  Jogging   49108702285000  0.736460  1.236201    6.1700177\n",
       "27            33  Jogging   49108812332000 -1.361561  0.002613    4.0180025\n",
       "28            33  Jogging   49108922378000 -0.291670 -0.893808    2.3699405\n",
       "29            33  Jogging   49109022293000 -0.766956  1.825739    4.7126403\n",
       "...          ...      ...              ...       ...       ...          ...\n",
       "1098174       19  Sitting  131622091524000  1.205587 -1.263769         2.22\n",
       "1098175       19  Sitting  131622131471000  1.211427 -1.269698         2.26\n",
       "1098176       19  Sitting  131622171541000  1.205587 -1.272663         2.18\n",
       "1098177       19  Sitting  131622211580000  1.195367 -1.286004         2.26\n",
       "1098178       19  Sitting  131622291475000  1.199747 -1.280074         2.41\n",
       "1098179       19  Sitting  131622331483000  1.205587 -1.269698          2.3\n",
       "1098180       19  Sitting  131622371522000  1.205587 -1.244499         2.26\n",
       "1098181       19  Sitting  131622451479000  1.211427 -1.241534         2.34\n",
       "1098182       19  Sitting  131622491487000  1.223108 -1.241534         2.41\n",
       "1098183       19  Sitting  131622531465000  1.199747 -1.241534         2.37\n",
       "1098184       19  Sitting  131622571443000  1.199747 -1.244499         2.37\n",
       "1098185       19  Sitting  131622611635000  1.205587 -1.257840         2.45\n",
       "1098186       19  Sitting  131622691469000  1.205587 -1.257840         2.45\n",
       "1098187       19  Sitting  131622731477000  1.183686 -1.272663         2.53\n",
       "1098188       19  Sitting  131622771486000  1.211427 -1.280074          2.6\n",
       "1098189       19  Sitting  131622851472000  1.144265 -1.297862         2.56\n",
       "1098190       19  Sitting  131622891511000  1.110683 -1.320097         2.11\n",
       "1098191       19  Sitting  131622931490000  1.211427 -1.291933          2.3\n",
       "1098192       19  Sitting  131622971498000  1.250849 -1.291933         2.26\n",
       "1098193       19  Sitting  131623051485000  1.195367 -1.257840         2.26\n",
       "1098194       19  Sitting  131623091524000  1.150105 -1.269698         2.49\n",
       "1098195       19  Sitting  131623131471000  1.167626 -1.269698         2.37\n",
       "1098196       19  Sitting  131623172578000  1.195367 -1.263769         2.18\n",
       "1098197       19  Sitting  131623251466000  1.233328 -1.280074         1.95\n",
       "1098198       19  Sitting  131623291475000  1.217268 -1.297862          1.8\n",
       "1098199       19  Sitting  131623331483000  1.217268 -1.308238         1.69\n",
       "1098200       19  Sitting  131623371431000  1.223108 -1.291933         1.73\n",
       "1098201       19  Sitting  131623411592000  1.228948 -1.280074         1.69\n",
       "1098202       19  Sitting  131623491487000  1.217268 -1.291933         1.73\n",
       "1098203       19  Sitting  131623531465000  1.199747 -1.272663         1.61\n",
       "\n",
       "[1098204 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-169114f33f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user-id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user-id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplot_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fb8ab1dffe65>\u001b[0m in \u001b[0;36mplot_subject\u001b[0;34m(subject, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb8ab1dffe65>\u001b[0m in \u001b[0;36mplot_axis\u001b[0;34m(ax, x, y, title)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6360\u001b[0m                                       skipna=skipna, ddof=ddof)\n\u001b[1;32m   6361\u001b[0m         return self._reduce(f, name, axis=axis, numeric_only=numeric_only,\n\u001b[0;32m-> 6362\u001b[0;31m                             skipna=skipna, ddof=ddof)\n\u001b[0m\u001b[1;32m   6363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   2379\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mbottleneck_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAJCCAYAAABj8z68AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d16e350b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#modify taerget(user-id)\n",
    "for subject in np.unique(dataset[\"user-id\"]):\n",
    "    subset = dataset[dataset[\"user-id\"] == subject][:40]\n",
    "    plot_subject(subject,subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-967f72759e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#saved data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_x.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_y.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_x.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_y.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#saved data\n",
    "np.save('train_x.npy',train_x)\n",
    "np.save('train_y.npy',train_y)\n",
    "np.save('test_x.npy',test_x)\n",
    "np.save('test_y.npy',test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.load('train_x.npy')\n",
    "train_y = np.load('train_y.npy')\n",
    "test_x = np.load('test_x.npy')\n",
    "test_y = np.load('test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'momentum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-06499ceaff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMomentumOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'momentum'"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "        with tf.name_scope('summary'):\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter('./logs', session.graph)\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 200\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17152, 1, 90, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  33.1968  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  35.6132  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  37.0849  Training Accuracy:  0.0433862\n",
      "Epoch:  3  Training Loss:  38.1057  Training Accuracy:  0.0447972\n",
      "Epoch:  4  Training Loss:  38.8707  Training Accuracy:  0.046796\n",
      "Epoch:  5  Training Loss:  39.5741  Training Accuracy:  0.0489124\n",
      "Epoch:  6  Training Loss:  40.1169  Training Accuracy:  0.052087\n",
      "Epoch:  7  Training Loss:  40.5513  Training Accuracy:  0.0562022\n",
      "Epoch:  8  Training Loss:  40.8959  Training Accuracy:  0.0589065\n",
      "Epoch:  9  Training Loss:  41.2386  Training Accuracy:  0.0607878\n",
      "Epoch:  10  Training Loss:  41.5907  Training Accuracy:  0.0634921\n",
      "Epoch:  11  Training Loss:  41.8969  Training Accuracy:  0.0676661\n",
      "Epoch:  12  Training Loss:  42.2046  Training Accuracy:  0.0738977\n",
      "Epoch:  13  Training Loss:  42.441  Training Accuracy:  0.0813051\n",
      "Epoch:  14  Training Loss:  42.6838  Training Accuracy:  0.0897119\n",
      "Epoch:  15  Training Loss:  42.8147  Training Accuracy:  0.0996473\n",
      "Epoch:  16  Training Loss:  42.9501  Training Accuracy:  0.107995\n",
      "Epoch:  17  Training Loss:  43.0746  Training Accuracy:  0.118342\n",
      "Epoch:  18  Training Loss:  43.2553  Training Accuracy:  0.127513\n",
      "Epoch:  19  Training Loss:  43.4385  Training Accuracy:  0.136155\n",
      "Epoch:  20  Training Loss:  43.6039  Training Accuracy:  0.146267\n",
      "Epoch:  21  Training Loss:  43.803  Training Accuracy:  0.154321\n",
      "Epoch:  22  Training Loss:  43.9347  Training Accuracy:  0.163962\n",
      "Epoch:  23  Training Loss:  44.0845  Training Accuracy:  0.172369\n",
      "Epoch:  24  Training Loss:  44.2035  Training Accuracy:  0.179953\n",
      "Epoch:  25  Training Loss:  44.3151  Training Accuracy:  0.187419\n",
      "Epoch:  26  Training Loss:  44.42  Training Accuracy:  0.195591\n",
      "Epoch:  27  Training Loss:  44.5295  Training Accuracy:  0.201587\n",
      "Epoch:  28  Training Loss:  44.6013  Training Accuracy:  0.20729\n",
      "Epoch:  29  Training Loss:  44.6635  Training Accuracy:  0.213227\n",
      "Epoch:  30  Training Loss:  44.7542  Training Accuracy:  0.219283\n",
      "Epoch:  31  Training Loss:  44.8565  Training Accuracy:  0.225162\n",
      "Epoch:  32  Training Loss:  44.9376  Training Accuracy:  0.2301\n",
      "Epoch:  33  Training Loss:  45.054  Training Accuracy:  0.235097\n",
      "Epoch:  34  Training Loss:  45.1899  Training Accuracy:  0.242034\n",
      "Epoch:  35  Training Loss:  45.3358  Training Accuracy:  0.247207\n",
      "Epoch:  36  Training Loss:  45.4726  Training Accuracy:  0.252616\n",
      "Epoch:  37  Training Loss:  45.6192  Training Accuracy:  0.256908\n",
      "Epoch:  38  Training Loss:  45.7782  Training Accuracy:  0.26114\n",
      "Epoch:  39  Training Loss:  45.9688  Training Accuracy:  0.265902\n",
      "Epoch:  40  Training Loss:  46.1383  Training Accuracy:  0.270194\n",
      "Epoch:  41  Training Loss:  46.3339  Training Accuracy:  0.274544\n",
      "Epoch:  42  Training Loss:  46.512  Training Accuracy:  0.280012\n",
      "Epoch:  43  Training Loss:  46.7262  Training Accuracy:  0.284715\n",
      "Epoch:  44  Training Loss:  46.9398  Training Accuracy:  0.288948\n",
      "Epoch:  45  Training Loss:  47.2041  Training Accuracy:  0.293651\n",
      "Epoch:  46  Training Loss:  47.4515  Training Accuracy:  0.297531\n",
      "Epoch:  47  Training Loss:  47.6919  Training Accuracy:  0.30147\n",
      "Epoch:  48  Training Loss:  47.9416  Training Accuracy:  0.305409\n",
      "Epoch:  49  Training Loss:  48.153  Training Accuracy:  0.308759\n",
      "Epoch:  50  Training Loss:  48.3501  Training Accuracy:  0.312052\n",
      "Epoch:  51  Training Loss:  48.6168  Training Accuracy:  0.316167\n",
      "Epoch:  52  Training Loss:  48.8484  Training Accuracy:  0.320223\n",
      "Epoch:  53  Training Loss:  49.0951  Training Accuracy:  0.323692\n",
      "Epoch:  54  Training Loss:  49.3391  Training Accuracy:  0.32816\n",
      "Epoch:  55  Training Loss:  49.5889  Training Accuracy:  0.332687\n",
      "Epoch:  56  Training Loss:  49.8479  Training Accuracy:  0.335861\n",
      "Epoch:  57  Training Loss:  50.104  Training Accuracy:  0.340094\n",
      "Epoch:  58  Training Loss:  50.3726  Training Accuracy:  0.34415\n",
      "Epoch:  59  Training Loss:  50.6562  Training Accuracy:  0.347325\n",
      "Epoch:  60  Training Loss:  50.9124  Training Accuracy:  0.350911\n",
      "Epoch:  61  Training Loss:  51.1791  Training Accuracy:  0.353968\n",
      "Epoch:  62  Training Loss:  51.4594  Training Accuracy:  0.35726\n",
      "Epoch:  63  Training Loss:  51.7276  Training Accuracy:  0.361493\n",
      "Epoch:  64  Training Loss:  51.9983  Training Accuracy:  0.36502\n",
      "Epoch:  65  Training Loss:  52.2483  Training Accuracy:  0.368665\n",
      "Epoch:  66  Training Loss:  52.546  Training Accuracy:  0.371605\n",
      "Epoch:  67  Training Loss:  52.8087  Training Accuracy:  0.374779\n",
      "Epoch:  68  Training Loss:  53.0613  Training Accuracy:  0.378483\n",
      "Epoch:  69  Training Loss:  53.2966  Training Accuracy:  0.381129\n",
      "Epoch:  70  Training Loss:  53.5453  Training Accuracy:  0.383892\n",
      "Epoch:  71  Training Loss:  53.7869  Training Accuracy:  0.38642\n",
      "Epoch:  72  Training Loss:  54.0343  Training Accuracy:  0.389183\n",
      "Epoch:  73  Training Loss:  54.2505  Training Accuracy:  0.392534\n",
      "Epoch:  74  Training Loss:  54.4895  Training Accuracy:  0.39512\n",
      "Epoch:  75  Training Loss:  54.7041  Training Accuracy:  0.398119\n",
      "Epoch:  76  Training Loss:  54.9118  Training Accuracy:  0.401117\n",
      "Epoch:  77  Training Loss:  55.0801  Training Accuracy:  0.403233\n",
      "Epoch:  78  Training Loss:  55.2179  Training Accuracy:  0.407055\n",
      "Epoch:  79  Training Loss:  55.3706  Training Accuracy:  0.409112\n",
      "Epoch:  80  Training Loss:  55.4592  Training Accuracy:  0.411816\n",
      "Epoch:  81  Training Loss:  55.5537  Training Accuracy:  0.413698\n",
      "Epoch:  82  Training Loss:  55.6407  Training Accuracy:  0.416343\n",
      "Epoch:  83  Training Loss:  55.7384  Training Accuracy:  0.419048\n",
      "Epoch:  84  Training Loss:  55.8323  Training Accuracy:  0.421575\n",
      "Epoch:  85  Training Loss:  55.9304  Training Accuracy:  0.424339\n",
      "Epoch:  86  Training Loss:  55.9929  Training Accuracy:  0.426455\n",
      "Epoch:  87  Training Loss:  56.0808  Training Accuracy:  0.428865\n",
      "Epoch:  88  Training Loss:  56.1349  Training Accuracy:  0.432099\n",
      "Epoch:  89  Training Loss:  56.1943  Training Accuracy:  0.434803\n",
      "Epoch:  90  Training Loss:  56.2285  Training Accuracy:  0.437801\n",
      "Epoch:  91  Training Loss:  56.2641  Training Accuracy:  0.441211\n",
      "Epoch:  92  Training Loss:  56.3248  Training Accuracy:  0.444973\n",
      "Epoch:  93  Training Loss:  56.3503  Training Accuracy:  0.447443\n",
      "Epoch:  94  Training Loss:  56.3539  Training Accuracy:  0.449794\n",
      "Epoch:  95  Training Loss:  56.3467  Training Accuracy:  0.452851\n",
      "Epoch:  96  Training Loss:  56.3822  Training Accuracy:  0.455497\n",
      "Epoch:  97  Training Loss:  56.3682  Training Accuracy:  0.457907\n",
      "Epoch:  98  Training Loss:  56.3809  Training Accuracy:  0.46067\n",
      "Epoch:  99  Training Loss:  56.3776  Training Accuracy:  0.463551\n",
      "Epoch:  100  Training Loss:  56.3766  Training Accuracy:  0.465667\n",
      "Epoch:  101  Training Loss:  56.3517  Training Accuracy:  0.469136\n",
      "Epoch:  102  Training Loss:  56.3406  Training Accuracy:  0.472134\n",
      "Epoch:  103  Training Loss:  56.2942  Training Accuracy:  0.475426\n",
      "Epoch:  104  Training Loss:  56.2762  Training Accuracy:  0.477954\n",
      "Epoch:  105  Training Loss:  56.2154  Training Accuracy:  0.480012\n",
      "Epoch:  106  Training Loss:  56.175  Training Accuracy:  0.482363\n",
      "Epoch:  107  Training Loss:  56.1581  Training Accuracy:  0.48448\n",
      "Epoch:  108  Training Loss:  56.082  Training Accuracy:  0.486008\n",
      "Epoch:  109  Training Loss:  56.0479  Training Accuracy:  0.488242\n",
      "Epoch:  110  Training Loss:  55.9894  Training Accuracy:  0.490241\n",
      "Epoch:  111  Training Loss:  55.9636  Training Accuracy:  0.49271\n",
      "Epoch:  112  Training Loss:  55.8782  Training Accuracy:  0.494532\n",
      "Epoch:  113  Training Loss:  55.8244  Training Accuracy:  0.497472\n",
      "Epoch:  114  Training Loss:  55.7648  Training Accuracy:  0.49953\n",
      "Epoch:  115  Training Loss:  55.6937  Training Accuracy:  0.501058\n",
      "Epoch:  116  Training Loss:  55.6003  Training Accuracy:  0.503586\n",
      "Epoch:  117  Training Loss:  55.4826  Training Accuracy:  0.505114\n",
      "Epoch:  118  Training Loss:  55.397  Training Accuracy:  0.506349\n",
      "Epoch:  119  Training Loss:  55.2966  Training Accuracy:  0.507819\n",
      "Epoch:  120  Training Loss:  55.1823  Training Accuracy:  0.509818\n",
      "Epoch:  121  Training Loss:  55.0636  Training Accuracy:  0.511699\n",
      "Epoch:  122  Training Loss:  54.9752  Training Accuracy:  0.513463\n",
      "Epoch:  123  Training Loss:  54.8612  Training Accuracy:  0.515461\n",
      "Epoch:  124  Training Loss:  54.7625  Training Accuracy:  0.517401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  54.6478  Training Accuracy:  0.5194\n",
      "Epoch:  126  Training Loss:  54.5284  Training Accuracy:  0.52087\n",
      "Epoch:  127  Training Loss:  54.4326  Training Accuracy:  0.522222\n",
      "Epoch:  128  Training Loss:  54.3429  Training Accuracy:  0.524221\n",
      "Epoch:  129  Training Loss:  54.2332  Training Accuracy:  0.525808\n",
      "Epoch:  130  Training Loss:  54.1012  Training Accuracy:  0.527337\n",
      "Epoch:  131  Training Loss:  53.9912  Training Accuracy:  0.529159\n",
      "Epoch:  132  Training Loss:  53.866  Training Accuracy:  0.531452\n",
      "Epoch:  133  Training Loss:  53.7499  Training Accuracy:  0.533862\n",
      "Epoch:  134  Training Loss:  53.6307  Training Accuracy:  0.536508\n",
      "Epoch:  135  Training Loss:  53.5146  Training Accuracy:  0.538389\n",
      "Epoch:  136  Training Loss:  53.4227  Training Accuracy:  0.540035\n",
      "Epoch:  137  Training Loss:  53.2831  Training Accuracy:  0.541975\n",
      "Epoch:  138  Training Loss:  53.2082  Training Accuracy:  0.543445\n",
      "Epoch:  139  Training Loss:  53.0808  Training Accuracy:  0.545209\n",
      "Epoch:  140  Training Loss:  52.9208  Training Accuracy:  0.546561\n",
      "Epoch:  141  Training Loss:  52.8081  Training Accuracy:  0.548089\n",
      "Epoch:  142  Training Loss:  52.6841  Training Accuracy:  0.550029\n",
      "Epoch:  143  Training Loss:  52.5719  Training Accuracy:  0.551617\n",
      "Epoch:  144  Training Loss:  52.474  Training Accuracy:  0.553086\n",
      "Epoch:  145  Training Loss:  52.3564  Training Accuracy:  0.555144\n",
      "Epoch:  146  Training Loss:  52.2928  Training Accuracy:  0.556672\n",
      "Epoch:  147  Training Loss:  52.1921  Training Accuracy:  0.558554\n",
      "Epoch:  148  Training Loss:  52.0915  Training Accuracy:  0.560082\n",
      "Epoch:  149  Training Loss:  52.0045  Training Accuracy:  0.561082\n",
      "Epoch:  150  Training Loss:  51.9181  Training Accuracy:  0.562963\n",
      "Epoch:  151  Training Loss:  51.8262  Training Accuracy:  0.564785\n",
      "Epoch:  152  Training Loss:  51.712  Training Accuracy:  0.566079\n",
      "Epoch:  153  Training Loss:  51.6287  Training Accuracy:  0.56749\n",
      "Epoch:  154  Training Loss:  51.5354  Training Accuracy:  0.568783\n",
      "Epoch:  155  Training Loss:  51.4448  Training Accuracy:  0.570311\n",
      "Epoch:  156  Training Loss:  51.3532  Training Accuracy:  0.571722\n",
      "Epoch:  157  Training Loss:  51.2764  Training Accuracy:  0.573839\n",
      "Epoch:  158  Training Loss:  51.1845  Training Accuracy:  0.575661\n",
      "Epoch:  159  Training Loss:  51.0924  Training Accuracy:  0.576778\n",
      "Epoch:  160  Training Loss:  51.016  Training Accuracy:  0.578601\n",
      "Epoch:  161  Training Loss:  50.9535  Training Accuracy:  0.580894\n",
      "Epoch:  162  Training Loss:  50.8631  Training Accuracy:  0.582834\n",
      "Epoch:  163  Training Loss:  50.811  Training Accuracy:  0.583598\n",
      "Epoch:  164  Training Loss:  50.7403  Training Accuracy:  0.585773\n",
      "Epoch:  165  Training Loss:  50.6446  Training Accuracy:  0.587301\n",
      "Epoch:  166  Training Loss:  50.5887  Training Accuracy:  0.588654\n",
      "Epoch:  167  Training Loss:  50.5275  Training Accuracy:  0.589947\n",
      "Epoch:  168  Training Loss:  50.4319  Training Accuracy:  0.591182\n",
      "Epoch:  169  Training Loss:  50.3584  Training Accuracy:  0.59318\n",
      "Epoch:  170  Training Loss:  50.2725  Training Accuracy:  0.594474\n",
      "Epoch:  171  Training Loss:  50.2097  Training Accuracy:  0.595591\n",
      "Epoch:  172  Training Loss:  50.1339  Training Accuracy:  0.596884\n",
      "Epoch:  173  Training Loss:  50.0546  Training Accuracy:  0.598295\n",
      "Epoch:  174  Training Loss:  49.9895  Training Accuracy:  0.599236\n",
      "Epoch:  175  Training Loss:  49.9331  Training Accuracy:  0.600353\n",
      "Epoch:  176  Training Loss:  49.828  Training Accuracy:  0.602175\n",
      "Epoch:  177  Training Loss:  49.753  Training Accuracy:  0.603704\n",
      "Epoch:  178  Training Loss:  49.6745  Training Accuracy:  0.605409\n",
      "Epoch:  179  Training Loss:  49.6008  Training Accuracy:  0.60682\n",
      "Epoch:  180  Training Loss:  49.5322  Training Accuracy:  0.607995\n",
      "Epoch:  181  Training Loss:  49.4688  Training Accuracy:  0.610112\n",
      "Epoch:  182  Training Loss:  49.3472  Training Accuracy:  0.611581\n",
      "Epoch:  183  Training Loss:  49.2977  Training Accuracy:  0.612522\n",
      "Epoch:  184  Training Loss:  49.1869  Training Accuracy:  0.613933\n",
      "Epoch:  185  Training Loss:  49.1168  Training Accuracy:  0.615873\n",
      "Epoch:  186  Training Loss:  49.0629  Training Accuracy:  0.617343\n",
      "Epoch:  187  Training Loss:  48.9715  Training Accuracy:  0.619342\n",
      "Epoch:  188  Training Loss:  48.9137  Training Accuracy:  0.620988\n",
      "Epoch:  189  Training Loss:  48.8395  Training Accuracy:  0.622575\n",
      "Epoch:  190  Training Loss:  48.8019  Training Accuracy:  0.623927\n",
      "Epoch:  191  Training Loss:  48.7475  Training Accuracy:  0.625632\n",
      "Epoch:  192  Training Loss:  48.7076  Training Accuracy:  0.626925\n",
      "Epoch:  193  Training Loss:  48.6719  Training Accuracy:  0.629042\n",
      "Epoch:  194  Training Loss:  48.643  Training Accuracy:  0.630805\n",
      "Epoch:  195  Training Loss:  48.5845  Training Accuracy:  0.632746\n",
      "Epoch:  196  Training Loss:  48.5408  Training Accuracy:  0.634862\n",
      "Epoch:  197  Training Loss:  48.4944  Training Accuracy:  0.636919\n",
      "Epoch:  198  Training Loss:  48.4386  Training Accuracy:  0.638977\n",
      "Epoch:  199  Training Loss:  48.3608  Training Accuracy:  0.641152\n",
      "Testing Accuracy: 0.622887\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  33.0511  Training Accuracy:  0.0409759\n",
      "Epoch:  1  Training Loss:  31.2921  Training Accuracy:  0.0409759\n",
      "Epoch:  2  Training Loss:  31.1127  Training Accuracy:  0.0421517\n",
      "Epoch:  3  Training Loss:  31.352  Training Accuracy:  0.0476778\n",
      "Epoch:  4  Training Loss:  31.6794  Training Accuracy:  0.0546737\n",
      "Epoch:  5  Training Loss:  32.0976  Training Accuracy:  0.0600235\n",
      "Epoch:  6  Training Loss:  32.5727  Training Accuracy:  0.0634921\n",
      "Epoch:  7  Training Loss:  33.2026  Training Accuracy:  0.0683715\n",
      "Epoch:  8  Training Loss:  33.8316  Training Accuracy:  0.0744856\n",
      "Epoch:  9  Training Loss:  34.5352  Training Accuracy:  0.0800118\n",
      "Epoch:  10  Training Loss:  35.0213  Training Accuracy:  0.0858907\n",
      "Epoch:  11  Training Loss:  35.4852  Training Accuracy:  0.0927102\n",
      "Epoch:  12  Training Loss:  35.8859  Training Accuracy:  0.09806\n",
      "Epoch:  13  Training Loss:  36.0901  Training Accuracy:  0.103645\n",
      "Epoch:  14  Training Loss:  36.2503  Training Accuracy:  0.108172\n",
      "Epoch:  15  Training Loss:  36.3395  Training Accuracy:  0.113345\n",
      "Epoch:  16  Training Loss:  36.3929  Training Accuracy:  0.118519\n",
      "Epoch:  17  Training Loss:  36.4144  Training Accuracy:  0.123457\n",
      "Epoch:  18  Training Loss:  36.4108  Training Accuracy:  0.128571\n",
      "Epoch:  19  Training Loss:  36.4152  Training Accuracy:  0.13398\n",
      "Epoch:  20  Training Loss:  36.3433  Training Accuracy:  0.137743\n",
      "Epoch:  21  Training Loss:  36.3534  Training Accuracy:  0.143857\n",
      "Epoch:  22  Training Loss:  36.3522  Training Accuracy:  0.148442\n",
      "Epoch:  23  Training Loss:  36.3141  Training Accuracy:  0.153439\n",
      "Epoch:  24  Training Loss:  36.3095  Training Accuracy:  0.158025\n",
      "Epoch:  25  Training Loss:  36.2833  Training Accuracy:  0.163551\n",
      "Epoch:  26  Training Loss:  36.2294  Training Accuracy:  0.16943\n",
      "Epoch:  27  Training Loss:  36.2077  Training Accuracy:  0.174897\n",
      "Epoch:  28  Training Loss:  36.1598  Training Accuracy:  0.180776\n",
      "Epoch:  29  Training Loss:  36.1102  Training Accuracy:  0.187066\n",
      "Epoch:  30  Training Loss:  36.1283  Training Accuracy:  0.192593\n",
      "Epoch:  31  Training Loss:  36.2336  Training Accuracy:  0.199118\n",
      "Epoch:  32  Training Loss:  36.3811  Training Accuracy:  0.204938\n",
      "Epoch:  33  Training Loss:  36.5196  Training Accuracy:  0.211111\n",
      "Epoch:  34  Training Loss:  36.7286  Training Accuracy:  0.216755\n",
      "Epoch:  35  Training Loss:  36.9429  Training Accuracy:  0.22281\n",
      "Epoch:  36  Training Loss:  37.1946  Training Accuracy:  0.227513\n",
      "Epoch:  37  Training Loss:  37.4894  Training Accuracy:  0.23251\n",
      "Epoch:  38  Training Loss:  37.7628  Training Accuracy:  0.238389\n",
      "Epoch:  39  Training Loss:  38.0779  Training Accuracy:  0.242857\n",
      "Epoch:  40  Training Loss:  38.4517  Training Accuracy:  0.248442\n",
      "Epoch:  41  Training Loss:  38.8065  Training Accuracy:  0.253968\n",
      "Epoch:  42  Training Loss:  39.2097  Training Accuracy:  0.258201\n",
      "Epoch:  43  Training Loss:  39.6261  Training Accuracy:  0.263022\n",
      "Epoch:  44  Training Loss:  40.0193  Training Accuracy:  0.268136\n",
      "Epoch:  45  Training Loss:  40.4968  Training Accuracy:  0.273016\n",
      "Epoch:  46  Training Loss:  40.8937  Training Accuracy:  0.276955\n",
      "Epoch:  47  Training Loss:  41.3699  Training Accuracy:  0.281717\n",
      "Epoch:  48  Training Loss:  41.8605  Training Accuracy:  0.285714\n",
      "Epoch:  49  Training Loss:  42.3798  Training Accuracy:  0.288889\n",
      "Epoch:  50  Training Loss:  42.9406  Training Accuracy:  0.292651\n",
      "Epoch:  51  Training Loss:  43.4656  Training Accuracy:  0.296884\n",
      "Epoch:  52  Training Loss:  44.0065  Training Accuracy:  0.301528\n",
      "Epoch:  53  Training Loss:  44.5383  Training Accuracy:  0.305938\n",
      "Epoch:  54  Training Loss:  45.0982  Training Accuracy:  0.310229\n",
      "Epoch:  55  Training Loss:  45.6215  Training Accuracy:  0.313227\n",
      "Epoch:  56  Training Loss:  46.1623  Training Accuracy:  0.317284\n",
      "Epoch:  57  Training Loss:  46.6726  Training Accuracy:  0.321634\n",
      "Epoch:  58  Training Loss:  47.1019  Training Accuracy:  0.325573\n",
      "Epoch:  59  Training Loss:  47.6434  Training Accuracy:  0.330276\n",
      "Epoch:  60  Training Loss:  48.0485  Training Accuracy:  0.33351\n",
      "Epoch:  61  Training Loss:  48.5231  Training Accuracy:  0.337213\n",
      "Epoch:  62  Training Loss:  48.956  Training Accuracy:  0.341623\n",
      "Epoch:  63  Training Loss:  49.3721  Training Accuracy:  0.345444\n",
      "Epoch:  64  Training Loss:  49.6685  Training Accuracy:  0.350147\n",
      "Epoch:  65  Training Loss:  50.0622  Training Accuracy:  0.353733\n",
      "Epoch:  66  Training Loss:  50.4152  Training Accuracy:  0.35726\n",
      "Epoch:  67  Training Loss:  50.7866  Training Accuracy:  0.361023\n",
      "Epoch:  68  Training Loss:  51.0229  Training Accuracy:  0.364785\n",
      "Epoch:  69  Training Loss:  51.3054  Training Accuracy:  0.367666\n",
      "Epoch:  70  Training Loss:  51.5885  Training Accuracy:  0.370958\n",
      "Epoch:  71  Training Loss:  51.8401  Training Accuracy:  0.37425\n",
      "Epoch:  72  Training Loss:  52.0921  Training Accuracy:  0.377307\n",
      "Epoch:  73  Training Loss:  52.3221  Training Accuracy:  0.38007\n",
      "Epoch:  74  Training Loss:  52.5257  Training Accuracy:  0.384186\n",
      "Epoch:  75  Training Loss:  52.7166  Training Accuracy:  0.387008\n",
      "Epoch:  76  Training Loss:  52.8962  Training Accuracy:  0.390241\n",
      "Epoch:  77  Training Loss:  53.0712  Training Accuracy:  0.393827\n",
      "Epoch:  78  Training Loss:  53.2506  Training Accuracy:  0.396414\n",
      "Epoch:  79  Training Loss:  53.3894  Training Accuracy:  0.4\n",
      "Epoch:  80  Training Loss:  53.535  Training Accuracy:  0.402645\n",
      "Epoch:  81  Training Loss:  53.6681  Training Accuracy:  0.405938\n",
      "Epoch:  82  Training Loss:  53.7867  Training Accuracy:  0.409406\n",
      "Epoch:  83  Training Loss:  53.9003  Training Accuracy:  0.413286\n",
      "Epoch:  84  Training Loss:  53.9824  Training Accuracy:  0.416402\n",
      "Epoch:  85  Training Loss:  54.0865  Training Accuracy:  0.419341\n",
      "Epoch:  86  Training Loss:  54.2246  Training Accuracy:  0.423104\n",
      "Epoch:  87  Training Loss:  54.3414  Training Accuracy:  0.426161\n",
      "Epoch:  88  Training Loss:  54.3578  Training Accuracy:  0.429512\n",
      "Epoch:  89  Training Loss:  54.457  Training Accuracy:  0.432863\n",
      "Epoch:  90  Training Loss:  54.5279  Training Accuracy:  0.436155\n",
      "Epoch:  91  Training Loss:  54.6174  Training Accuracy:  0.439682\n",
      "Epoch:  92  Training Loss:  54.6781  Training Accuracy:  0.442504\n",
      "Epoch:  93  Training Loss:  54.6566  Training Accuracy:  0.445267\n",
      "Epoch:  94  Training Loss:  54.7711  Training Accuracy:  0.449089\n",
      "Epoch:  95  Training Loss:  54.7942  Training Accuracy:  0.452792\n",
      "Epoch:  96  Training Loss:  54.7557  Training Accuracy:  0.455026\n",
      "Epoch:  97  Training Loss:  54.6821  Training Accuracy:  0.458201\n",
      "Epoch:  98  Training Loss:  54.6753  Training Accuracy:  0.461611\n",
      "Epoch:  99  Training Loss:  54.6213  Training Accuracy:  0.464668\n",
      "Epoch:  100  Training Loss:  54.5939  Training Accuracy:  0.468254\n",
      "Epoch:  101  Training Loss:  54.562  Training Accuracy:  0.471428\n",
      "Epoch:  102  Training Loss:  54.5085  Training Accuracy:  0.474309\n",
      "Epoch:  103  Training Loss:  54.3997  Training Accuracy:  0.476249\n",
      "Epoch:  104  Training Loss:  54.3717  Training Accuracy:  0.478718\n",
      "Epoch:  105  Training Loss:  54.2638  Training Accuracy:  0.480717\n",
      "Epoch:  106  Training Loss:  54.2739  Training Accuracy:  0.482834\n",
      "Epoch:  107  Training Loss:  54.2342  Training Accuracy:  0.485891\n",
      "Epoch:  108  Training Loss:  54.1796  Training Accuracy:  0.488066\n",
      "Epoch:  109  Training Loss:  54.1048  Training Accuracy:  0.489594\n",
      "Epoch:  110  Training Loss:  54.0076  Training Accuracy:  0.491358\n",
      "Epoch:  111  Training Loss:  53.926  Training Accuracy:  0.494121\n",
      "Epoch:  112  Training Loss:  53.7879  Training Accuracy:  0.495767\n",
      "Epoch:  113  Training Loss:  53.7968  Training Accuracy:  0.496943\n",
      "Epoch:  114  Training Loss:  53.7417  Training Accuracy:  0.499353\n",
      "Epoch:  115  Training Loss:  53.6699  Training Accuracy:  0.50147\n",
      "Epoch:  116  Training Loss:  53.6701  Training Accuracy:  0.503939\n",
      "Epoch:  117  Training Loss:  53.5936  Training Accuracy:  0.50582\n",
      "Epoch:  118  Training Loss:  53.5193  Training Accuracy:  0.50776\n",
      "Epoch:  119  Training Loss:  53.4492  Training Accuracy:  0.509053\n",
      "Epoch:  120  Training Loss:  53.3588  Training Accuracy:  0.510523\n",
      "Epoch:  121  Training Loss:  53.2835  Training Accuracy:  0.51264\n",
      "Epoch:  122  Training Loss:  53.2224  Training Accuracy:  0.514286\n",
      "Epoch:  123  Training Loss:  53.178  Training Accuracy:  0.515403\n",
      "Epoch:  124  Training Loss:  53.1099  Training Accuracy:  0.517107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  53.0287  Training Accuracy:  0.518871\n",
      "Epoch:  126  Training Loss:  52.9861  Training Accuracy:  0.520929\n",
      "Epoch:  127  Training Loss:  52.926  Training Accuracy:  0.523045\n",
      "Epoch:  128  Training Loss:  52.8907  Training Accuracy:  0.524456\n",
      "Epoch:  129  Training Loss:  52.868  Training Accuracy:  0.526043\n",
      "Epoch:  130  Training Loss:  52.8021  Training Accuracy:  0.527631\n",
      "Epoch:  131  Training Loss:  52.7642  Training Accuracy:  0.530041\n",
      "Epoch:  132  Training Loss:  52.7428  Training Accuracy:  0.532275\n",
      "Epoch:  133  Training Loss:  52.6499  Training Accuracy:  0.534744\n",
      "Epoch:  134  Training Loss:  52.5879  Training Accuracy:  0.536038\n",
      "Epoch:  135  Training Loss:  52.5524  Training Accuracy:  0.538036\n",
      "Epoch:  136  Training Loss:  52.5451  Training Accuracy:  0.540506\n",
      "Epoch:  137  Training Loss:  52.5079  Training Accuracy:  0.542269\n",
      "Epoch:  138  Training Loss:  52.4948  Training Accuracy:  0.543915\n",
      "Epoch:  139  Training Loss:  52.4884  Training Accuracy:  0.545855\n",
      "Epoch:  140  Training Loss:  52.4543  Training Accuracy:  0.547325\n",
      "Epoch:  141  Training Loss:  52.4435  Training Accuracy:  0.548795\n",
      "Epoch:  142  Training Loss:  52.4475  Training Accuracy:  0.549794\n",
      "Epoch:  143  Training Loss:  52.4536  Training Accuracy:  0.551675\n",
      "Epoch:  144  Training Loss:  52.4519  Training Accuracy:  0.553204\n",
      "Epoch:  145  Training Loss:  52.4942  Training Accuracy:  0.555085\n",
      "Epoch:  146  Training Loss:  52.4884  Training Accuracy:  0.556614\n",
      "Epoch:  147  Training Loss:  52.4853  Training Accuracy:  0.558319\n",
      "Epoch:  148  Training Loss:  52.4808  Training Accuracy:  0.559788\n",
      "Epoch:  149  Training Loss:  52.4779  Training Accuracy:  0.561787\n",
      "Epoch:  150  Training Loss:  52.4776  Training Accuracy:  0.563139\n",
      "Epoch:  151  Training Loss:  52.4762  Training Accuracy:  0.564491\n",
      "Epoch:  152  Training Loss:  52.4996  Training Accuracy:  0.56602\n",
      "Epoch:  153  Training Loss:  52.4855  Training Accuracy:  0.56796\n",
      "Epoch:  154  Training Loss:  52.5032  Training Accuracy:  0.56943\n",
      "Epoch:  155  Training Loss:  52.494  Training Accuracy:  0.570782\n",
      "Epoch:  156  Training Loss:  52.4968  Training Accuracy:  0.572369\n",
      "Epoch:  157  Training Loss:  52.4875  Training Accuracy:  0.574192\n",
      "Epoch:  158  Training Loss:  52.4968  Training Accuracy:  0.575838\n",
      "Epoch:  159  Training Loss:  52.4823  Training Accuracy:  0.577307\n",
      "Epoch:  160  Training Loss:  52.4708  Training Accuracy:  0.579541\n",
      "Epoch:  161  Training Loss:  52.4508  Training Accuracy:  0.581246\n",
      "Epoch:  162  Training Loss:  52.4931  Training Accuracy:  0.583186\n",
      "Epoch:  163  Training Loss:  52.4987  Training Accuracy:  0.585244\n",
      "Epoch:  164  Training Loss:  52.4846  Training Accuracy:  0.587478\n",
      "Epoch:  165  Training Loss:  52.5167  Training Accuracy:  0.589477\n",
      "Epoch:  166  Training Loss:  52.5188  Training Accuracy:  0.591064\n",
      "Epoch:  167  Training Loss:  52.5292  Training Accuracy:  0.592299\n",
      "Epoch:  168  Training Loss:  52.5667  Training Accuracy:  0.593592\n",
      "Epoch:  169  Training Loss:  52.5399  Training Accuracy:  0.594297\n",
      "Epoch:  170  Training Loss:  52.5786  Training Accuracy:  0.595356\n",
      "Epoch:  171  Training Loss:  52.5574  Training Accuracy:  0.596943\n",
      "Epoch:  172  Training Loss:  52.5782  Training Accuracy:  0.598765\n",
      "Epoch:  173  Training Loss:  52.5432  Training Accuracy:  0.600353\n",
      "Epoch:  174  Training Loss:  52.5316  Training Accuracy:  0.602116\n",
      "Epoch:  175  Training Loss:  52.5181  Training Accuracy:  0.603704\n",
      "Epoch:  176  Training Loss:  52.5609  Training Accuracy:  0.605467\n",
      "Epoch:  177  Training Loss:  52.5502  Training Accuracy:  0.60729\n",
      "Epoch:  178  Training Loss:  52.5411  Training Accuracy:  0.608701\n",
      "Epoch:  179  Training Loss:  52.5672  Training Accuracy:  0.610464\n",
      "Epoch:  180  Training Loss:  52.5637  Training Accuracy:  0.612052\n",
      "Epoch:  181  Training Loss:  52.5728  Training Accuracy:  0.613815\n",
      "Epoch:  182  Training Loss:  52.5597  Training Accuracy:  0.615226\n",
      "Epoch:  183  Training Loss:  52.5408  Training Accuracy:  0.616637\n",
      "Epoch:  184  Training Loss:  52.5467  Training Accuracy:  0.618401\n",
      "Epoch:  185  Training Loss:  52.514  Training Accuracy:  0.619871\n",
      "Epoch:  186  Training Loss:  52.5432  Training Accuracy:  0.621282\n",
      "Epoch:  187  Training Loss:  52.5254  Training Accuracy:  0.622751\n",
      "Epoch:  188  Training Loss:  52.5044  Training Accuracy:  0.624691\n",
      "Epoch:  189  Training Loss:  52.5258  Training Accuracy:  0.626279\n",
      "Epoch:  190  Training Loss:  52.5446  Training Accuracy:  0.627748\n",
      "Epoch:  191  Training Loss:  52.537  Training Accuracy:  0.629747\n",
      "Epoch:  192  Training Loss:  52.543  Training Accuracy:  0.631687\n",
      "Epoch:  193  Training Loss:  52.526  Training Accuracy:  0.633686\n",
      "Epoch:  194  Training Loss:  52.5112  Training Accuracy:  0.636332\n",
      "Epoch:  195  Training Loss:  52.5243  Training Accuracy:  0.637743\n",
      "Epoch:  196  Training Loss:  52.5067  Training Accuracy:  0.639565\n",
      "Epoch:  197  Training Loss:  52.5509  Training Accuracy:  0.641035\n",
      "Epoch:  198  Training Loss:  52.549  Training Accuracy:  0.642152\n",
      "Epoch:  199  Training Loss:  52.5426  Training Accuracy:  0.643092\n",
      "Epoch:  200  Training Loss:  52.5642  Training Accuracy:  0.644856\n",
      "Epoch:  201  Training Loss:  52.539  Training Accuracy:  0.646385\n",
      "Epoch:  202  Training Loss:  52.5484  Training Accuracy:  0.647678\n",
      "Epoch:  203  Training Loss:  52.537  Training Accuracy:  0.649324\n",
      "Epoch:  204  Training Loss:  52.5279  Training Accuracy:  0.650794\n",
      "Epoch:  205  Training Loss:  52.5329  Training Accuracy:  0.652028\n",
      "Epoch:  206  Training Loss:  52.5203  Training Accuracy:  0.654086\n",
      "Epoch:  207  Training Loss:  52.5569  Training Accuracy:  0.655791\n",
      "Epoch:  208  Training Loss:  52.5465  Training Accuracy:  0.657731\n",
      "Epoch:  209  Training Loss:  52.5337  Training Accuracy:  0.659201\n",
      "Epoch:  210  Training Loss:  52.5321  Training Accuracy:  0.660318\n",
      "Epoch:  211  Training Loss:  52.5049  Training Accuracy:  0.662493\n",
      "Epoch:  212  Training Loss:  52.5027  Training Accuracy:  0.663904\n",
      "Epoch:  213  Training Loss:  52.4638  Training Accuracy:  0.665315\n",
      "Epoch:  214  Training Loss:  52.4347  Training Accuracy:  0.666961\n",
      "Epoch:  215  Training Loss:  52.4205  Training Accuracy:  0.668724\n",
      "Epoch:  216  Training Loss:  52.3806  Training Accuracy:  0.670253\n",
      "Epoch:  217  Training Loss:  52.3637  Training Accuracy:  0.671664\n",
      "Epoch:  218  Training Loss:  52.3521  Training Accuracy:  0.67331\n",
      "Epoch:  219  Training Loss:  52.2976  Training Accuracy:  0.674897\n",
      "Epoch:  220  Training Loss:  52.2829  Training Accuracy:  0.67672\n",
      "Epoch:  221  Training Loss:  52.2656  Training Accuracy:  0.677895\n",
      "Epoch:  222  Training Loss:  52.2494  Training Accuracy:  0.679483\n",
      "Epoch:  223  Training Loss:  52.1658  Training Accuracy:  0.680894\n",
      "Epoch:  224  Training Loss:  52.1182  Training Accuracy:  0.681893\n",
      "Epoch:  225  Training Loss:  52.0913  Training Accuracy:  0.683539\n",
      "Epoch:  226  Training Loss:  52.0987  Training Accuracy:  0.684303\n",
      "Epoch:  227  Training Loss:  52.0445  Training Accuracy:  0.685656\n",
      "Epoch:  228  Training Loss:  52.0054  Training Accuracy:  0.686714\n",
      "Epoch:  229  Training Loss:  51.9634  Training Accuracy:  0.68836\n",
      "Epoch:  230  Training Loss:  51.8881  Training Accuracy:  0.689477\n",
      "Epoch:  231  Training Loss:  51.8178  Training Accuracy:  0.690829\n",
      "Epoch:  232  Training Loss:  51.7237  Training Accuracy:  0.692181\n",
      "Epoch:  233  Training Loss:  51.6432  Training Accuracy:  0.693475\n",
      "Epoch:  234  Training Loss:  51.59  Training Accuracy:  0.695003\n",
      "Epoch:  235  Training Loss:  51.4766  Training Accuracy:  0.696943\n",
      "Epoch:  236  Training Loss:  51.396  Training Accuracy:  0.698354\n",
      "Epoch:  237  Training Loss:  51.3056  Training Accuracy:  0.700588\n",
      "Epoch:  238  Training Loss:  51.2411  Training Accuracy:  0.701881\n",
      "Epoch:  239  Training Loss:  51.1661  Training Accuracy:  0.703351\n",
      "Epoch:  240  Training Loss:  51.098  Training Accuracy:  0.704645\n",
      "Epoch:  241  Training Loss:  51.0027  Training Accuracy:  0.706232\n",
      "Epoch:  242  Training Loss:  50.9369  Training Accuracy:  0.707643\n",
      "Epoch:  243  Training Loss:  50.8801  Training Accuracy:  0.709054\n",
      "Epoch:  244  Training Loss:  50.8062  Training Accuracy:  0.710229\n",
      "Epoch:  245  Training Loss:  50.7277  Training Accuracy:  0.711934\n",
      "Epoch:  246  Training Loss:  50.6466  Training Accuracy:  0.713286\n",
      "Epoch:  247  Training Loss:  50.5893  Training Accuracy:  0.715109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  50.4999  Training Accuracy:  0.716167\n",
      "Epoch:  249  Training Loss:  50.4028  Training Accuracy:  0.717813\n",
      "Epoch:  250  Training Loss:  50.3371  Training Accuracy:  0.719342\n",
      "Epoch:  251  Training Loss:  50.2374  Training Accuracy:  0.72087\n",
      "Epoch:  252  Training Loss:  50.1468  Training Accuracy:  0.722105\n",
      "Epoch:  253  Training Loss:  50.059  Training Accuracy:  0.723281\n",
      "Epoch:  254  Training Loss:  50.0083  Training Accuracy:  0.724574\n",
      "Epoch:  255  Training Loss:  49.9009  Training Accuracy:  0.725632\n",
      "Epoch:  256  Training Loss:  49.8435  Training Accuracy:  0.727043\n",
      "Epoch:  257  Training Loss:  49.7531  Training Accuracy:  0.728219\n",
      "Epoch:  258  Training Loss:  49.6853  Training Accuracy:  0.728866\n",
      "Epoch:  259  Training Loss:  49.6074  Training Accuracy:  0.730276\n",
      "Epoch:  260  Training Loss:  49.5498  Training Accuracy:  0.731452\n",
      "Epoch:  261  Training Loss:  49.4606  Training Accuracy:  0.732687\n",
      "Epoch:  262  Training Loss:  49.4022  Training Accuracy:  0.73351\n",
      "Epoch:  263  Training Loss:  49.3389  Training Accuracy:  0.734921\n",
      "Epoch:  264  Training Loss:  49.2888  Training Accuracy:  0.735979\n",
      "Epoch:  265  Training Loss:  49.1815  Training Accuracy:  0.73739\n",
      "Epoch:  266  Training Loss:  49.126  Training Accuracy:  0.738213\n",
      "Epoch:  267  Training Loss:  49.0721  Training Accuracy:  0.739859\n",
      "Epoch:  268  Training Loss:  49.0004  Training Accuracy:  0.741446\n",
      "Epoch:  269  Training Loss:  48.9198  Training Accuracy:  0.74227\n",
      "Epoch:  270  Training Loss:  48.8472  Training Accuracy:  0.743328\n",
      "Epoch:  271  Training Loss:  48.7672  Training Accuracy:  0.74421\n",
      "Epoch:  272  Training Loss:  48.6921  Training Accuracy:  0.744739\n",
      "Epoch:  273  Training Loss:  48.6141  Training Accuracy:  0.745914\n",
      "Epoch:  274  Training Loss:  48.5339  Training Accuracy:  0.747208\n",
      "Epoch:  275  Training Loss:  48.4625  Training Accuracy:  0.748384\n",
      "Epoch:  276  Training Loss:  48.3954  Training Accuracy:  0.749265\n",
      "Epoch:  277  Training Loss:  48.322  Training Accuracy:  0.750206\n",
      "Epoch:  278  Training Loss:  48.2933  Training Accuracy:  0.751382\n",
      "Epoch:  279  Training Loss:  48.218  Training Accuracy:  0.752734\n",
      "Epoch:  280  Training Loss:  48.2337  Training Accuracy:  0.753792\n",
      "Epoch:  281  Training Loss:  48.1914  Training Accuracy:  0.754615\n",
      "Epoch:  282  Training Loss:  48.1503  Training Accuracy:  0.75585\n",
      "Epoch:  283  Training Loss:  48.0871  Training Accuracy:  0.75679\n",
      "Epoch:  284  Training Loss:  48.0771  Training Accuracy:  0.758437\n",
      "Epoch:  285  Training Loss:  48.0495  Training Accuracy:  0.758848\n",
      "Epoch:  286  Training Loss:  48.0197  Training Accuracy:  0.759671\n",
      "Epoch:  287  Training Loss:  47.9751  Training Accuracy:  0.760788\n",
      "Epoch:  288  Training Loss:  47.9649  Training Accuracy:  0.761317\n",
      "Epoch:  289  Training Loss:  47.9167  Training Accuracy:  0.762317\n",
      "Epoch:  290  Training Loss:  47.8601  Training Accuracy:  0.763492\n",
      "Epoch:  291  Training Loss:  47.8575  Training Accuracy:  0.764374\n",
      "Epoch:  292  Training Loss:  47.8138  Training Accuracy:  0.765256\n",
      "Epoch:  293  Training Loss:  47.7846  Training Accuracy:  0.766314\n",
      "Epoch:  294  Training Loss:  47.7548  Training Accuracy:  0.767843\n",
      "Epoch:  295  Training Loss:  47.703  Training Accuracy:  0.768489\n",
      "Epoch:  296  Training Loss:  47.6849  Training Accuracy:  0.769018\n",
      "Epoch:  297  Training Loss:  47.6548  Training Accuracy:  0.769841\n",
      "Epoch:  298  Training Loss:  47.6236  Training Accuracy:  0.770841\n",
      "Epoch:  299  Training Loss:  47.5705  Training Accuracy:  0.771723\n",
      "Testing Accuracy: 0.72988\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  162.126  Training Accuracy:  0.0435038\n",
      "Epoch:  1  Training Loss:  179.855  Training Accuracy:  0.0493239\n",
      "Epoch:  2  Training Loss:  189.472  Training Accuracy:  0.0627866\n",
      "Epoch:  3  Training Loss:  194.607  Training Accuracy:  0.0793063\n",
      "Epoch:  4  Training Loss:  198.018  Training Accuracy:  0.0917108\n",
      "Epoch:  5  Training Loss:  200.349  Training Accuracy:  0.100999\n",
      "Epoch:  6  Training Loss:  201.925  Training Accuracy:  0.109465\n",
      "Epoch:  7  Training Loss:  202.946  Training Accuracy:  0.121046\n",
      "Epoch:  8  Training Loss:  203.642  Training Accuracy:  0.132863\n",
      "Epoch:  9  Training Loss:  204.072  Training Accuracy:  0.143151\n",
      "Epoch:  10  Training Loss:  204.317  Training Accuracy:  0.153792\n",
      "Epoch:  11  Training Loss:  204.423  Training Accuracy:  0.165021\n",
      "Epoch:  12  Training Loss:  204.41  Training Accuracy:  0.17378\n",
      "Epoch:  13  Training Loss:  204.314  Training Accuracy:  0.180482\n",
      "Epoch:  14  Training Loss:  204.165  Training Accuracy:  0.186537\n",
      "Epoch:  15  Training Loss:  203.979  Training Accuracy:  0.194827\n",
      "Epoch:  16  Training Loss:  203.772  Training Accuracy:  0.203527\n",
      "Epoch:  17  Training Loss:  203.527  Training Accuracy:  0.21164\n",
      "Epoch:  18  Training Loss:  203.271  Training Accuracy:  0.219694\n",
      "Epoch:  19  Training Loss:  203.002  Training Accuracy:  0.226102\n",
      "Epoch:  20  Training Loss:  202.701  Training Accuracy:  0.231511\n",
      "Epoch:  21  Training Loss:  202.407  Training Accuracy:  0.236449\n",
      "Epoch:  22  Training Loss:  202.077  Training Accuracy:  0.241505\n",
      "Epoch:  23  Training Loss:  201.741  Training Accuracy:  0.245679\n",
      "Epoch:  24  Training Loss:  201.372  Training Accuracy:  0.250147\n",
      "Epoch:  25  Training Loss:  201.008  Training Accuracy:  0.255614\n",
      "Epoch:  26  Training Loss:  200.62  Training Accuracy:  0.259906\n",
      "Epoch:  27  Training Loss:  200.198  Training Accuracy:  0.263845\n",
      "Epoch:  28  Training Loss:  199.761  Training Accuracy:  0.267725\n",
      "Epoch:  29  Training Loss:  199.302  Training Accuracy:  0.271722\n",
      "Epoch:  30  Training Loss:  198.864  Training Accuracy:  0.275779\n",
      "Epoch:  31  Training Loss:  198.413  Training Accuracy:  0.2806\n",
      "Epoch:  32  Training Loss:  197.935  Training Accuracy:  0.284538\n",
      "Epoch:  33  Training Loss:  197.477  Training Accuracy:  0.290065\n",
      "Epoch:  34  Training Loss:  196.974  Training Accuracy:  0.294297\n",
      "Epoch:  35  Training Loss:  196.485  Training Accuracy:  0.298648\n",
      "Epoch:  36  Training Loss:  195.983  Training Accuracy:  0.303468\n",
      "Epoch:  37  Training Loss:  195.524  Training Accuracy:  0.30776\n",
      "Epoch:  38  Training Loss:  195.038  Training Accuracy:  0.31264\n",
      "Epoch:  39  Training Loss:  194.556  Training Accuracy:  0.316284\n",
      "Epoch:  40  Training Loss:  194.075  Training Accuracy:  0.32087\n",
      "Epoch:  41  Training Loss:  193.6  Training Accuracy:  0.325691\n",
      "Epoch:  42  Training Loss:  193.149  Training Accuracy:  0.32963\n",
      "Epoch:  43  Training Loss:  192.697  Training Accuracy:  0.334156\n",
      "Epoch:  44  Training Loss:  192.263  Training Accuracy:  0.338154\n",
      "Epoch:  45  Training Loss:  191.84  Training Accuracy:  0.342034\n",
      "Epoch:  46  Training Loss:  191.401  Training Accuracy:  0.345326\n",
      "Epoch:  47  Training Loss:  190.984  Training Accuracy:  0.348971\n",
      "Epoch:  48  Training Loss:  190.557  Training Accuracy:  0.352263\n",
      "Epoch:  49  Training Loss:  190.161  Training Accuracy:  0.355908\n",
      "Epoch:  50  Training Loss:  189.784  Training Accuracy:  0.360023\n",
      "Epoch:  51  Training Loss:  189.402  Training Accuracy:  0.363374\n",
      "Epoch:  52  Training Loss:  189.036  Training Accuracy:  0.366784\n",
      "Epoch:  53  Training Loss:  188.676  Training Accuracy:  0.370311\n",
      "Epoch:  54  Training Loss:  188.344  Training Accuracy:  0.373251\n",
      "Epoch:  55  Training Loss:  187.994  Training Accuracy:  0.376249\n",
      "Epoch:  56  Training Loss:  187.662  Training Accuracy:  0.379894\n",
      "Epoch:  57  Training Loss:  187.35  Training Accuracy:  0.382951\n",
      "Epoch:  58  Training Loss:  187.04  Training Accuracy:  0.386361\n",
      "Epoch:  59  Training Loss:  186.723  Training Accuracy:  0.389535\n",
      "Epoch:  60  Training Loss:  186.433  Training Accuracy:  0.392534\n",
      "Epoch:  61  Training Loss:  186.141  Training Accuracy:  0.395414\n",
      "Epoch:  62  Training Loss:  185.867  Training Accuracy:  0.398177\n",
      "Epoch:  63  Training Loss:  185.61  Training Accuracy:  0.399765\n",
      "Epoch:  64  Training Loss:  185.348  Training Accuracy:  0.402704\n",
      "Epoch:  65  Training Loss:  185.119  Training Accuracy:  0.405938\n",
      "Epoch:  66  Training Loss:  184.872  Training Accuracy:  0.40823\n",
      "Epoch:  67  Training Loss:  184.651  Training Accuracy:  0.411346\n",
      "Epoch:  68  Training Loss:  184.424  Training Accuracy:  0.413874\n",
      "Epoch:  69  Training Loss:  184.205  Training Accuracy:  0.416402\n",
      "Epoch:  70  Training Loss:  183.983  Training Accuracy:  0.419048\n",
      "Epoch:  71  Training Loss:  183.793  Training Accuracy:  0.421811\n",
      "Epoch:  72  Training Loss:  183.578  Training Accuracy:  0.424162\n",
      "Epoch:  73  Training Loss:  183.386  Training Accuracy:  0.427278\n",
      "Epoch:  74  Training Loss:  183.192  Training Accuracy:  0.430276\n",
      "Epoch:  75  Training Loss:  183.023  Training Accuracy:  0.433157\n",
      "Epoch:  76  Training Loss:  182.848  Training Accuracy:  0.435685\n",
      "Epoch:  77  Training Loss:  182.679  Training Accuracy:  0.438448\n",
      "Epoch:  78  Training Loss:  182.508  Training Accuracy:  0.440329\n",
      "Epoch:  79  Training Loss:  182.355  Training Accuracy:  0.442857\n",
      "Epoch:  80  Training Loss:  182.196  Training Accuracy:  0.445385\n",
      "Epoch:  81  Training Loss:  182.057  Training Accuracy:  0.44856\n",
      "Epoch:  82  Training Loss:  181.901  Training Accuracy:  0.451029\n",
      "Epoch:  83  Training Loss:  181.782  Training Accuracy:  0.453968\n",
      "Epoch:  84  Training Loss:  181.647  Training Accuracy:  0.456085\n",
      "Epoch:  85  Training Loss:  181.528  Training Accuracy:  0.458025\n",
      "Epoch:  86  Training Loss:  181.407  Training Accuracy:  0.4602\n",
      "Epoch:  87  Training Loss:  181.296  Training Accuracy:  0.461963\n",
      "Epoch:  88  Training Loss:  181.178  Training Accuracy:  0.46455\n",
      "Epoch:  89  Training Loss:  181.057  Training Accuracy:  0.466431\n",
      "Epoch:  90  Training Loss:  180.941  Training Accuracy:  0.468901\n",
      "Epoch:  91  Training Loss:  180.842  Training Accuracy:  0.471193\n",
      "Epoch:  92  Training Loss:  180.721  Training Accuracy:  0.473133\n",
      "Epoch:  93  Training Loss:  180.632  Training Accuracy:  0.475426\n",
      "Epoch:  94  Training Loss:  180.529  Training Accuracy:  0.478248\n",
      "Epoch:  95  Training Loss:  180.439  Training Accuracy:  0.481246\n",
      "Epoch:  96  Training Loss:  180.352  Training Accuracy:  0.483304\n",
      "Epoch:  97  Training Loss:  180.268  Training Accuracy:  0.48642\n",
      "Epoch:  98  Training Loss:  180.155  Training Accuracy:  0.48883\n",
      "Epoch:  99  Training Loss:  180.071  Training Accuracy:  0.491123\n",
      "Epoch:  100  Training Loss:  179.995  Training Accuracy:  0.494239\n",
      "Epoch:  101  Training Loss:  179.911  Training Accuracy:  0.496473\n",
      "Epoch:  102  Training Loss:  179.809  Training Accuracy:  0.49806\n",
      "Epoch:  103  Training Loss:  179.749  Training Accuracy:  0.500941\n",
      "Epoch:  104  Training Loss:  179.652  Training Accuracy:  0.503468\n",
      "Epoch:  105  Training Loss:  179.59  Training Accuracy:  0.506643\n",
      "Epoch:  106  Training Loss:  179.514  Training Accuracy:  0.508583\n",
      "Epoch:  107  Training Loss:  179.446  Training Accuracy:  0.510523\n",
      "Epoch:  108  Training Loss:  179.358  Training Accuracy:  0.512522\n",
      "Epoch:  109  Training Loss:  179.305  Training Accuracy:  0.514697\n",
      "Epoch:  110  Training Loss:  179.233  Training Accuracy:  0.516226\n",
      "Epoch:  111  Training Loss:  179.163  Training Accuracy:  0.518812\n",
      "Epoch:  112  Training Loss:  179.098  Training Accuracy:  0.520929\n",
      "Epoch:  113  Training Loss:  179.049  Training Accuracy:  0.522398\n",
      "Epoch:  114  Training Loss:  179.0  Training Accuracy:  0.524691\n",
      "Epoch:  115  Training Loss:  178.931  Training Accuracy:  0.527043\n",
      "Epoch:  116  Training Loss:  178.872  Training Accuracy:  0.5291\n",
      "Epoch:  117  Training Loss:  178.834  Training Accuracy:  0.530982\n",
      "Epoch:  118  Training Loss:  178.774  Training Accuracy:  0.53351\n",
      "Epoch:  119  Training Loss:  178.722  Training Accuracy:  0.535391\n",
      "Epoch:  120  Training Loss:  178.675  Training Accuracy:  0.537507\n",
      "Epoch:  121  Training Loss:  178.595  Training Accuracy:  0.539624\n",
      "Epoch:  122  Training Loss:  178.546  Training Accuracy:  0.541564\n",
      "Epoch:  123  Training Loss:  178.479  Training Accuracy:  0.543386\n",
      "Epoch:  124  Training Loss:  178.427  Training Accuracy:  0.545209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  178.343  Training Accuracy:  0.547795\n",
      "Epoch:  126  Training Loss:  178.304  Training Accuracy:  0.550206\n",
      "Epoch:  127  Training Loss:  178.224  Training Accuracy:  0.552087\n",
      "Epoch:  128  Training Loss:  178.179  Training Accuracy:  0.553851\n",
      "Epoch:  129  Training Loss:  178.092  Training Accuracy:  0.55632\n",
      "Epoch:  130  Training Loss:  178.045  Training Accuracy:  0.557437\n",
      "Epoch:  131  Training Loss:  177.975  Training Accuracy:  0.559377\n",
      "Epoch:  132  Training Loss:  177.92  Training Accuracy:  0.561611\n",
      "Epoch:  133  Training Loss:  177.853  Training Accuracy:  0.562845\n",
      "Epoch:  134  Training Loss:  177.792  Training Accuracy:  0.564668\n",
      "Epoch:  135  Training Loss:  177.722  Training Accuracy:  0.56649\n",
      "Epoch:  136  Training Loss:  177.647  Training Accuracy:  0.568548\n",
      "Epoch:  137  Training Loss:  177.609  Training Accuracy:  0.570076\n",
      "Epoch:  138  Training Loss:  177.521  Training Accuracy:  0.572193\n",
      "Epoch:  139  Training Loss:  177.455  Training Accuracy:  0.574486\n",
      "Epoch:  140  Training Loss:  177.394  Training Accuracy:  0.575838\n",
      "Epoch:  141  Training Loss:  177.331  Training Accuracy:  0.577366\n",
      "Epoch:  142  Training Loss:  177.262  Training Accuracy:  0.579071\n",
      "Epoch:  143  Training Loss:  177.204  Training Accuracy:  0.580776\n",
      "Epoch:  144  Training Loss:  177.157  Training Accuracy:  0.582716\n",
      "Epoch:  145  Training Loss:  177.08  Training Accuracy:  0.583774\n",
      "Epoch:  146  Training Loss:  177.042  Training Accuracy:  0.585479\n",
      "Epoch:  147  Training Loss:  176.985  Training Accuracy:  0.586655\n",
      "Epoch:  148  Training Loss:  176.908  Training Accuracy:  0.588066\n",
      "Epoch:  149  Training Loss:  176.867  Training Accuracy:  0.589771\n",
      "Epoch:  150  Training Loss:  176.832  Training Accuracy:  0.591182\n",
      "Epoch:  151  Training Loss:  176.77  Training Accuracy:  0.592181\n",
      "Epoch:  152  Training Loss:  176.747  Training Accuracy:  0.59371\n",
      "Epoch:  153  Training Loss:  176.668  Training Accuracy:  0.595062\n",
      "Epoch:  154  Training Loss:  176.635  Training Accuracy:  0.596473\n",
      "Epoch:  155  Training Loss:  176.57  Training Accuracy:  0.597766\n",
      "Epoch:  156  Training Loss:  176.532  Training Accuracy:  0.599295\n",
      "Epoch:  157  Training Loss:  176.485  Training Accuracy:  0.601058\n",
      "Epoch:  158  Training Loss:  176.454  Training Accuracy:  0.602469\n",
      "Epoch:  159  Training Loss:  176.389  Training Accuracy:  0.604056\n",
      "Epoch:  160  Training Loss:  176.338  Training Accuracy:  0.605467\n",
      "Epoch:  161  Training Loss:  176.28  Training Accuracy:  0.606761\n",
      "Epoch:  162  Training Loss:  176.231  Training Accuracy:  0.608348\n",
      "Epoch:  163  Training Loss:  176.15  Training Accuracy:  0.609583\n",
      "Epoch:  164  Training Loss:  176.121  Training Accuracy:  0.6107\n",
      "Epoch:  165  Training Loss:  176.051  Training Accuracy:  0.611934\n",
      "Epoch:  166  Training Loss:  175.994  Training Accuracy:  0.613286\n",
      "Epoch:  167  Training Loss:  175.924  Training Accuracy:  0.614697\n",
      "Epoch:  168  Training Loss:  175.874  Training Accuracy:  0.616049\n",
      "Epoch:  169  Training Loss:  175.818  Training Accuracy:  0.617166\n",
      "Epoch:  170  Training Loss:  175.778  Training Accuracy:  0.618401\n",
      "Epoch:  171  Training Loss:  175.715  Training Accuracy:  0.619577\n",
      "Epoch:  172  Training Loss:  175.649  Training Accuracy:  0.621164\n",
      "Epoch:  173  Training Loss:  175.572  Training Accuracy:  0.622046\n",
      "Epoch:  174  Training Loss:  175.546  Training Accuracy:  0.623045\n",
      "Epoch:  175  Training Loss:  175.483  Training Accuracy:  0.62428\n",
      "Epoch:  176  Training Loss:  175.458  Training Accuracy:  0.625103\n",
      "Epoch:  177  Training Loss:  175.381  Training Accuracy:  0.626867\n",
      "Epoch:  178  Training Loss:  175.351  Training Accuracy:  0.628042\n",
      "Epoch:  179  Training Loss:  175.299  Training Accuracy:  0.62963\n",
      "Epoch:  180  Training Loss:  175.257  Training Accuracy:  0.631217\n",
      "Epoch:  181  Training Loss:  175.199  Training Accuracy:  0.631981\n",
      "Epoch:  182  Training Loss:  175.175  Training Accuracy:  0.633686\n",
      "Epoch:  183  Training Loss:  175.12  Training Accuracy:  0.634979\n",
      "Epoch:  184  Training Loss:  175.079  Training Accuracy:  0.635803\n",
      "Epoch:  185  Training Loss:  175.049  Training Accuracy:  0.636802\n",
      "Epoch:  186  Training Loss:  174.999  Training Accuracy:  0.63833\n",
      "Epoch:  187  Training Loss:  174.952  Training Accuracy:  0.639741\n",
      "Epoch:  188  Training Loss:  174.899  Training Accuracy:  0.640858\n",
      "Epoch:  189  Training Loss:  174.871  Training Accuracy:  0.642152\n",
      "Epoch:  190  Training Loss:  174.832  Training Accuracy:  0.64368\n",
      "Epoch:  191  Training Loss:  174.778  Training Accuracy:  0.644856\n",
      "Epoch:  192  Training Loss:  174.721  Training Accuracy:  0.645914\n",
      "Epoch:  193  Training Loss:  174.675  Training Accuracy:  0.647031\n",
      "Epoch:  194  Training Loss:  174.619  Training Accuracy:  0.648089\n",
      "Epoch:  195  Training Loss:  174.569  Training Accuracy:  0.648971\n",
      "Epoch:  196  Training Loss:  174.548  Training Accuracy:  0.650323\n",
      "Epoch:  197  Training Loss:  174.495  Training Accuracy:  0.651793\n",
      "Epoch:  198  Training Loss:  174.451  Training Accuracy:  0.653086\n",
      "Epoch:  199  Training Loss:  174.39  Training Accuracy:  0.654086\n",
      "Epoch:  200  Training Loss:  174.351  Training Accuracy:  0.654909\n",
      "Epoch:  201  Training Loss:  174.283  Training Accuracy:  0.656143\n",
      "Epoch:  202  Training Loss:  174.242  Training Accuracy:  0.657084\n",
      "Epoch:  203  Training Loss:  174.187  Training Accuracy:  0.657907\n",
      "Epoch:  204  Training Loss:  174.148  Training Accuracy:  0.659142\n",
      "Epoch:  205  Training Loss:  174.101  Training Accuracy:  0.660553\n",
      "Epoch:  206  Training Loss:  174.075  Training Accuracy:  0.661611\n",
      "Epoch:  207  Training Loss:  174.009  Training Accuracy:  0.663198\n",
      "Epoch:  208  Training Loss:  174.003  Training Accuracy:  0.664433\n",
      "Epoch:  209  Training Loss:  173.947  Training Accuracy:  0.665315\n",
      "Epoch:  210  Training Loss:  173.906  Training Accuracy:  0.66602\n",
      "Epoch:  211  Training Loss:  173.868  Training Accuracy:  0.666432\n",
      "Epoch:  212  Training Loss:  173.855  Training Accuracy:  0.667078\n",
      "Epoch:  213  Training Loss:  173.805  Training Accuracy:  0.66796\n",
      "Epoch:  214  Training Loss:  173.798  Training Accuracy:  0.668901\n",
      "Epoch:  215  Training Loss:  173.741  Training Accuracy:  0.669959\n",
      "Epoch:  216  Training Loss:  173.721  Training Accuracy:  0.670488\n",
      "Epoch:  217  Training Loss:  173.676  Training Accuracy:  0.671487\n",
      "Epoch:  218  Training Loss:  173.65  Training Accuracy:  0.672428\n",
      "Epoch:  219  Training Loss:  173.598  Training Accuracy:  0.673486\n",
      "Epoch:  220  Training Loss:  173.569  Training Accuracy:  0.674368\n",
      "Epoch:  221  Training Loss:  173.534  Training Accuracy:  0.675426\n",
      "Epoch:  222  Training Loss:  173.478  Training Accuracy:  0.676367\n",
      "Epoch:  223  Training Loss:  173.426  Training Accuracy:  0.677131\n",
      "Epoch:  224  Training Loss:  173.421  Training Accuracy:  0.677837\n",
      "Epoch:  225  Training Loss:  173.357  Training Accuracy:  0.678836\n",
      "Epoch:  226  Training Loss:  173.324  Training Accuracy:  0.679424\n",
      "Epoch:  227  Training Loss:  173.298  Training Accuracy:  0.6806\n",
      "Epoch:  228  Training Loss:  173.246  Training Accuracy:  0.68154\n",
      "Epoch:  229  Training Loss:  173.215  Training Accuracy:  0.68254\n",
      "Epoch:  230  Training Loss:  173.192  Training Accuracy:  0.683128\n",
      "Epoch:  231  Training Loss:  173.145  Training Accuracy:  0.684245\n",
      "Epoch:  232  Training Loss:  173.099  Training Accuracy:  0.685656\n",
      "Epoch:  233  Training Loss:  173.046  Training Accuracy:  0.686361\n",
      "Epoch:  234  Training Loss:  173.041  Training Accuracy:  0.687067\n",
      "Epoch:  235  Training Loss:  172.962  Training Accuracy:  0.688184\n",
      "Epoch:  236  Training Loss:  172.945  Training Accuracy:  0.689359\n",
      "Epoch:  237  Training Loss:  172.851  Training Accuracy:  0.690418\n",
      "Epoch:  238  Training Loss:  172.813  Training Accuracy:  0.690888\n",
      "Epoch:  239  Training Loss:  172.766  Training Accuracy:  0.691652\n",
      "Epoch:  240  Training Loss:  172.69  Training Accuracy:  0.69224\n",
      "Epoch:  241  Training Loss:  172.611  Training Accuracy:  0.693181\n",
      "Epoch:  242  Training Loss:  172.589  Training Accuracy:  0.693945\n",
      "Epoch:  243  Training Loss:  172.513  Training Accuracy:  0.694709\n",
      "Epoch:  244  Training Loss:  172.466  Training Accuracy:  0.695297\n",
      "Epoch:  245  Training Loss:  172.378  Training Accuracy:  0.69612\n",
      "Epoch:  246  Training Loss:  172.351  Training Accuracy:  0.696826\n",
      "Epoch:  247  Training Loss:  172.282  Training Accuracy:  0.69759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  172.218  Training Accuracy:  0.698354\n",
      "Epoch:  249  Training Loss:  172.147  Training Accuracy:  0.699177\n",
      "Epoch:  250  Training Loss:  172.125  Training Accuracy:  0.699589\n",
      "Epoch:  251  Training Loss:  172.07  Training Accuracy:  0.700353\n",
      "Epoch:  252  Training Loss:  172.003  Training Accuracy:  0.701235\n",
      "Epoch:  253  Training Loss:  171.936  Training Accuracy:  0.702175\n",
      "Epoch:  254  Training Loss:  171.886  Training Accuracy:  0.702763\n",
      "Epoch:  255  Training Loss:  171.814  Training Accuracy:  0.703469\n",
      "Epoch:  256  Training Loss:  171.781  Training Accuracy:  0.704292\n",
      "Epoch:  257  Training Loss:  171.75  Training Accuracy:  0.704821\n",
      "Epoch:  258  Training Loss:  171.653  Training Accuracy:  0.705291\n",
      "Epoch:  259  Training Loss:  171.619  Training Accuracy:  0.705938\n",
      "Epoch:  260  Training Loss:  171.528  Training Accuracy:  0.706467\n",
      "Epoch:  261  Training Loss:  171.491  Training Accuracy:  0.707584\n",
      "Epoch:  262  Training Loss:  171.415  Training Accuracy:  0.708289\n",
      "Epoch:  263  Training Loss:  171.344  Training Accuracy:  0.708818\n",
      "Epoch:  264  Training Loss:  171.312  Training Accuracy:  0.709171\n",
      "Epoch:  265  Training Loss:  171.24  Training Accuracy:  0.709935\n",
      "Epoch:  266  Training Loss:  171.172  Training Accuracy:  0.7107\n",
      "Epoch:  267  Training Loss:  171.131  Training Accuracy:  0.711699\n",
      "Epoch:  268  Training Loss:  171.128  Training Accuracy:  0.712405\n",
      "Epoch:  269  Training Loss:  170.966  Training Accuracy:  0.712757\n",
      "Epoch:  270  Training Loss:  170.947  Training Accuracy:  0.713463\n",
      "Epoch:  271  Training Loss:  170.85  Training Accuracy:  0.713874\n",
      "Epoch:  272  Training Loss:  170.795  Training Accuracy:  0.71458\n",
      "Epoch:  273  Training Loss:  170.745  Training Accuracy:  0.715168\n",
      "Epoch:  274  Training Loss:  170.673  Training Accuracy:  0.715403\n",
      "Epoch:  275  Training Loss:  170.661  Training Accuracy:  0.716285\n",
      "Epoch:  276  Training Loss:  170.53  Training Accuracy:  0.716696\n",
      "Epoch:  277  Training Loss:  170.465  Training Accuracy:  0.71746\n",
      "Epoch:  278  Training Loss:  170.426  Training Accuracy:  0.718107\n",
      "Epoch:  279  Training Loss:  170.302  Training Accuracy:  0.71893\n",
      "Epoch:  280  Training Loss:  170.346  Training Accuracy:  0.719518\n",
      "Epoch:  281  Training Loss:  170.234  Training Accuracy:  0.720165\n",
      "Epoch:  282  Training Loss:  170.114  Training Accuracy:  0.720694\n",
      "Epoch:  283  Training Loss:  170.062  Training Accuracy:  0.721635\n",
      "Epoch:  284  Training Loss:  170.011  Training Accuracy:  0.72234\n",
      "Epoch:  285  Training Loss:  169.917  Training Accuracy:  0.723045\n",
      "Epoch:  286  Training Loss:  169.87  Training Accuracy:  0.723457\n",
      "Epoch:  287  Training Loss:  169.814  Training Accuracy:  0.724162\n",
      "Epoch:  288  Training Loss:  169.704  Training Accuracy:  0.725456\n",
      "Epoch:  289  Training Loss:  169.659  Training Accuracy:  0.726161\n",
      "Epoch:  290  Training Loss:  169.565  Training Accuracy:  0.726926\n",
      "Epoch:  291  Training Loss:  169.511  Training Accuracy:  0.727455\n",
      "Epoch:  292  Training Loss:  169.386  Training Accuracy:  0.727925\n",
      "Epoch:  293  Training Loss:  169.349  Training Accuracy:  0.728748\n",
      "Epoch:  294  Training Loss:  169.278  Training Accuracy:  0.729453\n",
      "Epoch:  295  Training Loss:  169.141  Training Accuracy:  0.730276\n",
      "Epoch:  296  Training Loss:  169.099  Training Accuracy:  0.730453\n",
      "Epoch:  297  Training Loss:  168.992  Training Accuracy:  0.731041\n",
      "Epoch:  298  Training Loss:  168.853  Training Accuracy:  0.731746\n",
      "Epoch:  299  Training Loss:  168.804  Training Accuracy:  0.732217\n",
      "Epoch:  300  Training Loss:  168.7  Training Accuracy:  0.732746\n",
      "Epoch:  301  Training Loss:  168.591  Training Accuracy:  0.733451\n",
      "Epoch:  302  Training Loss:  168.508  Training Accuracy:  0.73398\n",
      "Epoch:  303  Training Loss:  168.435  Training Accuracy:  0.734392\n",
      "Epoch:  304  Training Loss:  168.292  Training Accuracy:  0.734744\n",
      "Epoch:  305  Training Loss:  168.236  Training Accuracy:  0.735274\n",
      "Epoch:  306  Training Loss:  168.097  Training Accuracy:  0.736155\n",
      "Epoch:  307  Training Loss:  168.041  Training Accuracy:  0.736743\n",
      "Epoch:  308  Training Loss:  167.918  Training Accuracy:  0.737214\n",
      "Epoch:  309  Training Loss:  167.825  Training Accuracy:  0.737566\n",
      "Epoch:  310  Training Loss:  167.678  Training Accuracy:  0.737743\n",
      "Epoch:  311  Training Loss:  167.698  Training Accuracy:  0.737919\n",
      "Epoch:  312  Training Loss:  167.5  Training Accuracy:  0.738566\n",
      "Epoch:  313  Training Loss:  167.402  Training Accuracy:  0.739095\n",
      "Epoch:  314  Training Loss:  167.336  Training Accuracy:  0.739624\n",
      "Epoch:  315  Training Loss:  167.176  Training Accuracy:  0.740212\n",
      "Epoch:  316  Training Loss:  167.118  Training Accuracy:  0.741152\n",
      "Epoch:  317  Training Loss:  166.969  Training Accuracy:  0.74174\n",
      "Epoch:  318  Training Loss:  166.907  Training Accuracy:  0.742328\n",
      "Epoch:  319  Training Loss:  166.784  Training Accuracy:  0.743034\n",
      "Epoch:  320  Training Loss:  166.71  Training Accuracy:  0.743563\n",
      "Epoch:  321  Training Loss:  166.566  Training Accuracy:  0.744033\n",
      "Epoch:  322  Training Loss:  166.516  Training Accuracy:  0.74468\n",
      "Epoch:  323  Training Loss:  166.354  Training Accuracy:  0.745209\n",
      "Epoch:  324  Training Loss:  166.283  Training Accuracy:  0.745562\n",
      "Epoch:  325  Training Loss:  166.177  Training Accuracy:  0.746208\n",
      "Epoch:  326  Training Loss:  166.072  Training Accuracy:  0.746679\n",
      "Epoch:  327  Training Loss:  165.942  Training Accuracy:  0.747031\n",
      "Epoch:  328  Training Loss:  165.907  Training Accuracy:  0.747854\n",
      "Epoch:  329  Training Loss:  165.765  Training Accuracy:  0.748619\n",
      "Epoch:  330  Training Loss:  165.658  Training Accuracy:  0.749265\n",
      "Epoch:  331  Training Loss:  165.526  Training Accuracy:  0.749794\n",
      "Epoch:  332  Training Loss:  165.449  Training Accuracy:  0.750265\n",
      "Epoch:  333  Training Loss:  165.298  Training Accuracy:  0.751029\n",
      "Epoch:  334  Training Loss:  165.262  Training Accuracy:  0.751676\n",
      "Epoch:  335  Training Loss:  165.166  Training Accuracy:  0.752146\n",
      "Epoch:  336  Training Loss:  165.045  Training Accuracy:  0.752499\n",
      "Epoch:  337  Training Loss:  164.918  Training Accuracy:  0.753204\n",
      "Epoch:  338  Training Loss:  164.781  Training Accuracy:  0.753733\n",
      "Epoch:  339  Training Loss:  164.705  Training Accuracy:  0.754204\n",
      "Epoch:  340  Training Loss:  164.604  Training Accuracy:  0.754674\n",
      "Epoch:  341  Training Loss:  164.498  Training Accuracy:  0.755085\n",
      "Epoch:  342  Training Loss:  164.444  Training Accuracy:  0.755203\n",
      "Epoch:  343  Training Loss:  164.324  Training Accuracy:  0.755967\n",
      "Epoch:  344  Training Loss:  164.212  Training Accuracy:  0.756261\n",
      "Epoch:  345  Training Loss:  164.064  Training Accuracy:  0.756908\n",
      "Epoch:  346  Training Loss:  163.957  Training Accuracy:  0.757319\n",
      "Epoch:  347  Training Loss:  163.83  Training Accuracy:  0.758025\n",
      "Epoch:  348  Training Loss:  163.673  Training Accuracy:  0.758554\n",
      "Epoch:  349  Training Loss:  163.572  Training Accuracy:  0.759201\n",
      "Epoch:  350  Training Loss:  163.424  Training Accuracy:  0.759789\n",
      "Epoch:  351  Training Loss:  163.344  Training Accuracy:  0.760141\n",
      "Epoch:  352  Training Loss:  163.15  Training Accuracy:  0.760788\n",
      "Epoch:  353  Training Loss:  163.045  Training Accuracy:  0.761258\n",
      "Epoch:  354  Training Loss:  162.934  Training Accuracy:  0.76167\n",
      "Epoch:  355  Training Loss:  162.829  Training Accuracy:  0.762434\n",
      "Epoch:  356  Training Loss:  162.686  Training Accuracy:  0.763022\n",
      "Epoch:  357  Training Loss:  162.59  Training Accuracy:  0.763022\n",
      "Epoch:  358  Training Loss:  162.429  Training Accuracy:  0.763904\n",
      "Epoch:  359  Training Loss:  162.331  Training Accuracy:  0.764257\n",
      "Epoch:  360  Training Loss:  162.139  Training Accuracy:  0.764551\n",
      "Epoch:  361  Training Loss:  162.088  Training Accuracy:  0.764962\n",
      "Epoch:  362  Training Loss:  161.871  Training Accuracy:  0.765315\n",
      "Epoch:  363  Training Loss:  161.784  Training Accuracy:  0.766079\n",
      "Epoch:  364  Training Loss:  161.661  Training Accuracy:  0.766491\n",
      "Epoch:  365  Training Loss:  161.531  Training Accuracy:  0.767314\n",
      "Epoch:  366  Training Loss:  161.344  Training Accuracy:  0.767666\n",
      "Epoch:  367  Training Loss:  161.246  Training Accuracy:  0.768489\n",
      "Epoch:  368  Training Loss:  161.02  Training Accuracy:  0.769254\n",
      "Epoch:  369  Training Loss:  160.979  Training Accuracy:  0.769724\n",
      "Epoch:  370  Training Loss:  160.824  Training Accuracy:  0.770253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  160.634  Training Accuracy:  0.770841\n",
      "Epoch:  372  Training Loss:  160.5  Training Accuracy:  0.771546\n",
      "Epoch:  373  Training Loss:  160.423  Training Accuracy:  0.772134\n",
      "Epoch:  374  Training Loss:  160.233  Training Accuracy:  0.772428\n",
      "Epoch:  375  Training Loss:  160.127  Training Accuracy:  0.772899\n",
      "Epoch:  376  Training Loss:  159.923  Training Accuracy:  0.773428\n",
      "Epoch:  377  Training Loss:  159.671  Training Accuracy:  0.774133\n",
      "Epoch:  378  Training Loss:  159.652  Training Accuracy:  0.774839\n",
      "Epoch:  379  Training Loss:  159.513  Training Accuracy:  0.77525\n",
      "Epoch:  380  Training Loss:  159.317  Training Accuracy:  0.775662\n",
      "Epoch:  381  Training Loss:  159.181  Training Accuracy:  0.776367\n",
      "Epoch:  382  Training Loss:  159.011  Training Accuracy:  0.776661\n",
      "Epoch:  383  Training Loss:  158.87  Training Accuracy:  0.777425\n",
      "Epoch:  384  Training Loss:  158.8  Training Accuracy:  0.77766\n",
      "Epoch:  385  Training Loss:  158.575  Training Accuracy:  0.778542\n",
      "Epoch:  386  Training Loss:  158.446  Training Accuracy:  0.778895\n",
      "Epoch:  387  Training Loss:  158.277  Training Accuracy:  0.77913\n",
      "Epoch:  388  Training Loss:  158.125  Training Accuracy:  0.779601\n",
      "Epoch:  389  Training Loss:  157.98  Training Accuracy:  0.779953\n",
      "Epoch:  390  Training Loss:  157.86  Training Accuracy:  0.780188\n",
      "Epoch:  391  Training Loss:  157.772  Training Accuracy:  0.780659\n",
      "Epoch:  392  Training Loss:  157.601  Training Accuracy:  0.780953\n",
      "Epoch:  393  Training Loss:  157.407  Training Accuracy:  0.781541\n",
      "Epoch:  394  Training Loss:  157.253  Training Accuracy:  0.782305\n",
      "Epoch:  395  Training Loss:  157.14  Training Accuracy:  0.783069\n",
      "Epoch:  396  Training Loss:  157.004  Training Accuracy:  0.783246\n",
      "Epoch:  397  Training Loss:  156.865  Training Accuracy:  0.783422\n",
      "Epoch:  398  Training Loss:  156.701  Training Accuracy:  0.784068\n",
      "Epoch:  399  Training Loss:  156.523  Training Accuracy:  0.784362\n",
      "Testing Accuracy: 0.702962\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 400\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  32.7082  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  28.506  Training Accuracy:  0.0409759\n",
      "Epoch:  2  Training Loss:  27.5811  Training Accuracy:  0.0423868\n",
      "Epoch:  3  Training Loss:  27.9487  Training Accuracy:  0.0482069\n",
      "Epoch:  4  Training Loss:  28.2934  Training Accuracy:  0.0531452\n",
      "Epoch:  5  Training Loss:  28.5934  Training Accuracy:  0.0579071\n",
      "Epoch:  6  Training Loss:  28.9419  Training Accuracy:  0.0613757\n",
      "Epoch:  7  Training Loss:  29.326  Training Accuracy:  0.0641387\n",
      "Epoch:  8  Training Loss:  29.6398  Training Accuracy:  0.0674309\n",
      "Epoch:  9  Training Loss:  29.8456  Training Accuracy:  0.0708407\n",
      "Epoch:  10  Training Loss:  30.0088  Training Accuracy:  0.0742504\n",
      "Epoch:  11  Training Loss:  30.1933  Training Accuracy:  0.0788948\n",
      "Epoch:  12  Training Loss:  30.3296  Training Accuracy:  0.0837742\n",
      "Epoch:  13  Training Loss:  30.5384  Training Accuracy:  0.0881246\n",
      "Epoch:  14  Training Loss:  30.7825  Training Accuracy:  0.0931217\n",
      "Epoch:  15  Training Loss:  31.03  Training Accuracy:  0.0986479\n",
      "Epoch:  16  Training Loss:  31.272  Training Accuracy:  0.103586\n",
      "Epoch:  17  Training Loss:  31.4781  Training Accuracy:  0.109759\n",
      "Epoch:  18  Training Loss:  31.668  Training Accuracy:  0.117519\n",
      "Epoch:  19  Training Loss:  31.8714  Training Accuracy:  0.125456\n",
      "Epoch:  20  Training Loss:  32.0833  Training Accuracy:  0.132216\n",
      "Epoch:  21  Training Loss:  32.3252  Training Accuracy:  0.139565\n",
      "Epoch:  22  Training Loss:  32.5501  Training Accuracy:  0.145385\n",
      "Epoch:  23  Training Loss:  32.7717  Training Accuracy:  0.152851\n",
      "Epoch:  24  Training Loss:  32.9747  Training Accuracy:  0.159377\n",
      "Epoch:  25  Training Loss:  33.1798  Training Accuracy:  0.16649\n",
      "Epoch:  26  Training Loss:  33.426  Training Accuracy:  0.172781\n",
      "Epoch:  27  Training Loss:  33.6906  Training Accuracy:  0.179012\n",
      "Epoch:  28  Training Loss:  33.9326  Training Accuracy:  0.185714\n",
      "Epoch:  29  Training Loss:  34.1762  Training Accuracy:  0.19224\n",
      "Epoch:  30  Training Loss:  34.4945  Training Accuracy:  0.197766\n",
      "Epoch:  31  Training Loss:  34.8685  Training Accuracy:  0.20341\n",
      "Epoch:  32  Training Loss:  35.2591  Training Accuracy:  0.209818\n",
      "Epoch:  33  Training Loss:  35.6659  Training Accuracy:  0.216814\n",
      "Epoch:  34  Training Loss:  36.1352  Training Accuracy:  0.222163\n",
      "Epoch:  35  Training Loss:  36.6564  Training Accuracy:  0.228277\n",
      "Epoch:  36  Training Loss:  37.2301  Training Accuracy:  0.233804\n",
      "Epoch:  37  Training Loss:  37.8466  Training Accuracy:  0.239565\n",
      "Epoch:  38  Training Loss:  38.4458  Training Accuracy:  0.244856\n",
      "Epoch:  39  Training Loss:  39.0663  Training Accuracy:  0.249441\n",
      "Epoch:  40  Training Loss:  39.6797  Training Accuracy:  0.254674\n",
      "Epoch:  41  Training Loss:  40.3313  Training Accuracy:  0.259142\n",
      "Epoch:  42  Training Loss:  41.0001  Training Accuracy:  0.264491\n",
      "Epoch:  43  Training Loss:  41.6727  Training Accuracy:  0.269136\n",
      "Epoch:  44  Training Loss:  42.3541  Training Accuracy:  0.274427\n",
      "Epoch:  45  Training Loss:  43.0483  Training Accuracy:  0.279541\n",
      "Epoch:  46  Training Loss:  43.7607  Training Accuracy:  0.284597\n",
      "Epoch:  47  Training Loss:  44.383  Training Accuracy:  0.289535\n",
      "Epoch:  48  Training Loss:  44.9137  Training Accuracy:  0.293416\n",
      "Epoch:  49  Training Loss:  45.5029  Training Accuracy:  0.297884\n",
      "Epoch:  50  Training Loss:  46.1769  Training Accuracy:  0.302528\n",
      "Epoch:  51  Training Loss:  46.9586  Training Accuracy:  0.307231\n",
      "Epoch:  52  Training Loss:  47.5995  Training Accuracy:  0.310464\n",
      "Epoch:  53  Training Loss:  48.2374  Training Accuracy:  0.313992\n",
      "Epoch:  54  Training Loss:  48.9485  Training Accuracy:  0.317989\n",
      "Epoch:  55  Training Loss:  49.5754  Training Accuracy:  0.321811\n",
      "Epoch:  56  Training Loss:  50.1923  Training Accuracy:  0.325044\n",
      "Epoch:  57  Training Loss:  50.6858  Training Accuracy:  0.32816\n",
      "Epoch:  58  Training Loss:  51.1279  Training Accuracy:  0.332216\n",
      "Epoch:  59  Training Loss:  51.6522  Training Accuracy:  0.335332\n",
      "Epoch:  60  Training Loss:  52.0921  Training Accuracy:  0.338213\n",
      "Epoch:  61  Training Loss:  52.5145  Training Accuracy:  0.341505\n",
      "Epoch:  62  Training Loss:  52.9179  Training Accuracy:  0.344797\n",
      "Epoch:  63  Training Loss:  53.3314  Training Accuracy:  0.347795\n",
      "Epoch:  64  Training Loss:  53.689  Training Accuracy:  0.35097\n",
      "Epoch:  65  Training Loss:  54.0436  Training Accuracy:  0.354791\n",
      "Epoch:  66  Training Loss:  54.4066  Training Accuracy:  0.358142\n",
      "Epoch:  67  Training Loss:  54.7329  Training Accuracy:  0.36067\n",
      "Epoch:  68  Training Loss:  54.979  Training Accuracy:  0.364197\n",
      "Epoch:  69  Training Loss:  55.1848  Training Accuracy:  0.36749\n",
      "Epoch:  70  Training Loss:  55.4048  Training Accuracy:  0.371428\n",
      "Epoch:  71  Training Loss:  55.6322  Training Accuracy:  0.37378\n",
      "Epoch:  72  Training Loss:  55.8959  Training Accuracy:  0.376778\n",
      "Epoch:  73  Training Loss:  56.1134  Training Accuracy:  0.379953\n",
      "Epoch:  74  Training Loss:  56.3509  Training Accuracy:  0.382775\n",
      "Epoch:  75  Training Loss:  56.514  Training Accuracy:  0.386067\n",
      "Epoch:  76  Training Loss:  56.6996  Training Accuracy:  0.38883\n",
      "Epoch:  77  Training Loss:  56.8174  Training Accuracy:  0.39224\n",
      "Epoch:  78  Training Loss:  57.0059  Training Accuracy:  0.395356\n",
      "Epoch:  79  Training Loss:  57.1847  Training Accuracy:  0.398707\n",
      "Epoch:  80  Training Loss:  57.3719  Training Accuracy:  0.401528\n",
      "Epoch:  81  Training Loss:  57.5656  Training Accuracy:  0.404997\n",
      "Epoch:  82  Training Loss:  57.7035  Training Accuracy:  0.407466\n",
      "Epoch:  83  Training Loss:  57.8666  Training Accuracy:  0.410699\n",
      "Epoch:  84  Training Loss:  57.9708  Training Accuracy:  0.413463\n",
      "Epoch:  85  Training Loss:  58.0581  Training Accuracy:  0.415873\n",
      "Epoch:  86  Training Loss:  58.2196  Training Accuracy:  0.418401\n",
      "Epoch:  87  Training Loss:  58.2617  Training Accuracy:  0.42087\n",
      "Epoch:  88  Training Loss:  58.4063  Training Accuracy:  0.423927\n",
      "Epoch:  89  Training Loss:  58.5085  Training Accuracy:  0.426572\n",
      "Epoch:  90  Training Loss:  58.6149  Training Accuracy:  0.429042\n",
      "Epoch:  91  Training Loss:  58.67  Training Accuracy:  0.43251\n",
      "Epoch:  92  Training Loss:  58.7207  Training Accuracy:  0.434803\n",
      "Epoch:  93  Training Loss:  58.6956  Training Accuracy:  0.437213\n",
      "Epoch:  94  Training Loss:  58.8124  Training Accuracy:  0.439447\n",
      "Epoch:  95  Training Loss:  58.8061  Training Accuracy:  0.441211\n",
      "Epoch:  96  Training Loss:  58.8681  Training Accuracy:  0.445267\n",
      "Epoch:  97  Training Loss:  58.9222  Training Accuracy:  0.448324\n",
      "Epoch:  98  Training Loss:  58.9452  Training Accuracy:  0.452146\n",
      "Epoch:  99  Training Loss:  58.9876  Training Accuracy:  0.456084\n",
      "Epoch:  100  Training Loss:  59.0714  Training Accuracy:  0.459024\n",
      "Epoch:  101  Training Loss:  59.0853  Training Accuracy:  0.461493\n",
      "Epoch:  102  Training Loss:  59.1473  Training Accuracy:  0.463962\n",
      "Epoch:  103  Training Loss:  59.1161  Training Accuracy:  0.466137\n",
      "Epoch:  104  Training Loss:  59.1698  Training Accuracy:  0.468548\n",
      "Epoch:  105  Training Loss:  59.1889  Training Accuracy:  0.471252\n",
      "Epoch:  106  Training Loss:  59.2234  Training Accuracy:  0.473839\n",
      "Epoch:  107  Training Loss:  59.255  Training Accuracy:  0.47619\n",
      "Epoch:  108  Training Loss:  59.3217  Training Accuracy:  0.478542\n",
      "Epoch:  109  Training Loss:  59.3275  Training Accuracy:  0.480835\n",
      "Epoch:  110  Training Loss:  59.3842  Training Accuracy:  0.483245\n",
      "Epoch:  111  Training Loss:  59.3733  Training Accuracy:  0.485126\n",
      "Epoch:  112  Training Loss:  59.4459  Training Accuracy:  0.487125\n",
      "Epoch:  113  Training Loss:  59.4641  Training Accuracy:  0.489947\n",
      "Epoch:  114  Training Loss:  59.5563  Training Accuracy:  0.492651\n",
      "Epoch:  115  Training Loss:  59.5525  Training Accuracy:  0.495532\n",
      "Epoch:  116  Training Loss:  59.6274  Training Accuracy:  0.496943\n",
      "Epoch:  117  Training Loss:  59.6116  Training Accuracy:  0.498824\n",
      "Epoch:  118  Training Loss:  59.6624  Training Accuracy:  0.500882\n",
      "Epoch:  119  Training Loss:  59.6297  Training Accuracy:  0.503527\n",
      "Epoch:  120  Training Loss:  59.6193  Training Accuracy:  0.505938\n",
      "Epoch:  121  Training Loss:  59.5496  Training Accuracy:  0.50823\n",
      "Epoch:  122  Training Loss:  59.5682  Training Accuracy:  0.51017\n",
      "Epoch:  123  Training Loss:  59.4882  Training Accuracy:  0.511522\n",
      "Epoch:  124  Training Loss:  59.4991  Training Accuracy:  0.513521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  59.461  Training Accuracy:  0.515226\n",
      "Epoch:  126  Training Loss:  59.442  Training Accuracy:  0.517401\n",
      "Epoch:  127  Training Loss:  59.376  Training Accuracy:  0.519753\n",
      "Epoch:  128  Training Loss:  59.3638  Training Accuracy:  0.521693\n",
      "Epoch:  129  Training Loss:  59.2831  Training Accuracy:  0.523751\n",
      "Epoch:  130  Training Loss:  59.2758  Training Accuracy:  0.525103\n",
      "Epoch:  131  Training Loss:  59.1384  Training Accuracy:  0.527043\n",
      "Epoch:  132  Training Loss:  59.1481  Training Accuracy:  0.528748\n",
      "Epoch:  133  Training Loss:  59.0565  Training Accuracy:  0.530394\n",
      "Epoch:  134  Training Loss:  59.0168  Training Accuracy:  0.532863\n",
      "Epoch:  135  Training Loss:  58.9062  Training Accuracy:  0.534568\n",
      "Epoch:  136  Training Loss:  58.8714  Training Accuracy:  0.535979\n",
      "Epoch:  137  Training Loss:  58.8074  Training Accuracy:  0.537625\n",
      "Epoch:  138  Training Loss:  58.7775  Training Accuracy:  0.539095\n",
      "Epoch:  139  Training Loss:  58.7052  Training Accuracy:  0.541387\n",
      "Epoch:  140  Training Loss:  58.6781  Training Accuracy:  0.543445\n",
      "Epoch:  141  Training Loss:  58.5886  Training Accuracy:  0.545326\n",
      "Epoch:  142  Training Loss:  58.5194  Training Accuracy:  0.54662\n",
      "Epoch:  143  Training Loss:  58.4044  Training Accuracy:  0.548854\n",
      "Epoch:  144  Training Loss:  58.3438  Training Accuracy:  0.550323\n",
      "Epoch:  145  Training Loss:  58.2663  Training Accuracy:  0.552616\n",
      "Epoch:  146  Training Loss:  58.2328  Training Accuracy:  0.554732\n",
      "Epoch:  147  Training Loss:  58.1384  Training Accuracy:  0.556672\n",
      "Epoch:  148  Training Loss:  58.0956  Training Accuracy:  0.558906\n",
      "Epoch:  149  Training Loss:  58.0073  Training Accuracy:  0.56114\n",
      "Epoch:  150  Training Loss:  57.9484  Training Accuracy:  0.563374\n",
      "Epoch:  151  Training Loss:  57.8784  Training Accuracy:  0.565726\n",
      "Epoch:  152  Training Loss:  57.8347  Training Accuracy:  0.567137\n",
      "Epoch:  153  Training Loss:  57.7141  Training Accuracy:  0.568607\n",
      "Epoch:  154  Training Loss:  57.6473  Training Accuracy:  0.57137\n",
      "Epoch:  155  Training Loss:  57.5272  Training Accuracy:  0.572898\n",
      "Epoch:  156  Training Loss:  57.5221  Training Accuracy:  0.574838\n",
      "Epoch:  157  Training Loss:  57.369  Training Accuracy:  0.57672\n",
      "Epoch:  158  Training Loss:  57.3586  Training Accuracy:  0.577895\n",
      "Epoch:  159  Training Loss:  57.305  Training Accuracy:  0.579424\n",
      "Epoch:  160  Training Loss:  57.2372  Training Accuracy:  0.58107\n",
      "Epoch:  161  Training Loss:  57.1369  Training Accuracy:  0.582716\n",
      "Epoch:  162  Training Loss:  57.0462  Training Accuracy:  0.584009\n",
      "Epoch:  163  Training Loss:  56.9746  Training Accuracy:  0.585538\n",
      "Epoch:  164  Training Loss:  56.9257  Training Accuracy:  0.586831\n",
      "Epoch:  165  Training Loss:  56.8124  Training Accuracy:  0.588419\n",
      "Epoch:  166  Training Loss:  56.6816  Training Accuracy:  0.590182\n",
      "Epoch:  167  Training Loss:  56.6789  Training Accuracy:  0.591476\n",
      "Epoch:  168  Training Loss:  56.5722  Training Accuracy:  0.592945\n",
      "Epoch:  169  Training Loss:  56.6165  Training Accuracy:  0.594121\n",
      "Epoch:  170  Training Loss:  56.441  Training Accuracy:  0.595885\n",
      "Epoch:  171  Training Loss:  56.4464  Training Accuracy:  0.597178\n",
      "Epoch:  172  Training Loss:  56.387  Training Accuracy:  0.599412\n",
      "Epoch:  173  Training Loss:  56.2942  Training Accuracy:  0.600882\n",
      "Epoch:  174  Training Loss:  56.2548  Training Accuracy:  0.602293\n",
      "Epoch:  175  Training Loss:  56.2032  Training Accuracy:  0.604174\n",
      "Epoch:  176  Training Loss:  56.1068  Training Accuracy:  0.605526\n",
      "Epoch:  177  Training Loss:  56.0843  Training Accuracy:  0.607466\n",
      "Epoch:  178  Training Loss:  55.9731  Training Accuracy:  0.609994\n",
      "Epoch:  179  Training Loss:  56.0041  Training Accuracy:  0.611111\n",
      "Epoch:  180  Training Loss:  55.8926  Training Accuracy:  0.612698\n",
      "Epoch:  181  Training Loss:  55.8941  Training Accuracy:  0.614051\n",
      "Epoch:  182  Training Loss:  55.7931  Training Accuracy:  0.615873\n",
      "Epoch:  183  Training Loss:  55.8074  Training Accuracy:  0.617049\n",
      "Epoch:  184  Training Loss:  55.6622  Training Accuracy:  0.618989\n",
      "Epoch:  185  Training Loss:  55.6874  Training Accuracy:  0.6204\n",
      "Epoch:  186  Training Loss:  55.5472  Training Accuracy:  0.621634\n",
      "Epoch:  187  Training Loss:  55.5581  Training Accuracy:  0.622928\n",
      "Epoch:  188  Training Loss:  55.4121  Training Accuracy:  0.624456\n",
      "Epoch:  189  Training Loss:  55.4248  Training Accuracy:  0.625338\n",
      "Epoch:  190  Training Loss:  55.3513  Training Accuracy:  0.627102\n",
      "Epoch:  191  Training Loss:  55.295  Training Accuracy:  0.628042\n",
      "Epoch:  192  Training Loss:  55.2549  Training Accuracy:  0.628983\n",
      "Epoch:  193  Training Loss:  55.1721  Training Accuracy:  0.630629\n",
      "Epoch:  194  Training Loss:  55.0243  Training Accuracy:  0.632099\n",
      "Epoch:  195  Training Loss:  55.0275  Training Accuracy:  0.633745\n",
      "Epoch:  196  Training Loss:  54.9355  Training Accuracy:  0.635391\n",
      "Epoch:  197  Training Loss:  54.9278  Training Accuracy:  0.637213\n",
      "Epoch:  198  Training Loss:  54.8588  Training Accuracy:  0.638566\n",
      "Epoch:  199  Training Loss:  54.824  Training Accuracy:  0.639506\n",
      "Epoch:  200  Training Loss:  54.7054  Training Accuracy:  0.640623\n",
      "Epoch:  201  Training Loss:  54.7353  Training Accuracy:  0.642857\n",
      "Epoch:  202  Training Loss:  54.6134  Training Accuracy:  0.644151\n",
      "Epoch:  203  Training Loss:  54.5587  Training Accuracy:  0.645209\n",
      "Epoch:  204  Training Loss:  54.4616  Training Accuracy:  0.646561\n",
      "Epoch:  205  Training Loss:  54.4654  Training Accuracy:  0.647737\n",
      "Epoch:  206  Training Loss:  54.4533  Training Accuracy:  0.649324\n",
      "Epoch:  207  Training Loss:  54.4247  Training Accuracy:  0.650676\n",
      "Epoch:  208  Training Loss:  54.3368  Training Accuracy:  0.652499\n",
      "Epoch:  209  Training Loss:  54.3559  Training Accuracy:  0.654145\n",
      "Epoch:  210  Training Loss:  54.1841  Training Accuracy:  0.655614\n",
      "Epoch:  211  Training Loss:  54.2185  Training Accuracy:  0.657025\n",
      "Epoch:  212  Training Loss:  54.1978  Training Accuracy:  0.658789\n",
      "Epoch:  213  Training Loss:  54.2186  Training Accuracy:  0.659671\n",
      "Epoch:  214  Training Loss:  54.1543  Training Accuracy:  0.661846\n",
      "Epoch:  215  Training Loss:  54.1844  Training Accuracy:  0.663022\n",
      "Epoch:  216  Training Loss:  54.0913  Training Accuracy:  0.663962\n",
      "Epoch:  217  Training Loss:  54.052  Training Accuracy:  0.665432\n",
      "Epoch:  218  Training Loss:  54.0325  Training Accuracy:  0.666432\n",
      "Epoch:  219  Training Loss:  53.9872  Training Accuracy:  0.667666\n",
      "Epoch:  220  Training Loss:  53.9611  Training Accuracy:  0.669724\n",
      "Epoch:  221  Training Loss:  54.0222  Training Accuracy:  0.67137\n",
      "Epoch:  222  Training Loss:  53.9305  Training Accuracy:  0.67284\n",
      "Epoch:  223  Training Loss:  53.8501  Training Accuracy:  0.673898\n",
      "Epoch:  224  Training Loss:  53.7978  Training Accuracy:  0.675426\n",
      "Epoch:  225  Training Loss:  53.8763  Training Accuracy:  0.676778\n",
      "Epoch:  226  Training Loss:  53.7705  Training Accuracy:  0.677543\n",
      "Epoch:  227  Training Loss:  53.7731  Training Accuracy:  0.67866\n",
      "Epoch:  228  Training Loss:  53.6879  Training Accuracy:  0.680012\n",
      "Epoch:  229  Training Loss:  53.6556  Training Accuracy:  0.681776\n",
      "Epoch:  230  Training Loss:  53.6089  Training Accuracy:  0.683422\n",
      "Epoch:  231  Training Loss:  53.6748  Training Accuracy:  0.684539\n",
      "Epoch:  232  Training Loss:  53.615  Training Accuracy:  0.685773\n",
      "Epoch:  233  Training Loss:  53.607  Training Accuracy:  0.687125\n",
      "Epoch:  234  Training Loss:  53.5777  Training Accuracy:  0.68789\n",
      "Epoch:  235  Training Loss:  53.6215  Training Accuracy:  0.689359\n",
      "Epoch:  236  Training Loss:  53.5489  Training Accuracy:  0.690947\n",
      "Epoch:  237  Training Loss:  53.5158  Training Accuracy:  0.692005\n",
      "Epoch:  238  Training Loss:  53.4919  Training Accuracy:  0.693416\n",
      "Epoch:  239  Training Loss:  53.6086  Training Accuracy:  0.694474\n",
      "Epoch:  240  Training Loss:  53.4388  Training Accuracy:  0.695473\n",
      "Epoch:  241  Training Loss:  53.5322  Training Accuracy:  0.697355\n",
      "Epoch:  242  Training Loss:  53.4189  Training Accuracy:  0.698883\n",
      "Epoch:  243  Training Loss:  53.4424  Training Accuracy:  0.699941\n",
      "Epoch:  244  Training Loss:  53.3718  Training Accuracy:  0.701176\n",
      "Epoch:  245  Training Loss:  53.3414  Training Accuracy:  0.702234\n",
      "Epoch:  246  Training Loss:  53.2923  Training Accuracy:  0.70294\n",
      "Epoch:  247  Training Loss:  53.351  Training Accuracy:  0.705174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  53.2594  Training Accuracy:  0.706467\n",
      "Epoch:  249  Training Loss:  53.2041  Training Accuracy:  0.70729\n",
      "Epoch:  250  Training Loss:  53.1179  Training Accuracy:  0.708466\n",
      "Epoch:  251  Training Loss:  53.2045  Training Accuracy:  0.709994\n",
      "Epoch:  252  Training Loss:  53.1018  Training Accuracy:  0.711405\n",
      "Epoch:  253  Training Loss:  53.1156  Training Accuracy:  0.712287\n",
      "Epoch:  254  Training Loss:  52.9894  Training Accuracy:  0.713404\n",
      "Epoch:  255  Training Loss:  53.0949  Training Accuracy:  0.714286\n",
      "Epoch:  256  Training Loss:  52.9864  Training Accuracy:  0.715932\n",
      "Epoch:  257  Training Loss:  53.003  Training Accuracy:  0.717637\n",
      "Epoch:  258  Training Loss:  52.9662  Training Accuracy:  0.718519\n",
      "Epoch:  259  Training Loss:  52.9738  Training Accuracy:  0.719636\n",
      "Epoch:  260  Training Loss:  52.9084  Training Accuracy:  0.720753\n",
      "Epoch:  261  Training Loss:  52.8822  Training Accuracy:  0.721928\n",
      "Epoch:  262  Training Loss:  52.8231  Training Accuracy:  0.723045\n",
      "Epoch:  263  Training Loss:  52.8758  Training Accuracy:  0.724162\n",
      "Epoch:  264  Training Loss:  52.7755  Training Accuracy:  0.725162\n",
      "Epoch:  265  Training Loss:  52.7412  Training Accuracy:  0.726455\n",
      "Epoch:  266  Training Loss:  52.7305  Training Accuracy:  0.728042\n",
      "Epoch:  267  Training Loss:  52.7192  Training Accuracy:  0.729512\n",
      "Epoch:  268  Training Loss:  52.6699  Training Accuracy:  0.730629\n",
      "Epoch:  269  Training Loss:  52.6816  Training Accuracy:  0.731629\n",
      "Epoch:  270  Training Loss:  52.601  Training Accuracy:  0.732275\n",
      "Epoch:  271  Training Loss:  52.6012  Training Accuracy:  0.733451\n",
      "Epoch:  272  Training Loss:  52.5301  Training Accuracy:  0.734862\n",
      "Epoch:  273  Training Loss:  52.5476  Training Accuracy:  0.736273\n",
      "Epoch:  274  Training Loss:  52.5112  Training Accuracy:  0.737331\n",
      "Epoch:  275  Training Loss:  52.4733  Training Accuracy:  0.738625\n",
      "Epoch:  276  Training Loss:  52.4193  Training Accuracy:  0.739506\n",
      "Epoch:  277  Training Loss:  52.4502  Training Accuracy:  0.740212\n",
      "Epoch:  278  Training Loss:  52.3255  Training Accuracy:  0.741388\n",
      "Epoch:  279  Training Loss:  52.325  Training Accuracy:  0.742681\n",
      "Epoch:  280  Training Loss:  52.2599  Training Accuracy:  0.743974\n",
      "Epoch:  281  Training Loss:  52.2037  Training Accuracy:  0.745033\n",
      "Epoch:  282  Training Loss:  52.1608  Training Accuracy:  0.746208\n",
      "Epoch:  283  Training Loss:  52.1307  Training Accuracy:  0.747208\n",
      "Epoch:  284  Training Loss:  52.0203  Training Accuracy:  0.747913\n",
      "Epoch:  285  Training Loss:  52.0433  Training Accuracy:  0.749383\n",
      "Epoch:  286  Training Loss:  51.9681  Training Accuracy:  0.750559\n",
      "Epoch:  287  Training Loss:  51.9441  Training Accuracy:  0.751617\n",
      "Epoch:  288  Training Loss:  51.9054  Training Accuracy:  0.752381\n",
      "Epoch:  289  Training Loss:  51.866  Training Accuracy:  0.753204\n",
      "Epoch:  290  Training Loss:  51.8432  Training Accuracy:  0.753792\n",
      "Epoch:  291  Training Loss:  51.8008  Training Accuracy:  0.755027\n",
      "Epoch:  292  Training Loss:  51.7868  Training Accuracy:  0.755556\n",
      "Epoch:  293  Training Loss:  51.7023  Training Accuracy:  0.756555\n",
      "Epoch:  294  Training Loss:  51.7459  Training Accuracy:  0.757672\n",
      "Epoch:  295  Training Loss:  51.6629  Training Accuracy:  0.759083\n",
      "Epoch:  296  Training Loss:  51.6775  Training Accuracy:  0.759495\n",
      "Epoch:  297  Training Loss:  51.5853  Training Accuracy:  0.760494\n",
      "Epoch:  298  Training Loss:  51.6139  Training Accuracy:  0.762199\n",
      "Epoch:  299  Training Loss:  51.4972  Training Accuracy:  0.76261\n",
      "Epoch:  300  Training Loss:  51.5154  Training Accuracy:  0.763551\n",
      "Epoch:  301  Training Loss:  51.4158  Training Accuracy:  0.765197\n",
      "Epoch:  302  Training Loss:  51.375  Training Accuracy:  0.766138\n",
      "Epoch:  303  Training Loss:  51.2825  Training Accuracy:  0.76749\n",
      "Epoch:  304  Training Loss:  51.2804  Training Accuracy:  0.768254\n",
      "Epoch:  305  Training Loss:  51.1937  Training Accuracy:  0.76896\n",
      "Epoch:  306  Training Loss:  51.1678  Training Accuracy:  0.769959\n",
      "Epoch:  307  Training Loss:  51.0947  Training Accuracy:  0.770723\n",
      "Epoch:  308  Training Loss:  51.0611  Training Accuracy:  0.771194\n",
      "Epoch:  309  Training Loss:  50.9803  Training Accuracy:  0.772311\n",
      "Epoch:  310  Training Loss:  50.9786  Training Accuracy:  0.773428\n",
      "Epoch:  311  Training Loss:  50.8964  Training Accuracy:  0.773957\n",
      "Epoch:  312  Training Loss:  50.8837  Training Accuracy:  0.774839\n",
      "Epoch:  313  Training Loss:  50.8044  Training Accuracy:  0.775897\n",
      "Epoch:  314  Training Loss:  50.7985  Training Accuracy:  0.776602\n",
      "Epoch:  315  Training Loss:  50.6936  Training Accuracy:  0.777484\n",
      "Epoch:  316  Training Loss:  50.7004  Training Accuracy:  0.778483\n",
      "Epoch:  317  Training Loss:  50.611  Training Accuracy:  0.779248\n",
      "Epoch:  318  Training Loss:  50.5657  Training Accuracy:  0.780012\n",
      "Epoch:  319  Training Loss:  50.5126  Training Accuracy:  0.7806\n",
      "Epoch:  320  Training Loss:  50.5117  Training Accuracy:  0.781717\n",
      "Epoch:  321  Training Loss:  50.4245  Training Accuracy:  0.782481\n",
      "Epoch:  322  Training Loss:  50.4246  Training Accuracy:  0.783304\n",
      "Epoch:  323  Training Loss:  50.3616  Training Accuracy:  0.784186\n",
      "Epoch:  324  Training Loss:  50.3898  Training Accuracy:  0.785068\n",
      "Epoch:  325  Training Loss:  50.3175  Training Accuracy:  0.785479\n",
      "Epoch:  326  Training Loss:  50.328  Training Accuracy:  0.78642\n",
      "Epoch:  327  Training Loss:  50.2855  Training Accuracy:  0.787419\n",
      "Epoch:  328  Training Loss:  50.3127  Training Accuracy:  0.788536\n",
      "Epoch:  329  Training Loss:  50.2902  Training Accuracy:  0.789712\n",
      "Epoch:  330  Training Loss:  50.2635  Training Accuracy:  0.790535\n",
      "Epoch:  331  Training Loss:  50.2671  Training Accuracy:  0.791535\n",
      "Epoch:  332  Training Loss:  50.2897  Training Accuracy:  0.792299\n",
      "Epoch:  333  Training Loss:  50.2348  Training Accuracy:  0.792887\n",
      "Epoch:  334  Training Loss:  50.2284  Training Accuracy:  0.793651\n",
      "Epoch:  335  Training Loss:  50.1534  Training Accuracy:  0.794415\n",
      "Epoch:  336  Training Loss:  50.1643  Training Accuracy:  0.794886\n",
      "Epoch:  337  Training Loss:  50.1584  Training Accuracy:  0.795474\n",
      "Epoch:  338  Training Loss:  50.112  Training Accuracy:  0.796473\n",
      "Epoch:  339  Training Loss:  50.1069  Training Accuracy:  0.797296\n",
      "Epoch:  340  Training Loss:  50.106  Training Accuracy:  0.798295\n",
      "Epoch:  341  Training Loss:  50.0399  Training Accuracy:  0.798883\n",
      "Epoch:  342  Training Loss:  50.0712  Training Accuracy:  0.799648\n",
      "Epoch:  343  Training Loss:  50.0289  Training Accuracy:  0.800588\n",
      "Epoch:  344  Training Loss:  50.0362  Training Accuracy:  0.801411\n",
      "Epoch:  345  Training Loss:  49.9836  Training Accuracy:  0.802175\n",
      "Epoch:  346  Training Loss:  50.0157  Training Accuracy:  0.803116\n",
      "Epoch:  347  Training Loss:  50.0066  Training Accuracy:  0.804174\n",
      "Epoch:  348  Training Loss:  50.0237  Training Accuracy:  0.804997\n",
      "Epoch:  349  Training Loss:  49.9861  Training Accuracy:  0.805879\n",
      "Epoch:  350  Training Loss:  49.991  Training Accuracy:  0.80682\n",
      "Epoch:  351  Training Loss:  49.9853  Training Accuracy:  0.807584\n",
      "Epoch:  352  Training Loss:  49.9837  Training Accuracy:  0.808466\n",
      "Epoch:  353  Training Loss:  49.9114  Training Accuracy:  0.809113\n",
      "Epoch:  354  Training Loss:  49.9063  Training Accuracy:  0.809759\n",
      "Epoch:  355  Training Loss:  49.9374  Training Accuracy:  0.810465\n",
      "Epoch:  356  Training Loss:  49.9168  Training Accuracy:  0.81117\n",
      "Epoch:  357  Training Loss:  49.9371  Training Accuracy:  0.811817\n",
      "Epoch:  358  Training Loss:  49.9211  Training Accuracy:  0.812464\n",
      "Epoch:  359  Training Loss:  49.9117  Training Accuracy:  0.813287\n",
      "Epoch:  360  Training Loss:  49.8876  Training Accuracy:  0.814404\n",
      "Epoch:  361  Training Loss:  49.8851  Training Accuracy:  0.815285\n",
      "Epoch:  362  Training Loss:  49.8526  Training Accuracy:  0.815815\n",
      "Epoch:  363  Training Loss:  49.8471  Training Accuracy:  0.81699\n",
      "Epoch:  364  Training Loss:  49.8096  Training Accuracy:  0.818049\n",
      "Epoch:  365  Training Loss:  49.8137  Training Accuracy:  0.818519\n",
      "Epoch:  366  Training Loss:  49.8094  Training Accuracy:  0.819048\n",
      "Epoch:  367  Training Loss:  49.7671  Training Accuracy:  0.820165\n",
      "Epoch:  368  Training Loss:  49.7261  Training Accuracy:  0.820341\n",
      "Epoch:  369  Training Loss:  49.6893  Training Accuracy:  0.821223\n",
      "Epoch:  370  Training Loss:  49.6667  Training Accuracy:  0.821282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  49.6706  Training Accuracy:  0.822105\n",
      "Epoch:  372  Training Loss:  49.642  Training Accuracy:  0.822693\n",
      "Epoch:  373  Training Loss:  49.6521  Training Accuracy:  0.823222\n",
      "Epoch:  374  Training Loss:  49.5958  Training Accuracy:  0.824104\n",
      "Epoch:  375  Training Loss:  49.5559  Training Accuracy:  0.825338\n",
      "Epoch:  376  Training Loss:  49.4912  Training Accuracy:  0.825574\n",
      "Epoch:  377  Training Loss:  49.5516  Training Accuracy:  0.826103\n",
      "Epoch:  378  Training Loss:  49.5041  Training Accuracy:  0.826749\n",
      "Epoch:  379  Training Loss:  49.5543  Training Accuracy:  0.827514\n",
      "Epoch:  380  Training Loss:  49.5001  Training Accuracy:  0.828101\n",
      "Epoch:  381  Training Loss:  49.4603  Training Accuracy:  0.828983\n",
      "Epoch:  382  Training Loss:  49.4168  Training Accuracy:  0.829571\n",
      "Epoch:  383  Training Loss:  49.4382  Training Accuracy:  0.830394\n",
      "Epoch:  384  Training Loss:  49.3717  Training Accuracy:  0.830571\n",
      "Epoch:  385  Training Loss:  49.438  Training Accuracy:  0.831217\n",
      "Epoch:  386  Training Loss:  49.3767  Training Accuracy:  0.831629\n",
      "Epoch:  387  Training Loss:  49.3711  Training Accuracy:  0.832452\n",
      "Epoch:  388  Training Loss:  49.2883  Training Accuracy:  0.83351\n",
      "Epoch:  389  Training Loss:  49.3342  Training Accuracy:  0.833922\n",
      "Epoch:  390  Training Loss:  49.2747  Training Accuracy:  0.834686\n",
      "Epoch:  391  Training Loss:  49.3157  Training Accuracy:  0.83545\n",
      "Epoch:  392  Training Loss:  49.2707  Training Accuracy:  0.83592\n",
      "Epoch:  393  Training Loss:  49.2427  Training Accuracy:  0.836685\n",
      "Epoch:  394  Training Loss:  49.2658  Training Accuracy:  0.837096\n",
      "Epoch:  395  Training Loss:  49.2084  Training Accuracy:  0.838037\n",
      "Epoch:  396  Training Loss:  49.1859  Training Accuracy:  0.838096\n",
      "Epoch:  397  Training Loss:  49.205  Training Accuracy:  0.838213\n",
      "Epoch:  398  Training Loss:  49.1896  Training Accuracy:  0.838625\n",
      "Epoch:  399  Training Loss:  49.214  Training Accuracy:  0.839213\n",
      "Epoch:  400  Training Loss:  49.1819  Training Accuracy:  0.839742\n",
      "Epoch:  401  Training Loss:  49.1927  Training Accuracy:  0.840094\n",
      "Epoch:  402  Training Loss:  49.0958  Training Accuracy:  0.840388\n",
      "Epoch:  403  Training Loss:  49.1182  Training Accuracy:  0.840741\n",
      "Epoch:  404  Training Loss:  49.1248  Training Accuracy:  0.841211\n",
      "Epoch:  405  Training Loss:  49.1352  Training Accuracy:  0.841917\n",
      "Epoch:  406  Training Loss:  49.0764  Training Accuracy:  0.843093\n",
      "Epoch:  407  Training Loss:  49.0525  Training Accuracy:  0.843034\n",
      "Epoch:  408  Training Loss:  49.0697  Training Accuracy:  0.843681\n",
      "Epoch:  409  Training Loss:  49.082  Training Accuracy:  0.843974\n",
      "Epoch:  410  Training Loss:  49.0564  Training Accuracy:  0.845091\n",
      "Epoch:  411  Training Loss:  49.0409  Training Accuracy:  0.845385\n",
      "Epoch:  412  Training Loss:  49.041  Training Accuracy:  0.845973\n",
      "Epoch:  413  Training Loss:  48.9806  Training Accuracy:  0.846679\n",
      "Epoch:  414  Training Loss:  48.9717  Training Accuracy:  0.847208\n",
      "Epoch:  415  Training Loss:  49.0179  Training Accuracy:  0.847678\n",
      "Epoch:  416  Training Loss:  48.9802  Training Accuracy:  0.84809\n",
      "Epoch:  417  Training Loss:  48.9428  Training Accuracy:  0.84903\n",
      "Epoch:  418  Training Loss:  48.9109  Training Accuracy:  0.849383\n",
      "Epoch:  419  Training Loss:  48.9427  Training Accuracy:  0.849795\n",
      "Epoch:  420  Training Loss:  48.9373  Training Accuracy:  0.849912\n",
      "Epoch:  421  Training Loss:  48.8996  Training Accuracy:  0.85097\n",
      "Epoch:  422  Training Loss:  48.8791  Training Accuracy:  0.851206\n",
      "Epoch:  423  Training Loss:  48.8849  Training Accuracy:  0.851441\n",
      "Epoch:  424  Training Loss:  48.9124  Training Accuracy:  0.851676\n",
      "Epoch:  425  Training Loss:  48.8494  Training Accuracy:  0.85391\n",
      "Epoch:  426  Training Loss:  48.8068  Training Accuracy:  0.853204\n",
      "Epoch:  427  Training Loss:  48.835  Training Accuracy:  0.853616\n",
      "Epoch:  428  Training Loss:  48.8901  Training Accuracy:  0.853498\n",
      "Epoch:  429  Training Loss:  48.862  Training Accuracy:  0.853557\n",
      "Epoch:  430  Training Loss:  48.8261  Training Accuracy:  0.856555\n",
      "Epoch:  431  Training Loss:  48.8152  Training Accuracy:  0.856673\n",
      "Epoch:  432  Training Loss:  48.816  Training Accuracy:  0.857202\n",
      "Epoch:  433  Training Loss:  48.8292  Training Accuracy:  0.857614\n",
      "Epoch:  434  Training Loss:  48.8537  Training Accuracy:  0.857731\n",
      "Epoch:  435  Training Loss:  48.7679  Training Accuracy:  0.859848\n",
      "Epoch:  436  Training Loss:  48.7424  Training Accuracy:  0.85973\n",
      "Epoch:  437  Training Loss:  48.7427  Training Accuracy:  0.860141\n",
      "Epoch:  438  Training Loss:  48.7682  Training Accuracy:  0.8602\n",
      "Epoch:  439  Training Loss:  48.7212  Training Accuracy:  0.861376\n",
      "Epoch:  440  Training Loss:  48.724  Training Accuracy:  0.861611\n",
      "Epoch:  441  Training Loss:  48.7348  Training Accuracy:  0.861964\n",
      "Epoch:  442  Training Loss:  48.7399  Training Accuracy:  0.862375\n",
      "Epoch:  443  Training Loss:  48.7153  Training Accuracy:  0.863198\n",
      "Epoch:  444  Training Loss:  48.6615  Training Accuracy:  0.86361\n",
      "Epoch:  445  Training Loss:  48.6826  Training Accuracy:  0.863904\n",
      "Epoch:  446  Training Loss:  48.7101  Training Accuracy:  0.864316\n",
      "Epoch:  447  Training Loss:  48.6617  Training Accuracy:  0.86508\n",
      "Epoch:  448  Training Loss:  48.6285  Training Accuracy:  0.865609\n",
      "Epoch:  449  Training Loss:  48.6415  Training Accuracy:  0.865844\n",
      "Epoch:  450  Training Loss:  48.6607  Training Accuracy:  0.86602\n",
      "Epoch:  451  Training Loss:  48.602  Training Accuracy:  0.866432\n",
      "Epoch:  452  Training Loss:  48.5462  Training Accuracy:  0.867314\n",
      "Epoch:  453  Training Loss:  48.5713  Training Accuracy:  0.867666\n",
      "Epoch:  454  Training Loss:  48.5516  Training Accuracy:  0.867902\n",
      "Epoch:  455  Training Loss:  48.459  Training Accuracy:  0.867549\n",
      "Epoch:  456  Training Loss:  48.424  Training Accuracy:  0.868842\n",
      "Epoch:  457  Training Loss:  48.4069  Training Accuracy:  0.869724\n",
      "Epoch:  458  Training Loss:  48.3714  Training Accuracy:  0.869548\n",
      "Epoch:  459  Training Loss:  48.3226  Training Accuracy:  0.870371\n",
      "Epoch:  460  Training Loss:  48.3429  Training Accuracy:  0.870841\n",
      "Epoch:  461  Training Loss:  48.2376  Training Accuracy:  0.870371\n",
      "Epoch:  462  Training Loss:  48.1842  Training Accuracy:  0.871311\n",
      "Epoch:  463  Training Loss:  48.2021  Training Accuracy:  0.872311\n",
      "Epoch:  464  Training Loss:  48.11  Training Accuracy:  0.871429\n",
      "Epoch:  465  Training Loss:  48.04  Training Accuracy:  0.872134\n",
      "Epoch:  466  Training Loss:  48.0945  Training Accuracy:  0.873134\n",
      "Epoch:  467  Training Loss:  47.9665  Training Accuracy:  0.872957\n",
      "Epoch:  468  Training Loss:  47.9494  Training Accuracy:  0.87331\n",
      "Epoch:  469  Training Loss:  47.9293  Training Accuracy:  0.874192\n",
      "Epoch:  470  Training Loss:  47.8237  Training Accuracy:  0.874074\n",
      "Epoch:  471  Training Loss:  47.7683  Training Accuracy:  0.874721\n",
      "Epoch:  472  Training Loss:  47.7604  Training Accuracy:  0.875309\n",
      "Epoch:  473  Training Loss:  47.6693  Training Accuracy:  0.874898\n",
      "Epoch:  474  Training Loss:  47.59  Training Accuracy:  0.875956\n",
      "Epoch:  475  Training Loss:  47.6064  Training Accuracy:  0.876426\n",
      "Epoch:  476  Training Loss:  47.5186  Training Accuracy:  0.876191\n",
      "Epoch:  477  Training Loss:  47.4454  Training Accuracy:  0.876661\n",
      "Epoch:  478  Training Loss:  47.4648  Training Accuracy:  0.877308\n",
      "Epoch:  479  Training Loss:  47.3685  Training Accuracy:  0.877132\n",
      "Epoch:  480  Training Loss:  47.3223  Training Accuracy:  0.877661\n",
      "Epoch:  481  Training Loss:  47.3037  Training Accuracy:  0.878542\n",
      "Epoch:  482  Training Loss:  47.2015  Training Accuracy:  0.878484\n",
      "Epoch:  483  Training Loss:  47.1394  Training Accuracy:  0.878954\n",
      "Epoch:  484  Training Loss:  47.1469  Training Accuracy:  0.879718\n",
      "Epoch:  485  Training Loss:  47.0671  Training Accuracy:  0.879777\n",
      "Epoch:  486  Training Loss:  46.9887  Training Accuracy:  0.880247\n",
      "Epoch:  487  Training Loss:  46.9865  Training Accuracy:  0.8806\n",
      "Epoch:  488  Training Loss:  46.8583  Training Accuracy:  0.880718\n",
      "Epoch:  489  Training Loss:  46.7851  Training Accuracy:  0.880835\n",
      "Epoch:  490  Training Loss:  46.6943  Training Accuracy:  0.881247\n",
      "Epoch:  491  Training Loss:  46.6204  Training Accuracy:  0.881658\n",
      "Epoch:  492  Training Loss:  46.5337  Training Accuracy:  0.882305\n",
      "Epoch:  493  Training Loss:  46.4706  Training Accuracy:  0.882775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  46.3554  Training Accuracy:  0.88301\n",
      "Epoch:  495  Training Loss:  46.2821  Training Accuracy:  0.883657\n",
      "Epoch:  496  Training Loss:  46.1896  Training Accuracy:  0.88354\n",
      "Epoch:  497  Training Loss:  46.1408  Training Accuracy:  0.884245\n",
      "Epoch:  498  Training Loss:  46.008  Training Accuracy:  0.884186\n",
      "Epoch:  499  Training Loss:  45.9296  Training Accuracy:  0.884892\n",
      "Testing Accuracy: 0.80698\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  14.6381  Training Accuracy:  0.0408583\n",
      "Epoch:  1  Training Loss:  23.345  Training Accuracy:  0.0430923\n",
      "Epoch:  2  Training Loss:  27.1044  Training Accuracy:  0.0495591\n",
      "Epoch:  3  Training Loss:  30.681  Training Accuracy:  0.0537331\n",
      "Epoch:  4  Training Loss:  34.3444  Training Accuracy:  0.0568489\n",
      "Epoch:  5  Training Loss:  36.8187  Training Accuracy:  0.060435\n",
      "Epoch:  6  Training Loss:  38.8072  Training Accuracy:  0.0659612\n",
      "Epoch:  7  Training Loss:  40.4415  Training Accuracy:  0.0717225\n",
      "Epoch:  8  Training Loss:  41.6326  Training Accuracy:  0.0801293\n",
      "Epoch:  9  Training Loss:  42.7347  Training Accuracy:  0.0920635\n",
      "Epoch:  10  Training Loss:  43.4051  Training Accuracy:  0.10341\n",
      "Epoch:  11  Training Loss:  43.8378  Training Accuracy:  0.113933\n",
      "Epoch:  12  Training Loss:  44.5414  Training Accuracy:  0.124633\n",
      "Epoch:  13  Training Loss:  45.213  Training Accuracy:  0.133392\n",
      "Epoch:  14  Training Loss:  45.9662  Training Accuracy:  0.141917\n",
      "Epoch:  15  Training Loss:  46.8266  Training Accuracy:  0.149971\n",
      "Epoch:  16  Training Loss:  47.7398  Training Accuracy:  0.156908\n",
      "Epoch:  17  Training Loss:  48.5024  Training Accuracy:  0.165432\n",
      "Epoch:  18  Training Loss:  49.1162  Training Accuracy:  0.170547\n",
      "Epoch:  19  Training Loss:  49.6787  Training Accuracy:  0.177366\n",
      "Epoch:  20  Training Loss:  50.156  Training Accuracy:  0.183304\n",
      "Epoch:  21  Training Loss:  50.6128  Training Accuracy:  0.1893\n",
      "Epoch:  22  Training Loss:  51.0121  Training Accuracy:  0.194533\n",
      "Epoch:  23  Training Loss:  51.4018  Training Accuracy:  0.200529\n",
      "Epoch:  24  Training Loss:  51.7463  Training Accuracy:  0.206232\n",
      "Epoch:  25  Training Loss:  52.1103  Training Accuracy:  0.212052\n",
      "Epoch:  26  Training Loss:  52.4378  Training Accuracy:  0.217813\n",
      "Epoch:  27  Training Loss:  52.7083  Training Accuracy:  0.224339\n",
      "Epoch:  28  Training Loss:  53.0084  Training Accuracy:  0.229277\n",
      "Epoch:  29  Training Loss:  53.3356  Training Accuracy:  0.234744\n",
      "Epoch:  30  Training Loss:  53.6701  Training Accuracy:  0.242093\n",
      "Epoch:  31  Training Loss:  54.0483  Training Accuracy:  0.248501\n",
      "Epoch:  32  Training Loss:  54.5347  Training Accuracy:  0.255144\n",
      "Epoch:  33  Training Loss:  54.8923  Training Accuracy:  0.258671\n",
      "Epoch:  34  Training Loss:  55.2571  Training Accuracy:  0.262904\n",
      "Epoch:  35  Training Loss:  55.6542  Training Accuracy:  0.268195\n",
      "Epoch:  36  Training Loss:  56.0171  Training Accuracy:  0.274368\n",
      "Epoch:  37  Training Loss:  56.4603  Training Accuracy:  0.280012\n",
      "Epoch:  38  Training Loss:  56.9394  Training Accuracy:  0.285714\n",
      "Epoch:  39  Training Loss:  57.4695  Training Accuracy:  0.291534\n",
      "Epoch:  40  Training Loss:  57.9626  Training Accuracy:  0.297002\n",
      "Epoch:  41  Training Loss:  58.4186  Training Accuracy:  0.302881\n",
      "Epoch:  42  Training Loss:  58.8845  Training Accuracy:  0.30923\n",
      "Epoch:  43  Training Loss:  59.4374  Training Accuracy:  0.315167\n",
      "Epoch:  44  Training Loss:  59.9514  Training Accuracy:  0.321164\n",
      "Epoch:  45  Training Loss:  60.5703  Training Accuracy:  0.32669\n",
      "Epoch:  46  Training Loss:  61.2402  Training Accuracy:  0.331217\n",
      "Epoch:  47  Training Loss:  61.9648  Training Accuracy:  0.335508\n",
      "Epoch:  48  Training Loss:  62.6933  Training Accuracy:  0.339447\n",
      "Epoch:  49  Training Loss:  63.4636  Training Accuracy:  0.343327\n",
      "Epoch:  50  Training Loss:  64.1674  Training Accuracy:  0.347207\n",
      "Epoch:  51  Training Loss:  64.9488  Training Accuracy:  0.352557\n",
      "Epoch:  52  Training Loss:  65.7011  Training Accuracy:  0.356555\n",
      "Epoch:  53  Training Loss:  66.4949  Training Accuracy:  0.360376\n",
      "Epoch:  54  Training Loss:  67.2193  Training Accuracy:  0.363551\n",
      "Epoch:  55  Training Loss:  67.9742  Training Accuracy:  0.36696\n",
      "Epoch:  56  Training Loss:  68.7054  Training Accuracy:  0.370605\n",
      "Epoch:  57  Training Loss:  69.512  Training Accuracy:  0.374485\n",
      "Epoch:  58  Training Loss:  70.2558  Training Accuracy:  0.377601\n",
      "Epoch:  59  Training Loss:  71.0979  Training Accuracy:  0.380717\n",
      "Epoch:  60  Training Loss:  71.8527  Training Accuracy:  0.384127\n",
      "Epoch:  61  Training Loss:  72.6999  Training Accuracy:  0.388536\n",
      "Epoch:  62  Training Loss:  73.5028  Training Accuracy:  0.393063\n",
      "Epoch:  63  Training Loss:  74.3537  Training Accuracy:  0.397472\n",
      "Epoch:  64  Training Loss:  75.129  Training Accuracy:  0.401352\n",
      "Epoch:  65  Training Loss:  75.9196  Training Accuracy:  0.404703\n",
      "Epoch:  66  Training Loss:  76.6696  Training Accuracy:  0.40923\n",
      "Epoch:  67  Training Loss:  77.3971  Training Accuracy:  0.412757\n",
      "Epoch:  68  Training Loss:  78.2028  Training Accuracy:  0.416284\n",
      "Epoch:  69  Training Loss:  78.9226  Training Accuracy:  0.419165\n",
      "Epoch:  70  Training Loss:  79.7649  Training Accuracy:  0.422751\n",
      "Epoch:  71  Training Loss:  80.4502  Training Accuracy:  0.425985\n",
      "Epoch:  72  Training Loss:  81.2999  Training Accuracy:  0.429218\n",
      "Epoch:  73  Training Loss:  81.968  Training Accuracy:  0.43251\n",
      "Epoch:  74  Training Loss:  82.4806  Training Accuracy:  0.43545\n",
      "Epoch:  75  Training Loss:  83.1471  Training Accuracy:  0.438742\n",
      "Epoch:  76  Training Loss:  83.804  Training Accuracy:  0.442563\n",
      "Epoch:  77  Training Loss:  84.3503  Training Accuracy:  0.445914\n",
      "Epoch:  78  Training Loss:  84.9818  Training Accuracy:  0.449618\n",
      "Epoch:  79  Training Loss:  85.4179  Training Accuracy:  0.453204\n",
      "Epoch:  80  Training Loss:  85.9854  Training Accuracy:  0.455849\n",
      "Epoch:  81  Training Loss:  86.3609  Training Accuracy:  0.459847\n",
      "Epoch:  82  Training Loss:  86.8541  Training Accuracy:  0.462669\n",
      "Epoch:  83  Training Loss:  87.1015  Training Accuracy:  0.465843\n",
      "Epoch:  84  Training Loss:  87.5784  Training Accuracy:  0.468901\n",
      "Epoch:  85  Training Loss:  88.0459  Training Accuracy:  0.472545\n",
      "Epoch:  86  Training Loss:  88.4379  Training Accuracy:  0.474486\n",
      "Epoch:  87  Training Loss:  88.708  Training Accuracy:  0.478072\n",
      "Epoch:  88  Training Loss:  89.1035  Training Accuracy:  0.480835\n",
      "Epoch:  89  Training Loss:  89.4265  Training Accuracy:  0.483539\n",
      "Epoch:  90  Training Loss:  89.5629  Training Accuracy:  0.48689\n",
      "Epoch:  91  Training Loss:  89.9332  Training Accuracy:  0.489653\n",
      "Epoch:  92  Training Loss:  90.2374  Training Accuracy:  0.493239\n",
      "Epoch:  93  Training Loss:  90.4018  Training Accuracy:  0.496296\n",
      "Epoch:  94  Training Loss:  90.7308  Training Accuracy:  0.499\n",
      "Epoch:  95  Training Loss:  91.0225  Training Accuracy:  0.501646\n",
      "Epoch:  96  Training Loss:  91.1125  Training Accuracy:  0.503998\n",
      "Epoch:  97  Training Loss:  91.3356  Training Accuracy:  0.506937\n",
      "Epoch:  98  Training Loss:  91.5131  Training Accuracy:  0.509524\n",
      "Epoch:  99  Training Loss:  91.494  Training Accuracy:  0.511699\n",
      "Epoch:  100  Training Loss:  91.7249  Training Accuracy:  0.514697\n",
      "Epoch:  101  Training Loss:  91.8278  Training Accuracy:  0.517343\n",
      "Epoch:  102  Training Loss:  91.915  Training Accuracy:  0.519165\n",
      "Epoch:  103  Training Loss:  91.9263  Training Accuracy:  0.521281\n",
      "Epoch:  104  Training Loss:  92.0428  Training Accuracy:  0.523515\n",
      "Epoch:  105  Training Loss:  92.107  Training Accuracy:  0.525279\n",
      "Epoch:  106  Training Loss:  92.1805  Training Accuracy:  0.527631\n",
      "Epoch:  107  Training Loss:  92.2506  Training Accuracy:  0.529923\n",
      "Epoch:  108  Training Loss:  92.2618  Training Accuracy:  0.532334\n",
      "Epoch:  109  Training Loss:  92.3485  Training Accuracy:  0.534215\n",
      "Epoch:  110  Training Loss:  92.2722  Training Accuracy:  0.536214\n",
      "Epoch:  111  Training Loss:  92.1631  Training Accuracy:  0.538448\n",
      "Epoch:  112  Training Loss:  92.2482  Training Accuracy:  0.541799\n",
      "Epoch:  113  Training Loss:  92.2085  Training Accuracy:  0.543563\n",
      "Epoch:  114  Training Loss:  91.9887  Training Accuracy:  0.545738\n",
      "Epoch:  115  Training Loss:  91.987  Training Accuracy:  0.547737\n",
      "Epoch:  116  Training Loss:  91.8736  Training Accuracy:  0.549735\n",
      "Epoch:  117  Training Loss:  91.8435  Training Accuracy:  0.551617\n",
      "Epoch:  118  Training Loss:  91.6432  Training Accuracy:  0.553674\n",
      "Epoch:  119  Training Loss:  91.564  Training Accuracy:  0.55532\n",
      "Epoch:  120  Training Loss:  91.3953  Training Accuracy:  0.557907\n",
      "Epoch:  121  Training Loss:  91.4229  Training Accuracy:  0.559612\n",
      "Epoch:  122  Training Loss:  91.2699  Training Accuracy:  0.561846\n",
      "Epoch:  123  Training Loss:  91.2574  Training Accuracy:  0.563433\n",
      "Epoch:  124  Training Loss:  91.0996  Training Accuracy:  0.565491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  91.1473  Training Accuracy:  0.567196\n",
      "Epoch:  126  Training Loss:  90.9861  Training Accuracy:  0.568783\n",
      "Epoch:  127  Training Loss:  91.0841  Training Accuracy:  0.570782\n",
      "Epoch:  128  Training Loss:  90.9053  Training Accuracy:  0.572781\n",
      "Epoch:  129  Training Loss:  90.9653  Training Accuracy:  0.57478\n",
      "Epoch:  130  Training Loss:  90.7914  Training Accuracy:  0.577484\n",
      "Epoch:  131  Training Loss:  90.9113  Training Accuracy:  0.579071\n",
      "Epoch:  132  Training Loss:  90.7377  Training Accuracy:  0.58107\n",
      "Epoch:  133  Training Loss:  90.8099  Training Accuracy:  0.583069\n",
      "Epoch:  134  Training Loss:  90.6028  Training Accuracy:  0.585949\n",
      "Epoch:  135  Training Loss:  90.7745  Training Accuracy:  0.587654\n",
      "Epoch:  136  Training Loss:  90.565  Training Accuracy:  0.5893\n",
      "Epoch:  137  Training Loss:  90.641  Training Accuracy:  0.590711\n",
      "Epoch:  138  Training Loss:  90.4961  Training Accuracy:  0.592299\n",
      "Epoch:  139  Training Loss:  90.5873  Training Accuracy:  0.593651\n",
      "Epoch:  140  Training Loss:  90.4368  Training Accuracy:  0.595708\n",
      "Epoch:  141  Training Loss:  90.5016  Training Accuracy:  0.597472\n",
      "Epoch:  142  Training Loss:  90.3279  Training Accuracy:  0.599118\n",
      "Epoch:  143  Training Loss:  90.4332  Training Accuracy:  0.600706\n",
      "Epoch:  144  Training Loss:  90.3235  Training Accuracy:  0.602293\n",
      "Epoch:  145  Training Loss:  90.4058  Training Accuracy:  0.603939\n",
      "Epoch:  146  Training Loss:  90.2824  Training Accuracy:  0.605409\n",
      "Epoch:  147  Training Loss:  90.3131  Training Accuracy:  0.606761\n",
      "Epoch:  148  Training Loss:  90.1984  Training Accuracy:  0.608172\n",
      "Epoch:  149  Training Loss:  90.2927  Training Accuracy:  0.610053\n",
      "Epoch:  150  Training Loss:  90.2317  Training Accuracy:  0.611346\n",
      "Epoch:  151  Training Loss:  90.3597  Training Accuracy:  0.613051\n",
      "Epoch:  152  Training Loss:  90.3273  Training Accuracy:  0.614345\n",
      "Epoch:  153  Training Loss:  90.3873  Training Accuracy:  0.616167\n",
      "Epoch:  154  Training Loss:  90.3842  Training Accuracy:  0.617754\n",
      "Epoch:  155  Training Loss:  90.4117  Training Accuracy:  0.619048\n",
      "Epoch:  156  Training Loss:  90.3841  Training Accuracy:  0.620635\n",
      "Epoch:  157  Training Loss:  90.3693  Training Accuracy:  0.62234\n",
      "Epoch:  158  Training Loss:  90.3506  Training Accuracy:  0.624574\n",
      "Epoch:  159  Training Loss:  90.3537  Training Accuracy:  0.625397\n",
      "Epoch:  160  Training Loss:  90.2641  Training Accuracy:  0.627043\n",
      "Epoch:  161  Training Loss:  90.2632  Training Accuracy:  0.628513\n",
      "Epoch:  162  Training Loss:  90.1178  Training Accuracy:  0.629924\n",
      "Epoch:  163  Training Loss:  90.0871  Training Accuracy:  0.631922\n",
      "Epoch:  164  Training Loss:  90.0876  Training Accuracy:  0.634098\n",
      "Epoch:  165  Training Loss:  90.0682  Training Accuracy:  0.635038\n",
      "Epoch:  166  Training Loss:  90.0167  Training Accuracy:  0.635979\n",
      "Epoch:  167  Training Loss:  90.0432  Training Accuracy:  0.637684\n",
      "Epoch:  168  Training Loss:  89.9244  Training Accuracy:  0.639036\n",
      "Epoch:  169  Training Loss:  90.014  Training Accuracy:  0.640741\n",
      "Epoch:  170  Training Loss:  89.9065  Training Accuracy:  0.642093\n",
      "Epoch:  171  Training Loss:  89.9312  Training Accuracy:  0.64368\n",
      "Epoch:  172  Training Loss:  89.9208  Training Accuracy:  0.64515\n",
      "Epoch:  173  Training Loss:  89.852  Training Accuracy:  0.646502\n",
      "Epoch:  174  Training Loss:  89.7902  Training Accuracy:  0.647795\n",
      "Epoch:  175  Training Loss:  89.7993  Training Accuracy:  0.6495\n",
      "Epoch:  176  Training Loss:  89.7829  Training Accuracy:  0.650735\n",
      "Epoch:  177  Training Loss:  89.6985  Training Accuracy:  0.652499\n",
      "Epoch:  178  Training Loss:  89.7113  Training Accuracy:  0.654145\n",
      "Epoch:  179  Training Loss:  89.6878  Training Accuracy:  0.655144\n",
      "Epoch:  180  Training Loss:  89.6595  Training Accuracy:  0.656379\n",
      "Epoch:  181  Training Loss:  89.5448  Training Accuracy:  0.658142\n",
      "Epoch:  182  Training Loss:  89.5598  Training Accuracy:  0.659318\n",
      "Epoch:  183  Training Loss:  89.5122  Training Accuracy:  0.66067\n",
      "Epoch:  184  Training Loss:  89.4696  Training Accuracy:  0.66167\n",
      "Epoch:  185  Training Loss:  89.4668  Training Accuracy:  0.662669\n",
      "Epoch:  186  Training Loss:  89.453  Training Accuracy:  0.663904\n",
      "Epoch:  187  Training Loss:  89.4358  Training Accuracy:  0.665021\n",
      "Epoch:  188  Training Loss:  89.3442  Training Accuracy:  0.666549\n",
      "Epoch:  189  Training Loss:  89.2173  Training Accuracy:  0.667607\n",
      "Epoch:  190  Training Loss:  89.2383  Training Accuracy:  0.669547\n",
      "Epoch:  191  Training Loss:  89.2256  Training Accuracy:  0.670664\n",
      "Epoch:  192  Training Loss:  89.2687  Training Accuracy:  0.672193\n",
      "Epoch:  193  Training Loss:  89.1605  Training Accuracy:  0.673192\n",
      "Epoch:  194  Training Loss:  89.1335  Training Accuracy:  0.674897\n",
      "Epoch:  195  Training Loss:  89.0523  Training Accuracy:  0.676426\n",
      "Epoch:  196  Training Loss:  88.9702  Training Accuracy:  0.677954\n",
      "Epoch:  197  Training Loss:  88.921  Training Accuracy:  0.679542\n",
      "Epoch:  198  Training Loss:  88.9147  Training Accuracy:  0.680659\n",
      "Epoch:  199  Training Loss:  88.8251  Training Accuracy:  0.681893\n",
      "Testing Accuracy: 0.660355\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 200\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  39.1417  Training Accuracy:  0.0391534\n",
      "Epoch:  1  Training Loss:  47.0061  Training Accuracy:  0.0466784\n",
      "Epoch:  2  Training Loss:  54.4406  Training Accuracy:  0.0470312\n",
      "Epoch:  3  Training Loss:  60.8398  Training Accuracy:  0.050147\n",
      "Epoch:  4  Training Loss:  67.7658  Training Accuracy:  0.0535567\n",
      "Epoch:  5  Training Loss:  73.8586  Training Accuracy:  0.0569665\n",
      "Epoch:  6  Training Loss:  78.777  Training Accuracy:  0.0610229\n",
      "Epoch:  7  Training Loss:  80.8754  Training Accuracy:  0.0638448\n",
      "Epoch:  8  Training Loss:  82.6952  Training Accuracy:  0.0674897\n",
      "Epoch:  9  Training Loss:  84.4109  Training Accuracy:  0.0707231\n",
      "Epoch:  10  Training Loss:  83.7744  Training Accuracy:  0.0756026\n",
      "Epoch:  11  Training Loss:  83.4048  Training Accuracy:  0.0813051\n",
      "Epoch:  12  Training Loss:  83.1021  Training Accuracy:  0.0870076\n",
      "Epoch:  13  Training Loss:  81.3249  Training Accuracy:  0.0929453\n",
      "Epoch:  14  Training Loss:  79.4958  Training Accuracy:  0.100176\n",
      "Epoch:  15  Training Loss:  77.7673  Training Accuracy:  0.107584\n",
      "Epoch:  16  Training Loss:  75.8482  Training Accuracy:  0.114991\n",
      "Epoch:  17  Training Loss:  73.9002  Training Accuracy:  0.123574\n",
      "Epoch:  18  Training Loss:  71.9077  Training Accuracy:  0.132628\n",
      "Epoch:  19  Training Loss:  69.9237  Training Accuracy:  0.140858\n",
      "Epoch:  20  Training Loss:  68.254  Training Accuracy:  0.149853\n",
      "Epoch:  21  Training Loss:  66.486  Training Accuracy:  0.15779\n",
      "Epoch:  22  Training Loss:  64.7848  Training Accuracy:  0.16555\n",
      "Epoch:  23  Training Loss:  63.1836  Training Accuracy:  0.173604\n",
      "Epoch:  24  Training Loss:  61.6671  Training Accuracy:  0.180717\n",
      "Epoch:  25  Training Loss:  60.3026  Training Accuracy:  0.186831\n",
      "Epoch:  26  Training Loss:  59.0813  Training Accuracy:  0.192357\n",
      "Epoch:  27  Training Loss:  58.0346  Training Accuracy:  0.198413\n",
      "Epoch:  28  Training Loss:  57.1443  Training Accuracy:  0.204585\n",
      "Epoch:  29  Training Loss:  56.5006  Training Accuracy:  0.211875\n",
      "Epoch:  30  Training Loss:  55.8712  Training Accuracy:  0.217166\n",
      "Epoch:  31  Training Loss:  55.3117  Training Accuracy:  0.223398\n",
      "Epoch:  32  Training Loss:  54.8185  Training Accuracy:  0.22963\n",
      "Epoch:  33  Training Loss:  54.3983  Training Accuracy:  0.235509\n",
      "Epoch:  34  Training Loss:  53.9779  Training Accuracy:  0.242328\n",
      "Epoch:  35  Training Loss:  53.6295  Training Accuracy:  0.248442\n",
      "Epoch:  36  Training Loss:  53.3465  Training Accuracy:  0.254968\n",
      "Epoch:  37  Training Loss:  53.1319  Training Accuracy:  0.260905\n",
      "Epoch:  38  Training Loss:  52.9944  Training Accuracy:  0.266784\n",
      "Epoch:  39  Training Loss:  52.8683  Training Accuracy:  0.272134\n",
      "Epoch:  40  Training Loss:  52.8015  Training Accuracy:  0.276778\n",
      "Epoch:  41  Training Loss:  52.8162  Training Accuracy:  0.28107\n",
      "Epoch:  42  Training Loss:  52.9238  Training Accuracy:  0.286478\n",
      "Epoch:  43  Training Loss:  53.0622  Training Accuracy:  0.291417\n",
      "Epoch:  44  Training Loss:  53.3033  Training Accuracy:  0.297002\n",
      "Epoch:  45  Training Loss:  53.5093  Training Accuracy:  0.302998\n",
      "Epoch:  46  Training Loss:  53.6736  Training Accuracy:  0.308936\n",
      "Epoch:  47  Training Loss:  53.8384  Training Accuracy:  0.314168\n",
      "Epoch:  48  Training Loss:  54.0327  Training Accuracy:  0.31846\n",
      "Epoch:  49  Training Loss:  54.3215  Training Accuracy:  0.321399\n",
      "Epoch:  50  Training Loss:  54.6075  Training Accuracy:  0.325808\n",
      "Epoch:  51  Training Loss:  54.9021  Training Accuracy:  0.329806\n",
      "Epoch:  52  Training Loss:  55.2347  Training Accuracy:  0.333745\n",
      "Epoch:  53  Training Loss:  55.5235  Training Accuracy:  0.339095\n",
      "Epoch:  54  Training Loss:  55.8473  Training Accuracy:  0.343739\n",
      "Epoch:  55  Training Loss:  56.1634  Training Accuracy:  0.348383\n",
      "Epoch:  56  Training Loss:  56.5495  Training Accuracy:  0.352028\n",
      "Epoch:  57  Training Loss:  56.912  Training Accuracy:  0.357084\n",
      "Epoch:  58  Training Loss:  57.3033  Training Accuracy:  0.362081\n",
      "Epoch:  59  Training Loss:  57.6535  Training Accuracy:  0.366079\n",
      "Epoch:  60  Training Loss:  57.9915  Training Accuracy:  0.370253\n",
      "Epoch:  61  Training Loss:  58.2834  Training Accuracy:  0.374427\n",
      "Epoch:  62  Training Loss:  58.5866  Training Accuracy:  0.378189\n",
      "Epoch:  63  Training Loss:  58.8418  Training Accuracy:  0.382422\n",
      "Epoch:  64  Training Loss:  59.0623  Training Accuracy:  0.38736\n",
      "Epoch:  65  Training Loss:  59.2696  Training Accuracy:  0.391475\n",
      "Epoch:  66  Training Loss:  59.5017  Training Accuracy:  0.395532\n",
      "Epoch:  67  Training Loss:  59.6857  Training Accuracy:  0.398824\n",
      "Epoch:  68  Training Loss:  59.8568  Training Accuracy:  0.402645\n",
      "Epoch:  69  Training Loss:  59.9976  Training Accuracy:  0.407231\n",
      "Epoch:  70  Training Loss:  60.171  Training Accuracy:  0.411052\n",
      "Epoch:  71  Training Loss:  60.3121  Training Accuracy:  0.414873\n",
      "Epoch:  72  Training Loss:  60.4267  Training Accuracy:  0.418812\n",
      "Epoch:  73  Training Loss:  60.5177  Training Accuracy:  0.42281\n",
      "Epoch:  74  Training Loss:  60.6123  Training Accuracy:  0.426455\n",
      "Epoch:  75  Training Loss:  60.6806  Training Accuracy:  0.430511\n",
      "Epoch:  76  Training Loss:  60.7181  Training Accuracy:  0.433803\n",
      "Epoch:  77  Training Loss:  60.7471  Training Accuracy:  0.437155\n",
      "Epoch:  78  Training Loss:  60.78  Training Accuracy:  0.440799\n",
      "Epoch:  79  Training Loss:  60.7856  Training Accuracy:  0.445032\n",
      "Epoch:  80  Training Loss:  60.754  Training Accuracy:  0.44903\n",
      "Epoch:  81  Training Loss:  60.7204  Training Accuracy:  0.452498\n",
      "Epoch:  82  Training Loss:  60.764  Training Accuracy:  0.45632\n",
      "Epoch:  83  Training Loss:  60.8042  Training Accuracy:  0.459965\n",
      "Epoch:  84  Training Loss:  60.8313  Training Accuracy:  0.463316\n",
      "Epoch:  85  Training Loss:  60.8701  Training Accuracy:  0.465961\n",
      "Epoch:  86  Training Loss:  60.8879  Training Accuracy:  0.468665\n",
      "Epoch:  87  Training Loss:  60.9274  Training Accuracy:  0.472428\n",
      "Epoch:  88  Training Loss:  60.9638  Training Accuracy:  0.47525\n",
      "Epoch:  89  Training Loss:  60.9882  Training Accuracy:  0.47913\n",
      "Epoch:  90  Training Loss:  61.0557  Training Accuracy:  0.48154\n",
      "Epoch:  91  Training Loss:  61.0683  Training Accuracy:  0.484597\n",
      "Epoch:  92  Training Loss:  61.0651  Training Accuracy:  0.488124\n",
      "Epoch:  93  Training Loss:  61.1131  Training Accuracy:  0.490711\n",
      "Epoch:  94  Training Loss:  61.1789  Training Accuracy:  0.493357\n",
      "Epoch:  95  Training Loss:  61.2388  Training Accuracy:  0.496649\n",
      "Epoch:  96  Training Loss:  61.2899  Training Accuracy:  0.49953\n",
      "Epoch:  97  Training Loss:  61.3514  Training Accuracy:  0.502763\n",
      "Epoch:  98  Training Loss:  61.4132  Training Accuracy:  0.505291\n",
      "Epoch:  99  Training Loss:  61.4856  Training Accuracy:  0.507819\n",
      "Epoch:  100  Training Loss:  61.4922  Training Accuracy:  0.510758\n",
      "Epoch:  101  Training Loss:  61.4929  Training Accuracy:  0.513227\n",
      "Epoch:  102  Training Loss:  61.4725  Training Accuracy:  0.516226\n",
      "Epoch:  103  Training Loss:  61.438  Training Accuracy:  0.519047\n",
      "Epoch:  104  Training Loss:  61.4059  Training Accuracy:  0.521575\n",
      "Epoch:  105  Training Loss:  61.3804  Training Accuracy:  0.523692\n",
      "Epoch:  106  Training Loss:  61.3213  Training Accuracy:  0.526396\n",
      "Epoch:  107  Training Loss:  61.2699  Training Accuracy:  0.528807\n",
      "Epoch:  108  Training Loss:  61.2104  Training Accuracy:  0.531452\n",
      "Epoch:  109  Training Loss:  61.2037  Training Accuracy:  0.534156\n",
      "Epoch:  110  Training Loss:  61.1833  Training Accuracy:  0.536861\n",
      "Epoch:  111  Training Loss:  61.171  Training Accuracy:  0.539212\n",
      "Epoch:  112  Training Loss:  61.1905  Training Accuracy:  0.541505\n",
      "Epoch:  113  Training Loss:  61.2034  Training Accuracy:  0.544092\n",
      "Epoch:  114  Training Loss:  61.1762  Training Accuracy:  0.546149\n",
      "Epoch:  115  Training Loss:  61.1232  Training Accuracy:  0.547972\n",
      "Epoch:  116  Training Loss:  61.0892  Training Accuracy:  0.550206\n",
      "Epoch:  117  Training Loss:  61.0607  Training Accuracy:  0.553086\n",
      "Epoch:  118  Training Loss:  61.0294  Training Accuracy:  0.55632\n",
      "Epoch:  119  Training Loss:  61.0081  Training Accuracy:  0.559142\n",
      "Epoch:  120  Training Loss:  60.9747  Training Accuracy:  0.56167\n",
      "Epoch:  121  Training Loss:  60.9951  Training Accuracy:  0.564491\n",
      "Epoch:  122  Training Loss:  60.9845  Training Accuracy:  0.566843\n",
      "Epoch:  123  Training Loss:  61.0062  Training Accuracy:  0.568959\n",
      "Epoch:  124  Training Loss:  60.9613  Training Accuracy:  0.571076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  60.9481  Training Accuracy:  0.573369\n",
      "Epoch:  126  Training Loss:  60.9062  Training Accuracy:  0.576249\n",
      "Epoch:  127  Training Loss:  60.9466  Training Accuracy:  0.57866\n",
      "Epoch:  128  Training Loss:  60.9587  Training Accuracy:  0.580423\n",
      "Epoch:  129  Training Loss:  60.9996  Training Accuracy:  0.582304\n",
      "Epoch:  130  Training Loss:  61.072  Training Accuracy:  0.583892\n",
      "Epoch:  131  Training Loss:  61.1072  Training Accuracy:  0.585597\n",
      "Epoch:  132  Training Loss:  61.2107  Training Accuracy:  0.587066\n",
      "Epoch:  133  Training Loss:  61.2475  Training Accuracy:  0.588654\n",
      "Epoch:  134  Training Loss:  61.3411  Training Accuracy:  0.58983\n",
      "Epoch:  135  Training Loss:  61.424  Training Accuracy:  0.59124\n",
      "Epoch:  136  Training Loss:  61.5192  Training Accuracy:  0.592593\n",
      "Epoch:  137  Training Loss:  61.6406  Training Accuracy:  0.594885\n",
      "Epoch:  138  Training Loss:  61.7887  Training Accuracy:  0.596237\n",
      "Epoch:  139  Training Loss:  61.9492  Training Accuracy:  0.59806\n",
      "Epoch:  140  Training Loss:  62.0938  Training Accuracy:  0.600353\n",
      "Epoch:  141  Training Loss:  62.2457  Training Accuracy:  0.602234\n",
      "Epoch:  142  Training Loss:  62.4409  Training Accuracy:  0.603821\n",
      "Epoch:  143  Training Loss:  62.608  Training Accuracy:  0.605115\n",
      "Epoch:  144  Training Loss:  62.7861  Training Accuracy:  0.606937\n",
      "Epoch:  145  Training Loss:  63.0015  Training Accuracy:  0.608936\n",
      "Epoch:  146  Training Loss:  63.2077  Training Accuracy:  0.610112\n",
      "Epoch:  147  Training Loss:  63.4245  Training Accuracy:  0.612346\n",
      "Epoch:  148  Training Loss:  63.6428  Training Accuracy:  0.614756\n",
      "Epoch:  149  Training Loss:  63.862  Training Accuracy:  0.616226\n",
      "Epoch:  150  Training Loss:  64.1405  Training Accuracy:  0.618342\n",
      "Epoch:  151  Training Loss:  64.3838  Training Accuracy:  0.620223\n",
      "Epoch:  152  Training Loss:  64.659  Training Accuracy:  0.62187\n",
      "Epoch:  153  Training Loss:  64.8932  Training Accuracy:  0.623163\n",
      "Epoch:  154  Training Loss:  65.1985  Training Accuracy:  0.624985\n",
      "Epoch:  155  Training Loss:  65.505  Training Accuracy:  0.625926\n",
      "Epoch:  156  Training Loss:  65.7638  Training Accuracy:  0.627748\n",
      "Epoch:  157  Training Loss:  66.066  Training Accuracy:  0.629571\n",
      "Epoch:  158  Training Loss:  66.2981  Training Accuracy:  0.631335\n",
      "Epoch:  159  Training Loss:  66.5355  Training Accuracy:  0.632334\n",
      "Epoch:  160  Training Loss:  66.8385  Training Accuracy:  0.634274\n",
      "Epoch:  161  Training Loss:  67.146  Training Accuracy:  0.635626\n",
      "Epoch:  162  Training Loss:  67.4664  Training Accuracy:  0.636684\n",
      "Epoch:  163  Training Loss:  67.7476  Training Accuracy:  0.638036\n",
      "Epoch:  164  Training Loss:  68.0362  Training Accuracy:  0.639565\n",
      "Epoch:  165  Training Loss:  68.3134  Training Accuracy:  0.641917\n",
      "Epoch:  166  Training Loss:  68.5674  Training Accuracy:  0.643269\n",
      "Epoch:  167  Training Loss:  68.7812  Training Accuracy:  0.645091\n",
      "Epoch:  168  Training Loss:  69.0153  Training Accuracy:  0.646914\n",
      "Epoch:  169  Training Loss:  69.2317  Training Accuracy:  0.648912\n",
      "Epoch:  170  Training Loss:  69.4854  Training Accuracy:  0.650382\n",
      "Epoch:  171  Training Loss:  69.7086  Training Accuracy:  0.651911\n",
      "Epoch:  172  Training Loss:  69.9613  Training Accuracy:  0.653145\n",
      "Epoch:  173  Training Loss:  70.2365  Training Accuracy:  0.654615\n",
      "Epoch:  174  Training Loss:  70.5208  Training Accuracy:  0.656261\n",
      "Epoch:  175  Training Loss:  70.76  Training Accuracy:  0.657907\n",
      "Epoch:  176  Training Loss:  71.0319  Training Accuracy:  0.659083\n",
      "Epoch:  177  Training Loss:  71.2684  Training Accuracy:  0.660318\n",
      "Epoch:  178  Training Loss:  71.5621  Training Accuracy:  0.661552\n",
      "Epoch:  179  Training Loss:  71.8214  Training Accuracy:  0.663139\n",
      "Epoch:  180  Training Loss:  72.089  Training Accuracy:  0.664727\n",
      "Epoch:  181  Training Loss:  72.2709  Training Accuracy:  0.666196\n",
      "Epoch:  182  Training Loss:  72.6006  Training Accuracy:  0.667901\n",
      "Epoch:  183  Training Loss:  72.7599  Training Accuracy:  0.669136\n",
      "Epoch:  184  Training Loss:  73.0502  Training Accuracy:  0.67037\n",
      "Epoch:  185  Training Loss:  73.2376  Training Accuracy:  0.671605\n",
      "Epoch:  186  Training Loss:  73.5262  Training Accuracy:  0.673075\n",
      "Epoch:  187  Training Loss:  73.7128  Training Accuracy:  0.674603\n",
      "Epoch:  188  Training Loss:  74.0101  Training Accuracy:  0.676191\n",
      "Epoch:  189  Training Loss:  74.1957  Training Accuracy:  0.677308\n",
      "Epoch:  190  Training Loss:  74.4335  Training Accuracy:  0.678895\n",
      "Epoch:  191  Training Loss:  74.5803  Training Accuracy:  0.680541\n",
      "Epoch:  192  Training Loss:  74.8432  Training Accuracy:  0.681776\n",
      "Epoch:  193  Training Loss:  74.9655  Training Accuracy:  0.683892\n",
      "Epoch:  194  Training Loss:  75.2389  Training Accuracy:  0.685185\n",
      "Epoch:  195  Training Loss:  75.2792  Training Accuracy:  0.686655\n",
      "Epoch:  196  Training Loss:  75.663  Training Accuracy:  0.688242\n",
      "Epoch:  197  Training Loss:  75.6163  Training Accuracy:  0.689536\n",
      "Epoch:  198  Training Loss:  75.9606  Training Accuracy:  0.690829\n",
      "Epoch:  199  Training Loss:  75.9021  Training Accuracy:  0.692299\n",
      "Epoch:  200  Training Loss:  76.2262  Training Accuracy:  0.693886\n",
      "Epoch:  201  Training Loss:  76.1539  Training Accuracy:  0.695885\n",
      "Epoch:  202  Training Loss:  76.5207  Training Accuracy:  0.697002\n",
      "Epoch:  203  Training Loss:  76.4001  Training Accuracy:  0.697884\n",
      "Epoch:  204  Training Loss:  76.7386  Training Accuracy:  0.698824\n",
      "Epoch:  205  Training Loss:  76.5278  Training Accuracy:  0.699883\n",
      "Epoch:  206  Training Loss:  76.8367  Training Accuracy:  0.701411\n",
      "Epoch:  207  Training Loss:  76.7273  Training Accuracy:  0.702822\n",
      "Epoch:  208  Training Loss:  77.0192  Training Accuracy:  0.70435\n",
      "Epoch:  209  Training Loss:  76.9663  Training Accuracy:  0.705938\n",
      "Epoch:  210  Training Loss:  77.2502  Training Accuracy:  0.706643\n",
      "Epoch:  211  Training Loss:  77.2032  Training Accuracy:  0.70776\n",
      "Epoch:  212  Training Loss:  77.4249  Training Accuracy:  0.709171\n",
      "Epoch:  213  Training Loss:  77.4209  Training Accuracy:  0.710817\n",
      "Epoch:  214  Training Loss:  77.7011  Training Accuracy:  0.711582\n",
      "Epoch:  215  Training Loss:  77.5861  Training Accuracy:  0.713287\n",
      "Epoch:  216  Training Loss:  77.8297  Training Accuracy:  0.713992\n",
      "Epoch:  217  Training Loss:  77.6807  Training Accuracy:  0.715756\n",
      "Epoch:  218  Training Loss:  77.8836  Training Accuracy:  0.71699\n",
      "Epoch:  219  Training Loss:  77.7896  Training Accuracy:  0.718519\n",
      "Epoch:  220  Training Loss:  77.9969  Training Accuracy:  0.719694\n",
      "Epoch:  221  Training Loss:  77.8255  Training Accuracy:  0.721517\n",
      "Epoch:  222  Training Loss:  78.0057  Training Accuracy:  0.722693\n",
      "Epoch:  223  Training Loss:  77.931  Training Accuracy:  0.724162\n",
      "Epoch:  224  Training Loss:  78.2124  Training Accuracy:  0.725573\n",
      "Epoch:  225  Training Loss:  77.9428  Training Accuracy:  0.727572\n",
      "Epoch:  226  Training Loss:  78.2016  Training Accuracy:  0.729159\n",
      "Epoch:  227  Training Loss:  78.0391  Training Accuracy:  0.73057\n",
      "Epoch:  228  Training Loss:  78.2941  Training Accuracy:  0.731805\n",
      "Epoch:  229  Training Loss:  78.0791  Training Accuracy:  0.73304\n",
      "Epoch:  230  Training Loss:  78.3302  Training Accuracy:  0.734627\n",
      "Epoch:  231  Training Loss:  78.0793  Training Accuracy:  0.735861\n",
      "Epoch:  232  Training Loss:  78.3131  Training Accuracy:  0.736861\n",
      "Epoch:  233  Training Loss:  78.0367  Training Accuracy:  0.738625\n",
      "Epoch:  234  Training Loss:  78.2401  Training Accuracy:  0.7398\n",
      "Epoch:  235  Training Loss:  77.9816  Training Accuracy:  0.740388\n",
      "Epoch:  236  Training Loss:  78.1757  Training Accuracy:  0.741094\n",
      "Epoch:  237  Training Loss:  77.8849  Training Accuracy:  0.742211\n",
      "Epoch:  238  Training Loss:  78.0787  Training Accuracy:  0.743151\n",
      "Epoch:  239  Training Loss:  77.7715  Training Accuracy:  0.744445\n",
      "Epoch:  240  Training Loss:  77.9913  Training Accuracy:  0.745444\n",
      "Epoch:  241  Training Loss:  77.7191  Training Accuracy:  0.747149\n",
      "Epoch:  242  Training Loss:  77.922  Training Accuracy:  0.748442\n",
      "Epoch:  243  Training Loss:  77.6507  Training Accuracy:  0.750441\n",
      "Epoch:  244  Training Loss:  77.8422  Training Accuracy:  0.751852\n",
      "Epoch:  245  Training Loss:  77.6733  Training Accuracy:  0.753146\n",
      "Epoch:  246  Training Loss:  77.8491  Training Accuracy:  0.753969\n",
      "Epoch:  247  Training Loss:  77.4854  Training Accuracy:  0.755085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  77.7314  Training Accuracy:  0.756261\n",
      "Epoch:  249  Training Loss:  77.3733  Training Accuracy:  0.757319\n",
      "Epoch:  250  Training Loss:  77.6209  Training Accuracy:  0.758143\n",
      "Epoch:  251  Training Loss:  77.2558  Training Accuracy:  0.759318\n",
      "Epoch:  252  Training Loss:  77.3951  Training Accuracy:  0.760141\n",
      "Epoch:  253  Training Loss:  77.4804  Training Accuracy:  0.760964\n",
      "Epoch:  254  Training Loss:  77.0626  Training Accuracy:  0.762199\n",
      "Epoch:  255  Training Loss:  77.2348  Training Accuracy:  0.763434\n",
      "Epoch:  256  Training Loss:  76.8398  Training Accuracy:  0.764492\n",
      "Epoch:  257  Training Loss:  76.8865  Training Accuracy:  0.765138\n",
      "Epoch:  258  Training Loss:  76.9728  Training Accuracy:  0.765785\n",
      "Epoch:  259  Training Loss:  76.5207  Training Accuracy:  0.767314\n",
      "Epoch:  260  Training Loss:  76.6211  Training Accuracy:  0.768078\n",
      "Epoch:  261  Training Loss:  76.5101  Training Accuracy:  0.769195\n",
      "Epoch:  262  Training Loss:  76.1513  Training Accuracy:  0.769665\n",
      "Epoch:  263  Training Loss:  76.1537  Training Accuracy:  0.770135\n",
      "Epoch:  264  Training Loss:  76.2457  Training Accuracy:  0.770606\n",
      "Epoch:  265  Training Loss:  75.7227  Training Accuracy:  0.772075\n",
      "Epoch:  266  Training Loss:  75.8014  Training Accuracy:  0.773134\n",
      "Epoch:  267  Training Loss:  75.5081  Training Accuracy:  0.774251\n",
      "Epoch:  268  Training Loss:  75.5687  Training Accuracy:  0.775015\n",
      "Epoch:  269  Training Loss:  75.0974  Training Accuracy:  0.776191\n",
      "Epoch:  270  Training Loss:  75.23  Training Accuracy:  0.777073\n",
      "Epoch:  271  Training Loss:  74.8096  Training Accuracy:  0.777896\n",
      "Epoch:  272  Training Loss:  74.8983  Training Accuracy:  0.778719\n",
      "Epoch:  273  Training Loss:  74.8012  Training Accuracy:  0.779659\n",
      "Epoch:  274  Training Loss:  74.3634  Training Accuracy:  0.780776\n",
      "Epoch:  275  Training Loss:  74.5024  Training Accuracy:  0.781305\n",
      "Epoch:  276  Training Loss:  74.0638  Training Accuracy:  0.782716\n",
      "Epoch:  277  Training Loss:  74.1851  Training Accuracy:  0.783304\n",
      "Epoch:  278  Training Loss:  73.9205  Training Accuracy:  0.784362\n",
      "Epoch:  279  Training Loss:  73.9896  Training Accuracy:  0.784892\n",
      "Epoch:  280  Training Loss:  73.5411  Training Accuracy:  0.786008\n",
      "Epoch:  281  Training Loss:  73.7013  Training Accuracy:  0.786655\n",
      "Epoch:  282  Training Loss:  73.2981  Training Accuracy:  0.787537\n",
      "Epoch:  283  Training Loss:  73.3298  Training Accuracy:  0.788184\n",
      "Epoch:  284  Training Loss:  73.3986  Training Accuracy:  0.789007\n",
      "Epoch:  285  Training Loss:  72.9145  Training Accuracy:  0.790535\n",
      "Epoch:  286  Training Loss:  73.003  Training Accuracy:  0.791241\n",
      "Epoch:  287  Training Loss:  72.6543  Training Accuracy:  0.792123\n",
      "Epoch:  288  Training Loss:  72.6785  Training Accuracy:  0.792475\n",
      "Epoch:  289  Training Loss:  72.6179  Training Accuracy:  0.793298\n",
      "Epoch:  290  Training Loss:  72.2358  Training Accuracy:  0.794709\n",
      "Epoch:  291  Training Loss:  72.2699  Training Accuracy:  0.795003\n",
      "Epoch:  292  Training Loss:  71.9655  Training Accuracy:  0.796885\n",
      "Epoch:  293  Training Loss:  71.9718  Training Accuracy:  0.797002\n",
      "Epoch:  294  Training Loss:  71.587  Training Accuracy:  0.798119\n",
      "Epoch:  295  Training Loss:  71.6885  Training Accuracy:  0.798001\n",
      "Epoch:  296  Training Loss:  71.2707  Training Accuracy:  0.799118\n",
      "Epoch:  297  Training Loss:  71.3226  Training Accuracy:  0.8\n",
      "Epoch:  298  Training Loss:  71.0237  Training Accuracy:  0.800706\n",
      "Epoch:  299  Training Loss:  71.0195  Training Accuracy:  0.80147\n",
      "Testing Accuracy: 0.754633\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  17.9125  Training Accuracy:  0.0409759\n",
      "Epoch:  1  Training Loss:  31.2156  Training Accuracy:  0.0410347\n",
      "Epoch:  2  Training Loss:  41.4486  Training Accuracy:  0.0414462\n",
      "Epoch:  3  Training Loss:  52.2145  Training Accuracy:  0.0442681\n",
      "Epoch:  4  Training Loss:  58.19  Training Accuracy:  0.0496179\n",
      "Epoch:  5  Training Loss:  62.3194  Training Accuracy:  0.0579071\n",
      "Epoch:  6  Training Loss:  66.1681  Training Accuracy:  0.0662551\n",
      "Epoch:  7  Training Loss:  68.3259  Training Accuracy:  0.0725456\n",
      "Epoch:  8  Training Loss:  68.2633  Training Accuracy:  0.0779541\n",
      "Epoch:  9  Training Loss:  69.3013  Training Accuracy:  0.0845973\n",
      "Epoch:  10  Training Loss:  70.3947  Training Accuracy:  0.0898883\n",
      "Epoch:  11  Training Loss:  70.8521  Training Accuracy:  0.0948266\n",
      "Epoch:  12  Training Loss:  71.5679  Training Accuracy:  0.099177\n",
      "Epoch:  13  Training Loss:  72.1913  Training Accuracy:  0.107113\n",
      "Epoch:  14  Training Loss:  73.014  Training Accuracy:  0.115873\n",
      "Epoch:  15  Training Loss:  72.2183  Training Accuracy:  0.123574\n",
      "Epoch:  16  Training Loss:  71.5228  Training Accuracy:  0.133451\n",
      "Epoch:  17  Training Loss:  71.0942  Training Accuracy:  0.143621\n",
      "Epoch:  18  Training Loss:  70.2899  Training Accuracy:  0.151911\n",
      "Epoch:  19  Training Loss:  69.7522  Training Accuracy:  0.158613\n",
      "Epoch:  20  Training Loss:  68.8582  Training Accuracy:  0.16649\n",
      "Epoch:  21  Training Loss:  67.9908  Training Accuracy:  0.173486\n",
      "Epoch:  22  Training Loss:  67.3287  Training Accuracy:  0.180658\n",
      "Epoch:  23  Training Loss:  66.3213  Training Accuracy:  0.188477\n",
      "Epoch:  24  Training Loss:  65.3828  Training Accuracy:  0.19512\n",
      "Epoch:  25  Training Loss:  64.5915  Training Accuracy:  0.202234\n",
      "Epoch:  26  Training Loss:  63.9045  Training Accuracy:  0.208583\n",
      "Epoch:  27  Training Loss:  63.2973  Training Accuracy:  0.214932\n",
      "Epoch:  28  Training Loss:  62.7339  Training Accuracy:  0.221811\n",
      "Epoch:  29  Training Loss:  62.1869  Training Accuracy:  0.228454\n",
      "Epoch:  30  Training Loss:  61.6513  Training Accuracy:  0.235508\n",
      "Epoch:  31  Training Loss:  61.1226  Training Accuracy:  0.24274\n",
      "Epoch:  32  Training Loss:  60.6683  Training Accuracy:  0.249089\n",
      "Epoch:  33  Training Loss:  60.1948  Training Accuracy:  0.254615\n",
      "Epoch:  34  Training Loss:  59.7215  Training Accuracy:  0.262199\n",
      "Epoch:  35  Training Loss:  59.294  Training Accuracy:  0.269253\n",
      "Epoch:  36  Training Loss:  58.995  Training Accuracy:  0.276778\n",
      "Epoch:  37  Training Loss:  58.5941  Training Accuracy:  0.283069\n",
      "Epoch:  38  Training Loss:  58.3359  Training Accuracy:  0.290417\n",
      "Epoch:  39  Training Loss:  58.2109  Training Accuracy:  0.296296\n",
      "Epoch:  40  Training Loss:  58.1013  Training Accuracy:  0.303351\n",
      "Epoch:  41  Training Loss:  58.1031  Training Accuracy:  0.308172\n",
      "Epoch:  42  Training Loss:  58.0366  Training Accuracy:  0.31164\n",
      "Epoch:  43  Training Loss:  57.9855  Training Accuracy:  0.316284\n",
      "Epoch:  44  Training Loss:  57.9573  Training Accuracy:  0.320635\n",
      "Epoch:  45  Training Loss:  57.9756  Training Accuracy:  0.325456\n",
      "Epoch:  46  Training Loss:  58.0524  Training Accuracy:  0.329923\n",
      "Epoch:  47  Training Loss:  58.147  Training Accuracy:  0.334391\n",
      "Epoch:  48  Training Loss:  58.253  Training Accuracy:  0.339095\n",
      "Epoch:  49  Training Loss:  58.3784  Training Accuracy:  0.34368\n",
      "Epoch:  50  Training Loss:  58.5333  Training Accuracy:  0.348089\n",
      "Epoch:  51  Training Loss:  58.6609  Training Accuracy:  0.352498\n",
      "Epoch:  52  Training Loss:  58.7181  Training Accuracy:  0.356555\n",
      "Epoch:  53  Training Loss:  58.859  Training Accuracy:  0.360905\n",
      "Epoch:  54  Training Loss:  59.3063  Training Accuracy:  0.365844\n",
      "Epoch:  55  Training Loss:  59.5701  Training Accuracy:  0.371193\n",
      "Epoch:  56  Training Loss:  59.8387  Training Accuracy:  0.375485\n",
      "Epoch:  57  Training Loss:  60.0868  Training Accuracy:  0.379012\n",
      "Epoch:  58  Training Loss:  60.3288  Training Accuracy:  0.382657\n",
      "Epoch:  59  Training Loss:  60.5387  Training Accuracy:  0.386478\n",
      "Epoch:  60  Training Loss:  60.7131  Training Accuracy:  0.390829\n",
      "Epoch:  61  Training Loss:  60.8695  Training Accuracy:  0.394768\n",
      "Epoch:  62  Training Loss:  61.0273  Training Accuracy:  0.398883\n",
      "Epoch:  63  Training Loss:  61.1922  Training Accuracy:  0.402822\n",
      "Epoch:  64  Training Loss:  61.3753  Training Accuracy:  0.407055\n",
      "Epoch:  65  Training Loss:  61.5938  Training Accuracy:  0.410523\n",
      "Epoch:  66  Training Loss:  61.7662  Training Accuracy:  0.414344\n",
      "Epoch:  67  Training Loss:  61.9556  Training Accuracy:  0.417637\n",
      "Epoch:  68  Training Loss:  62.1268  Training Accuracy:  0.421928\n",
      "Epoch:  69  Training Loss:  62.3132  Training Accuracy:  0.425162\n",
      "Epoch:  70  Training Loss:  62.4335  Training Accuracy:  0.428924\n",
      "Epoch:  71  Training Loss:  62.5302  Training Accuracy:  0.432216\n",
      "Epoch:  72  Training Loss:  62.579  Training Accuracy:  0.435391\n",
      "Epoch:  73  Training Loss:  62.5978  Training Accuracy:  0.439153\n",
      "Epoch:  74  Training Loss:  62.5878  Training Accuracy:  0.441975\n",
      "Epoch:  75  Training Loss:  62.716  Training Accuracy:  0.445914\n",
      "Epoch:  76  Training Loss:  62.7345  Training Accuracy:  0.448853\n",
      "Epoch:  77  Training Loss:  62.7297  Training Accuracy:  0.45144\n",
      "Epoch:  78  Training Loss:  62.7215  Training Accuracy:  0.454615\n",
      "Epoch:  79  Training Loss:  62.7043  Training Accuracy:  0.457437\n",
      "Epoch:  80  Training Loss:  62.7236  Training Accuracy:  0.460964\n",
      "Epoch:  81  Training Loss:  62.7237  Training Accuracy:  0.464374\n",
      "Epoch:  82  Training Loss:  62.6991  Training Accuracy:  0.467842\n",
      "Epoch:  83  Training Loss:  62.6769  Training Accuracy:  0.47037\n",
      "Epoch:  84  Training Loss:  62.6322  Training Accuracy:  0.472604\n",
      "Epoch:  85  Training Loss:  62.6156  Training Accuracy:  0.475426\n",
      "Epoch:  86  Training Loss:  62.591  Training Accuracy:  0.478777\n",
      "Epoch:  87  Training Loss:  62.6023  Training Accuracy:  0.482304\n",
      "Epoch:  88  Training Loss:  62.5987  Training Accuracy:  0.485479\n",
      "Epoch:  89  Training Loss:  62.5455  Training Accuracy:  0.487654\n",
      "Epoch:  90  Training Loss:  62.5544  Training Accuracy:  0.49077\n",
      "Epoch:  91  Training Loss:  62.5267  Training Accuracy:  0.494121\n",
      "Epoch:  92  Training Loss:  62.4718  Training Accuracy:  0.496531\n",
      "Epoch:  93  Training Loss:  62.4566  Training Accuracy:  0.500235\n",
      "Epoch:  94  Training Loss:  62.4549  Training Accuracy:  0.503116\n",
      "Epoch:  95  Training Loss:  62.4456  Training Accuracy:  0.506173\n",
      "Epoch:  96  Training Loss:  62.3996  Training Accuracy:  0.508465\n",
      "Epoch:  97  Training Loss:  62.3892  Training Accuracy:  0.510053\n",
      "Epoch:  98  Training Loss:  62.3522  Training Accuracy:  0.512875\n",
      "Epoch:  99  Training Loss:  62.2973  Training Accuracy:  0.514991\n",
      "Epoch:  100  Training Loss:  62.2597  Training Accuracy:  0.51846\n",
      "Epoch:  101  Training Loss:  62.2208  Training Accuracy:  0.520576\n",
      "Epoch:  102  Training Loss:  62.1458  Training Accuracy:  0.523163\n",
      "Epoch:  103  Training Loss:  62.1054  Training Accuracy:  0.526043\n",
      "Epoch:  104  Training Loss:  62.1045  Training Accuracy:  0.528336\n",
      "Epoch:  105  Training Loss:  62.0976  Training Accuracy:  0.530747\n",
      "Epoch:  106  Training Loss:  62.0995  Training Accuracy:  0.533921\n",
      "Epoch:  107  Training Loss:  62.1246  Training Accuracy:  0.536096\n",
      "Epoch:  108  Training Loss:  62.153  Training Accuracy:  0.539153\n",
      "Epoch:  109  Training Loss:  62.1941  Training Accuracy:  0.541799\n",
      "Epoch:  110  Training Loss:  62.2312  Training Accuracy:  0.54368\n",
      "Epoch:  111  Training Loss:  62.3053  Training Accuracy:  0.545855\n",
      "Epoch:  112  Training Loss:  62.3853  Training Accuracy:  0.547266\n",
      "Epoch:  113  Training Loss:  62.4904  Training Accuracy:  0.549324\n",
      "Epoch:  114  Training Loss:  62.5559  Training Accuracy:  0.551205\n",
      "Epoch:  115  Training Loss:  62.6173  Training Accuracy:  0.553792\n",
      "Epoch:  116  Training Loss:  62.7823  Training Accuracy:  0.555379\n",
      "Epoch:  117  Training Loss:  62.8889  Training Accuracy:  0.557378\n",
      "Epoch:  118  Training Loss:  63.0828  Training Accuracy:  0.559142\n",
      "Epoch:  119  Training Loss:  63.1534  Training Accuracy:  0.560964\n",
      "Epoch:  120  Training Loss:  63.2764  Training Accuracy:  0.563257\n",
      "Epoch:  121  Training Loss:  63.4286  Training Accuracy:  0.56555\n",
      "Epoch:  122  Training Loss:  63.6194  Training Accuracy:  0.567784\n",
      "Epoch:  123  Training Loss:  63.7215  Training Accuracy:  0.569488\n",
      "Epoch:  124  Training Loss:  63.8936  Training Accuracy:  0.571546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  64.0653  Training Accuracy:  0.572957\n",
      "Epoch:  126  Training Loss:  64.2297  Training Accuracy:  0.574721\n",
      "Epoch:  127  Training Loss:  64.4254  Training Accuracy:  0.576661\n",
      "Epoch:  128  Training Loss:  64.5516  Training Accuracy:  0.578601\n",
      "Epoch:  129  Training Loss:  64.7123  Training Accuracy:  0.580129\n",
      "Epoch:  130  Training Loss:  64.8421  Training Accuracy:  0.581775\n",
      "Epoch:  131  Training Loss:  64.966  Training Accuracy:  0.583833\n",
      "Epoch:  132  Training Loss:  65.1022  Training Accuracy:  0.585126\n",
      "Epoch:  133  Training Loss:  65.2839  Training Accuracy:  0.587066\n",
      "Epoch:  134  Training Loss:  65.3086  Training Accuracy:  0.588948\n",
      "Epoch:  135  Training Loss:  65.4705  Training Accuracy:  0.5903\n",
      "Epoch:  136  Training Loss:  65.5642  Training Accuracy:  0.592063\n",
      "Epoch:  137  Training Loss:  65.7154  Training Accuracy:  0.592945\n",
      "Epoch:  138  Training Loss:  65.9317  Training Accuracy:  0.594709\n",
      "Epoch:  139  Training Loss:  66.0983  Training Accuracy:  0.596179\n",
      "Epoch:  140  Training Loss:  66.3375  Training Accuracy:  0.597354\n",
      "Epoch:  141  Training Loss:  66.453  Training Accuracy:  0.598765\n",
      "Epoch:  142  Training Loss:  66.6323  Training Accuracy:  0.600823\n",
      "Epoch:  143  Training Loss:  66.8317  Training Accuracy:  0.602234\n",
      "Epoch:  144  Training Loss:  67.0425  Training Accuracy:  0.604056\n",
      "Epoch:  145  Training Loss:  67.2527  Training Accuracy:  0.605761\n",
      "Epoch:  146  Training Loss:  67.4472  Training Accuracy:  0.607113\n",
      "Epoch:  147  Training Loss:  67.6586  Training Accuracy:  0.608524\n",
      "Epoch:  148  Training Loss:  67.8578  Training Accuracy:  0.609641\n",
      "Epoch:  149  Training Loss:  68.0982  Training Accuracy:  0.611111\n",
      "Epoch:  150  Training Loss:  68.3176  Training Accuracy:  0.612522\n",
      "Epoch:  151  Training Loss:  68.5525  Training Accuracy:  0.614051\n",
      "Epoch:  152  Training Loss:  68.7452  Training Accuracy:  0.615403\n",
      "Epoch:  153  Training Loss:  68.9349  Training Accuracy:  0.616931\n",
      "Epoch:  154  Training Loss:  69.1773  Training Accuracy:  0.618989\n",
      "Epoch:  155  Training Loss:  69.3773  Training Accuracy:  0.620694\n",
      "Epoch:  156  Training Loss:  69.5814  Training Accuracy:  0.62234\n",
      "Epoch:  157  Training Loss:  69.8003  Training Accuracy:  0.624515\n",
      "Epoch:  158  Training Loss:  70.0213  Training Accuracy:  0.627396\n",
      "Epoch:  159  Training Loss:  70.2929  Training Accuracy:  0.628748\n",
      "Epoch:  160  Training Loss:  70.4461  Training Accuracy:  0.630394\n",
      "Epoch:  161  Training Loss:  70.6922  Training Accuracy:  0.631981\n",
      "Epoch:  162  Training Loss:  70.9266  Training Accuracy:  0.633686\n",
      "Epoch:  163  Training Loss:  71.138  Training Accuracy:  0.634979\n",
      "Epoch:  164  Training Loss:  71.3117  Training Accuracy:  0.636332\n",
      "Epoch:  165  Training Loss:  71.5397  Training Accuracy:  0.638095\n",
      "Epoch:  166  Training Loss:  71.7124  Training Accuracy:  0.639389\n",
      "Epoch:  167  Training Loss:  71.8936  Training Accuracy:  0.641093\n",
      "Epoch:  168  Training Loss:  72.121  Training Accuracy:  0.642504\n",
      "Epoch:  169  Training Loss:  72.2436  Training Accuracy:  0.643269\n",
      "Epoch:  170  Training Loss:  72.409  Training Accuracy:  0.644797\n",
      "Epoch:  171  Training Loss:  72.6889  Training Accuracy:  0.64662\n",
      "Epoch:  172  Training Loss:  72.8263  Training Accuracy:  0.648207\n",
      "Epoch:  173  Training Loss:  73.0922  Training Accuracy:  0.649324\n",
      "Epoch:  174  Training Loss:  73.2462  Training Accuracy:  0.650559\n",
      "Epoch:  175  Training Loss:  73.4504  Training Accuracy:  0.652028\n",
      "Epoch:  176  Training Loss:  73.6544  Training Accuracy:  0.65391\n",
      "Epoch:  177  Training Loss:  73.8642  Training Accuracy:  0.655262\n",
      "Epoch:  178  Training Loss:  74.0438  Training Accuracy:  0.65679\n",
      "Epoch:  179  Training Loss:  74.2629  Training Accuracy:  0.658201\n",
      "Epoch:  180  Training Loss:  74.4218  Training Accuracy:  0.659318\n",
      "Epoch:  181  Training Loss:  74.611  Training Accuracy:  0.660847\n",
      "Epoch:  182  Training Loss:  74.8123  Training Accuracy:  0.662081\n",
      "Epoch:  183  Training Loss:  74.9948  Training Accuracy:  0.663316\n",
      "Epoch:  184  Training Loss:  75.2508  Training Accuracy:  0.664786\n",
      "Epoch:  185  Training Loss:  75.3769  Training Accuracy:  0.666255\n",
      "Epoch:  186  Training Loss:  75.6428  Training Accuracy:  0.668313\n",
      "Epoch:  187  Training Loss:  75.7531  Training Accuracy:  0.669959\n",
      "Epoch:  188  Training Loss:  75.9931  Training Accuracy:  0.67184\n",
      "Epoch:  189  Training Loss:  76.0891  Training Accuracy:  0.673721\n",
      "Epoch:  190  Training Loss:  76.3288  Training Accuracy:  0.67525\n",
      "Epoch:  191  Training Loss:  76.4024  Training Accuracy:  0.676014\n",
      "Epoch:  192  Training Loss:  76.6102  Training Accuracy:  0.677602\n",
      "Epoch:  193  Training Loss:  76.7685  Training Accuracy:  0.679483\n",
      "Epoch:  194  Training Loss:  76.8883  Training Accuracy:  0.680835\n",
      "Epoch:  195  Training Loss:  77.0598  Training Accuracy:  0.68254\n",
      "Epoch:  196  Training Loss:  77.2131  Training Accuracy:  0.68348\n",
      "Epoch:  197  Training Loss:  77.4328  Training Accuracy:  0.685068\n",
      "Epoch:  198  Training Loss:  77.4993  Training Accuracy:  0.68689\n",
      "Epoch:  199  Training Loss:  77.6333  Training Accuracy:  0.68789\n",
      "Epoch:  200  Training Loss:  77.7802  Training Accuracy:  0.688654\n",
      "Epoch:  201  Training Loss:  77.9319  Training Accuracy:  0.689653\n",
      "Epoch:  202  Training Loss:  78.0654  Training Accuracy:  0.691064\n",
      "Epoch:  203  Training Loss:  78.1966  Training Accuracy:  0.692475\n",
      "Epoch:  204  Training Loss:  78.2628  Training Accuracy:  0.693827\n",
      "Epoch:  205  Training Loss:  78.3864  Training Accuracy:  0.695356\n",
      "Epoch:  206  Training Loss:  78.509  Training Accuracy:  0.697119\n",
      "Epoch:  207  Training Loss:  78.6062  Training Accuracy:  0.698589\n",
      "Epoch:  208  Training Loss:  78.7846  Training Accuracy:  0.699883\n",
      "Epoch:  209  Training Loss:  78.8287  Training Accuracy:  0.701411\n",
      "Epoch:  210  Training Loss:  78.8958  Training Accuracy:  0.702587\n",
      "Epoch:  211  Training Loss:  78.9568  Training Accuracy:  0.703704\n",
      "Epoch:  212  Training Loss:  79.0225  Training Accuracy:  0.705291\n",
      "Epoch:  213  Training Loss:  79.1014  Training Accuracy:  0.707055\n",
      "Epoch:  214  Training Loss:  79.1608  Training Accuracy:  0.707702\n",
      "Epoch:  215  Training Loss:  79.2918  Training Accuracy:  0.70876\n",
      "Epoch:  216  Training Loss:  79.2447  Training Accuracy:  0.710229\n",
      "Epoch:  217  Training Loss:  79.2923  Training Accuracy:  0.71164\n",
      "Epoch:  218  Training Loss:  79.2844  Training Accuracy:  0.712992\n",
      "Epoch:  219  Training Loss:  79.3007  Training Accuracy:  0.714109\n",
      "Epoch:  220  Training Loss:  79.2842  Training Accuracy:  0.715403\n",
      "Epoch:  221  Training Loss:  79.255  Training Accuracy:  0.716814\n",
      "Epoch:  222  Training Loss:  79.3166  Training Accuracy:  0.71799\n",
      "Epoch:  223  Training Loss:  79.2528  Training Accuracy:  0.719518\n",
      "Epoch:  224  Training Loss:  79.2952  Training Accuracy:  0.720576\n",
      "Epoch:  225  Training Loss:  79.2233  Training Accuracy:  0.721752\n",
      "Epoch:  226  Training Loss:  79.2006  Training Accuracy:  0.723104\n",
      "Epoch:  227  Training Loss:  79.1742  Training Accuracy:  0.724221\n",
      "Epoch:  228  Training Loss:  79.103  Training Accuracy:  0.725338\n",
      "Epoch:  229  Training Loss:  79.0455  Training Accuracy:  0.726161\n",
      "Epoch:  230  Training Loss:  79.0924  Training Accuracy:  0.727572\n",
      "Epoch:  231  Training Loss:  79.0067  Training Accuracy:  0.728513\n",
      "Epoch:  232  Training Loss:  78.9842  Training Accuracy:  0.730041\n",
      "Epoch:  233  Training Loss:  78.92  Training Accuracy:  0.731041\n",
      "Epoch:  234  Training Loss:  78.838  Training Accuracy:  0.732981\n",
      "Epoch:  235  Training Loss:  78.8058  Training Accuracy:  0.734568\n",
      "Epoch:  236  Training Loss:  78.6996  Training Accuracy:  0.735626\n",
      "Epoch:  237  Training Loss:  78.6803  Training Accuracy:  0.736391\n",
      "Epoch:  238  Training Loss:  78.5597  Training Accuracy:  0.737743\n",
      "Epoch:  239  Training Loss:  78.4906  Training Accuracy:  0.738683\n",
      "Epoch:  240  Training Loss:  78.4109  Training Accuracy:  0.739859\n",
      "Epoch:  241  Training Loss:  78.369  Training Accuracy:  0.741094\n",
      "Epoch:  242  Training Loss:  78.2767  Training Accuracy:  0.741976\n",
      "Epoch:  243  Training Loss:  78.2541  Training Accuracy:  0.742975\n",
      "Epoch:  244  Training Loss:  78.1548  Training Accuracy:  0.744445\n",
      "Epoch:  245  Training Loss:  78.0892  Training Accuracy:  0.745385\n",
      "Epoch:  246  Training Loss:  77.9796  Training Accuracy:  0.747561\n",
      "Epoch:  247  Training Loss:  77.9363  Training Accuracy:  0.748795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  77.8139  Training Accuracy:  0.75003\n",
      "Epoch:  249  Training Loss:  77.7547  Training Accuracy:  0.750794\n",
      "Epoch:  250  Training Loss:  77.652  Training Accuracy:  0.751617\n",
      "Epoch:  251  Training Loss:  77.5966  Training Accuracy:  0.752851\n",
      "Epoch:  252  Training Loss:  77.471  Training Accuracy:  0.754204\n",
      "Epoch:  253  Training Loss:  77.401  Training Accuracy:  0.755144\n",
      "Epoch:  254  Training Loss:  77.295  Training Accuracy:  0.75585\n",
      "Epoch:  255  Training Loss:  77.2504  Training Accuracy:  0.756555\n",
      "Epoch:  256  Training Loss:  77.1261  Training Accuracy:  0.757437\n",
      "Epoch:  257  Training Loss:  77.0376  Training Accuracy:  0.757966\n",
      "Epoch:  258  Training Loss:  76.9485  Training Accuracy:  0.758848\n",
      "Epoch:  259  Training Loss:  76.8285  Training Accuracy:  0.759495\n",
      "Epoch:  260  Training Loss:  76.6745  Training Accuracy:  0.760083\n",
      "Epoch:  261  Training Loss:  76.5881  Training Accuracy:  0.7612\n",
      "Epoch:  262  Training Loss:  76.4125  Training Accuracy:  0.761611\n",
      "Epoch:  263  Training Loss:  76.3255  Training Accuracy:  0.762728\n",
      "Epoch:  264  Training Loss:  76.1157  Training Accuracy:  0.763963\n",
      "Epoch:  265  Training Loss:  76.0292  Training Accuracy:  0.764492\n",
      "Epoch:  266  Training Loss:  75.8754  Training Accuracy:  0.765785\n",
      "Epoch:  267  Training Loss:  75.7718  Training Accuracy:  0.766608\n",
      "Epoch:  268  Training Loss:  75.5398  Training Accuracy:  0.767901\n",
      "Epoch:  269  Training Loss:  75.4366  Training Accuracy:  0.768901\n",
      "Epoch:  270  Training Loss:  75.2082  Training Accuracy:  0.770077\n",
      "Epoch:  271  Training Loss:  75.0924  Training Accuracy:  0.771076\n",
      "Epoch:  272  Training Loss:  74.898  Training Accuracy:  0.771899\n",
      "Epoch:  273  Training Loss:  74.7549  Training Accuracy:  0.772722\n",
      "Epoch:  274  Training Loss:  74.5517  Training Accuracy:  0.773428\n",
      "Epoch:  275  Training Loss:  74.4407  Training Accuracy:  0.774251\n",
      "Epoch:  276  Training Loss:  74.2117  Training Accuracy:  0.775133\n",
      "Epoch:  277  Training Loss:  74.135  Training Accuracy:  0.776191\n",
      "Epoch:  278  Training Loss:  73.9076  Training Accuracy:  0.777425\n",
      "Epoch:  279  Training Loss:  73.7479  Training Accuracy:  0.778777\n",
      "Epoch:  280  Training Loss:  73.5507  Training Accuracy:  0.779659\n",
      "Epoch:  281  Training Loss:  73.3791  Training Accuracy:  0.780776\n",
      "Epoch:  282  Training Loss:  73.1775  Training Accuracy:  0.78207\n",
      "Epoch:  283  Training Loss:  73.0445  Training Accuracy:  0.782775\n",
      "Epoch:  284  Training Loss:  72.7812  Training Accuracy:  0.783481\n",
      "Epoch:  285  Training Loss:  72.6284  Training Accuracy:  0.78448\n",
      "Epoch:  286  Training Loss:  72.3863  Training Accuracy:  0.785362\n",
      "Epoch:  287  Training Loss:  72.2383  Training Accuracy:  0.78595\n",
      "Epoch:  288  Training Loss:  71.9839  Training Accuracy:  0.787302\n",
      "Epoch:  289  Training Loss:  71.8057  Training Accuracy:  0.788301\n",
      "Epoch:  290  Training Loss:  71.5678  Training Accuracy:  0.78883\n",
      "Epoch:  291  Training Loss:  71.4278  Training Accuracy:  0.789183\n",
      "Epoch:  292  Training Loss:  71.2412  Training Accuracy:  0.790418\n",
      "Epoch:  293  Training Loss:  71.0532  Training Accuracy:  0.791652\n",
      "Epoch:  294  Training Loss:  70.8586  Training Accuracy:  0.792652\n",
      "Epoch:  295  Training Loss:  70.7037  Training Accuracy:  0.793651\n",
      "Epoch:  296  Training Loss:  70.4932  Training Accuracy:  0.794709\n",
      "Epoch:  297  Training Loss:  70.2973  Training Accuracy:  0.796061\n",
      "Epoch:  298  Training Loss:  70.0372  Training Accuracy:  0.797531\n",
      "Epoch:  299  Training Loss:  69.9426  Training Accuracy:  0.798237\n",
      "Epoch:  300  Training Loss:  69.7682  Training Accuracy:  0.799001\n",
      "Epoch:  301  Training Loss:  69.5472  Training Accuracy:  0.799648\n",
      "Epoch:  302  Training Loss:  69.3479  Training Accuracy:  0.800706\n",
      "Epoch:  303  Training Loss:  69.1973  Training Accuracy:  0.801529\n",
      "Epoch:  304  Training Loss:  68.9789  Training Accuracy:  0.802881\n",
      "Epoch:  305  Training Loss:  68.7731  Training Accuracy:  0.803351\n",
      "Epoch:  306  Training Loss:  68.5514  Training Accuracy:  0.804527\n",
      "Epoch:  307  Training Loss:  68.3768  Training Accuracy:  0.805056\n",
      "Epoch:  308  Training Loss:  68.1643  Training Accuracy:  0.805644\n",
      "Epoch:  309  Training Loss:  67.9747  Training Accuracy:  0.806232\n",
      "Epoch:  310  Training Loss:  67.7674  Training Accuracy:  0.806643\n",
      "Epoch:  311  Training Loss:  67.5555  Training Accuracy:  0.807408\n",
      "Epoch:  312  Training Loss:  67.3618  Training Accuracy:  0.807996\n",
      "Epoch:  313  Training Loss:  67.1579  Training Accuracy:  0.808877\n",
      "Epoch:  314  Training Loss:  66.9796  Training Accuracy:  0.809877\n",
      "Epoch:  315  Training Loss:  66.7709  Training Accuracy:  0.810817\n",
      "Epoch:  316  Training Loss:  66.5662  Training Accuracy:  0.811641\n",
      "Epoch:  317  Training Loss:  66.3472  Training Accuracy:  0.812464\n",
      "Epoch:  318  Training Loss:  66.1596  Training Accuracy:  0.813345\n",
      "Epoch:  319  Training Loss:  65.9328  Training Accuracy:  0.813816\n",
      "Epoch:  320  Training Loss:  65.7463  Training Accuracy:  0.814345\n",
      "Epoch:  321  Training Loss:  65.5351  Training Accuracy:  0.815285\n",
      "Epoch:  322  Training Loss:  65.3413  Training Accuracy:  0.815932\n",
      "Epoch:  323  Training Loss:  65.1079  Training Accuracy:  0.816638\n",
      "Epoch:  324  Training Loss:  64.9168  Training Accuracy:  0.817402\n",
      "Epoch:  325  Training Loss:  64.6926  Training Accuracy:  0.818049\n",
      "Epoch:  326  Training Loss:  64.497  Training Accuracy:  0.818754\n",
      "Epoch:  327  Training Loss:  64.2911  Training Accuracy:  0.819753\n",
      "Epoch:  328  Training Loss:  64.1161  Training Accuracy:  0.820518\n",
      "Epoch:  329  Training Loss:  63.8939  Training Accuracy:  0.820929\n",
      "Epoch:  330  Training Loss:  63.7233  Training Accuracy:  0.821458\n",
      "Epoch:  331  Training Loss:  63.4949  Training Accuracy:  0.822164\n",
      "Epoch:  332  Training Loss:  63.3254  Training Accuracy:  0.82281\n",
      "Epoch:  333  Training Loss:  63.1036  Training Accuracy:  0.823457\n",
      "Epoch:  334  Training Loss:  62.9111  Training Accuracy:  0.824045\n",
      "Epoch:  335  Training Loss:  62.6949  Training Accuracy:  0.824692\n",
      "Epoch:  336  Training Loss:  62.5206  Training Accuracy:  0.825338\n",
      "Epoch:  337  Training Loss:  62.2853  Training Accuracy:  0.82622\n",
      "Epoch:  338  Training Loss:  62.1097  Training Accuracy:  0.826926\n",
      "Epoch:  339  Training Loss:  61.8943  Training Accuracy:  0.827514\n",
      "Epoch:  340  Training Loss:  61.7038  Training Accuracy:  0.828278\n",
      "Epoch:  341  Training Loss:  61.5026  Training Accuracy:  0.829042\n",
      "Epoch:  342  Training Loss:  61.3165  Training Accuracy:  0.829101\n",
      "Epoch:  343  Training Loss:  61.1036  Training Accuracy:  0.82963\n",
      "Epoch:  344  Training Loss:  60.9131  Training Accuracy:  0.830335\n",
      "Epoch:  345  Training Loss:  60.7359  Training Accuracy:  0.830747\n",
      "Epoch:  346  Training Loss:  60.5028  Training Accuracy:  0.8311\n",
      "Epoch:  347  Training Loss:  60.3618  Training Accuracy:  0.831688\n",
      "Epoch:  348  Training Loss:  60.1668  Training Accuracy:  0.832158\n",
      "Epoch:  349  Training Loss:  59.972  Training Accuracy:  0.832805\n",
      "Epoch:  350  Training Loss:  59.839  Training Accuracy:  0.833157\n",
      "Epoch:  351  Training Loss:  59.6449  Training Accuracy:  0.833804\n",
      "Epoch:  352  Training Loss:  59.4594  Training Accuracy:  0.834333\n",
      "Epoch:  353  Training Loss:  59.2603  Training Accuracy:  0.835039\n",
      "Epoch:  354  Training Loss:  59.0624  Training Accuracy:  0.835509\n",
      "Epoch:  355  Training Loss:  58.8757  Training Accuracy:  0.83592\n",
      "Epoch:  356  Training Loss:  58.6539  Training Accuracy:  0.836861\n",
      "Epoch:  357  Training Loss:  58.4767  Training Accuracy:  0.837508\n",
      "Epoch:  358  Training Loss:  58.2906  Training Accuracy:  0.838096\n",
      "Epoch:  359  Training Loss:  58.1328  Training Accuracy:  0.83886\n",
      "Epoch:  360  Training Loss:  57.9036  Training Accuracy:  0.839683\n",
      "Epoch:  361  Training Loss:  57.7108  Training Accuracy:  0.840153\n",
      "Epoch:  362  Training Loss:  57.5146  Training Accuracy:  0.840741\n",
      "Epoch:  363  Training Loss:  57.3169  Training Accuracy:  0.841329\n",
      "Epoch:  364  Training Loss:  57.1465  Training Accuracy:  0.84174\n",
      "Epoch:  365  Training Loss:  56.959  Training Accuracy:  0.842681\n",
      "Epoch:  366  Training Loss:  56.7846  Training Accuracy:  0.843093\n",
      "Epoch:  367  Training Loss:  56.6251  Training Accuracy:  0.843798\n",
      "Epoch:  368  Training Loss:  56.4396  Training Accuracy:  0.844504\n",
      "Epoch:  369  Training Loss:  56.27  Training Accuracy:  0.84515\n",
      "Epoch:  370  Training Loss:  56.086  Training Accuracy:  0.845503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  55.9267  Training Accuracy:  0.846032\n",
      "Epoch:  372  Training Loss:  55.7344  Training Accuracy:  0.846796\n",
      "Epoch:  373  Training Loss:  55.6186  Training Accuracy:  0.847384\n",
      "Epoch:  374  Training Loss:  55.4049  Training Accuracy:  0.848031\n",
      "Epoch:  375  Training Loss:  55.2584  Training Accuracy:  0.848325\n",
      "Epoch:  376  Training Loss:  55.0782  Training Accuracy:  0.848795\n",
      "Epoch:  377  Training Loss:  54.9336  Training Accuracy:  0.849148\n",
      "Epoch:  378  Training Loss:  54.7515  Training Accuracy:  0.849501\n",
      "Epoch:  379  Training Loss:  54.5762  Training Accuracy:  0.850383\n",
      "Epoch:  380  Training Loss:  54.4206  Training Accuracy:  0.850853\n",
      "Epoch:  381  Training Loss:  54.2727  Training Accuracy:  0.851323\n",
      "Epoch:  382  Training Loss:  54.0888  Training Accuracy:  0.851793\n",
      "Epoch:  383  Training Loss:  53.9237  Training Accuracy:  0.852264\n",
      "Epoch:  384  Training Loss:  53.7591  Training Accuracy:  0.852616\n",
      "Epoch:  385  Training Loss:  53.6059  Training Accuracy:  0.853322\n",
      "Epoch:  386  Training Loss:  53.4389  Training Accuracy:  0.853969\n",
      "Epoch:  387  Training Loss:  53.2678  Training Accuracy:  0.85438\n",
      "Epoch:  388  Training Loss:  53.0711  Training Accuracy:  0.855086\n",
      "Epoch:  389  Training Loss:  52.9194  Training Accuracy:  0.855438\n",
      "Epoch:  390  Training Loss:  52.7715  Training Accuracy:  0.855967\n",
      "Epoch:  391  Training Loss:  52.5862  Training Accuracy:  0.856379\n",
      "Epoch:  392  Training Loss:  52.3882  Training Accuracy:  0.856849\n",
      "Epoch:  393  Training Loss:  52.2422  Training Accuracy:  0.85732\n",
      "Epoch:  394  Training Loss:  52.083  Training Accuracy:  0.857849\n",
      "Epoch:  395  Training Loss:  51.8998  Training Accuracy:  0.858378\n",
      "Epoch:  396  Training Loss:  51.7079  Training Accuracy:  0.858789\n",
      "Epoch:  397  Training Loss:  51.5431  Training Accuracy:  0.859201\n",
      "Epoch:  398  Training Loss:  51.3798  Training Accuracy:  0.859495\n",
      "Epoch:  399  Training Loss:  51.208  Training Accuracy:  0.859789\n",
      "Testing Accuracy: 0.796835\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 400\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  65.2185  Training Accuracy:  0.0393886\n",
      "Epoch:  1  Training Loss:  42.5247  Training Accuracy:  0.0393886\n",
      "Epoch:  2  Training Loss:  45.5513  Training Accuracy:  0.0429747\n",
      "Epoch:  3  Training Loss:  49.4864  Training Accuracy:  0.0444444\n",
      "Epoch:  4  Training Loss:  52.2846  Training Accuracy:  0.045973\n",
      "Epoch:  5  Training Loss:  54.6056  Training Accuracy:  0.047619\n",
      "Epoch:  6  Training Loss:  55.3263  Training Accuracy:  0.0503233\n",
      "Epoch:  7  Training Loss:  55.9798  Training Accuracy:  0.0538507\n",
      "Epoch:  8  Training Loss:  56.7656  Training Accuracy:  0.0587889\n",
      "Epoch:  9  Training Loss:  56.7429  Training Accuracy:  0.0625514\n",
      "Epoch:  10  Training Loss:  57.0772  Training Accuracy:  0.0688419\n",
      "Epoch:  11  Training Loss:  57.0337  Training Accuracy:  0.0733098\n",
      "Epoch:  12  Training Loss:  56.6778  Training Accuracy:  0.0783069\n",
      "Epoch:  13  Training Loss:  56.1767  Training Accuracy:  0.0834215\n",
      "Epoch:  14  Training Loss:  55.6762  Training Accuracy:  0.0911229\n",
      "Epoch:  15  Training Loss:  55.5465  Training Accuracy:  0.0999412\n",
      "Epoch:  16  Training Loss:  55.0555  Training Accuracy:  0.108289\n",
      "Epoch:  17  Training Loss:  54.6086  Training Accuracy:  0.117989\n",
      "Epoch:  18  Training Loss:  54.0174  Training Accuracy:  0.12669\n",
      "Epoch:  19  Training Loss:  53.6729  Training Accuracy:  0.136332\n",
      "Epoch:  20  Training Loss:  53.2193  Training Accuracy:  0.145385\n",
      "Epoch:  21  Training Loss:  52.8147  Training Accuracy:  0.154968\n",
      "Epoch:  22  Training Loss:  52.465  Training Accuracy:  0.163845\n",
      "Epoch:  23  Training Loss:  52.1454  Training Accuracy:  0.171487\n",
      "Epoch:  24  Training Loss:  51.8174  Training Accuracy:  0.180012\n",
      "Epoch:  25  Training Loss:  51.5399  Training Accuracy:  0.188948\n",
      "Epoch:  26  Training Loss:  51.2523  Training Accuracy:  0.19659\n",
      "Epoch:  27  Training Loss:  50.9416  Training Accuracy:  0.206232\n",
      "Epoch:  28  Training Loss:  50.6534  Training Accuracy:  0.215403\n",
      "Epoch:  29  Training Loss:  50.367  Training Accuracy:  0.223398\n",
      "Epoch:  30  Training Loss:  50.1778  Training Accuracy:  0.231746\n",
      "Epoch:  31  Training Loss:  50.1574  Training Accuracy:  0.239565\n",
      "Epoch:  32  Training Loss:  50.1268  Training Accuracy:  0.246149\n",
      "Epoch:  33  Training Loss:  50.0691  Training Accuracy:  0.252322\n",
      "Epoch:  34  Training Loss:  50.0458  Training Accuracy:  0.260023\n",
      "Epoch:  35  Training Loss:  50.0615  Training Accuracy:  0.265021\n",
      "Epoch:  36  Training Loss:  50.112  Training Accuracy:  0.270723\n",
      "Epoch:  37  Training Loss:  50.1928  Training Accuracy:  0.277543\n",
      "Epoch:  38  Training Loss:  50.3098  Training Accuracy:  0.282304\n",
      "Epoch:  39  Training Loss:  50.5179  Training Accuracy:  0.287419\n",
      "Epoch:  40  Training Loss:  50.789  Training Accuracy:  0.29224\n",
      "Epoch:  41  Training Loss:  51.0301  Training Accuracy:  0.297237\n",
      "Epoch:  42  Training Loss:  51.2003  Training Accuracy:  0.302116\n",
      "Epoch:  43  Training Loss:  51.342  Training Accuracy:  0.306584\n",
      "Epoch:  44  Training Loss:  51.5061  Training Accuracy:  0.311934\n",
      "Epoch:  45  Training Loss:  51.6572  Training Accuracy:  0.315697\n",
      "Epoch:  46  Training Loss:  51.8485  Training Accuracy:  0.319871\n",
      "Epoch:  47  Training Loss:  52.002  Training Accuracy:  0.324397\n",
      "Epoch:  48  Training Loss:  52.2011  Training Accuracy:  0.32863\n",
      "Epoch:  49  Training Loss:  52.3817  Training Accuracy:  0.333098\n",
      "Epoch:  50  Training Loss:  52.58  Training Accuracy:  0.336625\n",
      "Epoch:  51  Training Loss:  52.8014  Training Accuracy:  0.340388\n",
      "Epoch:  52  Training Loss:  53.0289  Training Accuracy:  0.342328\n",
      "Epoch:  53  Training Loss:  53.2319  Training Accuracy:  0.345797\n",
      "Epoch:  54  Training Loss:  53.4274  Training Accuracy:  0.350264\n",
      "Epoch:  55  Training Loss:  53.6467  Training Accuracy:  0.353615\n",
      "Epoch:  56  Training Loss:  53.8758  Training Accuracy:  0.357084\n",
      "Epoch:  57  Training Loss:  54.1054  Training Accuracy:  0.361082\n",
      "Epoch:  58  Training Loss:  54.3794  Training Accuracy:  0.364609\n",
      "Epoch:  59  Training Loss:  54.6311  Training Accuracy:  0.369253\n",
      "Epoch:  60  Training Loss:  54.8989  Training Accuracy:  0.373604\n",
      "Epoch:  61  Training Loss:  55.2597  Training Accuracy:  0.377131\n",
      "Epoch:  62  Training Loss:  55.464  Training Accuracy:  0.381599\n",
      "Epoch:  63  Training Loss:  55.6897  Training Accuracy:  0.385949\n",
      "Epoch:  64  Training Loss:  55.9264  Training Accuracy:  0.39077\n",
      "Epoch:  65  Training Loss:  56.1373  Training Accuracy:  0.394415\n",
      "Epoch:  66  Training Loss:  56.345  Training Accuracy:  0.398295\n",
      "Epoch:  67  Training Loss:  56.6031  Training Accuracy:  0.402763\n",
      "Epoch:  68  Training Loss:  56.8775  Training Accuracy:  0.406819\n",
      "Epoch:  69  Training Loss:  57.0972  Training Accuracy:  0.410347\n",
      "Epoch:  70  Training Loss:  57.3152  Training Accuracy:  0.413521\n",
      "Epoch:  71  Training Loss:  57.5397  Training Accuracy:  0.41699\n",
      "Epoch:  72  Training Loss:  57.7382  Training Accuracy:  0.421164\n",
      "Epoch:  73  Training Loss:  57.9427  Training Accuracy:  0.425044\n",
      "Epoch:  74  Training Loss:  58.1529  Training Accuracy:  0.427572\n",
      "Epoch:  75  Training Loss:  58.3683  Training Accuracy:  0.430453\n",
      "Epoch:  76  Training Loss:  58.55  Training Accuracy:  0.433451\n",
      "Epoch:  77  Training Loss:  58.6962  Training Accuracy:  0.436978\n",
      "Epoch:  78  Training Loss:  58.8422  Training Accuracy:  0.439918\n",
      "Epoch:  79  Training Loss:  58.9798  Training Accuracy:  0.442798\n",
      "Epoch:  80  Training Loss:  59.1391  Training Accuracy:  0.445973\n",
      "Epoch:  81  Training Loss:  59.3078  Training Accuracy:  0.449559\n",
      "Epoch:  82  Training Loss:  59.3718  Training Accuracy:  0.453086\n",
      "Epoch:  83  Training Loss:  59.5085  Training Accuracy:  0.456026\n",
      "Epoch:  84  Training Loss:  59.6192  Training Accuracy:  0.458965\n",
      "Epoch:  85  Training Loss:  59.7823  Training Accuracy:  0.462022\n",
      "Epoch:  86  Training Loss:  59.8859  Training Accuracy:  0.465256\n",
      "Epoch:  87  Training Loss:  60.0367  Training Accuracy:  0.46843\n",
      "Epoch:  88  Training Loss:  60.168  Training Accuracy:  0.471017\n",
      "Epoch:  89  Training Loss:  60.3338  Training Accuracy:  0.473662\n",
      "Epoch:  90  Training Loss:  60.4672  Training Accuracy:  0.476543\n",
      "Epoch:  91  Training Loss:  60.6329  Training Accuracy:  0.479776\n",
      "Epoch:  92  Training Loss:  60.7624  Training Accuracy:  0.48301\n",
      "Epoch:  93  Training Loss:  60.9231  Training Accuracy:  0.485949\n",
      "Epoch:  94  Training Loss:  61.0963  Training Accuracy:  0.488654\n",
      "Epoch:  95  Training Loss:  61.2749  Training Accuracy:  0.490358\n",
      "Epoch:  96  Training Loss:  61.4398  Training Accuracy:  0.493239\n",
      "Epoch:  97  Training Loss:  61.5548  Training Accuracy:  0.495826\n",
      "Epoch:  98  Training Loss:  61.6816  Training Accuracy:  0.499647\n",
      "Epoch:  99  Training Loss:  61.7829  Training Accuracy:  0.501999\n",
      "Epoch:  100  Training Loss:  61.8637  Training Accuracy:  0.505879\n",
      "Epoch:  101  Training Loss:  61.9866  Training Accuracy:  0.508818\n",
      "Epoch:  102  Training Loss:  62.0827  Training Accuracy:  0.511758\n",
      "Epoch:  103  Training Loss:  62.2067  Training Accuracy:  0.514815\n",
      "Epoch:  104  Training Loss:  62.2806  Training Accuracy:  0.517519\n",
      "Epoch:  105  Training Loss:  62.3908  Training Accuracy:  0.520341\n",
      "Epoch:  106  Training Loss:  62.4973  Training Accuracy:  0.522692\n",
      "Epoch:  107  Training Loss:  62.6358  Training Accuracy:  0.524691\n",
      "Epoch:  108  Training Loss:  62.7641  Training Accuracy:  0.527396\n",
      "Epoch:  109  Training Loss:  62.9168  Training Accuracy:  0.530041\n",
      "Epoch:  110  Training Loss:  63.1079  Training Accuracy:  0.532451\n",
      "Epoch:  111  Training Loss:  63.2357  Training Accuracy:  0.534921\n",
      "Epoch:  112  Training Loss:  63.4114  Training Accuracy:  0.537154\n",
      "Epoch:  113  Training Loss:  63.5612  Training Accuracy:  0.539271\n",
      "Epoch:  114  Training Loss:  63.6651  Training Accuracy:  0.540858\n",
      "Epoch:  115  Training Loss:  63.8189  Training Accuracy:  0.542563\n",
      "Epoch:  116  Training Loss:  63.9363  Training Accuracy:  0.54468\n",
      "Epoch:  117  Training Loss:  64.1205  Training Accuracy:  0.546972\n",
      "Epoch:  118  Training Loss:  64.273  Training Accuracy:  0.549441\n",
      "Epoch:  119  Training Loss:  64.4488  Training Accuracy:  0.550852\n",
      "Epoch:  120  Training Loss:  64.6144  Training Accuracy:  0.552734\n",
      "Epoch:  121  Training Loss:  64.803  Training Accuracy:  0.554438\n",
      "Epoch:  122  Training Loss:  65.0306  Training Accuracy:  0.556966\n",
      "Epoch:  123  Training Loss:  65.2589  Training Accuracy:  0.558906\n",
      "Epoch:  124  Training Loss:  65.5201  Training Accuracy:  0.560905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  65.8169  Training Accuracy:  0.562022\n",
      "Epoch:  126  Training Loss:  66.0055  Training Accuracy:  0.564315\n",
      "Epoch:  127  Training Loss:  66.2191  Training Accuracy:  0.56649\n",
      "Epoch:  128  Training Loss:  66.3996  Training Accuracy:  0.568019\n",
      "Epoch:  129  Training Loss:  66.55  Training Accuracy:  0.570135\n",
      "Epoch:  130  Training Loss:  66.7354  Training Accuracy:  0.571252\n",
      "Epoch:  131  Training Loss:  66.9212  Training Accuracy:  0.573016\n",
      "Epoch:  132  Training Loss:  67.1221  Training Accuracy:  0.575191\n",
      "Epoch:  133  Training Loss:  67.3388  Training Accuracy:  0.576367\n",
      "Epoch:  134  Training Loss:  67.5796  Training Accuracy:  0.578189\n",
      "Epoch:  135  Training Loss:  67.7609  Training Accuracy:  0.580541\n",
      "Epoch:  136  Training Loss:  67.9946  Training Accuracy:  0.582128\n",
      "Epoch:  137  Training Loss:  68.1877  Training Accuracy:  0.583186\n",
      "Epoch:  138  Training Loss:  68.4262  Training Accuracy:  0.585773\n",
      "Epoch:  139  Training Loss:  68.607  Training Accuracy:  0.58736\n",
      "Epoch:  140  Training Loss:  68.8259  Training Accuracy:  0.58836\n",
      "Epoch:  141  Training Loss:  68.9979  Training Accuracy:  0.589888\n",
      "Epoch:  142  Training Loss:  69.1977  Training Accuracy:  0.591887\n",
      "Epoch:  143  Training Loss:  69.4145  Training Accuracy:  0.593298\n",
      "Epoch:  144  Training Loss:  69.6303  Training Accuracy:  0.594885\n",
      "Epoch:  145  Training Loss:  69.8046  Training Accuracy:  0.596473\n",
      "Epoch:  146  Training Loss:  69.981  Training Accuracy:  0.598236\n",
      "Epoch:  147  Training Loss:  70.1638  Training Accuracy:  0.59953\n",
      "Epoch:  148  Training Loss:  70.3479  Training Accuracy:  0.601293\n",
      "Epoch:  149  Training Loss:  70.5222  Training Accuracy:  0.602998\n",
      "Epoch:  150  Training Loss:  70.6933  Training Accuracy:  0.604586\n",
      "Epoch:  151  Training Loss:  70.8379  Training Accuracy:  0.605938\n",
      "Epoch:  152  Training Loss:  71.0444  Training Accuracy:  0.607113\n",
      "Epoch:  153  Training Loss:  71.2513  Training Accuracy:  0.608407\n",
      "Epoch:  154  Training Loss:  71.41  Training Accuracy:  0.610347\n",
      "Epoch:  155  Training Loss:  71.6275  Training Accuracy:  0.611346\n",
      "Epoch:  156  Training Loss:  71.8695  Training Accuracy:  0.612992\n",
      "Epoch:  157  Training Loss:  72.1073  Training Accuracy:  0.613992\n",
      "Epoch:  158  Training Loss:  72.2547  Training Accuracy:  0.615226\n",
      "Epoch:  159  Training Loss:  72.4319  Training Accuracy:  0.61652\n",
      "Epoch:  160  Training Loss:  72.5771  Training Accuracy:  0.617754\n",
      "Epoch:  161  Training Loss:  72.9066  Training Accuracy:  0.6194\n",
      "Epoch:  162  Training Loss:  73.101  Training Accuracy:  0.620752\n",
      "Epoch:  163  Training Loss:  73.3335  Training Accuracy:  0.622281\n",
      "Epoch:  164  Training Loss:  73.4555  Training Accuracy:  0.623457\n",
      "Epoch:  165  Training Loss:  73.7331  Training Accuracy:  0.625279\n",
      "Epoch:  166  Training Loss:  73.9289  Training Accuracy:  0.626514\n",
      "Epoch:  167  Training Loss:  74.0967  Training Accuracy:  0.62816\n",
      "Epoch:  168  Training Loss:  74.2088  Training Accuracy:  0.629571\n",
      "Epoch:  169  Training Loss:  74.429  Training Accuracy:  0.630688\n",
      "Epoch:  170  Training Loss:  74.6032  Training Accuracy:  0.632393\n",
      "Epoch:  171  Training Loss:  74.7594  Training Accuracy:  0.633862\n",
      "Epoch:  172  Training Loss:  74.953  Training Accuracy:  0.635509\n",
      "Epoch:  173  Training Loss:  75.1564  Training Accuracy:  0.636743\n",
      "Epoch:  174  Training Loss:  75.3432  Training Accuracy:  0.63833\n",
      "Epoch:  175  Training Loss:  75.5159  Training Accuracy:  0.639977\n",
      "Epoch:  176  Training Loss:  75.7202  Training Accuracy:  0.64174\n",
      "Epoch:  177  Training Loss:  75.8695  Training Accuracy:  0.64321\n",
      "Epoch:  178  Training Loss:  76.0532  Training Accuracy:  0.64468\n",
      "Epoch:  179  Training Loss:  76.2086  Training Accuracy:  0.646149\n",
      "Epoch:  180  Training Loss:  76.3641  Training Accuracy:  0.648089\n",
      "Epoch:  181  Training Loss:  76.548  Training Accuracy:  0.649383\n",
      "Epoch:  182  Training Loss:  76.6882  Training Accuracy:  0.650911\n",
      "Epoch:  183  Training Loss:  76.8516  Training Accuracy:  0.652028\n",
      "Epoch:  184  Training Loss:  76.9904  Training Accuracy:  0.653616\n",
      "Epoch:  185  Training Loss:  77.1867  Training Accuracy:  0.655262\n",
      "Epoch:  186  Training Loss:  77.2599  Training Accuracy:  0.657084\n",
      "Epoch:  187  Training Loss:  77.4423  Training Accuracy:  0.658848\n",
      "Epoch:  188  Training Loss:  77.5917  Training Accuracy:  0.660141\n",
      "Epoch:  189  Training Loss:  77.7016  Training Accuracy:  0.661493\n",
      "Epoch:  190  Training Loss:  77.8163  Training Accuracy:  0.662963\n",
      "Epoch:  191  Training Loss:  77.924  Training Accuracy:  0.664256\n",
      "Epoch:  192  Training Loss:  78.0751  Training Accuracy:  0.665667\n",
      "Epoch:  193  Training Loss:  78.2007  Training Accuracy:  0.667313\n",
      "Epoch:  194  Training Loss:  78.2457  Training Accuracy:  0.668489\n",
      "Epoch:  195  Training Loss:  78.3881  Training Accuracy:  0.670312\n",
      "Epoch:  196  Training Loss:  78.4848  Training Accuracy:  0.672311\n",
      "Epoch:  197  Training Loss:  78.5588  Training Accuracy:  0.673369\n",
      "Epoch:  198  Training Loss:  78.6352  Training Accuracy:  0.674545\n",
      "Epoch:  199  Training Loss:  78.6947  Training Accuracy:  0.675661\n",
      "Epoch:  200  Training Loss:  78.7644  Training Accuracy:  0.677249\n",
      "Epoch:  201  Training Loss:  78.8063  Training Accuracy:  0.678483\n",
      "Epoch:  202  Training Loss:  78.869  Training Accuracy:  0.679659\n",
      "Epoch:  203  Training Loss:  78.8906  Training Accuracy:  0.681129\n",
      "Epoch:  204  Training Loss:  78.9291  Training Accuracy:  0.682187\n",
      "Epoch:  205  Training Loss:  78.9662  Training Accuracy:  0.683363\n",
      "Epoch:  206  Training Loss:  78.9522  Training Accuracy:  0.684891\n",
      "Epoch:  207  Training Loss:  78.994  Training Accuracy:  0.686243\n",
      "Epoch:  208  Training Loss:  79.0282  Training Accuracy:  0.688007\n",
      "Epoch:  209  Training Loss:  79.0495  Training Accuracy:  0.689301\n",
      "Epoch:  210  Training Loss:  79.0727  Training Accuracy:  0.690947\n",
      "Epoch:  211  Training Loss:  79.1  Training Accuracy:  0.69224\n",
      "Epoch:  212  Training Loss:  79.1083  Training Accuracy:  0.693827\n",
      "Epoch:  213  Training Loss:  79.1598  Training Accuracy:  0.694592\n",
      "Epoch:  214  Training Loss:  79.2225  Training Accuracy:  0.696473\n",
      "Epoch:  215  Training Loss:  79.2998  Training Accuracy:  0.697884\n",
      "Epoch:  216  Training Loss:  79.3017  Training Accuracy:  0.699118\n",
      "Epoch:  217  Training Loss:  79.3717  Training Accuracy:  0.700999\n",
      "Epoch:  218  Training Loss:  79.3998  Training Accuracy:  0.703175\n",
      "Epoch:  219  Training Loss:  79.3897  Training Accuracy:  0.704527\n",
      "Epoch:  220  Training Loss:  79.4152  Training Accuracy:  0.706173\n",
      "Epoch:  221  Training Loss:  79.4272  Training Accuracy:  0.707349\n",
      "Epoch:  222  Training Loss:  79.4513  Training Accuracy:  0.708407\n",
      "Epoch:  223  Training Loss:  79.4499  Training Accuracy:  0.710582\n",
      "Epoch:  224  Training Loss:  79.4566  Training Accuracy:  0.712228\n",
      "Epoch:  225  Training Loss:  79.5215  Training Accuracy:  0.713522\n",
      "Epoch:  226  Training Loss:  79.5392  Training Accuracy:  0.714933\n",
      "Epoch:  227  Training Loss:  79.5457  Training Accuracy:  0.716402\n",
      "Epoch:  228  Training Loss:  79.5647  Training Accuracy:  0.717872\n",
      "Epoch:  229  Training Loss:  79.5753  Training Accuracy:  0.71893\n",
      "Epoch:  230  Training Loss:  79.5629  Training Accuracy:  0.719694\n",
      "Epoch:  231  Training Loss:  79.5389  Training Accuracy:  0.720694\n",
      "Epoch:  232  Training Loss:  79.5063  Training Accuracy:  0.721752\n",
      "Epoch:  233  Training Loss:  79.4871  Training Accuracy:  0.723045\n",
      "Epoch:  234  Training Loss:  79.4939  Training Accuracy:  0.723575\n",
      "Epoch:  235  Training Loss:  79.4446  Training Accuracy:  0.724927\n",
      "Epoch:  236  Training Loss:  79.4608  Training Accuracy:  0.726396\n",
      "Epoch:  237  Training Loss:  79.4853  Training Accuracy:  0.728101\n",
      "Epoch:  238  Training Loss:  79.4672  Training Accuracy:  0.728748\n",
      "Epoch:  239  Training Loss:  79.456  Training Accuracy:  0.72963\n",
      "Epoch:  240  Training Loss:  79.4304  Training Accuracy:  0.730688\n",
      "Epoch:  241  Training Loss:  79.4451  Training Accuracy:  0.73204\n",
      "Epoch:  242  Training Loss:  79.4266  Training Accuracy:  0.733392\n",
      "Epoch:  243  Training Loss:  79.4232  Training Accuracy:  0.734215\n",
      "Epoch:  244  Training Loss:  79.4126  Training Accuracy:  0.735038\n",
      "Epoch:  245  Training Loss:  79.41  Training Accuracy:  0.73592\n",
      "Epoch:  246  Training Loss:  79.4167  Training Accuracy:  0.73692\n",
      "Epoch:  247  Training Loss:  79.4011  Training Accuracy:  0.73786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  79.3849  Training Accuracy:  0.738801\n",
      "Epoch:  249  Training Loss:  79.3323  Training Accuracy:  0.739918\n",
      "Epoch:  250  Training Loss:  79.2605  Training Accuracy:  0.741094\n",
      "Epoch:  251  Training Loss:  79.2302  Training Accuracy:  0.742093\n",
      "Epoch:  252  Training Loss:  79.1755  Training Accuracy:  0.742799\n",
      "Epoch:  253  Training Loss:  79.1218  Training Accuracy:  0.743798\n",
      "Epoch:  254  Training Loss:  79.1087  Training Accuracy:  0.744856\n",
      "Epoch:  255  Training Loss:  79.0598  Training Accuracy:  0.745679\n",
      "Epoch:  256  Training Loss:  78.9959  Training Accuracy:  0.74662\n",
      "Epoch:  257  Training Loss:  78.9228  Training Accuracy:  0.747737\n",
      "Epoch:  258  Training Loss:  78.8654  Training Accuracy:  0.748795\n",
      "Epoch:  259  Training Loss:  78.7912  Training Accuracy:  0.749677\n",
      "Epoch:  260  Training Loss:  78.7209  Training Accuracy:  0.750676\n",
      "Epoch:  261  Training Loss:  78.6637  Training Accuracy:  0.751441\n",
      "Epoch:  262  Training Loss:  78.5997  Training Accuracy:  0.75197\n",
      "Epoch:  263  Training Loss:  78.4894  Training Accuracy:  0.752381\n",
      "Epoch:  264  Training Loss:  78.4215  Training Accuracy:  0.752851\n",
      "Epoch:  265  Training Loss:  78.2907  Training Accuracy:  0.753968\n",
      "Epoch:  266  Training Loss:  78.198  Training Accuracy:  0.75485\n",
      "Epoch:  267  Training Loss:  78.0812  Training Accuracy:  0.755321\n",
      "Epoch:  268  Training Loss:  77.9785  Training Accuracy:  0.755908\n",
      "Epoch:  269  Training Loss:  77.8798  Training Accuracy:  0.757084\n",
      "Epoch:  270  Training Loss:  77.7477  Training Accuracy:  0.758143\n",
      "Epoch:  271  Training Loss:  77.6463  Training Accuracy:  0.758672\n",
      "Epoch:  272  Training Loss:  77.4866  Training Accuracy:  0.7602\n",
      "Epoch:  273  Training Loss:  77.3447  Training Accuracy:  0.761082\n",
      "Epoch:  274  Training Loss:  77.1812  Training Accuracy:  0.76214\n",
      "Epoch:  275  Training Loss:  77.0511  Training Accuracy:  0.763081\n",
      "Epoch:  276  Training Loss:  76.8964  Training Accuracy:  0.763904\n",
      "Epoch:  277  Training Loss:  76.7673  Training Accuracy:  0.764844\n",
      "Epoch:  278  Training Loss:  76.6091  Training Accuracy:  0.765609\n",
      "Epoch:  279  Training Loss:  76.4697  Training Accuracy:  0.766432\n",
      "Epoch:  280  Training Loss:  76.258  Training Accuracy:  0.767078\n",
      "Epoch:  281  Training Loss:  76.1024  Training Accuracy:  0.768607\n",
      "Epoch:  282  Training Loss:  75.8861  Training Accuracy:  0.769724\n",
      "Epoch:  283  Training Loss:  75.7257  Training Accuracy:  0.770429\n",
      "Epoch:  284  Training Loss:  75.5238  Training Accuracy:  0.771429\n",
      "Epoch:  285  Training Loss:  75.3656  Training Accuracy:  0.772369\n",
      "Epoch:  286  Training Loss:  75.1587  Training Accuracy:  0.773486\n",
      "Epoch:  287  Training Loss:  74.9981  Training Accuracy:  0.774545\n",
      "Epoch:  288  Training Loss:  74.7852  Training Accuracy:  0.775309\n",
      "Epoch:  289  Training Loss:  74.6299  Training Accuracy:  0.776308\n",
      "Epoch:  290  Training Loss:  74.4345  Training Accuracy:  0.776955\n",
      "Epoch:  291  Training Loss:  74.2639  Training Accuracy:  0.777484\n",
      "Epoch:  292  Training Loss:  74.0431  Training Accuracy:  0.77866\n",
      "Epoch:  293  Training Loss:  73.8839  Training Accuracy:  0.779601\n",
      "Epoch:  294  Training Loss:  73.6422  Training Accuracy:  0.780071\n",
      "Epoch:  295  Training Loss:  73.4694  Training Accuracy:  0.780717\n",
      "Epoch:  296  Training Loss:  73.266  Training Accuracy:  0.78207\n",
      "Epoch:  297  Training Loss:  73.086  Training Accuracy:  0.782716\n",
      "Epoch:  298  Training Loss:  72.8734  Training Accuracy:  0.783422\n",
      "Epoch:  299  Training Loss:  72.7179  Training Accuracy:  0.784774\n",
      "Epoch:  300  Training Loss:  72.5057  Training Accuracy:  0.785479\n",
      "Epoch:  301  Training Loss:  72.3618  Training Accuracy:  0.786008\n",
      "Epoch:  302  Training Loss:  72.1797  Training Accuracy:  0.786773\n",
      "Epoch:  303  Training Loss:  72.004  Training Accuracy:  0.787772\n",
      "Epoch:  304  Training Loss:  71.8155  Training Accuracy:  0.788595\n",
      "Epoch:  305  Training Loss:  71.6542  Training Accuracy:  0.789595\n",
      "Epoch:  306  Training Loss:  71.4666  Training Accuracy:  0.790535\n",
      "Epoch:  307  Training Loss:  71.3098  Training Accuracy:  0.791064\n",
      "Epoch:  308  Training Loss:  71.0993  Training Accuracy:  0.79177\n",
      "Epoch:  309  Training Loss:  70.922  Training Accuracy:  0.792593\n",
      "Epoch:  310  Training Loss:  70.6806  Training Accuracy:  0.79324\n",
      "Epoch:  311  Training Loss:  70.4866  Training Accuracy:  0.794357\n",
      "Epoch:  312  Training Loss:  70.2388  Training Accuracy:  0.795415\n",
      "Epoch:  313  Training Loss:  70.0231  Training Accuracy:  0.796061\n",
      "Epoch:  314  Training Loss:  69.7792  Training Accuracy:  0.796943\n",
      "Epoch:  315  Training Loss:  69.5866  Training Accuracy:  0.797825\n",
      "Epoch:  316  Training Loss:  69.3255  Training Accuracy:  0.798766\n",
      "Epoch:  317  Training Loss:  69.1182  Training Accuracy:  0.799765\n",
      "Epoch:  318  Training Loss:  68.877  Training Accuracy:  0.800471\n",
      "Epoch:  319  Training Loss:  68.678  Training Accuracy:  0.801176\n",
      "Epoch:  320  Training Loss:  68.4489  Training Accuracy:  0.802175\n",
      "Epoch:  321  Training Loss:  68.2546  Training Accuracy:  0.803528\n",
      "Epoch:  322  Training Loss:  68.0204  Training Accuracy:  0.804057\n",
      "Epoch:  323  Training Loss:  67.8215  Training Accuracy:  0.805056\n",
      "Epoch:  324  Training Loss:  67.5689  Training Accuracy:  0.805703\n",
      "Epoch:  325  Training Loss:  67.3994  Training Accuracy:  0.806643\n",
      "Epoch:  326  Training Loss:  67.1665  Training Accuracy:  0.807584\n",
      "Epoch:  327  Training Loss:  66.9991  Training Accuracy:  0.808466\n",
      "Epoch:  328  Training Loss:  66.779  Training Accuracy:  0.809583\n",
      "Epoch:  329  Training Loss:  66.5889  Training Accuracy:  0.810935\n",
      "Epoch:  330  Training Loss:  66.3733  Training Accuracy:  0.811876\n",
      "Epoch:  331  Training Loss:  66.1664  Training Accuracy:  0.812875\n",
      "Epoch:  332  Training Loss:  65.9317  Training Accuracy:  0.813169\n",
      "Epoch:  333  Training Loss:  65.7173  Training Accuracy:  0.813933\n",
      "Epoch:  334  Training Loss:  65.4631  Training Accuracy:  0.814815\n",
      "Epoch:  335  Training Loss:  65.3055  Training Accuracy:  0.815579\n",
      "Epoch:  336  Training Loss:  65.1015  Training Accuracy:  0.816814\n",
      "Epoch:  337  Training Loss:  64.8682  Training Accuracy:  0.817343\n",
      "Epoch:  338  Training Loss:  64.6646  Training Accuracy:  0.817637\n",
      "Epoch:  339  Training Loss:  64.4155  Training Accuracy:  0.81846\n",
      "Epoch:  340  Training Loss:  64.1976  Training Accuracy:  0.819636\n",
      "Epoch:  341  Training Loss:  63.9402  Training Accuracy:  0.820106\n",
      "Epoch:  342  Training Loss:  63.729  Training Accuracy:  0.820929\n",
      "Epoch:  343  Training Loss:  63.4791  Training Accuracy:  0.821341\n",
      "Epoch:  344  Training Loss:  63.2764  Training Accuracy:  0.821987\n",
      "Epoch:  345  Training Loss:  62.9825  Training Accuracy:  0.822928\n",
      "Epoch:  346  Training Loss:  62.7859  Training Accuracy:  0.823457\n",
      "Epoch:  347  Training Loss:  62.5496  Training Accuracy:  0.823986\n",
      "Epoch:  348  Training Loss:  62.2909  Training Accuracy:  0.824515\n",
      "Epoch:  349  Training Loss:  62.0948  Training Accuracy:  0.825338\n",
      "Epoch:  350  Training Loss:  61.8238  Training Accuracy:  0.825691\n",
      "Epoch:  351  Training Loss:  61.642  Training Accuracy:  0.826397\n",
      "Epoch:  352  Training Loss:  61.3727  Training Accuracy:  0.826984\n",
      "Epoch:  353  Training Loss:  61.1881  Training Accuracy:  0.827749\n",
      "Epoch:  354  Training Loss:  60.935  Training Accuracy:  0.828513\n",
      "Epoch:  355  Training Loss:  60.7526  Training Accuracy:  0.829101\n",
      "Epoch:  356  Training Loss:  60.5229  Training Accuracy:  0.829983\n",
      "Epoch:  357  Training Loss:  60.3139  Training Accuracy:  0.830512\n",
      "Epoch:  358  Training Loss:  60.0761  Training Accuracy:  0.831511\n",
      "Epoch:  359  Training Loss:  59.9026  Training Accuracy:  0.832334\n",
      "Epoch:  360  Training Loss:  59.6333  Training Accuracy:  0.832981\n",
      "Epoch:  361  Training Loss:  59.472  Training Accuracy:  0.833392\n",
      "Epoch:  362  Training Loss:  59.277  Training Accuracy:  0.833804\n",
      "Epoch:  363  Training Loss:  59.0362  Training Accuracy:  0.834216\n",
      "Epoch:  364  Training Loss:  58.858  Training Accuracy:  0.835156\n",
      "Epoch:  365  Training Loss:  58.5604  Training Accuracy:  0.836097\n",
      "Epoch:  366  Training Loss:  58.4076  Training Accuracy:  0.837037\n",
      "Epoch:  367  Training Loss:  58.2028  Training Accuracy:  0.838096\n",
      "Epoch:  368  Training Loss:  57.9543  Training Accuracy:  0.83886\n",
      "Epoch:  369  Training Loss:  57.7173  Training Accuracy:  0.839507\n",
      "Epoch:  370  Training Loss:  57.4468  Training Accuracy:  0.840036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  57.2533  Training Accuracy:  0.840682\n",
      "Epoch:  372  Training Loss:  56.9899  Training Accuracy:  0.840976\n",
      "Epoch:  373  Training Loss:  56.7807  Training Accuracy:  0.841388\n",
      "Epoch:  374  Training Loss:  56.4752  Training Accuracy:  0.841917\n",
      "Epoch:  375  Training Loss:  56.3193  Training Accuracy:  0.84227\n",
      "Epoch:  376  Training Loss:  56.0929  Training Accuracy:  0.842622\n",
      "Epoch:  377  Training Loss:  55.8326  Training Accuracy:  0.84321\n",
      "Epoch:  378  Training Loss:  55.6409  Training Accuracy:  0.843504\n",
      "Epoch:  379  Training Loss:  55.3799  Training Accuracy:  0.843857\n",
      "Epoch:  380  Training Loss:  55.175  Training Accuracy:  0.844562\n",
      "Epoch:  381  Training Loss:  54.8655  Training Accuracy:  0.845209\n",
      "Epoch:  382  Training Loss:  54.7099  Training Accuracy:  0.845503\n",
      "Epoch:  383  Training Loss:  54.4977  Training Accuracy:  0.845915\n",
      "Epoch:  384  Training Loss:  54.1896  Training Accuracy:  0.846502\n",
      "Epoch:  385  Training Loss:  54.0358  Training Accuracy:  0.847149\n",
      "Epoch:  386  Training Loss:  53.8184  Training Accuracy:  0.847619\n",
      "Epoch:  387  Training Loss:  53.5489  Training Accuracy:  0.847972\n",
      "Epoch:  388  Training Loss:  53.3693  Training Accuracy:  0.848442\n",
      "Epoch:  389  Training Loss:  53.1103  Training Accuracy:  0.848913\n",
      "Epoch:  390  Training Loss:  52.9286  Training Accuracy:  0.849324\n",
      "Epoch:  391  Training Loss:  52.6536  Training Accuracy:  0.85003\n",
      "Epoch:  392  Training Loss:  52.5009  Training Accuracy:  0.850618\n",
      "Epoch:  393  Training Loss:  52.2405  Training Accuracy:  0.851147\n",
      "Epoch:  394  Training Loss:  52.0857  Training Accuracy:  0.851558\n",
      "Epoch:  395  Training Loss:  51.8057  Training Accuracy:  0.852264\n",
      "Epoch:  396  Training Loss:  51.6487  Training Accuracy:  0.852852\n",
      "Epoch:  397  Training Loss:  51.4708  Training Accuracy:  0.853381\n",
      "Epoch:  398  Training Loss:  51.2229  Training Accuracy:  0.853792\n",
      "Epoch:  399  Training Loss:  51.0655  Training Accuracy:  0.85438\n",
      "Epoch:  400  Training Loss:  50.7817  Training Accuracy:  0.855086\n",
      "Epoch:  401  Training Loss:  50.6691  Training Accuracy:  0.855497\n",
      "Epoch:  402  Training Loss:  50.4981  Training Accuracy:  0.855967\n",
      "Epoch:  403  Training Loss:  50.2775  Training Accuracy:  0.856438\n",
      "Epoch:  404  Training Loss:  50.0686  Training Accuracy:  0.856908\n",
      "Epoch:  405  Training Loss:  49.8991  Training Accuracy:  0.857378\n",
      "Epoch:  406  Training Loss:  49.7075  Training Accuracy:  0.858025\n",
      "Epoch:  407  Training Loss:  49.5266  Training Accuracy:  0.85826\n",
      "Epoch:  408  Training Loss:  49.3375  Training Accuracy:  0.858848\n",
      "Epoch:  409  Training Loss:  49.1452  Training Accuracy:  0.859554\n",
      "Epoch:  410  Training Loss:  48.9726  Training Accuracy:  0.859848\n",
      "Epoch:  411  Training Loss:  48.7671  Training Accuracy:  0.860083\n",
      "Epoch:  412  Training Loss:  48.5785  Training Accuracy:  0.860553\n",
      "Epoch:  413  Training Loss:  48.4039  Training Accuracy:  0.861259\n",
      "Epoch:  414  Training Loss:  48.2167  Training Accuracy:  0.861552\n",
      "Epoch:  415  Training Loss:  48.0486  Training Accuracy:  0.862023\n",
      "Epoch:  416  Training Loss:  47.8709  Training Accuracy:  0.862493\n",
      "Epoch:  417  Training Loss:  47.6677  Training Accuracy:  0.862905\n",
      "Epoch:  418  Training Loss:  47.4997  Training Accuracy:  0.863551\n",
      "Epoch:  419  Training Loss:  47.2923  Training Accuracy:  0.863728\n",
      "Epoch:  420  Training Loss:  47.1228  Training Accuracy:  0.864374\n",
      "Epoch:  421  Training Loss:  46.9154  Training Accuracy:  0.864609\n",
      "Epoch:  422  Training Loss:  46.7452  Training Accuracy:  0.864668\n",
      "Epoch:  423  Training Loss:  46.5459  Training Accuracy:  0.865256\n",
      "Epoch:  424  Training Loss:  46.3551  Training Accuracy:  0.865668\n",
      "Epoch:  425  Training Loss:  46.1848  Training Accuracy:  0.86602\n",
      "Epoch:  426  Training Loss:  45.9813  Training Accuracy:  0.866373\n",
      "Epoch:  427  Training Loss:  45.8123  Training Accuracy:  0.866608\n",
      "Epoch:  428  Training Loss:  45.6117  Training Accuracy:  0.866843\n",
      "Epoch:  429  Training Loss:  45.4362  Training Accuracy:  0.867372\n",
      "Epoch:  430  Training Loss:  45.2482  Training Accuracy:  0.867725\n",
      "Epoch:  431  Training Loss:  45.0696  Training Accuracy:  0.868372\n",
      "Epoch:  432  Training Loss:  44.8499  Training Accuracy:  0.86896\n",
      "Epoch:  433  Training Loss:  44.728  Training Accuracy:  0.869313\n",
      "Epoch:  434  Training Loss:  44.5052  Training Accuracy:  0.869606\n",
      "Epoch:  435  Training Loss:  44.3462  Training Accuracy:  0.869959\n",
      "Epoch:  436  Training Loss:  44.0904  Training Accuracy:  0.870371\n",
      "Epoch:  437  Training Loss:  43.9919  Training Accuracy:  0.871017\n",
      "Epoch:  438  Training Loss:  43.8259  Training Accuracy:  0.871605\n",
      "Epoch:  439  Training Loss:  43.5499  Training Accuracy:  0.87184\n",
      "Epoch:  440  Training Loss:  43.4567  Training Accuracy:  0.872076\n",
      "Epoch:  441  Training Loss:  43.2688  Training Accuracy:  0.872605\n",
      "Epoch:  442  Training Loss:  43.009  Training Accuracy:  0.873016\n",
      "Epoch:  443  Training Loss:  42.9099  Training Accuracy:  0.873722\n",
      "Epoch:  444  Training Loss:  42.7386  Training Accuracy:  0.874133\n",
      "Epoch:  445  Training Loss:  42.5219  Training Accuracy:  0.874721\n",
      "Epoch:  446  Training Loss:  42.3371  Training Accuracy:  0.875192\n",
      "Epoch:  447  Training Loss:  42.1376  Training Accuracy:  0.875485\n",
      "Epoch:  448  Training Loss:  41.9845  Training Accuracy:  0.875956\n",
      "Epoch:  449  Training Loss:  41.7153  Training Accuracy:  0.876661\n",
      "Epoch:  450  Training Loss:  41.6358  Training Accuracy:  0.876838\n",
      "Epoch:  451  Training Loss:  41.4412  Training Accuracy:  0.877132\n",
      "Epoch:  452  Training Loss:  41.2034  Training Accuracy:  0.877425\n",
      "Epoch:  453  Training Loss:  41.0644  Training Accuracy:  0.877543\n",
      "Epoch:  454  Training Loss:  40.8759  Training Accuracy:  0.878131\n",
      "Epoch:  455  Training Loss:  40.6135  Training Accuracy:  0.878366\n",
      "Epoch:  456  Training Loss:  40.4879  Training Accuracy:  0.878248\n",
      "Epoch:  457  Training Loss:  40.3029  Training Accuracy:  0.878542\n",
      "Epoch:  458  Training Loss:  40.0194  Training Accuracy:  0.878895\n",
      "Epoch:  459  Training Loss:  39.9019  Training Accuracy:  0.87913\n",
      "Epoch:  460  Training Loss:  39.7245  Training Accuracy:  0.879366\n",
      "Epoch:  461  Training Loss:  39.4821  Training Accuracy:  0.880071\n",
      "Epoch:  462  Training Loss:  39.3315  Training Accuracy:  0.880012\n",
      "Epoch:  463  Training Loss:  39.1842  Training Accuracy:  0.880424\n",
      "Epoch:  464  Training Loss:  38.9496  Training Accuracy:  0.880776\n",
      "Epoch:  465  Training Loss:  38.8011  Training Accuracy:  0.881247\n",
      "Epoch:  466  Training Loss:  38.5909  Training Accuracy:  0.881423\n",
      "Epoch:  467  Training Loss:  38.3516  Training Accuracy:  0.881717\n",
      "Epoch:  468  Training Loss:  38.2612  Training Accuracy:  0.882187\n",
      "Epoch:  469  Training Loss:  38.086  Training Accuracy:  0.882481\n",
      "Epoch:  470  Training Loss:  37.8034  Training Accuracy:  0.882716\n",
      "Epoch:  471  Training Loss:  37.7435  Training Accuracy:  0.882952\n",
      "Epoch:  472  Training Loss:  37.5702  Training Accuracy:  0.883363\n",
      "Epoch:  473  Training Loss:  37.3053  Training Accuracy:  0.884127\n",
      "Epoch:  474  Training Loss:  37.1868  Training Accuracy:  0.884539\n",
      "Epoch:  475  Training Loss:  37.0464  Training Accuracy:  0.884656\n",
      "Epoch:  476  Training Loss:  36.7817  Training Accuracy:  0.885303\n",
      "Epoch:  477  Training Loss:  36.6728  Training Accuracy:  0.885362\n",
      "Epoch:  478  Training Loss:  36.5035  Training Accuracy:  0.885832\n",
      "Epoch:  479  Training Loss:  36.3333  Training Accuracy:  0.886303\n",
      "Epoch:  480  Training Loss:  36.1291  Training Accuracy:  0.886832\n",
      "Epoch:  481  Training Loss:  36.0011  Training Accuracy:  0.887361\n",
      "Epoch:  482  Training Loss:  35.7392  Training Accuracy:  0.887537\n",
      "Epoch:  483  Training Loss:  35.638  Training Accuracy:  0.887831\n",
      "Epoch:  484  Training Loss:  35.4787  Training Accuracy:  0.888008\n",
      "Epoch:  485  Training Loss:  35.2318  Training Accuracy:  0.888184\n",
      "Epoch:  486  Training Loss:  35.133  Training Accuracy:  0.888537\n",
      "Epoch:  487  Training Loss:  34.9813  Training Accuracy:  0.889124\n",
      "Epoch:  488  Training Loss:  34.7473  Training Accuracy:  0.889418\n",
      "Epoch:  489  Training Loss:  34.64  Training Accuracy:  0.889654\n",
      "Epoch:  490  Training Loss:  34.4739  Training Accuracy:  0.890065\n",
      "Epoch:  491  Training Loss:  34.2242  Training Accuracy:  0.890241\n",
      "Epoch:  492  Training Loss:  34.1285  Training Accuracy:  0.890653\n",
      "Epoch:  493  Training Loss:  33.9836  Training Accuracy:  0.890947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  33.7555  Training Accuracy:  0.891006\n",
      "Epoch:  495  Training Loss:  33.6385  Training Accuracy:  0.891652\n",
      "Epoch:  496  Training Loss:  33.4937  Training Accuracy:  0.892064\n",
      "Epoch:  497  Training Loss:  33.2549  Training Accuracy:  0.892711\n",
      "Epoch:  498  Training Loss:  33.1754  Training Accuracy:  0.893063\n",
      "Epoch:  499  Training Loss:  33.0073  Training Accuracy:  0.893592\n",
      "Testing Accuracy: 0.816719\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  32.2218  Training Accuracy:  0.0400353\n",
      "Epoch:  1  Training Loss:  37.5637  Training Accuracy:  0.0389183\n",
      "Epoch:  2  Training Loss:  40.4896  Training Accuracy:  0.0402116\n",
      "Epoch:  3  Training Loss:  47.2207  Training Accuracy:  0.0464433\n",
      "Epoch:  4  Training Loss:  53.4705  Training Accuracy:  0.0513815\n",
      "Epoch:  5  Training Loss:  58.6604  Training Accuracy:  0.0567313\n",
      "Epoch:  6  Training Loss:  63.0149  Training Accuracy:  0.0622575\n",
      "Epoch:  7  Training Loss:  64.3488  Training Accuracy:  0.0673721\n",
      "Epoch:  8  Training Loss:  65.0686  Training Accuracy:  0.0711934\n",
      "Epoch:  9  Training Loss:  66.0268  Training Accuracy:  0.0759553\n",
      "Epoch:  10  Training Loss:  67.3553  Training Accuracy:  0.081893\n",
      "Epoch:  11  Training Loss:  67.9285  Training Accuracy:  0.0878895\n",
      "Epoch:  12  Training Loss:  68.0664  Training Accuracy:  0.0938272\n",
      "Epoch:  13  Training Loss:  68.2146  Training Accuracy:  0.0997648\n",
      "Epoch:  14  Training Loss:  68.3915  Training Accuracy:  0.10682\n",
      "Epoch:  15  Training Loss:  68.5085  Training Accuracy:  0.114051\n",
      "Epoch:  16  Training Loss:  68.4092  Training Accuracy:  0.122163\n",
      "Epoch:  17  Training Loss:  68.2599  Training Accuracy:  0.129277\n",
      "Epoch:  18  Training Loss:  68.3095  Training Accuracy:  0.137625\n",
      "Epoch:  19  Training Loss:  68.1289  Training Accuracy:  0.145091\n",
      "Epoch:  20  Training Loss:  67.8134  Training Accuracy:  0.151852\n",
      "Epoch:  21  Training Loss:  67.3105  Training Accuracy:  0.156908\n",
      "Epoch:  22  Training Loss:  66.919  Training Accuracy:  0.163668\n",
      "Epoch:  23  Training Loss:  66.2971  Training Accuracy:  0.170312\n",
      "Epoch:  24  Training Loss:  65.773  Training Accuracy:  0.176249\n",
      "Epoch:  25  Training Loss:  65.2643  Training Accuracy:  0.182363\n",
      "Epoch:  26  Training Loss:  64.8279  Training Accuracy:  0.188125\n",
      "Epoch:  27  Training Loss:  64.3868  Training Accuracy:  0.194885\n",
      "Epoch:  28  Training Loss:  63.8559  Training Accuracy:  0.201411\n",
      "Epoch:  29  Training Loss:  63.3954  Training Accuracy:  0.208172\n",
      "Epoch:  30  Training Loss:  62.9383  Training Accuracy:  0.21458\n",
      "Epoch:  31  Training Loss:  62.411  Training Accuracy:  0.220576\n",
      "Epoch:  32  Training Loss:  61.858  Training Accuracy:  0.225985\n",
      "Epoch:  33  Training Loss:  61.2853  Training Accuracy:  0.231864\n",
      "Epoch:  34  Training Loss:  60.765  Training Accuracy:  0.238683\n",
      "Epoch:  35  Training Loss:  60.2301  Training Accuracy:  0.244503\n",
      "Epoch:  36  Training Loss:  59.7625  Training Accuracy:  0.25097\n",
      "Epoch:  37  Training Loss:  59.164  Training Accuracy:  0.257319\n",
      "Epoch:  38  Training Loss:  58.6831  Training Accuracy:  0.262081\n",
      "Epoch:  39  Training Loss:  58.2565  Training Accuracy:  0.268665\n",
      "Epoch:  40  Training Loss:  57.832  Training Accuracy:  0.273898\n",
      "Epoch:  41  Training Loss:  57.3898  Training Accuracy:  0.281305\n",
      "Epoch:  42  Training Loss:  56.9949  Training Accuracy:  0.287302\n",
      "Epoch:  43  Training Loss:  56.5842  Training Accuracy:  0.293063\n",
      "Epoch:  44  Training Loss:  56.1821  Training Accuracy:  0.299118\n",
      "Epoch:  45  Training Loss:  55.7779  Training Accuracy:  0.304644\n",
      "Epoch:  46  Training Loss:  55.457  Training Accuracy:  0.310288\n",
      "Epoch:  47  Training Loss:  55.1588  Training Accuracy:  0.315991\n",
      "Epoch:  48  Training Loss:  54.8703  Training Accuracy:  0.321752\n",
      "Epoch:  49  Training Loss:  54.65  Training Accuracy:  0.325867\n",
      "Epoch:  50  Training Loss:  54.4287  Training Accuracy:  0.330805\n",
      "Epoch:  51  Training Loss:  54.2425  Training Accuracy:  0.335391\n",
      "Epoch:  52  Training Loss:  54.122  Training Accuracy:  0.3398\n",
      "Epoch:  53  Training Loss:  54.0099  Training Accuracy:  0.345091\n",
      "Epoch:  54  Training Loss:  53.9154  Training Accuracy:  0.349677\n",
      "Epoch:  55  Training Loss:  53.8545  Training Accuracy:  0.354615\n",
      "Epoch:  56  Training Loss:  53.7811  Training Accuracy:  0.359142\n",
      "Epoch:  57  Training Loss:  53.7868  Training Accuracy:  0.363845\n",
      "Epoch:  58  Training Loss:  53.7381  Training Accuracy:  0.368136\n",
      "Epoch:  59  Training Loss:  53.6732  Training Accuracy:  0.372369\n",
      "Epoch:  60  Training Loss:  53.6693  Training Accuracy:  0.376484\n",
      "Epoch:  61  Training Loss:  53.667  Training Accuracy:  0.380306\n",
      "Epoch:  62  Training Loss:  53.7047  Training Accuracy:  0.384538\n",
      "Epoch:  63  Training Loss:  53.754  Training Accuracy:  0.387478\n",
      "Epoch:  64  Training Loss:  53.8328  Training Accuracy:  0.39077\n",
      "Epoch:  65  Training Loss:  53.9119  Training Accuracy:  0.394885\n",
      "Epoch:  66  Training Loss:  53.9756  Training Accuracy:  0.399647\n",
      "Epoch:  67  Training Loss:  54.0768  Training Accuracy:  0.404233\n",
      "Epoch:  68  Training Loss:  54.1881  Training Accuracy:  0.407231\n",
      "Epoch:  69  Training Loss:  54.2837  Training Accuracy:  0.411287\n",
      "Epoch:  70  Training Loss:  54.4015  Training Accuracy:  0.415579\n",
      "Epoch:  71  Training Loss:  54.5355  Training Accuracy:  0.419929\n",
      "Epoch:  72  Training Loss:  54.733  Training Accuracy:  0.423986\n",
      "Epoch:  73  Training Loss:  54.9324  Training Accuracy:  0.427043\n",
      "Epoch:  74  Training Loss:  55.1113  Training Accuracy:  0.43057\n",
      "Epoch:  75  Training Loss:  55.3103  Training Accuracy:  0.434156\n",
      "Epoch:  76  Training Loss:  55.5271  Training Accuracy:  0.437919\n",
      "Epoch:  77  Training Loss:  55.7887  Training Accuracy:  0.44127\n",
      "Epoch:  78  Training Loss:  56.0492  Training Accuracy:  0.445267\n",
      "Epoch:  79  Training Loss:  56.2938  Training Accuracy:  0.450029\n",
      "Epoch:  80  Training Loss:  56.5938  Training Accuracy:  0.452969\n",
      "Epoch:  81  Training Loss:  56.9344  Training Accuracy:  0.456849\n",
      "Epoch:  82  Training Loss:  57.2452  Training Accuracy:  0.459377\n",
      "Epoch:  83  Training Loss:  57.5735  Training Accuracy:  0.462316\n",
      "Epoch:  84  Training Loss:  57.9023  Training Accuracy:  0.465197\n",
      "Epoch:  85  Training Loss:  58.2892  Training Accuracy:  0.46796\n",
      "Epoch:  86  Training Loss:  58.6408  Training Accuracy:  0.470311\n",
      "Epoch:  87  Training Loss:  59.0281  Training Accuracy:  0.472957\n",
      "Epoch:  88  Training Loss:  59.3755  Training Accuracy:  0.47572\n",
      "Epoch:  89  Training Loss:  59.7235  Training Accuracy:  0.478895\n",
      "Epoch:  90  Training Loss:  60.073  Training Accuracy:  0.482246\n",
      "Epoch:  91  Training Loss:  60.4846  Training Accuracy:  0.485185\n",
      "Epoch:  92  Training Loss:  60.964  Training Accuracy:  0.487066\n",
      "Epoch:  93  Training Loss:  61.4064  Training Accuracy:  0.489241\n",
      "Epoch:  94  Training Loss:  61.8158  Training Accuracy:  0.491946\n",
      "Epoch:  95  Training Loss:  62.2178  Training Accuracy:  0.494826\n",
      "Epoch:  96  Training Loss:  62.6427  Training Accuracy:  0.497707\n",
      "Epoch:  97  Training Loss:  63.0626  Training Accuracy:  0.499412\n",
      "Epoch:  98  Training Loss:  63.5041  Training Accuracy:  0.502528\n",
      "Epoch:  99  Training Loss:  63.8896  Training Accuracy:  0.504056\n",
      "Epoch:  100  Training Loss:  64.3219  Training Accuracy:  0.506937\n",
      "Epoch:  101  Training Loss:  64.6975  Training Accuracy:  0.509171\n",
      "Epoch:  102  Training Loss:  65.1215  Training Accuracy:  0.511699\n",
      "Epoch:  103  Training Loss:  65.4812  Training Accuracy:  0.514462\n",
      "Epoch:  104  Training Loss:  65.8616  Training Accuracy:  0.516637\n",
      "Epoch:  105  Training Loss:  66.2025  Training Accuracy:  0.519459\n",
      "Epoch:  106  Training Loss:  66.6288  Training Accuracy:  0.521752\n",
      "Epoch:  107  Training Loss:  67.0236  Training Accuracy:  0.52428\n",
      "Epoch:  108  Training Loss:  67.3517  Training Accuracy:  0.527337\n",
      "Epoch:  109  Training Loss:  67.6815  Training Accuracy:  0.529394\n",
      "Epoch:  110  Training Loss:  68.0237  Training Accuracy:  0.531805\n",
      "Epoch:  111  Training Loss:  68.3497  Training Accuracy:  0.533745\n",
      "Epoch:  112  Training Loss:  68.681  Training Accuracy:  0.535626\n",
      "Epoch:  113  Training Loss:  69.007  Training Accuracy:  0.538036\n",
      "Epoch:  114  Training Loss:  69.3136  Training Accuracy:  0.540447\n",
      "Epoch:  115  Training Loss:  69.6999  Training Accuracy:  0.543033\n",
      "Epoch:  116  Training Loss:  69.9891  Training Accuracy:  0.544797\n",
      "Epoch:  117  Training Loss:  70.2144  Training Accuracy:  0.546678\n",
      "Epoch:  118  Training Loss:  70.6863  Training Accuracy:  0.548971\n",
      "Epoch:  119  Training Loss:  70.9812  Training Accuracy:  0.550676\n",
      "Epoch:  120  Training Loss:  71.2749  Training Accuracy:  0.552792\n",
      "Epoch:  121  Training Loss:  71.5668  Training Accuracy:  0.554968\n",
      "Epoch:  122  Training Loss:  71.8838  Training Accuracy:  0.55632\n",
      "Epoch:  123  Training Loss:  72.0913  Training Accuracy:  0.558319\n",
      "Epoch:  124  Training Loss:  72.2749  Training Accuracy:  0.560082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  72.5344  Training Accuracy:  0.56261\n",
      "Epoch:  126  Training Loss:  72.7922  Training Accuracy:  0.565256\n",
      "Epoch:  127  Training Loss:  73.0005  Training Accuracy:  0.567137\n",
      "Epoch:  128  Training Loss:  73.1991  Training Accuracy:  0.569253\n",
      "Epoch:  129  Training Loss:  73.3753  Training Accuracy:  0.571135\n",
      "Epoch:  130  Training Loss:  73.553  Training Accuracy:  0.572839\n",
      "Epoch:  131  Training Loss:  73.7402  Training Accuracy:  0.575015\n",
      "Epoch:  132  Training Loss:  73.8427  Training Accuracy:  0.576837\n",
      "Epoch:  133  Training Loss:  73.9527  Training Accuracy:  0.578542\n",
      "Epoch:  134  Training Loss:  74.051  Training Accuracy:  0.579894\n",
      "Epoch:  135  Training Loss:  74.2228  Training Accuracy:  0.581775\n",
      "Epoch:  136  Training Loss:  74.3039  Training Accuracy:  0.583539\n",
      "Epoch:  137  Training Loss:  74.3848  Training Accuracy:  0.585185\n",
      "Epoch:  138  Training Loss:  74.4707  Training Accuracy:  0.58689\n",
      "Epoch:  139  Training Loss:  74.6277  Training Accuracy:  0.589653\n",
      "Epoch:  140  Training Loss:  74.6851  Training Accuracy:  0.592005\n",
      "Epoch:  141  Training Loss:  74.7781  Training Accuracy:  0.594121\n",
      "Epoch:  142  Training Loss:  74.8343  Training Accuracy:  0.596179\n",
      "Epoch:  143  Training Loss:  74.8382  Training Accuracy:  0.597825\n",
      "Epoch:  144  Training Loss:  74.8888  Training Accuracy:  0.600118\n",
      "Epoch:  145  Training Loss:  74.9667  Training Accuracy:  0.602469\n",
      "Epoch:  146  Training Loss:  75.0675  Training Accuracy:  0.603939\n",
      "Epoch:  147  Training Loss:  75.1007  Training Accuracy:  0.605879\n",
      "Epoch:  148  Training Loss:  75.2058  Training Accuracy:  0.607878\n",
      "Epoch:  149  Training Loss:  75.2454  Training Accuracy:  0.609877\n",
      "Epoch:  150  Training Loss:  75.2498  Training Accuracy:  0.612522\n",
      "Epoch:  151  Training Loss:  75.381  Training Accuracy:  0.61458\n",
      "Epoch:  152  Training Loss:  75.378  Training Accuracy:  0.616167\n",
      "Epoch:  153  Training Loss:  75.3623  Training Accuracy:  0.617519\n",
      "Epoch:  154  Training Loss:  75.3434  Training Accuracy:  0.619694\n",
      "Epoch:  155  Training Loss:  75.3528  Training Accuracy:  0.622046\n",
      "Epoch:  156  Training Loss:  75.4687  Training Accuracy:  0.623927\n",
      "Epoch:  157  Training Loss:  75.3614  Training Accuracy:  0.62575\n",
      "Epoch:  158  Training Loss:  75.3725  Training Accuracy:  0.627102\n",
      "Epoch:  159  Training Loss:  75.2647  Training Accuracy:  0.629277\n",
      "Epoch:  160  Training Loss:  75.4769  Training Accuracy:  0.631158\n",
      "Epoch:  161  Training Loss:  75.3171  Training Accuracy:  0.633098\n",
      "Epoch:  162  Training Loss:  75.1882  Training Accuracy:  0.634568\n",
      "Epoch:  163  Training Loss:  75.3437  Training Accuracy:  0.636508\n",
      "Epoch:  164  Training Loss:  75.2818  Training Accuracy:  0.637919\n",
      "Epoch:  165  Training Loss:  75.2878  Training Accuracy:  0.639565\n",
      "Epoch:  166  Training Loss:  75.2078  Training Accuracy:  0.641094\n",
      "Epoch:  167  Training Loss:  75.3234  Training Accuracy:  0.643151\n",
      "Epoch:  168  Training Loss:  75.2328  Training Accuracy:  0.644268\n",
      "Epoch:  169  Training Loss:  75.194  Training Accuracy:  0.645855\n",
      "Epoch:  170  Training Loss:  75.1718  Training Accuracy:  0.647795\n",
      "Epoch:  171  Training Loss:  75.1711  Training Accuracy:  0.649206\n",
      "Epoch:  172  Training Loss:  75.2992  Training Accuracy:  0.650676\n",
      "Epoch:  173  Training Loss:  75.1718  Training Accuracy:  0.652381\n",
      "Epoch:  174  Training Loss:  75.3536  Training Accuracy:  0.654086\n",
      "Epoch:  175  Training Loss:  75.2076  Training Accuracy:  0.65585\n",
      "Epoch:  176  Training Loss:  75.2355  Training Accuracy:  0.658319\n",
      "Epoch:  177  Training Loss:  75.2837  Training Accuracy:  0.659847\n",
      "Epoch:  178  Training Loss:  75.2484  Training Accuracy:  0.660729\n",
      "Epoch:  179  Training Loss:  75.1614  Training Accuracy:  0.662669\n",
      "Epoch:  180  Training Loss:  75.2572  Training Accuracy:  0.66455\n",
      "Epoch:  181  Training Loss:  75.2412  Training Accuracy:  0.666079\n",
      "Epoch:  182  Training Loss:  75.1926  Training Accuracy:  0.667255\n",
      "Epoch:  183  Training Loss:  75.1941  Training Accuracy:  0.669253\n",
      "Epoch:  184  Training Loss:  75.1436  Training Accuracy:  0.671252\n",
      "Epoch:  185  Training Loss:  74.889  Training Accuracy:  0.672722\n",
      "Epoch:  186  Training Loss:  74.9614  Training Accuracy:  0.67478\n",
      "Epoch:  187  Training Loss:  74.9441  Training Accuracy:  0.675779\n",
      "Epoch:  188  Training Loss:  74.9338  Training Accuracy:  0.677837\n",
      "Epoch:  189  Training Loss:  74.8192  Training Accuracy:  0.678542\n",
      "Epoch:  190  Training Loss:  74.8001  Training Accuracy:  0.679659\n",
      "Epoch:  191  Training Loss:  74.7443  Training Accuracy:  0.680894\n",
      "Epoch:  192  Training Loss:  74.7334  Training Accuracy:  0.682246\n",
      "Epoch:  193  Training Loss:  74.6474  Training Accuracy:  0.683657\n",
      "Epoch:  194  Training Loss:  74.6491  Training Accuracy:  0.684774\n",
      "Epoch:  195  Training Loss:  74.5764  Training Accuracy:  0.686067\n",
      "Epoch:  196  Training Loss:  74.5302  Training Accuracy:  0.687537\n",
      "Epoch:  197  Training Loss:  74.4443  Training Accuracy:  0.688889\n",
      "Epoch:  198  Training Loss:  74.4029  Training Accuracy:  0.68983\n",
      "Epoch:  199  Training Loss:  74.2649  Training Accuracy:  0.690888\n",
      "Epoch:  200  Training Loss:  74.2085  Training Accuracy:  0.692299\n",
      "Epoch:  201  Training Loss:  74.1046  Training Accuracy:  0.693533\n",
      "Epoch:  202  Training Loss:  74.0429  Training Accuracy:  0.694944\n",
      "Epoch:  203  Training Loss:  73.9496  Training Accuracy:  0.696532\n",
      "Epoch:  204  Training Loss:  73.8808  Training Accuracy:  0.697413\n",
      "Epoch:  205  Training Loss:  73.7743  Training Accuracy:  0.698648\n",
      "Epoch:  206  Training Loss:  73.7123  Training Accuracy:  0.699941\n",
      "Epoch:  207  Training Loss:  73.6493  Training Accuracy:  0.701352\n",
      "Epoch:  208  Training Loss:  73.5578  Training Accuracy:  0.702528\n",
      "Epoch:  209  Training Loss:  73.5305  Training Accuracy:  0.704233\n",
      "Epoch:  210  Training Loss:  73.5821  Training Accuracy:  0.705997\n",
      "Epoch:  211  Training Loss:  73.3531  Training Accuracy:  0.707702\n",
      "Epoch:  212  Training Loss:  73.2736  Training Accuracy:  0.709171\n",
      "Epoch:  213  Training Loss:  73.1003  Training Accuracy:  0.710229\n",
      "Epoch:  214  Training Loss:  73.1249  Training Accuracy:  0.711582\n",
      "Epoch:  215  Training Loss:  73.1813  Training Accuracy:  0.712699\n",
      "Epoch:  216  Training Loss:  72.9922  Training Accuracy:  0.714051\n",
      "Epoch:  217  Training Loss:  72.7342  Training Accuracy:  0.715344\n",
      "Epoch:  218  Training Loss:  72.8393  Training Accuracy:  0.716461\n",
      "Epoch:  219  Training Loss:  72.5283  Training Accuracy:  0.718048\n",
      "Epoch:  220  Training Loss:  72.5269  Training Accuracy:  0.719283\n",
      "Epoch:  221  Training Loss:  72.2663  Training Accuracy:  0.720929\n",
      "Epoch:  222  Training Loss:  72.3767  Training Accuracy:  0.722575\n",
      "Epoch:  223  Training Loss:  72.1132  Training Accuracy:  0.723163\n",
      "Epoch:  224  Training Loss:  72.1744  Training Accuracy:  0.725044\n",
      "Epoch:  225  Training Loss:  71.9049  Training Accuracy:  0.725809\n",
      "Epoch:  226  Training Loss:  72.0122  Training Accuracy:  0.727102\n",
      "Epoch:  227  Training Loss:  71.7098  Training Accuracy:  0.728278\n",
      "Epoch:  228  Training Loss:  71.8048  Training Accuracy:  0.729395\n",
      "Epoch:  229  Training Loss:  71.4659  Training Accuracy:  0.730864\n",
      "Epoch:  230  Training Loss:  71.5394  Training Accuracy:  0.731981\n",
      "Epoch:  231  Training Loss:  71.236  Training Accuracy:  0.732804\n",
      "Epoch:  232  Training Loss:  71.3302  Training Accuracy:  0.734039\n",
      "Epoch:  233  Training Loss:  71.0301  Training Accuracy:  0.734686\n",
      "Epoch:  234  Training Loss:  71.1142  Training Accuracy:  0.73592\n",
      "Epoch:  235  Training Loss:  70.8045  Training Accuracy:  0.73692\n",
      "Epoch:  236  Training Loss:  70.8998  Training Accuracy:  0.738507\n",
      "Epoch:  237  Training Loss:  70.5861  Training Accuracy:  0.739683\n",
      "Epoch:  238  Training Loss:  70.7556  Training Accuracy:  0.740271\n",
      "Epoch:  239  Training Loss:  70.3732  Training Accuracy:  0.741505\n",
      "Epoch:  240  Training Loss:  70.5468  Training Accuracy:  0.742446\n",
      "Epoch:  241  Training Loss:  70.2263  Training Accuracy:  0.744268\n",
      "Epoch:  242  Training Loss:  70.1177  Training Accuracy:  0.745444\n",
      "Epoch:  243  Training Loss:  70.011  Training Accuracy:  0.746208\n",
      "Epoch:  244  Training Loss:  69.9022  Training Accuracy:  0.747149\n",
      "Epoch:  245  Training Loss:  70.0087  Training Accuracy:  0.747972\n",
      "Epoch:  246  Training Loss:  69.6684  Training Accuracy:  0.748854\n",
      "Epoch:  247  Training Loss:  69.7308  Training Accuracy:  0.749794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  69.451  Training Accuracy:  0.750618\n",
      "Epoch:  249  Training Loss:  69.6461  Training Accuracy:  0.751911\n",
      "Epoch:  250  Training Loss:  69.2156  Training Accuracy:  0.75244\n",
      "Epoch:  251  Training Loss:  69.3186  Training Accuracy:  0.753263\n",
      "Epoch:  252  Training Loss:  68.9731  Training Accuracy:  0.754615\n",
      "Epoch:  253  Training Loss:  69.0317  Training Accuracy:  0.755321\n",
      "Epoch:  254  Training Loss:  68.7254  Training Accuracy:  0.75632\n",
      "Epoch:  255  Training Loss:  68.7795  Training Accuracy:  0.757907\n",
      "Epoch:  256  Training Loss:  68.4769  Training Accuracy:  0.758789\n",
      "Epoch:  257  Training Loss:  68.5575  Training Accuracy:  0.760259\n",
      "Epoch:  258  Training Loss:  68.2534  Training Accuracy:  0.760964\n",
      "Epoch:  259  Training Loss:  68.3123  Training Accuracy:  0.761964\n",
      "Epoch:  260  Training Loss:  68.1177  Training Accuracy:  0.762728\n",
      "Epoch:  261  Training Loss:  67.891  Training Accuracy:  0.76361\n",
      "Epoch:  262  Training Loss:  67.8267  Training Accuracy:  0.764844\n",
      "Epoch:  263  Training Loss:  67.6665  Training Accuracy:  0.765609\n",
      "Epoch:  264  Training Loss:  67.4405  Training Accuracy:  0.766608\n",
      "Epoch:  265  Training Loss:  67.5654  Training Accuracy:  0.766961\n",
      "Epoch:  266  Training Loss:  67.2747  Training Accuracy:  0.768195\n",
      "Epoch:  267  Training Loss:  67.2742  Training Accuracy:  0.769254\n",
      "Epoch:  268  Training Loss:  66.9846  Training Accuracy:  0.770371\n",
      "Epoch:  269  Training Loss:  66.9785  Training Accuracy:  0.771135\n",
      "Epoch:  270  Training Loss:  66.7024  Training Accuracy:  0.772311\n",
      "Epoch:  271  Training Loss:  66.6454  Training Accuracy:  0.77331\n",
      "Epoch:  272  Training Loss:  66.3525  Training Accuracy:  0.774545\n",
      "Epoch:  273  Training Loss:  66.3436  Training Accuracy:  0.775191\n",
      "Epoch:  274  Training Loss:  66.04  Training Accuracy:  0.776367\n",
      "Epoch:  275  Training Loss:  66.0378  Training Accuracy:  0.776779\n",
      "Epoch:  276  Training Loss:  65.7207  Training Accuracy:  0.778013\n",
      "Epoch:  277  Training Loss:  65.7224  Training Accuracy:  0.778895\n",
      "Epoch:  278  Training Loss:  65.4013  Training Accuracy:  0.779777\n",
      "Epoch:  279  Training Loss:  65.4188  Training Accuracy:  0.780659\n",
      "Epoch:  280  Training Loss:  65.2059  Training Accuracy:  0.782128\n",
      "Epoch:  281  Training Loss:  64.9266  Training Accuracy:  0.783422\n",
      "Epoch:  282  Training Loss:  64.6678  Training Accuracy:  0.784715\n",
      "Epoch:  283  Training Loss:  64.7551  Training Accuracy:  0.785421\n",
      "Epoch:  284  Training Loss:  64.4876  Training Accuracy:  0.786538\n",
      "Epoch:  285  Training Loss:  64.3444  Training Accuracy:  0.787537\n",
      "Epoch:  286  Training Loss:  64.1704  Training Accuracy:  0.78836\n",
      "Epoch:  287  Training Loss:  63.8302  Training Accuracy:  0.789889\n",
      "Epoch:  288  Training Loss:  64.0244  Training Accuracy:  0.790947\n",
      "Epoch:  289  Training Loss:  63.6276  Training Accuracy:  0.792064\n",
      "Epoch:  290  Training Loss:  63.7262  Training Accuracy:  0.793122\n",
      "Epoch:  291  Training Loss:  63.3343  Training Accuracy:  0.794239\n",
      "Epoch:  292  Training Loss:  63.2284  Training Accuracy:  0.795238\n",
      "Epoch:  293  Training Loss:  62.9749  Training Accuracy:  0.795944\n",
      "Epoch:  294  Training Loss:  62.7885  Training Accuracy:  0.796355\n",
      "Epoch:  295  Training Loss:  62.8725  Training Accuracy:  0.797002\n",
      "Epoch:  296  Training Loss:  62.6404  Training Accuracy:  0.797707\n",
      "Epoch:  297  Training Loss:  62.3844  Training Accuracy:  0.798237\n",
      "Epoch:  298  Training Loss:  62.1341  Training Accuracy:  0.799236\n",
      "Epoch:  299  Training Loss:  62.1412  Training Accuracy:  0.799824\n",
      "Epoch:  300  Training Loss:  61.9012  Training Accuracy:  0.800294\n",
      "Epoch:  301  Training Loss:  61.754  Training Accuracy:  0.800882\n",
      "Epoch:  302  Training Loss:  61.5369  Training Accuracy:  0.801646\n",
      "Epoch:  303  Training Loss:  61.7127  Training Accuracy:  0.802176\n",
      "Epoch:  304  Training Loss:  61.3211  Training Accuracy:  0.802763\n",
      "Epoch:  305  Training Loss:  61.187  Training Accuracy:  0.803763\n",
      "Epoch:  306  Training Loss:  61.2525  Training Accuracy:  0.804645\n",
      "Epoch:  307  Training Loss:  60.9759  Training Accuracy:  0.805703\n",
      "Epoch:  308  Training Loss:  60.6843  Training Accuracy:  0.806526\n",
      "Epoch:  309  Training Loss:  60.873  Training Accuracy:  0.806937\n",
      "Epoch:  310  Training Loss:  60.5457  Training Accuracy:  0.807702\n",
      "Epoch:  311  Training Loss:  60.3473  Training Accuracy:  0.809113\n",
      "Epoch:  312  Training Loss:  60.1081  Training Accuracy:  0.81023\n",
      "Epoch:  313  Training Loss:  60.2827  Training Accuracy:  0.810641\n",
      "Epoch:  314  Training Loss:  59.9681  Training Accuracy:  0.810935\n",
      "Epoch:  315  Training Loss:  59.7571  Training Accuracy:  0.812228\n",
      "Epoch:  316  Training Loss:  59.5227  Training Accuracy:  0.813228\n",
      "Epoch:  317  Training Loss:  59.6705  Training Accuracy:  0.814051\n",
      "Epoch:  318  Training Loss:  59.341  Training Accuracy:  0.814521\n",
      "Epoch:  319  Training Loss:  59.3986  Training Accuracy:  0.814933\n",
      "Epoch:  320  Training Loss:  58.9339  Training Accuracy:  0.815521\n",
      "Epoch:  321  Training Loss:  58.913  Training Accuracy:  0.81605\n",
      "Epoch:  322  Training Loss:  58.5869  Training Accuracy:  0.817108\n",
      "Epoch:  323  Training Loss:  58.4414  Training Accuracy:  0.818225\n",
      "Epoch:  324  Training Loss:  58.5155  Training Accuracy:  0.818401\n",
      "Epoch:  325  Training Loss:  58.2978  Training Accuracy:  0.818871\n",
      "Epoch:  326  Training Loss:  57.9134  Training Accuracy:  0.819401\n",
      "Epoch:  327  Training Loss:  57.7602  Training Accuracy:  0.820341\n",
      "Epoch:  328  Training Loss:  57.7883  Training Accuracy:  0.820812\n",
      "Epoch:  329  Training Loss:  57.5539  Training Accuracy:  0.821458\n",
      "Epoch:  330  Training Loss:  57.4866  Training Accuracy:  0.82187\n",
      "Epoch:  331  Training Loss:  57.2048  Training Accuracy:  0.822223\n",
      "Epoch:  332  Training Loss:  57.153  Training Accuracy:  0.82281\n",
      "Epoch:  333  Training Loss:  57.0965  Training Accuracy:  0.822987\n",
      "Epoch:  334  Training Loss:  56.6592  Training Accuracy:  0.823927\n",
      "Epoch:  335  Training Loss:  56.4413  Training Accuracy:  0.825162\n",
      "Epoch:  336  Training Loss:  56.4454  Training Accuracy:  0.825397\n",
      "Epoch:  337  Training Loss:  56.2413  Training Accuracy:  0.826161\n",
      "Epoch:  338  Training Loss:  55.9611  Training Accuracy:  0.826573\n",
      "Epoch:  339  Training Loss:  55.8743  Training Accuracy:  0.827337\n",
      "Epoch:  340  Training Loss:  55.6263  Training Accuracy:  0.827749\n",
      "Epoch:  341  Training Loss:  55.4419  Training Accuracy:  0.828572\n",
      "Epoch:  342  Training Loss:  55.2983  Training Accuracy:  0.829219\n",
      "Epoch:  343  Training Loss:  55.0482  Training Accuracy:  0.829983\n",
      "Epoch:  344  Training Loss:  55.1473  Training Accuracy:  0.830394\n",
      "Epoch:  345  Training Loss:  54.7951  Training Accuracy:  0.8311\n",
      "Epoch:  346  Training Loss:  54.818  Training Accuracy:  0.831688\n",
      "Epoch:  347  Training Loss:  54.7601  Training Accuracy:  0.832569\n",
      "Epoch:  348  Training Loss:  54.1651  Training Accuracy:  0.832981\n",
      "Epoch:  349  Training Loss:  54.3271  Training Accuracy:  0.833686\n",
      "Epoch:  350  Training Loss:  53.9548  Training Accuracy:  0.834216\n",
      "Epoch:  351  Training Loss:  53.6698  Training Accuracy:  0.834803\n",
      "Epoch:  352  Training Loss:  53.4763  Training Accuracy:  0.835274\n",
      "Epoch:  353  Training Loss:  53.5796  Training Accuracy:  0.835862\n",
      "Epoch:  354  Training Loss:  53.3387  Training Accuracy:  0.836156\n",
      "Epoch:  355  Training Loss:  53.0271  Training Accuracy:  0.836802\n",
      "Epoch:  356  Training Loss:  52.6903  Training Accuracy:  0.837273\n",
      "Epoch:  357  Training Loss:  52.847  Training Accuracy:  0.838037\n",
      "Epoch:  358  Training Loss:  52.7048  Training Accuracy:  0.83839\n",
      "Epoch:  359  Training Loss:  52.1634  Training Accuracy:  0.839213\n",
      "Epoch:  360  Training Loss:  52.0326  Training Accuracy:  0.840388\n",
      "Epoch:  361  Training Loss:  51.8757  Training Accuracy:  0.841035\n",
      "Epoch:  362  Training Loss:  51.6265  Training Accuracy:  0.841858\n",
      "Epoch:  363  Training Loss:  51.495  Training Accuracy:  0.84227\n",
      "Epoch:  364  Training Loss:  51.2691  Training Accuracy:  0.843093\n",
      "Epoch:  365  Training Loss:  51.407  Training Accuracy:  0.843328\n",
      "Epoch:  366  Training Loss:  51.1627  Training Accuracy:  0.843798\n",
      "Epoch:  367  Training Loss:  50.7591  Training Accuracy:  0.844739\n",
      "Epoch:  368  Training Loss:  50.7284  Training Accuracy:  0.845679\n",
      "Epoch:  369  Training Loss:  50.519  Training Accuracy:  0.84615\n",
      "Epoch:  370  Training Loss:  50.1298  Training Accuracy:  0.847267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  50.2272  Training Accuracy:  0.847267\n",
      "Epoch:  372  Training Loss:  50.0529  Training Accuracy:  0.847913\n",
      "Epoch:  373  Training Loss:  49.705  Training Accuracy:  0.848442\n",
      "Epoch:  374  Training Loss:  49.4019  Training Accuracy:  0.84903\n",
      "Epoch:  375  Training Loss:  49.2894  Training Accuracy:  0.849501\n",
      "Epoch:  376  Training Loss:  49.4054  Training Accuracy:  0.849795\n",
      "Epoch:  377  Training Loss:  48.9747  Training Accuracy:  0.850559\n",
      "Epoch:  378  Training Loss:  49.0245  Training Accuracy:  0.851088\n",
      "Epoch:  379  Training Loss:  48.9627  Training Accuracy:  0.851441\n",
      "Epoch:  380  Training Loss:  48.4013  Training Accuracy:  0.852205\n",
      "Epoch:  381  Training Loss:  48.0657  Training Accuracy:  0.853028\n",
      "Epoch:  382  Training Loss:  48.2255  Training Accuracy:  0.853087\n",
      "Epoch:  383  Training Loss:  47.6875  Training Accuracy:  0.854145\n",
      "Epoch:  384  Training Loss:  47.8778  Training Accuracy:  0.854321\n",
      "Epoch:  385  Training Loss:  47.3221  Training Accuracy:  0.855438\n",
      "Epoch:  386  Training Loss:  47.4653  Training Accuracy:  0.856379\n",
      "Epoch:  387  Training Loss:  47.1211  Training Accuracy:  0.856967\n",
      "Epoch:  388  Training Loss:  46.9943  Training Accuracy:  0.857496\n",
      "Epoch:  389  Training Loss:  46.9237  Training Accuracy:  0.857731\n",
      "Epoch:  390  Training Loss:  46.5587  Training Accuracy:  0.858437\n",
      "Epoch:  391  Training Loss:  46.2046  Training Accuracy:  0.858966\n",
      "Epoch:  392  Training Loss:  45.9045  Training Accuracy:  0.859671\n",
      "Epoch:  393  Training Loss:  46.0696  Training Accuracy:  0.860083\n",
      "Epoch:  394  Training Loss:  45.8507  Training Accuracy:  0.860553\n",
      "Epoch:  395  Training Loss:  45.5246  Training Accuracy:  0.860906\n",
      "Epoch:  396  Training Loss:  45.4274  Training Accuracy:  0.860964\n",
      "Epoch:  397  Training Loss:  45.0054  Training Accuracy:  0.861552\n",
      "Epoch:  398  Training Loss:  44.8003  Training Accuracy:  0.861788\n",
      "Epoch:  399  Training Loss:  44.8652  Training Accuracy:  0.862552\n",
      "Epoch:  400  Training Loss:  44.7756  Training Accuracy:  0.862963\n",
      "Epoch:  401  Training Loss:  44.5064  Training Accuracy:  0.863081\n",
      "Epoch:  402  Training Loss:  44.0812  Training Accuracy:  0.863728\n",
      "Epoch:  403  Training Loss:  43.883  Training Accuracy:  0.864433\n",
      "Epoch:  404  Training Loss:  43.9033  Training Accuracy:  0.864845\n",
      "Epoch:  405  Training Loss:  43.861  Training Accuracy:  0.865139\n",
      "Epoch:  406  Training Loss:  43.2744  Training Accuracy:  0.86555\n",
      "Epoch:  407  Training Loss:  43.3405  Training Accuracy:  0.86602\n",
      "Epoch:  408  Training Loss:  43.2544  Training Accuracy:  0.866314\n",
      "Epoch:  409  Training Loss:  42.9428  Training Accuracy:  0.866608\n",
      "Epoch:  410  Training Loss:  42.5471  Training Accuracy:  0.866961\n",
      "Epoch:  411  Training Loss:  42.5627  Training Accuracy:  0.867196\n",
      "Epoch:  412  Training Loss:  42.1728  Training Accuracy:  0.867255\n",
      "Epoch:  413  Training Loss:  41.9478  Training Accuracy:  0.867725\n",
      "Epoch:  414  Training Loss:  42.0234  Training Accuracy:  0.868078\n",
      "Epoch:  415  Training Loss:  41.8367  Training Accuracy:  0.868666\n",
      "Epoch:  416  Training Loss:  41.3615  Training Accuracy:  0.869077\n",
      "Epoch:  417  Training Loss:  41.4468  Training Accuracy:  0.869783\n",
      "Epoch:  418  Training Loss:  40.9428  Training Accuracy:  0.87043\n",
      "Epoch:  419  Training Loss:  41.0036  Training Accuracy:  0.870841\n",
      "Epoch:  420  Training Loss:  40.8262  Training Accuracy:  0.870841\n",
      "Epoch:  421  Training Loss:  40.327  Training Accuracy:  0.872193\n",
      "Epoch:  422  Training Loss:  40.4786  Training Accuracy:  0.872017\n",
      "Epoch:  423  Training Loss:  40.3241  Training Accuracy:  0.872252\n",
      "Epoch:  424  Training Loss:  39.9791  Training Accuracy:  0.872958\n",
      "Epoch:  425  Training Loss:  39.9207  Training Accuracy:  0.872958\n",
      "Epoch:  426  Training Loss:  39.7895  Training Accuracy:  0.87331\n",
      "Epoch:  427  Training Loss:  39.5147  Training Accuracy:  0.873604\n",
      "Epoch:  428  Training Loss:  39.175  Training Accuracy:  0.874662\n",
      "Epoch:  429  Training Loss:  38.9201  Training Accuracy:  0.875485\n",
      "Epoch:  430  Training Loss:  38.7046  Training Accuracy:  0.876191\n",
      "Epoch:  431  Training Loss:  38.7173  Training Accuracy:  0.876191\n",
      "Epoch:  432  Training Loss:  38.2404  Training Accuracy:  0.876838\n",
      "Epoch:  433  Training Loss:  38.3435  Training Accuracy:  0.877132\n",
      "Epoch:  434  Training Loss:  38.2625  Training Accuracy:  0.877425\n",
      "Epoch:  435  Training Loss:  37.8547  Training Accuracy:  0.878131\n",
      "Epoch:  436  Training Loss:  37.8272  Training Accuracy:  0.878072\n",
      "Epoch:  437  Training Loss:  37.6027  Training Accuracy:  0.878425\n",
      "Epoch:  438  Training Loss:  37.4865  Training Accuracy:  0.878601\n",
      "Epoch:  439  Training Loss:  37.1449  Training Accuracy:  0.878836\n",
      "Epoch:  440  Training Loss:  36.7536  Training Accuracy:  0.879424\n",
      "Epoch:  441  Training Loss:  36.8428  Training Accuracy:  0.879601\n",
      "Epoch:  442  Training Loss:  36.6467  Training Accuracy:  0.879953\n",
      "Epoch:  443  Training Loss:  36.4082  Training Accuracy:  0.880189\n",
      "Epoch:  444  Training Loss:  36.0149  Training Accuracy:  0.8806\n",
      "Epoch:  445  Training Loss:  36.0593  Training Accuracy:  0.881012\n",
      "Epoch:  446  Training Loss:  35.9463  Training Accuracy:  0.881129\n",
      "Epoch:  447  Training Loss:  35.6557  Training Accuracy:  0.881364\n",
      "Epoch:  448  Training Loss:  35.33  Training Accuracy:  0.881893\n",
      "Epoch:  449  Training Loss:  35.3254  Training Accuracy:  0.882011\n",
      "Epoch:  450  Training Loss:  35.2318  Training Accuracy:  0.882187\n",
      "Epoch:  451  Training Loss:  34.9008  Training Accuracy:  0.882599\n",
      "Epoch:  452  Training Loss:  34.6036  Training Accuracy:  0.883128\n",
      "Epoch:  453  Training Loss:  34.6003  Training Accuracy:  0.883363\n",
      "Epoch:  454  Training Loss:  34.5128  Training Accuracy:  0.883775\n",
      "Epoch:  455  Training Loss:  34.2201  Training Accuracy:  0.884069\n",
      "Epoch:  456  Training Loss:  33.913  Training Accuracy:  0.884421\n",
      "Epoch:  457  Training Loss:  33.9487  Training Accuracy:  0.884715\n",
      "Epoch:  458  Training Loss:  33.7164  Training Accuracy:  0.885068\n",
      "Epoch:  459  Training Loss:  33.3604  Training Accuracy:  0.885127\n",
      "Epoch:  460  Training Loss:  33.3979  Training Accuracy:  0.885362\n",
      "Epoch:  461  Training Loss:  33.284  Training Accuracy:  0.885891\n",
      "Epoch:  462  Training Loss:  33.1235  Training Accuracy:  0.886126\n",
      "Epoch:  463  Training Loss:  33.042  Training Accuracy:  0.886303\n",
      "Epoch:  464  Training Loss:  32.594  Training Accuracy:  0.886655\n",
      "Epoch:  465  Training Loss:  32.3205  Training Accuracy:  0.887067\n",
      "Epoch:  466  Training Loss:  32.4148  Training Accuracy:  0.887361\n",
      "Epoch:  467  Training Loss:  32.2513  Training Accuracy:  0.887772\n",
      "Epoch:  468  Training Loss:  32.1311  Training Accuracy:  0.88836\n",
      "Epoch:  469  Training Loss:  31.9602  Training Accuracy:  0.888595\n",
      "Epoch:  470  Training Loss:  31.6861  Training Accuracy:  0.888948\n",
      "Epoch:  471  Training Loss:  31.3469  Training Accuracy:  0.889654\n",
      "Epoch:  472  Training Loss:  31.3497  Training Accuracy:  0.889595\n",
      "Epoch:  473  Training Loss:  31.237  Training Accuracy:  0.890183\n",
      "Epoch:  474  Training Loss:  31.0656  Training Accuracy:  0.890359\n",
      "Epoch:  475  Training Loss:  30.9185  Training Accuracy:  0.890653\n",
      "Epoch:  476  Training Loss:  30.6498  Training Accuracy:  0.890888\n",
      "Epoch:  477  Training Loss:  30.4545  Training Accuracy:  0.891535\n",
      "Epoch:  478  Training Loss:  30.1432  Training Accuracy:  0.89177\n",
      "Epoch:  479  Training Loss:  30.1553  Training Accuracy:  0.892299\n",
      "Epoch:  480  Training Loss:  30.0393  Training Accuracy:  0.892417\n",
      "Epoch:  481  Training Loss:  29.9236  Training Accuracy:  0.892534\n",
      "Epoch:  482  Training Loss:  29.7409  Training Accuracy:  0.892711\n",
      "Epoch:  483  Training Loss:  29.4747  Training Accuracy:  0.893416\n",
      "Epoch:  484  Training Loss:  29.3974  Training Accuracy:  0.893416\n",
      "Epoch:  485  Training Loss:  29.293  Training Accuracy:  0.893651\n",
      "Epoch:  486  Training Loss:  29.111  Training Accuracy:  0.893886\n",
      "Epoch:  487  Training Loss:  28.9906  Training Accuracy:  0.894122\n",
      "Epoch:  488  Training Loss:  28.8297  Training Accuracy:  0.894239\n",
      "Epoch:  489  Training Loss:  28.6023  Training Accuracy:  0.894592\n",
      "Epoch:  490  Training Loss:  28.4215  Training Accuracy:  0.895062\n",
      "Epoch:  491  Training Loss:  28.1503  Training Accuracy:  0.895415\n",
      "Epoch:  492  Training Loss:  28.1597  Training Accuracy:  0.89565\n",
      "Epoch:  493  Training Loss:  28.0279  Training Accuracy:  0.896003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  27.8155  Training Accuracy:  0.896179\n",
      "Epoch:  495  Training Loss:  27.6149  Training Accuracy:  0.896473\n",
      "Epoch:  496  Training Loss:  27.3414  Training Accuracy:  0.897061\n",
      "Epoch:  497  Training Loss:  27.3676  Training Accuracy:  0.897237\n",
      "Epoch:  498  Training Loss:  27.2555  Training Accuracy:  0.897355\n",
      "Epoch:  499  Training Loss:  27.0859  Training Accuracy:  0.897649\n",
      "Epoch:  500  Training Loss:  26.9759  Training Accuracy:  0.898119\n",
      "Epoch:  501  Training Loss:  26.7992  Training Accuracy:  0.898354\n",
      "Epoch:  502  Training Loss:  26.6066  Training Accuracy:  0.898707\n",
      "Epoch:  503  Training Loss:  26.4227  Training Accuracy:  0.899177\n",
      "Epoch:  504  Training Loss:  26.2823  Training Accuracy:  0.89953\n",
      "Epoch:  505  Training Loss:  26.0864  Training Accuracy:  0.899883\n",
      "Epoch:  506  Training Loss:  25.8466  Training Accuracy:  0.9\n",
      "Epoch:  507  Training Loss:  25.8447  Training Accuracy:  0.900236\n",
      "Epoch:  508  Training Loss:  25.7534  Training Accuracy:  0.900471\n",
      "Epoch:  509  Training Loss:  25.6042  Training Accuracy:  0.900706\n",
      "Epoch:  510  Training Loss:  25.4886  Training Accuracy:  0.901176\n",
      "Epoch:  511  Training Loss:  25.3341  Training Accuracy:  0.901411\n",
      "Epoch:  512  Training Loss:  25.2081  Training Accuracy:  0.901705\n",
      "Epoch:  513  Training Loss:  25.0776  Training Accuracy:  0.901764\n",
      "Epoch:  514  Training Loss:  24.963  Training Accuracy:  0.902058\n",
      "Epoch:  515  Training Loss:  24.7949  Training Accuracy:  0.902411\n",
      "Epoch:  516  Training Loss:  24.6976  Training Accuracy:  0.902646\n",
      "Epoch:  517  Training Loss:  24.5334  Training Accuracy:  0.90294\n",
      "Epoch:  518  Training Loss:  24.4231  Training Accuracy:  0.903234\n",
      "Epoch:  519  Training Loss:  24.2458  Training Accuracy:  0.903469\n",
      "Epoch:  520  Training Loss:  24.1583  Training Accuracy:  0.903998\n",
      "Epoch:  521  Training Loss:  23.9673  Training Accuracy:  0.904292\n",
      "Epoch:  522  Training Loss:  23.9034  Training Accuracy:  0.904468\n",
      "Epoch:  523  Training Loss:  23.7382  Training Accuracy:  0.904762\n",
      "Epoch:  524  Training Loss:  23.6432  Training Accuracy:  0.90535\n",
      "Epoch:  525  Training Loss:  23.4848  Training Accuracy:  0.905468\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 600\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  406.32  Training Accuracy:  0.0409865\n",
      "Epoch:  1  Training Loss:  351.361  Training Accuracy:  0.0421525\n",
      "Epoch:  2  Training Loss:  324.969  Training Accuracy:  0.0430854\n",
      "Epoch:  3  Training Loss:  308.678  Training Accuracy:  0.0486241\n",
      "Epoch:  4  Training Loss:  296.975  Training Accuracy:  0.0571945\n",
      "Epoch:  5  Training Loss:  288.892  Training Accuracy:  0.0624417\n",
      "Epoch:  6  Training Loss:  282.659  Training Accuracy:  0.0680387\n",
      "Epoch:  7  Training Loss:  277.205  Training Accuracy:  0.0735774\n",
      "Epoch:  8  Training Loss:  272.355  Training Accuracy:  0.0780084\n",
      "Epoch:  9  Training Loss:  268.178  Training Accuracy:  0.0811567\n",
      "Epoch:  10  Training Loss:  264.245  Training Accuracy:  0.0858792\n",
      "Epoch:  11  Training Loss:  260.81  Training Accuracy:  0.0915345\n",
      "Epoch:  12  Training Loss:  257.501  Training Accuracy:  0.0979478\n",
      "Epoch:  13  Training Loss:  254.102  Training Accuracy:  0.104594\n",
      "Epoch:  14  Training Loss:  250.879  Training Accuracy:  0.111765\n",
      "Epoch:  15  Training Loss:  248.761  Training Accuracy:  0.117071\n",
      "Epoch:  16  Training Loss:  246.727  Training Accuracy:  0.123368\n",
      "Epoch:  17  Training Loss:  244.337  Training Accuracy:  0.130539\n",
      "Epoch:  18  Training Loss:  242.018  Training Accuracy:  0.138118\n",
      "Epoch:  19  Training Loss:  239.455  Training Accuracy:  0.144881\n",
      "Epoch:  20  Training Loss:  237.394  Training Accuracy:  0.150886\n",
      "Epoch:  21  Training Loss:  235.067  Training Accuracy:  0.157125\n",
      "Epoch:  22  Training Loss:  232.724  Training Accuracy:  0.164121\n",
      "Epoch:  23  Training Loss:  230.436  Training Accuracy:  0.171933\n",
      "Epoch:  24  Training Loss:  228.126  Training Accuracy:  0.177764\n",
      "Epoch:  25  Training Loss:  225.865  Training Accuracy:  0.185226\n",
      "Epoch:  26  Training Loss:  223.762  Training Accuracy:  0.192164\n",
      "Epoch:  27  Training Loss:  221.873  Training Accuracy:  0.200152\n",
      "Epoch:  28  Training Loss:  219.924  Training Accuracy:  0.207381\n",
      "Epoch:  29  Training Loss:  217.761  Training Accuracy:  0.214202\n",
      "Epoch:  30  Training Loss:  215.173  Training Accuracy:  0.220674\n",
      "Epoch:  31  Training Loss:  212.536  Training Accuracy:  0.228603\n",
      "Epoch:  32  Training Loss:  209.859  Training Accuracy:  0.234783\n",
      "Epoch:  33  Training Loss:  207.13  Training Accuracy:  0.240555\n",
      "Epoch:  34  Training Loss:  204.515  Training Accuracy:  0.246968\n",
      "Epoch:  35  Training Loss:  201.725  Training Accuracy:  0.253207\n",
      "Epoch:  36  Training Loss:  199.032  Training Accuracy:  0.25857\n",
      "Epoch:  37  Training Loss:  196.3  Training Accuracy:  0.2651\n",
      "Epoch:  38  Training Loss:  193.522  Training Accuracy:  0.269298\n",
      "Epoch:  39  Training Loss:  190.768  Training Accuracy:  0.274662\n",
      "Epoch:  40  Training Loss:  188.075  Training Accuracy:  0.280434\n",
      "Epoch:  41  Training Loss:  185.279  Training Accuracy:  0.284981\n",
      "Epoch:  42  Training Loss:  182.623  Training Accuracy:  0.290229\n",
      "Epoch:  43  Training Loss:  179.993  Training Accuracy:  0.294426\n",
      "Epoch:  44  Training Loss:  177.433  Training Accuracy:  0.29909\n",
      "Epoch:  45  Training Loss:  174.85  Training Accuracy:  0.304163\n",
      "Epoch:  46  Training Loss:  172.247  Training Accuracy:  0.308535\n",
      "Epoch:  47  Training Loss:  169.664  Training Accuracy:  0.31215\n",
      "Epoch:  48  Training Loss:  167.194  Training Accuracy:  0.316348\n",
      "Epoch:  49  Training Loss:  164.606  Training Accuracy:  0.320312\n",
      "Epoch:  50  Training Loss:  162.134  Training Accuracy:  0.324277\n",
      "Epoch:  51  Training Loss:  159.779  Training Accuracy:  0.328183\n",
      "Epoch:  52  Training Loss:  157.408  Training Accuracy:  0.332381\n",
      "Epoch:  53  Training Loss:  155.129  Training Accuracy:  0.336462\n",
      "Epoch:  54  Training Loss:  152.851  Training Accuracy:  0.340019\n",
      "Epoch:  55  Training Loss:  150.545  Training Accuracy:  0.343109\n",
      "Epoch:  56  Training Loss:  148.347  Training Accuracy:  0.347423\n",
      "Epoch:  57  Training Loss:  146.139  Training Accuracy:  0.350804\n",
      "Epoch:  58  Training Loss:  144.038  Training Accuracy:  0.354478\n",
      "Epoch:  59  Training Loss:  141.967  Training Accuracy:  0.358151\n",
      "Epoch:  60  Training Loss:  139.722  Training Accuracy:  0.361532\n",
      "Epoch:  61  Training Loss:  137.72  Training Accuracy:  0.366429\n",
      "Epoch:  62  Training Loss:  135.685  Training Accuracy:  0.370103\n",
      "Epoch:  63  Training Loss:  133.623  Training Accuracy:  0.372843\n",
      "Epoch:  64  Training Loss:  131.578  Training Accuracy:  0.376457\n",
      "Epoch:  65  Training Loss:  129.75  Training Accuracy:  0.379956\n",
      "Epoch:  66  Training Loss:  127.816  Training Accuracy:  0.383279\n",
      "Epoch:  67  Training Loss:  126.003  Training Accuracy:  0.386719\n",
      "Epoch:  68  Training Loss:  124.107  Training Accuracy:  0.389925\n",
      "Epoch:  69  Training Loss:  122.283  Training Accuracy:  0.392549\n",
      "Epoch:  70  Training Loss:  120.458  Training Accuracy:  0.395114\n",
      "Epoch:  71  Training Loss:  118.591  Training Accuracy:  0.398612\n",
      "Epoch:  72  Training Loss:  116.745  Training Accuracy:  0.401644\n",
      "Epoch:  73  Training Loss:  114.982  Training Accuracy:  0.404676\n",
      "Epoch:  74  Training Loss:  113.244  Training Accuracy:  0.407999\n",
      "Epoch:  75  Training Loss:  111.45  Training Accuracy:  0.411555\n",
      "Epoch:  76  Training Loss:  109.76  Training Accuracy:  0.414062\n",
      "Epoch:  77  Training Loss:  108.035  Training Accuracy:  0.416453\n",
      "Epoch:  78  Training Loss:  106.396  Training Accuracy:  0.418435\n",
      "Epoch:  79  Training Loss:  104.774  Training Accuracy:  0.421758\n",
      "Epoch:  80  Training Loss:  103.211  Training Accuracy:  0.42444\n",
      "Epoch:  81  Training Loss:  101.625  Training Accuracy:  0.427005\n",
      "Epoch:  82  Training Loss:  100.123  Training Accuracy:  0.429746\n",
      "Epoch:  83  Training Loss:  98.571  Training Accuracy:  0.432661\n",
      "Epoch:  84  Training Loss:  97.1194  Training Accuracy:  0.434818\n",
      "Epoch:  85  Training Loss:  95.6286  Training Accuracy:  0.437908\n",
      "Epoch:  86  Training Loss:  94.331  Training Accuracy:  0.44129\n",
      "Epoch:  87  Training Loss:  93.1693  Training Accuracy:  0.444846\n",
      "Epoch:  88  Training Loss:  91.8589  Training Accuracy:  0.448169\n",
      "Epoch:  89  Training Loss:  90.4111  Training Accuracy:  0.450851\n",
      "Epoch:  90  Training Loss:  89.1378  Training Accuracy:  0.453416\n",
      "Epoch:  91  Training Loss:  87.8581  Training Accuracy:  0.455923\n",
      "Epoch:  92  Training Loss:  86.6708  Training Accuracy:  0.458664\n",
      "Epoch:  93  Training Loss:  85.3517  Training Accuracy:  0.460762\n",
      "Epoch:  94  Training Loss:  84.2454  Training Accuracy:  0.464319\n",
      "Epoch:  95  Training Loss:  82.8845  Training Accuracy:  0.467059\n",
      "Epoch:  96  Training Loss:  81.852  Training Accuracy:  0.470557\n",
      "Epoch:  97  Training Loss:  80.4879  Training Accuracy:  0.472889\n",
      "Epoch:  98  Training Loss:  79.4393  Training Accuracy:  0.474289\n",
      "Epoch:  99  Training Loss:  78.254  Training Accuracy:  0.476271\n",
      "Epoch:  100  Training Loss:  77.254  Training Accuracy:  0.477903\n",
      "Epoch:  101  Training Loss:  76.0562  Training Accuracy:  0.480352\n",
      "Epoch:  102  Training Loss:  75.086  Training Accuracy:  0.483617\n",
      "Epoch:  103  Training Loss:  73.9483  Training Accuracy:  0.486474\n",
      "Epoch:  104  Training Loss:  73.0347  Training Accuracy:  0.487698\n",
      "Epoch:  105  Training Loss:  71.9338  Training Accuracy:  0.488922\n",
      "Epoch:  106  Training Loss:  71.0052  Training Accuracy:  0.492129\n",
      "Epoch:  107  Training Loss:  69.9632  Training Accuracy:  0.495394\n",
      "Epoch:  108  Training Loss:  69.0704  Training Accuracy:  0.497376\n",
      "Epoch:  109  Training Loss:  68.0665  Training Accuracy:  0.500116\n",
      "Epoch:  110  Training Loss:  67.1917  Training Accuracy:  0.502215\n",
      "Epoch:  111  Training Loss:  66.2977  Training Accuracy:  0.503498\n",
      "Epoch:  112  Training Loss:  65.3805  Training Accuracy:  0.50583\n",
      "Epoch:  113  Training Loss:  64.4889  Training Accuracy:  0.507929\n",
      "Epoch:  114  Training Loss:  63.6114  Training Accuracy:  0.510203\n",
      "Epoch:  115  Training Loss:  62.6744  Training Accuracy:  0.511894\n",
      "Epoch:  116  Training Loss:  61.9098  Training Accuracy:  0.513992\n",
      "Epoch:  117  Training Loss:  61.0509  Training Accuracy:  0.5158\n",
      "Epoch:  118  Training Loss:  60.2536  Training Accuracy:  0.517432\n",
      "Epoch:  119  Training Loss:  59.5714  Training Accuracy:  0.519589\n",
      "Epoch:  120  Training Loss:  58.7914  Training Accuracy:  0.521572\n",
      "Epoch:  121  Training Loss:  58.0174  Training Accuracy:  0.523204\n",
      "Epoch:  122  Training Loss:  57.2716  Training Accuracy:  0.526119\n",
      "Epoch:  123  Training Loss:  56.5096  Training Accuracy:  0.527402\n",
      "Epoch:  124  Training Loss:  55.7831  Training Accuracy:  0.528976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  55.086  Training Accuracy:  0.530783\n",
      "Epoch:  126  Training Loss:  54.4521  Training Accuracy:  0.532124\n",
      "Epoch:  127  Training Loss:  53.7637  Training Accuracy:  0.533873\n",
      "Epoch:  128  Training Loss:  53.1214  Training Accuracy:  0.535448\n",
      "Epoch:  129  Training Loss:  52.4772  Training Accuracy:  0.536847\n",
      "Epoch:  130  Training Loss:  51.8728  Training Accuracy:  0.538363\n",
      "Epoch:  131  Training Loss:  51.2478  Training Accuracy:  0.540228\n",
      "Epoch:  132  Training Loss:  50.6164  Training Accuracy:  0.541394\n",
      "Epoch:  133  Training Loss:  50.0509  Training Accuracy:  0.542794\n",
      "Epoch:  134  Training Loss:  49.431  Training Accuracy:  0.544601\n",
      "Epoch:  135  Training Loss:  48.903  Training Accuracy:  0.546233\n",
      "Epoch:  136  Training Loss:  48.3299  Training Accuracy:  0.548041\n",
      "Epoch:  137  Training Loss:  47.7674  Training Accuracy:  0.550373\n",
      "Epoch:  138  Training Loss:  47.2029  Training Accuracy:  0.552472\n",
      "Epoch:  139  Training Loss:  46.7154  Training Accuracy:  0.553988\n",
      "Epoch:  140  Training Loss:  46.1507  Training Accuracy:  0.555853\n",
      "Epoch:  141  Training Loss:  45.667  Training Accuracy:  0.557602\n",
      "Epoch:  142  Training Loss:  45.1788  Training Accuracy:  0.558885\n",
      "Epoch:  143  Training Loss:  44.6746  Training Accuracy:  0.560751\n",
      "Epoch:  144  Training Loss:  44.2388  Training Accuracy:  0.562733\n",
      "Epoch:  145  Training Loss:  43.779  Training Accuracy:  0.564424\n",
      "Epoch:  146  Training Loss:  43.3386  Training Accuracy:  0.566464\n",
      "Epoch:  147  Training Loss:  42.8832  Training Accuracy:  0.567689\n",
      "Epoch:  148  Training Loss:  42.4432  Training Accuracy:  0.568971\n",
      "Epoch:  149  Training Loss:  42.0252  Training Accuracy:  0.570662\n",
      "Epoch:  150  Training Loss:  41.5958  Training Accuracy:  0.571886\n",
      "Epoch:  151  Training Loss:  41.2032  Training Accuracy:  0.573052\n",
      "Epoch:  152  Training Loss:  40.7665  Training Accuracy:  0.573752\n",
      "Epoch:  153  Training Loss:  40.3774  Training Accuracy:  0.575093\n",
      "Epoch:  154  Training Loss:  39.9855  Training Accuracy:  0.576551\n",
      "Epoch:  155  Training Loss:  39.6339  Training Accuracy:  0.578008\n",
      "Epoch:  156  Training Loss:  39.2557  Training Accuracy:  0.57999\n",
      "Epoch:  157  Training Loss:  38.8897  Training Accuracy:  0.582148\n",
      "Epoch:  158  Training Loss:  38.5564  Training Accuracy:  0.583722\n",
      "Epoch:  159  Training Loss:  38.1796  Training Accuracy:  0.585354\n",
      "Epoch:  160  Training Loss:  37.8428  Training Accuracy:  0.587453\n",
      "Epoch:  161  Training Loss:  37.5072  Training Accuracy:  0.588911\n",
      "Epoch:  162  Training Loss:  37.0939  Training Accuracy:  0.590718\n",
      "Epoch:  163  Training Loss:  36.7968  Training Accuracy:  0.592001\n",
      "Epoch:  164  Training Loss:  36.4781  Training Accuracy:  0.593575\n",
      "Epoch:  165  Training Loss:  36.167  Training Accuracy:  0.595091\n",
      "Epoch:  166  Training Loss:  35.8748  Training Accuracy:  0.596781\n",
      "Epoch:  167  Training Loss:  35.5658  Training Accuracy:  0.598647\n",
      "Epoch:  168  Training Loss:  35.2181  Training Accuracy:  0.600163\n",
      "Epoch:  169  Training Loss:  34.9491  Training Accuracy:  0.601212\n",
      "Epoch:  170  Training Loss:  34.649  Training Accuracy:  0.60267\n",
      "Epoch:  171  Training Loss:  34.3326  Training Accuracy:  0.604477\n",
      "Epoch:  172  Training Loss:  34.0493  Training Accuracy:  0.605993\n",
      "Epoch:  173  Training Loss:  33.7529  Training Accuracy:  0.607567\n",
      "Epoch:  174  Training Loss:  33.4712  Training Accuracy:  0.609142\n",
      "Epoch:  175  Training Loss:  33.2134  Training Accuracy:  0.610249\n",
      "Epoch:  176  Training Loss:  32.9096  Training Accuracy:  0.611882\n",
      "Epoch:  177  Training Loss:  32.6404  Training Accuracy:  0.613339\n",
      "Epoch:  178  Training Loss:  32.387  Training Accuracy:  0.61538\n",
      "Epoch:  179  Training Loss:  32.1531  Training Accuracy:  0.616896\n",
      "Epoch:  180  Training Loss:  31.8696  Training Accuracy:  0.618237\n",
      "Epoch:  181  Training Loss:  31.6222  Training Accuracy:  0.619869\n",
      "Epoch:  182  Training Loss:  31.4015  Training Accuracy:  0.621035\n",
      "Epoch:  183  Training Loss:  31.1377  Training Accuracy:  0.622609\n",
      "Epoch:  184  Training Loss:  30.8877  Training Accuracy:  0.623542\n",
      "Epoch:  185  Training Loss:  30.6363  Training Accuracy:  0.624883\n",
      "Epoch:  186  Training Loss:  30.4281  Training Accuracy:  0.625991\n",
      "Epoch:  187  Training Loss:  30.1591  Training Accuracy:  0.627157\n",
      "Epoch:  188  Training Loss:  29.9644  Training Accuracy:  0.628206\n",
      "Epoch:  189  Training Loss:  29.7201  Training Accuracy:  0.629955\n",
      "Epoch:  190  Training Loss:  29.5228  Training Accuracy:  0.631763\n",
      "Epoch:  191  Training Loss:  29.3051  Training Accuracy:  0.633803\n",
      "Epoch:  192  Training Loss:  29.0758  Training Accuracy:  0.635552\n",
      "Epoch:  193  Training Loss:  28.8502  Training Accuracy:  0.636952\n",
      "Epoch:  194  Training Loss:  28.6615  Training Accuracy:  0.638118\n",
      "Epoch:  195  Training Loss:  28.4517  Training Accuracy:  0.639692\n",
      "Epoch:  196  Training Loss:  28.2718  Training Accuracy:  0.641033\n",
      "Epoch:  197  Training Loss:  28.0357  Training Accuracy:  0.642199\n",
      "Epoch:  198  Training Loss:  27.8348  Training Accuracy:  0.64389\n",
      "Epoch:  199  Training Loss:  27.6227  Training Accuracy:  0.644881\n",
      "Epoch:  200  Training Loss:  27.4387  Training Accuracy:  0.646105\n",
      "Epoch:  201  Training Loss:  27.2509  Training Accuracy:  0.648204\n",
      "Epoch:  202  Training Loss:  27.0816  Training Accuracy:  0.649545\n",
      "Epoch:  203  Training Loss:  26.888  Training Accuracy:  0.650886\n",
      "Epoch:  204  Training Loss:  26.6916  Training Accuracy:  0.652169\n",
      "Epoch:  205  Training Loss:  26.5282  Training Accuracy:  0.653335\n",
      "Epoch:  206  Training Loss:  26.3364  Training Accuracy:  0.654676\n",
      "Epoch:  207  Training Loss:  26.1511  Training Accuracy:  0.656716\n",
      "Epoch:  208  Training Loss:  25.9938  Training Accuracy:  0.658115\n",
      "Epoch:  209  Training Loss:  25.8113  Training Accuracy:  0.659398\n",
      "Epoch:  210  Training Loss:  25.652  Training Accuracy:  0.660797\n",
      "Epoch:  211  Training Loss:  25.4721  Training Accuracy:  0.662838\n",
      "Epoch:  212  Training Loss:  25.2906  Training Accuracy:  0.664062\n",
      "Epoch:  213  Training Loss:  25.1339  Training Accuracy:  0.665986\n",
      "Epoch:  214  Training Loss:  24.9568  Training Accuracy:  0.667269\n",
      "Epoch:  215  Training Loss:  24.8095  Training Accuracy:  0.668668\n",
      "Epoch:  216  Training Loss:  24.6235  Training Accuracy:  0.670126\n",
      "Epoch:  217  Training Loss:  24.4845  Training Accuracy:  0.672108\n",
      "Epoch:  218  Training Loss:  24.333  Training Accuracy:  0.672924\n",
      "Epoch:  219  Training Loss:  24.2056  Training Accuracy:  0.674557\n",
      "Epoch:  220  Training Loss:  24.06  Training Accuracy:  0.676014\n",
      "Epoch:  221  Training Loss:  23.8966  Training Accuracy:  0.677472\n",
      "Epoch:  222  Training Loss:  23.7709  Training Accuracy:  0.678696\n",
      "Epoch:  223  Training Loss:  23.6438  Training Accuracy:  0.679862\n",
      "Epoch:  224  Training Loss:  23.4742  Training Accuracy:  0.681028\n",
      "Epoch:  225  Training Loss:  23.3453  Training Accuracy:  0.682078\n",
      "Epoch:  226  Training Loss:  23.2074  Training Accuracy:  0.68371\n",
      "Epoch:  227  Training Loss:  23.0596  Training Accuracy:  0.685109\n",
      "Epoch:  228  Training Loss:  22.9347  Training Accuracy:  0.686917\n",
      "Epoch:  229  Training Loss:  22.7842  Training Accuracy:  0.688024\n",
      "Epoch:  230  Training Loss:  22.6581  Training Accuracy:  0.689599\n",
      "Epoch:  231  Training Loss:  22.5385  Training Accuracy:  0.690473\n",
      "Epoch:  232  Training Loss:  22.4103  Training Accuracy:  0.691348\n",
      "Epoch:  233  Training Loss:  22.2869  Training Accuracy:  0.692747\n",
      "Epoch:  234  Training Loss:  22.1576  Training Accuracy:  0.694438\n",
      "Epoch:  235  Training Loss:  22.0624  Training Accuracy:  0.696187\n",
      "Epoch:  236  Training Loss:  21.9249  Training Accuracy:  0.697586\n",
      "Epoch:  237  Training Loss:  21.8048  Training Accuracy:  0.698752\n",
      "Epoch:  238  Training Loss:  21.6796  Training Accuracy:  0.700268\n",
      "Epoch:  239  Training Loss:  21.5731  Training Accuracy:  0.702017\n",
      "Epoch:  240  Training Loss:  21.4214  Training Accuracy:  0.703183\n",
      "Epoch:  241  Training Loss:  21.3164  Training Accuracy:  0.704291\n",
      "Epoch:  242  Training Loss:  21.1983  Training Accuracy:  0.705457\n",
      "Epoch:  243  Training Loss:  21.0789  Training Accuracy:  0.706856\n",
      "Epoch:  244  Training Loss:  20.9455  Training Accuracy:  0.708547\n",
      "Epoch:  245  Training Loss:  20.8683  Training Accuracy:  0.709713\n",
      "Epoch:  246  Training Loss:  20.7223  Training Accuracy:  0.711229\n",
      "Epoch:  247  Training Loss:  20.6187  Training Accuracy:  0.712803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  20.5028  Training Accuracy:  0.714085\n",
      "Epoch:  249  Training Loss:  20.3803  Training Accuracy:  0.714785\n",
      "Epoch:  250  Training Loss:  20.2621  Training Accuracy:  0.715893\n",
      "Epoch:  251  Training Loss:  20.1589  Training Accuracy:  0.716884\n",
      "Epoch:  252  Training Loss:  20.0295  Training Accuracy:  0.718342\n",
      "Epoch:  253  Training Loss:  19.9296  Training Accuracy:  0.719916\n",
      "Epoch:  254  Training Loss:  19.7958  Training Accuracy:  0.721315\n",
      "Epoch:  255  Training Loss:  19.7052  Training Accuracy:  0.722772\n",
      "Epoch:  256  Training Loss:  19.6015  Training Accuracy:  0.724347\n",
      "Epoch:  257  Training Loss:  19.4857  Training Accuracy:  0.725221\n",
      "Epoch:  258  Training Loss:  19.3742  Training Accuracy:  0.726737\n",
      "Epoch:  259  Training Loss:  19.2583  Training Accuracy:  0.727786\n",
      "Epoch:  260  Training Loss:  19.1557  Training Accuracy:  0.729186\n",
      "Epoch:  261  Training Loss:  19.0511  Training Accuracy:  0.730527\n",
      "Epoch:  262  Training Loss:  18.9283  Training Accuracy:  0.731984\n",
      "Epoch:  263  Training Loss:  18.8547  Training Accuracy:  0.733092\n",
      "Epoch:  264  Training Loss:  18.6973  Training Accuracy:  0.734375\n",
      "Epoch:  265  Training Loss:  18.6125  Training Accuracy:  0.735541\n",
      "Epoch:  266  Training Loss:  18.5211  Training Accuracy:  0.736765\n",
      "Epoch:  267  Training Loss:  18.4072  Training Accuracy:  0.737406\n",
      "Epoch:  268  Training Loss:  18.3129  Training Accuracy:  0.738514\n",
      "Epoch:  269  Training Loss:  18.1883  Training Accuracy:  0.739797\n",
      "Epoch:  270  Training Loss:  18.1054  Training Accuracy:  0.740846\n",
      "Epoch:  271  Training Loss:  17.9771  Training Accuracy:  0.74242\n",
      "Epoch:  272  Training Loss:  17.8994  Training Accuracy:  0.743703\n",
      "Epoch:  273  Training Loss:  17.8028  Training Accuracy:  0.744869\n",
      "Epoch:  274  Training Loss:  17.7133  Training Accuracy:  0.745685\n",
      "Epoch:  275  Training Loss:  17.5953  Training Accuracy:  0.746851\n",
      "Epoch:  276  Training Loss:  17.514  Training Accuracy:  0.747784\n",
      "Epoch:  277  Training Loss:  17.4053  Training Accuracy:  0.748892\n",
      "Epoch:  278  Training Loss:  17.3162  Training Accuracy:  0.750233\n",
      "Epoch:  279  Training Loss:  17.1935  Training Accuracy:  0.751166\n",
      "Epoch:  280  Training Loss:  17.1076  Training Accuracy:  0.752099\n",
      "Epoch:  281  Training Loss:  17.0105  Training Accuracy:  0.753206\n",
      "Epoch:  282  Training Loss:  16.9043  Training Accuracy:  0.753964\n",
      "Epoch:  283  Training Loss:  16.7955  Training Accuracy:  0.754664\n",
      "Epoch:  284  Training Loss:  16.7211  Training Accuracy:  0.75583\n",
      "Epoch:  285  Training Loss:  16.6021  Training Accuracy:  0.756704\n",
      "Epoch:  286  Training Loss:  16.5153  Training Accuracy:  0.757637\n",
      "Epoch:  287  Training Loss:  16.4206  Training Accuracy:  0.758628\n",
      "Epoch:  288  Training Loss:  16.3267  Training Accuracy:  0.760144\n",
      "Epoch:  289  Training Loss:  16.2469  Training Accuracy:  0.761777\n",
      "Epoch:  290  Training Loss:  16.1453  Training Accuracy:  0.762476\n",
      "Epoch:  291  Training Loss:  16.0611  Training Accuracy:  0.763876\n",
      "Epoch:  292  Training Loss:  15.9654  Training Accuracy:  0.765158\n",
      "Epoch:  293  Training Loss:  15.8914  Training Accuracy:  0.766499\n",
      "Epoch:  294  Training Loss:  15.786  Training Accuracy:  0.767549\n",
      "Epoch:  295  Training Loss:  15.7064  Training Accuracy:  0.768481\n",
      "Epoch:  296  Training Loss:  15.6121  Training Accuracy:  0.769473\n",
      "Epoch:  297  Training Loss:  15.5182  Training Accuracy:  0.770464\n",
      "Epoch:  298  Training Loss:  15.429  Training Accuracy:  0.77163\n",
      "Epoch:  299  Training Loss:  15.3434  Training Accuracy:  0.772621\n",
      "Epoch:  300  Training Loss:  15.2471  Training Accuracy:  0.773845\n",
      "Epoch:  301  Training Loss:  15.1557  Training Accuracy:  0.77472\n",
      "Epoch:  302  Training Loss:  15.0566  Training Accuracy:  0.775886\n",
      "Epoch:  303  Training Loss:  14.9812  Training Accuracy:  0.77676\n",
      "Epoch:  304  Training Loss:  14.8756  Training Accuracy:  0.777635\n",
      "Epoch:  305  Training Loss:  14.7927  Training Accuracy:  0.779034\n",
      "Epoch:  306  Training Loss:  14.7007  Training Accuracy:  0.780025\n",
      "Epoch:  307  Training Loss:  14.6082  Training Accuracy:  0.781016\n",
      "Epoch:  308  Training Loss:  14.528  Training Accuracy:  0.782416\n",
      "Epoch:  309  Training Loss:  14.4325  Training Accuracy:  0.783407\n",
      "Epoch:  310  Training Loss:  14.3396  Training Accuracy:  0.784456\n",
      "Epoch:  311  Training Loss:  14.2664  Training Accuracy:  0.785447\n",
      "Epoch:  312  Training Loss:  14.1788  Training Accuracy:  0.78638\n",
      "Epoch:  313  Training Loss:  14.0928  Training Accuracy:  0.787313\n",
      "Epoch:  314  Training Loss:  14.0142  Training Accuracy:  0.788129\n",
      "Epoch:  315  Training Loss:  13.9379  Training Accuracy:  0.788712\n",
      "Epoch:  316  Training Loss:  13.8441  Training Accuracy:  0.789587\n",
      "Epoch:  317  Training Loss:  13.7613  Training Accuracy:  0.790811\n",
      "Epoch:  318  Training Loss:  13.6744  Training Accuracy:  0.79221\n",
      "Epoch:  319  Training Loss:  13.5928  Training Accuracy:  0.793435\n",
      "Epoch:  320  Training Loss:  13.5106  Training Accuracy:  0.794134\n",
      "Epoch:  321  Training Loss:  13.4371  Training Accuracy:  0.795009\n",
      "Epoch:  322  Training Loss:  13.3399  Training Accuracy:  0.795767\n",
      "Epoch:  323  Training Loss:  13.2687  Training Accuracy:  0.796758\n",
      "Epoch:  324  Training Loss:  13.1839  Training Accuracy:  0.797516\n",
      "Epoch:  325  Training Loss:  13.1046  Training Accuracy:  0.797982\n",
      "Epoch:  326  Training Loss:  13.011  Training Accuracy:  0.798973\n",
      "Epoch:  327  Training Loss:  12.9444  Training Accuracy:  0.799615\n",
      "Epoch:  328  Training Loss:  12.8568  Training Accuracy:  0.800781\n",
      "Epoch:  329  Training Loss:  12.776  Training Accuracy:  0.801889\n",
      "Epoch:  330  Training Loss:  12.6914  Training Accuracy:  0.802763\n",
      "Epoch:  331  Training Loss:  12.6144  Training Accuracy:  0.803463\n",
      "Epoch:  332  Training Loss:  12.5415  Training Accuracy:  0.804104\n",
      "Epoch:  333  Training Loss:  12.4622  Training Accuracy:  0.80492\n",
      "Epoch:  334  Training Loss:  12.3799  Training Accuracy:  0.806028\n",
      "Epoch:  335  Training Loss:  12.3032  Training Accuracy:  0.806728\n",
      "Epoch:  336  Training Loss:  12.2255  Training Accuracy:  0.807369\n",
      "Epoch:  337  Training Loss:  12.1498  Training Accuracy:  0.808244\n",
      "Epoch:  338  Training Loss:  12.0585  Training Accuracy:  0.808885\n",
      "Epoch:  339  Training Loss:  11.9889  Training Accuracy:  0.809526\n",
      "Epoch:  340  Training Loss:  11.8961  Training Accuracy:  0.810051\n",
      "Epoch:  341  Training Loss:  11.8313  Training Accuracy:  0.810984\n",
      "Epoch:  342  Training Loss:  11.7431  Training Accuracy:  0.8118\n",
      "Epoch:  343  Training Loss:  11.6821  Training Accuracy:  0.812733\n",
      "Epoch:  344  Training Loss:  11.5906  Training Accuracy:  0.813432\n",
      "Epoch:  345  Training Loss:  11.5247  Training Accuracy:  0.814482\n",
      "Epoch:  346  Training Loss:  11.4422  Training Accuracy:  0.815473\n",
      "Epoch:  347  Training Loss:  11.3649  Training Accuracy:  0.816114\n",
      "Epoch:  348  Training Loss:  11.2842  Training Accuracy:  0.81693\n",
      "Epoch:  349  Training Loss:  11.2148  Training Accuracy:  0.818155\n",
      "Epoch:  350  Training Loss:  11.1341  Training Accuracy:  0.818913\n",
      "Epoch:  351  Training Loss:  11.0538  Training Accuracy:  0.819263\n",
      "Epoch:  352  Training Loss:  10.9896  Training Accuracy:  0.819729\n",
      "Epoch:  353  Training Loss:  10.9046  Training Accuracy:  0.820312\n",
      "Epoch:  354  Training Loss:  10.8332  Training Accuracy:  0.820953\n",
      "Epoch:  355  Training Loss:  10.7609  Training Accuracy:  0.822178\n",
      "Epoch:  356  Training Loss:  10.6867  Training Accuracy:  0.822702\n",
      "Epoch:  357  Training Loss:  10.618  Training Accuracy:  0.823169\n",
      "Epoch:  358  Training Loss:  10.545  Training Accuracy:  0.823694\n",
      "Epoch:  359  Training Loss:  10.4712  Training Accuracy:  0.824626\n",
      "Epoch:  360  Training Loss:  10.3885  Training Accuracy:  0.825034\n",
      "Epoch:  361  Training Loss:  10.3249  Training Accuracy:  0.825326\n",
      "Epoch:  362  Training Loss:  10.2568  Training Accuracy:  0.825792\n",
      "Epoch:  363  Training Loss:  10.1834  Training Accuracy:  0.826375\n",
      "Epoch:  364  Training Loss:  10.1135  Training Accuracy:  0.82725\n",
      "Epoch:  365  Training Loss:  10.0458  Training Accuracy:  0.82795\n",
      "Epoch:  366  Training Loss:  9.98064  Training Accuracy:  0.828591\n",
      "Epoch:  367  Training Loss:  9.91912  Training Accuracy:  0.829291\n",
      "Epoch:  368  Training Loss:  9.8621  Training Accuracy:  0.829932\n",
      "Epoch:  369  Training Loss:  9.78817  Training Accuracy:  0.830515\n",
      "Epoch:  370  Training Loss:  9.74458  Training Accuracy:  0.831156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  9.66998  Training Accuracy:  0.831798\n",
      "Epoch:  372  Training Loss:  9.60784  Training Accuracy:  0.832322\n",
      "Epoch:  373  Training Loss:  9.5591  Training Accuracy:  0.833022\n",
      "Epoch:  374  Training Loss:  9.48601  Training Accuracy:  0.833547\n",
      "Epoch:  375  Training Loss:  9.42569  Training Accuracy:  0.834363\n",
      "Epoch:  376  Training Loss:  9.3679  Training Accuracy:  0.834888\n",
      "Epoch:  377  Training Loss:  9.3073  Training Accuracy:  0.835471\n",
      "Epoch:  378  Training Loss:  9.24348  Training Accuracy:  0.836054\n",
      "Epoch:  379  Training Loss:  9.18737  Training Accuracy:  0.836287\n",
      "Epoch:  380  Training Loss:  9.1352  Training Accuracy:  0.836987\n",
      "Epoch:  381  Training Loss:  9.0707  Training Accuracy:  0.837803\n",
      "Epoch:  382  Training Loss:  9.02525  Training Accuracy:  0.838502\n",
      "Epoch:  383  Training Loss:  8.96442  Training Accuracy:  0.83926\n",
      "Epoch:  384  Training Loss:  8.90829  Training Accuracy:  0.840251\n",
      "Epoch:  385  Training Loss:  8.85886  Training Accuracy:  0.841068\n",
      "Epoch:  386  Training Loss:  8.79752  Training Accuracy:  0.841651\n",
      "Epoch:  387  Training Loss:  8.74615  Training Accuracy:  0.842525\n",
      "Epoch:  388  Training Loss:  8.70319  Training Accuracy:  0.842933\n",
      "Epoch:  389  Training Loss:  8.6417  Training Accuracy:  0.843458\n",
      "Epoch:  390  Training Loss:  8.59531  Training Accuracy:  0.844158\n",
      "Epoch:  391  Training Loss:  8.54447  Training Accuracy:  0.844624\n",
      "Epoch:  392  Training Loss:  8.49768  Training Accuracy:  0.845032\n",
      "Epoch:  393  Training Loss:  8.44242  Training Accuracy:  0.845557\n",
      "Epoch:  394  Training Loss:  8.39441  Training Accuracy:  0.84614\n",
      "Epoch:  395  Training Loss:  8.34815  Training Accuracy:  0.846898\n",
      "Epoch:  396  Training Loss:  8.2953  Training Accuracy:  0.847656\n",
      "Epoch:  397  Training Loss:  8.24784  Training Accuracy:  0.848355\n",
      "Epoch:  398  Training Loss:  8.19874  Training Accuracy:  0.849347\n",
      "Epoch:  399  Training Loss:  8.1457  Training Accuracy:  0.849813\n",
      "Epoch:  400  Training Loss:  8.10604  Training Accuracy:  0.850396\n",
      "Epoch:  401  Training Loss:  8.05681  Training Accuracy:  0.851037\n",
      "Epoch:  402  Training Loss:  8.00716  Training Accuracy:  0.851562\n",
      "Epoch:  403  Training Loss:  7.96856  Training Accuracy:  0.852145\n",
      "Epoch:  404  Training Loss:  7.9236  Training Accuracy:  0.852786\n",
      "Epoch:  405  Training Loss:  7.87529  Training Accuracy:  0.853194\n",
      "Epoch:  406  Training Loss:  7.83307  Training Accuracy:  0.853719\n",
      "Epoch:  407  Training Loss:  7.79194  Training Accuracy:  0.854011\n",
      "Epoch:  408  Training Loss:  7.74696  Training Accuracy:  0.854769\n",
      "Epoch:  409  Training Loss:  7.70979  Training Accuracy:  0.855293\n",
      "Epoch:  410  Training Loss:  7.66795  Training Accuracy:  0.855585\n",
      "Epoch:  411  Training Loss:  7.62267  Training Accuracy:  0.856343\n",
      "Epoch:  412  Training Loss:  7.58194  Training Accuracy:  0.856926\n",
      "Epoch:  413  Training Loss:  7.54748  Training Accuracy:  0.85745\n",
      "Epoch:  414  Training Loss:  7.50648  Training Accuracy:  0.858034\n",
      "Epoch:  415  Training Loss:  7.46455  Training Accuracy:  0.858617\n",
      "Epoch:  416  Training Loss:  7.43857  Training Accuracy:  0.859083\n",
      "Epoch:  417  Training Loss:  7.39433  Training Accuracy:  0.859491\n",
      "Epoch:  418  Training Loss:  7.36227  Training Accuracy:  0.859783\n",
      "Epoch:  419  Training Loss:  7.32035  Training Accuracy:  0.860657\n",
      "Epoch:  420  Training Loss:  7.28532  Training Accuracy:  0.861124\n",
      "Epoch:  421  Training Loss:  7.25189  Training Accuracy:  0.861648\n",
      "Epoch:  422  Training Loss:  7.21556  Training Accuracy:  0.862523\n",
      "Epoch:  423  Training Loss:  7.17148  Training Accuracy:  0.862756\n",
      "Epoch:  424  Training Loss:  7.13936  Training Accuracy:  0.863164\n",
      "Epoch:  425  Training Loss:  7.09963  Training Accuracy:  0.864155\n",
      "Epoch:  426  Training Loss:  7.05579  Training Accuracy:  0.864622\n",
      "Epoch:  427  Training Loss:  7.01971  Training Accuracy:  0.864913\n",
      "Epoch:  428  Training Loss:  6.98479  Training Accuracy:  0.865671\n",
      "Epoch:  429  Training Loss:  6.95261  Training Accuracy:  0.866079\n",
      "Epoch:  430  Training Loss:  6.90997  Training Accuracy:  0.866721\n",
      "Epoch:  431  Training Loss:  6.88551  Training Accuracy:  0.867245\n",
      "Epoch:  432  Training Loss:  6.84513  Training Accuracy:  0.867828\n",
      "Epoch:  433  Training Loss:  6.81175  Training Accuracy:  0.868295\n",
      "Epoch:  434  Training Loss:  6.77257  Training Accuracy:  0.868761\n",
      "Epoch:  435  Training Loss:  6.74026  Training Accuracy:  0.869286\n",
      "Epoch:  436  Training Loss:  6.70609  Training Accuracy:  0.869985\n",
      "Epoch:  437  Training Loss:  6.68005  Training Accuracy:  0.870335\n",
      "Epoch:  438  Training Loss:  6.6402  Training Accuracy:  0.870743\n",
      "Epoch:  439  Training Loss:  6.60999  Training Accuracy:  0.871385\n",
      "Epoch:  440  Training Loss:  6.57502  Training Accuracy:  0.871909\n",
      "Epoch:  441  Training Loss:  6.54387  Training Accuracy:  0.872259\n",
      "Epoch:  442  Training Loss:  6.50695  Training Accuracy:  0.872901\n",
      "Epoch:  443  Training Loss:  6.48463  Training Accuracy:  0.873425\n",
      "Epoch:  444  Training Loss:  6.44414  Training Accuracy:  0.873659\n",
      "Epoch:  445  Training Loss:  6.41735  Training Accuracy:  0.874242\n",
      "Epoch:  446  Training Loss:  6.38577  Training Accuracy:  0.874883\n",
      "Epoch:  447  Training Loss:  6.35161  Training Accuracy:  0.875058\n",
      "Epoch:  448  Training Loss:  6.31709  Training Accuracy:  0.875349\n",
      "Epoch:  449  Training Loss:  6.28715  Training Accuracy:  0.876166\n",
      "Epoch:  450  Training Loss:  6.25089  Training Accuracy:  0.876749\n",
      "Epoch:  451  Training Loss:  6.22591  Training Accuracy:  0.87704\n",
      "Epoch:  452  Training Loss:  6.19853  Training Accuracy:  0.877273\n",
      "Epoch:  453  Training Loss:  6.16208  Training Accuracy:  0.877973\n",
      "Epoch:  454  Training Loss:  6.13675  Training Accuracy:  0.878381\n",
      "Epoch:  455  Training Loss:  6.11174  Training Accuracy:  0.879022\n",
      "Epoch:  456  Training Loss:  6.07249  Training Accuracy:  0.879664\n",
      "Epoch:  457  Training Loss:  6.04917  Training Accuracy:  0.880072\n",
      "Epoch:  458  Training Loss:  6.01924  Training Accuracy:  0.880422\n",
      "Epoch:  459  Training Loss:  5.98569  Training Accuracy:  0.880771\n",
      "Epoch:  460  Training Loss:  5.95604  Training Accuracy:  0.881529\n",
      "Epoch:  461  Training Loss:  5.93418  Training Accuracy:  0.881996\n",
      "Epoch:  462  Training Loss:  5.89304  Training Accuracy:  0.88252\n",
      "Epoch:  463  Training Loss:  5.86964  Training Accuracy:  0.883162\n",
      "Epoch:  464  Training Loss:  5.84498  Training Accuracy:  0.88357\n",
      "Epoch:  465  Training Loss:  5.80986  Training Accuracy:  0.883861\n",
      "Epoch:  466  Training Loss:  5.78343  Training Accuracy:  0.884328\n",
      "Epoch:  467  Training Loss:  5.75439  Training Accuracy:  0.884736\n",
      "Epoch:  468  Training Loss:  5.72545  Training Accuracy:  0.885377\n",
      "Epoch:  469  Training Loss:  5.69127  Training Accuracy:  0.88631\n",
      "Epoch:  470  Training Loss:  5.67222  Training Accuracy:  0.886602\n",
      "Epoch:  471  Training Loss:  5.63655  Training Accuracy:  0.886893\n",
      "Epoch:  472  Training Loss:  5.60907  Training Accuracy:  0.887301\n",
      "Epoch:  473  Training Loss:  5.58445  Training Accuracy:  0.887768\n",
      "Epoch:  474  Training Loss:  5.55544  Training Accuracy:  0.888351\n",
      "Epoch:  475  Training Loss:  5.52734  Training Accuracy:  0.888992\n",
      "Epoch:  476  Training Loss:  5.50248  Training Accuracy:  0.889633\n",
      "Epoch:  477  Training Loss:  5.47595  Training Accuracy:  0.889925\n",
      "Epoch:  478  Training Loss:  5.4502  Training Accuracy:  0.89045\n",
      "Epoch:  479  Training Loss:  5.41841  Training Accuracy:  0.890916\n",
      "Epoch:  480  Training Loss:  5.39285  Training Accuracy:  0.891324\n",
      "Epoch:  481  Training Loss:  5.36757  Training Accuracy:  0.891732\n",
      "Epoch:  482  Training Loss:  5.34153  Training Accuracy:  0.891907\n",
      "Epoch:  483  Training Loss:  5.3185  Training Accuracy:  0.892257\n",
      "Epoch:  484  Training Loss:  5.29023  Training Accuracy:  0.893073\n",
      "Epoch:  485  Training Loss:  5.27038  Training Accuracy:  0.893365\n",
      "Epoch:  486  Training Loss:  5.24526  Training Accuracy:  0.893831\n",
      "Epoch:  487  Training Loss:  5.21697  Training Accuracy:  0.894298\n",
      "Epoch:  488  Training Loss:  5.19158  Training Accuracy:  0.894531\n",
      "Epoch:  489  Training Loss:  5.17034  Training Accuracy:  0.895055\n",
      "Epoch:  490  Training Loss:  5.14246  Training Accuracy:  0.89523\n",
      "Epoch:  491  Training Loss:  5.11931  Training Accuracy:  0.895813\n",
      "Epoch:  492  Training Loss:  5.08932  Training Accuracy:  0.896047\n",
      "Epoch:  493  Training Loss:  5.073  Training Accuracy:  0.896338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  5.0434  Training Accuracy:  0.896921\n",
      "Epoch:  495  Training Loss:  5.0257  Training Accuracy:  0.897329\n",
      "Epoch:  496  Training Loss:  4.99344  Training Accuracy:  0.897796\n",
      "Epoch:  497  Training Loss:  4.97396  Training Accuracy:  0.898029\n",
      "Epoch:  498  Training Loss:  4.95386  Training Accuracy:  0.89832\n",
      "Epoch:  499  Training Loss:  4.92946  Training Accuracy:  0.89867\n",
      "Epoch:  500  Training Loss:  4.90425  Training Accuracy:  0.898903\n",
      "Epoch:  501  Training Loss:  4.8896  Training Accuracy:  0.899312\n",
      "Epoch:  502  Training Loss:  4.86286  Training Accuracy:  0.899661\n",
      "Epoch:  503  Training Loss:  4.84523  Training Accuracy:  0.900186\n",
      "Epoch:  504  Training Loss:  4.82032  Training Accuracy:  0.900361\n",
      "Epoch:  505  Training Loss:  4.79984  Training Accuracy:  0.900769\n",
      "Epoch:  506  Training Loss:  4.77455  Training Accuracy:  0.901002\n",
      "Epoch:  507  Training Loss:  4.75505  Training Accuracy:  0.901177\n",
      "Epoch:  508  Training Loss:  4.73514  Training Accuracy:  0.901702\n",
      "Epoch:  509  Training Loss:  4.71378  Training Accuracy:  0.901819\n",
      "Epoch:  510  Training Loss:  4.69247  Training Accuracy:  0.902052\n",
      "Epoch:  511  Training Loss:  4.67312  Training Accuracy:  0.902343\n",
      "Epoch:  512  Training Loss:  4.65332  Training Accuracy:  0.902518\n",
      "Epoch:  513  Training Loss:  4.63512  Training Accuracy:  0.90281\n",
      "Epoch:  514  Training Loss:  4.61155  Training Accuracy:  0.903276\n",
      "Epoch:  515  Training Loss:  4.5976  Training Accuracy:  0.903393\n",
      "Epoch:  516  Training Loss:  4.57603  Training Accuracy:  0.903743\n",
      "Epoch:  517  Training Loss:  4.55746  Training Accuracy:  0.904151\n",
      "Epoch:  518  Training Loss:  4.536  Training Accuracy:  0.904617\n",
      "Epoch:  519  Training Loss:  4.51859  Training Accuracy:  0.904792\n",
      "Epoch:  520  Training Loss:  4.49868  Training Accuracy:  0.905258\n",
      "Epoch:  521  Training Loss:  4.48158  Training Accuracy:  0.905666\n",
      "Epoch:  522  Training Loss:  4.45839  Training Accuracy:  0.906133\n",
      "Epoch:  523  Training Loss:  4.44251  Training Accuracy:  0.906541\n",
      "Epoch:  524  Training Loss:  4.42317  Training Accuracy:  0.906891\n",
      "Epoch:  525  Training Loss:  4.40273  Training Accuracy:  0.90759\n",
      "Epoch:  526  Training Loss:  4.38754  Training Accuracy:  0.907824\n",
      "Epoch:  527  Training Loss:  4.36756  Training Accuracy:  0.907765\n",
      "Epoch:  528  Training Loss:  4.34707  Training Accuracy:  0.908232\n",
      "Epoch:  529  Training Loss:  4.33068  Training Accuracy:  0.908465\n",
      "Epoch:  530  Training Loss:  4.31087  Training Accuracy:  0.90864\n",
      "Epoch:  531  Training Loss:  4.29429  Training Accuracy:  0.909048\n",
      "Epoch:  532  Training Loss:  4.27772  Training Accuracy:  0.90899\n",
      "Epoch:  533  Training Loss:  4.26205  Training Accuracy:  0.909048\n",
      "Epoch:  534  Training Loss:  4.23917  Training Accuracy:  0.909223\n",
      "Epoch:  535  Training Loss:  4.22589  Training Accuracy:  0.909456\n",
      "Epoch:  536  Training Loss:  4.21109  Training Accuracy:  0.909456\n",
      "Epoch:  537  Training Loss:  4.19203  Training Accuracy:  0.909398\n",
      "Epoch:  538  Training Loss:  4.17158  Training Accuracy:  0.909514\n",
      "Epoch:  539  Training Loss:  4.15857  Training Accuracy:  0.909631\n",
      "Epoch:  540  Training Loss:  4.13673  Training Accuracy:  0.909864\n",
      "Epoch:  541  Training Loss:  4.12595  Training Accuracy:  0.910039\n",
      "Epoch:  542  Training Loss:  4.10742  Training Accuracy:  0.910039\n",
      "Epoch:  543  Training Loss:  4.0952  Training Accuracy:  0.910097\n",
      "Epoch:  544  Training Loss:  4.07338  Training Accuracy:  0.910564\n",
      "Epoch:  545  Training Loss:  4.06608  Training Accuracy:  0.910622\n",
      "Epoch:  546  Training Loss:  4.04239  Training Accuracy:  0.910622\n",
      "Epoch:  547  Training Loss:  4.03226  Training Accuracy:  0.910914\n",
      "Epoch:  548  Training Loss:  4.01187  Training Accuracy:  0.910739\n",
      "Epoch:  549  Training Loss:  4.00027  Training Accuracy:  0.911147\n",
      "Epoch:  550  Training Loss:  3.97986  Training Accuracy:  0.91138\n",
      "Epoch:  551  Training Loss:  3.97128  Training Accuracy:  0.911497\n",
      "Epoch:  552  Training Loss:  3.94782  Training Accuracy:  0.911905\n",
      "Epoch:  553  Training Loss:  3.9385  Training Accuracy:  0.912138\n",
      "Epoch:  554  Training Loss:  3.92054  Training Accuracy:  0.912371\n",
      "Epoch:  555  Training Loss:  3.90396  Training Accuracy:  0.912546\n",
      "Epoch:  556  Training Loss:  3.88939  Training Accuracy:  0.912546\n",
      "Epoch:  557  Training Loss:  3.88088  Training Accuracy:  0.912604\n",
      "Epoch:  558  Training Loss:  3.85929  Training Accuracy:  0.912896\n",
      "Epoch:  559  Training Loss:  3.84787  Training Accuracy:  0.913013\n",
      "Epoch:  560  Training Loss:  3.83517  Training Accuracy:  0.913246\n",
      "Epoch:  561  Training Loss:  3.82237  Training Accuracy:  0.913421\n",
      "Epoch:  562  Training Loss:  3.80445  Training Accuracy:  0.914062\n",
      "Epoch:  563  Training Loss:  3.79726  Training Accuracy:  0.914179\n",
      "Epoch:  564  Training Loss:  3.77689  Training Accuracy:  0.914412\n",
      "Epoch:  565  Training Loss:  3.76683  Training Accuracy:  0.914645\n",
      "Epoch:  566  Training Loss:  3.75461  Training Accuracy:  0.915111\n",
      "Epoch:  567  Training Loss:  3.73949  Training Accuracy:  0.915228\n",
      "Epoch:  568  Training Loss:  3.72676  Training Accuracy:  0.915345\n",
      "Epoch:  569  Training Loss:  3.71582  Training Accuracy:  0.915519\n",
      "Epoch:  570  Training Loss:  3.70206  Training Accuracy:  0.915636\n",
      "Epoch:  571  Training Loss:  3.68859  Training Accuracy:  0.916103\n",
      "Epoch:  572  Training Loss:  3.67563  Training Accuracy:  0.916277\n",
      "Epoch:  573  Training Loss:  3.66406  Training Accuracy:  0.916744\n",
      "Epoch:  574  Training Loss:  3.65185  Training Accuracy:  0.916977\n",
      "Epoch:  575  Training Loss:  3.64438  Training Accuracy:  0.916977\n",
      "Epoch:  576  Training Loss:  3.62727  Training Accuracy:  0.91721\n",
      "Epoch:  577  Training Loss:  3.6182  Training Accuracy:  0.917677\n",
      "Epoch:  578  Training Loss:  3.60404  Training Accuracy:  0.91791\n",
      "Epoch:  579  Training Loss:  3.59433  Training Accuracy:  0.918085\n",
      "Epoch:  580  Training Loss:  3.57978  Training Accuracy:  0.918493\n",
      "Epoch:  581  Training Loss:  3.57357  Training Accuracy:  0.918668\n",
      "Epoch:  582  Training Loss:  3.56092  Training Accuracy:  0.918901\n",
      "Epoch:  583  Training Loss:  3.5495  Training Accuracy:  0.918959\n",
      "Epoch:  584  Training Loss:  3.54021  Training Accuracy:  0.919018\n",
      "Epoch:  585  Training Loss:  3.52851  Training Accuracy:  0.919309\n",
      "Epoch:  586  Training Loss:  3.516  Training Accuracy:  0.919426\n",
      "Epoch:  587  Training Loss:  3.50482  Training Accuracy:  0.919717\n",
      "Epoch:  588  Training Loss:  3.49221  Training Accuracy:  0.9203\n",
      "Epoch:  589  Training Loss:  3.48356  Training Accuracy:  0.920475\n",
      "Epoch:  590  Training Loss:  3.46949  Training Accuracy:  0.92065\n",
      "Epoch:  591  Training Loss:  3.4585  Training Accuracy:  0.920883\n",
      "Epoch:  592  Training Loss:  3.44461  Training Accuracy:  0.921058\n",
      "Epoch:  593  Training Loss:  3.43641  Training Accuracy:  0.921233\n",
      "Epoch:  594  Training Loss:  3.42302  Training Accuracy:  0.921641\n",
      "Epoch:  595  Training Loss:  3.41121  Training Accuracy:  0.9217\n",
      "Epoch:  596  Training Loss:  3.39736  Training Accuracy:  0.921991\n",
      "Epoch:  597  Training Loss:  3.3915  Training Accuracy:  0.922341\n",
      "Epoch:  598  Training Loss:  3.37248  Training Accuracy:  0.922458\n",
      "Epoch:  599  Training Loss:  3.37152  Training Accuracy:  0.922982\n",
      "Epoch:  600  Training Loss:  3.34858  Training Accuracy:  0.923041\n",
      "Epoch:  601  Training Loss:  3.34562  Training Accuracy:  0.92339\n",
      "Epoch:  602  Training Loss:  3.32707  Training Accuracy:  0.92339\n",
      "Epoch:  603  Training Loss:  3.319  Training Accuracy:  0.92374\n",
      "Epoch:  604  Training Loss:  3.30346  Training Accuracy:  0.923682\n",
      "Epoch:  605  Training Loss:  3.29837  Training Accuracy:  0.924207\n",
      "Epoch:  606  Training Loss:  3.27811  Training Accuracy:  0.924615\n",
      "Epoch:  607  Training Loss:  3.27347  Training Accuracy:  0.924964\n",
      "Epoch:  608  Training Loss:  3.2552  Training Accuracy:  0.925023\n",
      "Epoch:  609  Training Loss:  3.24865  Training Accuracy:  0.925314\n",
      "Epoch:  610  Training Loss:  3.23016  Training Accuracy:  0.925548\n",
      "Epoch:  611  Training Loss:  3.2255  Training Accuracy:  0.925548\n",
      "Epoch:  612  Training Loss:  3.2072  Training Accuracy:  0.925781\n",
      "Epoch:  613  Training Loss:  3.20249  Training Accuracy:  0.926131\n",
      "Epoch:  614  Training Loss:  3.18623  Training Accuracy:  0.926364\n",
      "Epoch:  615  Training Loss:  3.17572  Training Accuracy:  0.926539\n",
      "Epoch:  616  Training Loss:  3.16126  Training Accuracy:  0.926655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  3.15542  Training Accuracy:  0.926772\n",
      "Epoch:  618  Training Loss:  3.13823  Training Accuracy:  0.92683\n",
      "Epoch:  619  Training Loss:  3.12983  Training Accuracy:  0.92718\n",
      "Epoch:  620  Training Loss:  3.11521  Training Accuracy:  0.927472\n",
      "Epoch:  621  Training Loss:  3.10427  Training Accuracy:  0.927646\n",
      "Epoch:  622  Training Loss:  3.09477  Training Accuracy:  0.928055\n",
      "Epoch:  623  Training Loss:  3.08533  Training Accuracy:  0.928463\n",
      "Epoch:  624  Training Loss:  3.07071  Training Accuracy:  0.928579\n",
      "Epoch:  625  Training Loss:  3.06038  Training Accuracy:  0.928754\n",
      "Epoch:  626  Training Loss:  3.05014  Training Accuracy:  0.928871\n",
      "Epoch:  627  Training Loss:  3.03835  Training Accuracy:  0.928987\n",
      "Epoch:  628  Training Loss:  3.02842  Training Accuracy:  0.929279\n",
      "Epoch:  629  Training Loss:  3.01863  Training Accuracy:  0.92957\n",
      "Epoch:  630  Training Loss:  3.00448  Training Accuracy:  0.929687\n",
      "Epoch:  631  Training Loss:  2.99845  Training Accuracy:  0.929862\n",
      "Epoch:  632  Training Loss:  2.98818  Training Accuracy:  0.930037\n",
      "Epoch:  633  Training Loss:  2.97819  Training Accuracy:  0.93027\n",
      "Epoch:  634  Training Loss:  2.9631  Training Accuracy:  0.930561\n",
      "Epoch:  635  Training Loss:  2.95938  Training Accuracy:  0.930561\n",
      "Epoch:  636  Training Loss:  2.94499  Training Accuracy:  0.93062\n",
      "Epoch:  637  Training Loss:  2.93778  Training Accuracy:  0.930853\n",
      "Epoch:  638  Training Loss:  2.92464  Training Accuracy:  0.931028\n",
      "Epoch:  639  Training Loss:  2.91808  Training Accuracy:  0.931378\n",
      "Epoch:  640  Training Loss:  2.9071  Training Accuracy:  0.931553\n",
      "Epoch:  641  Training Loss:  2.89571  Training Accuracy:  0.931844\n",
      "Epoch:  642  Training Loss:  2.88803  Training Accuracy:  0.932311\n",
      "Epoch:  643  Training Loss:  2.87293  Training Accuracy:  0.932719\n",
      "Epoch:  644  Training Loss:  2.86896  Training Accuracy:  0.932894\n",
      "Epoch:  645  Training Loss:  2.85554  Training Accuracy:  0.932952\n",
      "Epoch:  646  Training Loss:  2.85286  Training Accuracy:  0.933127\n",
      "Epoch:  647  Training Loss:  2.83853  Training Accuracy:  0.933302\n",
      "Epoch:  648  Training Loss:  2.83097  Training Accuracy:  0.93336\n",
      "Epoch:  649  Training Loss:  2.82119  Training Accuracy:  0.933593\n",
      "Epoch:  650  Training Loss:  2.81263  Training Accuracy:  0.93371\n",
      "Epoch:  651  Training Loss:  2.80557  Training Accuracy:  0.934001\n",
      "Epoch:  652  Training Loss:  2.79481  Training Accuracy:  0.93406\n",
      "Epoch:  653  Training Loss:  2.78628  Training Accuracy:  0.934351\n",
      "Epoch:  654  Training Loss:  2.7786  Training Accuracy:  0.934526\n",
      "Epoch:  655  Training Loss:  2.77095  Training Accuracy:  0.934643\n",
      "Epoch:  656  Training Loss:  2.76513  Training Accuracy:  0.934876\n",
      "Epoch:  657  Training Loss:  2.75266  Training Accuracy:  0.935051\n",
      "Epoch:  658  Training Loss:  2.747  Training Accuracy:  0.935342\n",
      "Epoch:  659  Training Loss:  2.73955  Training Accuracy:  0.935575\n",
      "Epoch:  660  Training Loss:  2.72774  Training Accuracy:  0.935692\n",
      "Epoch:  661  Training Loss:  2.72166  Training Accuracy:  0.935809\n",
      "Epoch:  662  Training Loss:  2.71316  Training Accuracy:  0.935867\n",
      "Epoch:  663  Training Loss:  2.69807  Training Accuracy:  0.9361\n",
      "Epoch:  664  Training Loss:  2.69506  Training Accuracy:  0.936217\n",
      "Epoch:  665  Training Loss:  2.68521  Training Accuracy:  0.936392\n",
      "Epoch:  666  Training Loss:  2.67545  Training Accuracy:  0.936508\n",
      "Epoch:  667  Training Loss:  2.6692  Training Accuracy:  0.936683\n",
      "Epoch:  668  Training Loss:  2.6597  Training Accuracy:  0.936858\n",
      "Epoch:  669  Training Loss:  2.65001  Training Accuracy:  0.937033\n",
      "Epoch:  670  Training Loss:  2.64427  Training Accuracy:  0.937266\n",
      "Epoch:  671  Training Loss:  2.63721  Training Accuracy:  0.937558\n",
      "Epoch:  672  Training Loss:  2.62347  Training Accuracy:  0.937616\n",
      "Epoch:  673  Training Loss:  2.61782  Training Accuracy:  0.937674\n",
      "Epoch:  674  Training Loss:  2.61334  Training Accuracy:  0.937674\n",
      "Epoch:  675  Training Loss:  2.59808  Training Accuracy:  0.937849\n",
      "Epoch:  676  Training Loss:  2.59457  Training Accuracy:  0.937966\n",
      "Epoch:  677  Training Loss:  2.58705  Training Accuracy:  0.938316\n",
      "Epoch:  678  Training Loss:  2.576  Training Accuracy:  0.938491\n",
      "Epoch:  679  Training Loss:  2.56858  Training Accuracy:  0.938666\n",
      "Epoch:  680  Training Loss:  2.56477  Training Accuracy:  0.938957\n",
      "Epoch:  681  Training Loss:  2.55182  Training Accuracy:  0.938957\n",
      "Epoch:  682  Training Loss:  2.54704  Training Accuracy:  0.939074\n",
      "Epoch:  683  Training Loss:  2.53659  Training Accuracy:  0.939307\n",
      "Epoch:  684  Training Loss:  2.53057  Training Accuracy:  0.93954\n",
      "Epoch:  685  Training Loss:  2.52215  Training Accuracy:  0.939773\n",
      "Epoch:  686  Training Loss:  2.51851  Training Accuracy:  0.939831\n",
      "Epoch:  687  Training Loss:  2.50849  Training Accuracy:  0.939948\n",
      "Epoch:  688  Training Loss:  2.50395  Training Accuracy:  0.940123\n",
      "Epoch:  689  Training Loss:  2.49455  Training Accuracy:  0.94024\n",
      "Epoch:  690  Training Loss:  2.48987  Training Accuracy:  0.940298\n",
      "Epoch:  691  Training Loss:  2.47686  Training Accuracy:  0.940415\n",
      "Epoch:  692  Training Loss:  2.47687  Training Accuracy:  0.940473\n",
      "Epoch:  693  Training Loss:  2.46575  Training Accuracy:  0.940473\n",
      "Epoch:  694  Training Loss:  2.46046  Training Accuracy:  0.940706\n",
      "Epoch:  695  Training Loss:  2.45409  Training Accuracy:  0.940881\n",
      "Epoch:  696  Training Loss:  2.44398  Training Accuracy:  0.941114\n",
      "Epoch:  697  Training Loss:  2.43957  Training Accuracy:  0.941347\n",
      "Epoch:  698  Training Loss:  2.43121  Training Accuracy:  0.941464\n",
      "Epoch:  699  Training Loss:  2.4243  Training Accuracy:  0.941697\n",
      "Epoch:  700  Training Loss:  2.41607  Training Accuracy:  0.941872\n",
      "Epoch:  701  Training Loss:  2.41098  Training Accuracy:  0.942047\n",
      "Epoch:  702  Training Loss:  2.40071  Training Accuracy:  0.942222\n",
      "Epoch:  703  Training Loss:  2.39325  Training Accuracy:  0.942222\n",
      "Epoch:  704  Training Loss:  2.38639  Training Accuracy:  0.942513\n",
      "Epoch:  705  Training Loss:  2.38197  Training Accuracy:  0.942572\n",
      "Epoch:  706  Training Loss:  2.37201  Training Accuracy:  0.942688\n",
      "Epoch:  707  Training Loss:  2.36611  Training Accuracy:  0.942922\n",
      "Epoch:  708  Training Loss:  2.35705  Training Accuracy:  0.943096\n",
      "Epoch:  709  Training Loss:  2.34715  Training Accuracy:  0.943213\n",
      "Epoch:  710  Training Loss:  2.34442  Training Accuracy:  0.943271\n",
      "Epoch:  711  Training Loss:  2.33499  Training Accuracy:  0.94333\n",
      "Epoch:  712  Training Loss:  2.33087  Training Accuracy:  0.943388\n",
      "Epoch:  713  Training Loss:  2.32147  Training Accuracy:  0.943505\n",
      "Epoch:  714  Training Loss:  2.31589  Training Accuracy:  0.943738\n",
      "Epoch:  715  Training Loss:  2.30806  Training Accuracy:  0.943796\n",
      "Epoch:  716  Training Loss:  2.29995  Training Accuracy:  0.943913\n",
      "Epoch:  717  Training Loss:  2.29326  Training Accuracy:  0.944029\n",
      "Epoch:  718  Training Loss:  2.28816  Training Accuracy:  0.944029\n",
      "Epoch:  719  Training Loss:  2.28073  Training Accuracy:  0.944321\n",
      "Epoch:  720  Training Loss:  2.27234  Training Accuracy:  0.944671\n",
      "Epoch:  721  Training Loss:  2.26695  Training Accuracy:  0.944787\n",
      "Epoch:  722  Training Loss:  2.25994  Training Accuracy:  0.944962\n",
      "Epoch:  723  Training Loss:  2.25356  Training Accuracy:  0.945079\n",
      "Epoch:  724  Training Loss:  2.24267  Training Accuracy:  0.945079\n",
      "Epoch:  725  Training Loss:  2.24102  Training Accuracy:  0.945195\n",
      "Epoch:  726  Training Loss:  2.23326  Training Accuracy:  0.945254\n",
      "Epoch:  727  Training Loss:  2.22559  Training Accuracy:  0.945429\n",
      "Epoch:  728  Training Loss:  2.21817  Training Accuracy:  0.945487\n",
      "Epoch:  729  Training Loss:  2.21405  Training Accuracy:  0.945545\n",
      "Epoch:  730  Training Loss:  2.20676  Training Accuracy:  0.945603\n",
      "Epoch:  731  Training Loss:  2.19981  Training Accuracy:  0.945837\n",
      "Epoch:  732  Training Loss:  2.19167  Training Accuracy:  0.945953\n",
      "Epoch:  733  Training Loss:  2.18791  Training Accuracy:  0.946186\n",
      "Epoch:  734  Training Loss:  2.18167  Training Accuracy:  0.946361\n",
      "Epoch:  735  Training Loss:  2.17291  Training Accuracy:  0.946536\n",
      "Epoch:  736  Training Loss:  2.16681  Training Accuracy:  0.946711\n",
      "Epoch:  737  Training Loss:  2.15916  Training Accuracy:  0.946944\n",
      "Epoch:  738  Training Loss:  2.15349  Training Accuracy:  0.947294\n",
      "Epoch:  739  Training Loss:  2.14565  Training Accuracy:  0.947469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  740  Training Loss:  2.1412  Training Accuracy:  0.947586\n",
      "Epoch:  741  Training Loss:  2.13538  Training Accuracy:  0.947702\n",
      "Epoch:  742  Training Loss:  2.12549  Training Accuracy:  0.947877\n",
      "Epoch:  743  Training Loss:  2.1196  Training Accuracy:  0.948169\n",
      "Epoch:  744  Training Loss:  2.11484  Training Accuracy:  0.948285\n",
      "Epoch:  745  Training Loss:  2.10828  Training Accuracy:  0.948577\n",
      "Epoch:  746  Training Loss:  2.09926  Training Accuracy:  0.948577\n",
      "Epoch:  747  Training Loss:  2.09443  Training Accuracy:  0.948693\n",
      "Epoch:  748  Training Loss:  2.08827  Training Accuracy:  0.948752\n",
      "Epoch:  749  Training Loss:  2.08216  Training Accuracy:  0.948985\n",
      "Epoch:  750  Training Loss:  2.07586  Training Accuracy:  0.949218\n",
      "Epoch:  751  Training Loss:  2.07039  Training Accuracy:  0.949276\n",
      "Epoch:  752  Training Loss:  2.06438  Training Accuracy:  0.949335\n",
      "Epoch:  753  Training Loss:  2.05656  Training Accuracy:  0.949451\n",
      "Epoch:  754  Training Loss:  2.05131  Training Accuracy:  0.949568\n",
      "Epoch:  755  Training Loss:  2.04695  Training Accuracy:  0.949743\n",
      "Epoch:  756  Training Loss:  2.03696  Training Accuracy:  0.949918\n",
      "Epoch:  757  Training Loss:  2.03378  Training Accuracy:  0.950268\n",
      "Epoch:  758  Training Loss:  2.02817  Training Accuracy:  0.950384\n",
      "Epoch:  759  Training Loss:  2.02298  Training Accuracy:  0.950559\n",
      "Epoch:  760  Training Loss:  2.0154  Training Accuracy:  0.950617\n",
      "Epoch:  761  Training Loss:  2.00929  Training Accuracy:  0.950676\n",
      "Epoch:  762  Training Loss:  2.00395  Training Accuracy:  0.950792\n",
      "Epoch:  763  Training Loss:  1.9967  Training Accuracy:  0.950909\n",
      "Epoch:  764  Training Loss:  1.99169  Training Accuracy:  0.950909\n",
      "Epoch:  765  Training Loss:  1.98748  Training Accuracy:  0.951142\n",
      "Epoch:  766  Training Loss:  1.9785  Training Accuracy:  0.9512\n",
      "Epoch:  767  Training Loss:  1.97382  Training Accuracy:  0.951317\n",
      "Epoch:  768  Training Loss:  1.97154  Training Accuracy:  0.951317\n",
      "Epoch:  769  Training Loss:  1.96258  Training Accuracy:  0.95155\n",
      "Epoch:  770  Training Loss:  1.95967  Training Accuracy:  0.951667\n",
      "Epoch:  771  Training Loss:  1.9515  Training Accuracy:  0.951842\n",
      "Epoch:  772  Training Loss:  1.94852  Training Accuracy:  0.951842\n",
      "Epoch:  773  Training Loss:  1.94148  Training Accuracy:  0.952017\n",
      "Epoch:  774  Training Loss:  1.93581  Training Accuracy:  0.952017\n",
      "Epoch:  775  Training Loss:  1.93291  Training Accuracy:  0.952075\n",
      "Epoch:  776  Training Loss:  1.92499  Training Accuracy:  0.952366\n",
      "Epoch:  777  Training Loss:  1.92078  Training Accuracy:  0.952483\n",
      "Epoch:  778  Training Loss:  1.91562  Training Accuracy:  0.952658\n",
      "Epoch:  779  Training Loss:  1.90752  Training Accuracy:  0.952775\n",
      "Epoch:  780  Training Loss:  1.90423  Training Accuracy:  0.952775\n",
      "Epoch:  781  Training Loss:  1.90022  Training Accuracy:  0.952775\n",
      "Epoch:  782  Training Loss:  1.89175  Training Accuracy:  0.952833\n",
      "Epoch:  783  Training Loss:  1.888  Training Accuracy:  0.952833\n",
      "Epoch:  784  Training Loss:  1.88375  Training Accuracy:  0.952891\n",
      "Epoch:  785  Training Loss:  1.87505  Training Accuracy:  0.95295\n",
      "Epoch:  786  Training Loss:  1.87416  Training Accuracy:  0.953299\n",
      "Epoch:  787  Training Loss:  1.86865  Training Accuracy:  0.953416\n",
      "Epoch:  788  Training Loss:  1.85968  Training Accuracy:  0.953649\n",
      "Epoch:  789  Training Loss:  1.85785  Training Accuracy:  0.953941\n",
      "Epoch:  790  Training Loss:  1.85243  Training Accuracy:  0.954057\n",
      "Epoch:  791  Training Loss:  1.84509  Training Accuracy:  0.954174\n",
      "Epoch:  792  Training Loss:  1.84225  Training Accuracy:  0.954232\n",
      "Epoch:  793  Training Loss:  1.83891  Training Accuracy:  0.954291\n",
      "Epoch:  794  Training Loss:  1.82991  Training Accuracy:  0.954524\n",
      "Epoch:  795  Training Loss:  1.82818  Training Accuracy:  0.95464\n",
      "Epoch:  796  Training Loss:  1.8241  Training Accuracy:  0.954757\n",
      "Epoch:  797  Training Loss:  1.81623  Training Accuracy:  0.95499\n",
      "Epoch:  798  Training Loss:  1.81394  Training Accuracy:  0.955048\n",
      "Epoch:  799  Training Loss:  1.80979  Training Accuracy:  0.955107\n",
      "Epoch:  800  Training Loss:  1.8035  Training Accuracy:  0.955457\n",
      "Epoch:  801  Training Loss:  1.80074  Training Accuracy:  0.955515\n",
      "Epoch:  802  Training Loss:  1.79545  Training Accuracy:  0.95569\n",
      "Epoch:  803  Training Loss:  1.78852  Training Accuracy:  0.955806\n",
      "Epoch:  804  Training Loss:  1.78617  Training Accuracy:  0.955923\n",
      "Epoch:  805  Training Loss:  1.77976  Training Accuracy:  0.955981\n",
      "Epoch:  806  Training Loss:  1.77635  Training Accuracy:  0.956098\n",
      "Epoch:  807  Training Loss:  1.76991  Training Accuracy:  0.956214\n",
      "Epoch:  808  Training Loss:  1.76772  Training Accuracy:  0.956156\n",
      "Epoch:  809  Training Loss:  1.76206  Training Accuracy:  0.956331\n",
      "Epoch:  810  Training Loss:  1.75594  Training Accuracy:  0.956448\n",
      "Epoch:  811  Training Loss:  1.75376  Training Accuracy:  0.956564\n",
      "Epoch:  812  Training Loss:  1.74987  Training Accuracy:  0.956564\n",
      "Epoch:  813  Training Loss:  1.74308  Training Accuracy:  0.956564\n",
      "Epoch:  814  Training Loss:  1.7398  Training Accuracy:  0.956681\n",
      "Epoch:  815  Training Loss:  1.73881  Training Accuracy:  0.956739\n",
      "Epoch:  816  Training Loss:  1.73001  Training Accuracy:  0.956856\n",
      "Epoch:  817  Training Loss:  1.7255  Training Accuracy:  0.957031\n",
      "Epoch:  818  Training Loss:  1.7236  Training Accuracy:  0.957147\n",
      "Epoch:  819  Training Loss:  1.7179  Training Accuracy:  0.957147\n",
      "Epoch:  820  Training Loss:  1.71331  Training Accuracy:  0.957264\n",
      "Epoch:  821  Training Loss:  1.70964  Training Accuracy:  0.95738\n",
      "Epoch:  822  Training Loss:  1.70508  Training Accuracy:  0.957497\n",
      "Epoch:  823  Training Loss:  1.69986  Training Accuracy:  0.957614\n",
      "Epoch:  824  Training Loss:  1.69725  Training Accuracy:  0.957672\n",
      "Epoch:  825  Training Loss:  1.69281  Training Accuracy:  0.957789\n",
      "Epoch:  826  Training Loss:  1.68628  Training Accuracy:  0.957789\n",
      "Epoch:  827  Training Loss:  1.68478  Training Accuracy:  0.957847\n",
      "Epoch:  828  Training Loss:  1.67987  Training Accuracy:  0.957964\n",
      "Epoch:  829  Training Loss:  1.67565  Training Accuracy:  0.958138\n",
      "Epoch:  830  Training Loss:  1.66973  Training Accuracy:  0.958197\n",
      "Epoch:  831  Training Loss:  1.66714  Training Accuracy:  0.958372\n",
      "Epoch:  832  Training Loss:  1.66348  Training Accuracy:  0.95843\n",
      "Epoch:  833  Training Loss:  1.6574  Training Accuracy:  0.958547\n",
      "Epoch:  834  Training Loss:  1.65449  Training Accuracy:  0.958605\n",
      "Epoch:  835  Training Loss:  1.64849  Training Accuracy:  0.958721\n",
      "Epoch:  836  Training Loss:  1.64328  Training Accuracy:  0.958838\n",
      "Epoch:  837  Training Loss:  1.64165  Training Accuracy:  0.958955\n",
      "Epoch:  838  Training Loss:  1.63533  Training Accuracy:  0.959013\n",
      "Epoch:  839  Training Loss:  1.63153  Training Accuracy:  0.959188\n",
      "Epoch:  840  Training Loss:  1.62833  Training Accuracy:  0.959188\n",
      "Epoch:  841  Training Loss:  1.62245  Training Accuracy:  0.959304\n",
      "Epoch:  842  Training Loss:  1.6193  Training Accuracy:  0.959479\n",
      "Epoch:  843  Training Loss:  1.61561  Training Accuracy:  0.959479\n",
      "Epoch:  844  Training Loss:  1.61044  Training Accuracy:  0.959596\n",
      "Epoch:  845  Training Loss:  1.60582  Training Accuracy:  0.959713\n",
      "Epoch:  846  Training Loss:  1.6019  Training Accuracy:  0.959771\n",
      "Epoch:  847  Training Loss:  1.59996  Training Accuracy:  0.959771\n",
      "Epoch:  848  Training Loss:  1.59353  Training Accuracy:  0.959946\n",
      "Epoch:  849  Training Loss:  1.58772  Training Accuracy:  0.960004\n",
      "Epoch:  850  Training Loss:  1.58564  Training Accuracy:  0.960062\n",
      "Epoch:  851  Training Loss:  1.58206  Training Accuracy:  0.960237\n",
      "Epoch:  852  Training Loss:  1.57683  Training Accuracy:  0.960471\n",
      "Epoch:  853  Training Loss:  1.5708  Training Accuracy:  0.960587\n",
      "Epoch:  854  Training Loss:  1.56927  Training Accuracy:  0.960645\n",
      "Epoch:  855  Training Loss:  1.56403  Training Accuracy:  0.960762\n",
      "Epoch:  856  Training Loss:  1.56196  Training Accuracy:  0.960879\n",
      "Epoch:  857  Training Loss:  1.55754  Training Accuracy:  0.960937\n",
      "Epoch:  858  Training Loss:  1.5515  Training Accuracy:  0.960937\n",
      "Epoch:  859  Training Loss:  1.55029  Training Accuracy:  0.961054\n",
      "Epoch:  860  Training Loss:  1.54433  Training Accuracy:  0.961112\n",
      "Epoch:  861  Training Loss:  1.54086  Training Accuracy:  0.96117\n",
      "Epoch:  862  Training Loss:  1.53664  Training Accuracy:  0.96117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  863  Training Loss:  1.53264  Training Accuracy:  0.96117\n",
      "Epoch:  864  Training Loss:  1.52875  Training Accuracy:  0.961228\n",
      "Epoch:  865  Training Loss:  1.52607  Training Accuracy:  0.961403\n",
      "Epoch:  866  Training Loss:  1.51995  Training Accuracy:  0.961578\n",
      "Epoch:  867  Training Loss:  1.51638  Training Accuracy:  0.961637\n",
      "Epoch:  868  Training Loss:  1.51351  Training Accuracy:  0.961695\n",
      "Epoch:  869  Training Loss:  1.50967  Training Accuracy:  0.961928\n",
      "Epoch:  870  Training Loss:  1.50479  Training Accuracy:  0.961928\n",
      "Epoch:  871  Training Loss:  1.50127  Training Accuracy:  0.962161\n",
      "Epoch:  872  Training Loss:  1.49856  Training Accuracy:  0.962161\n",
      "Epoch:  873  Training Loss:  1.49451  Training Accuracy:  0.962278\n",
      "Epoch:  874  Training Loss:  1.49031  Training Accuracy:  0.962336\n",
      "Epoch:  875  Training Loss:  1.48547  Training Accuracy:  0.962394\n",
      "Epoch:  876  Training Loss:  1.4808  Training Accuracy:  0.962394\n",
      "Epoch:  877  Training Loss:  1.47761  Training Accuracy:  0.962394\n",
      "Epoch:  878  Training Loss:  1.47559  Training Accuracy:  0.962511\n",
      "Epoch:  879  Training Loss:  1.47025  Training Accuracy:  0.962628\n",
      "Epoch:  880  Training Loss:  1.46655  Training Accuracy:  0.962628\n",
      "Epoch:  881  Training Loss:  1.46113  Training Accuracy:  0.962686\n",
      "Epoch:  882  Training Loss:  1.45833  Training Accuracy:  0.962686\n",
      "Epoch:  883  Training Loss:  1.45652  Training Accuracy:  0.962919\n",
      "Epoch:  884  Training Loss:  1.45106  Training Accuracy:  0.963094\n",
      "Epoch:  885  Training Loss:  1.44668  Training Accuracy:  0.963094\n",
      "Epoch:  886  Training Loss:  1.44374  Training Accuracy:  0.963269\n",
      "Epoch:  887  Training Loss:  1.44188  Training Accuracy:  0.963386\n",
      "Epoch:  888  Training Loss:  1.43529  Training Accuracy:  0.963444\n",
      "Epoch:  889  Training Loss:  1.43243  Training Accuracy:  0.963677\n",
      "Epoch:  890  Training Loss:  1.43016  Training Accuracy:  0.963852\n",
      "Epoch:  891  Training Loss:  1.42458  Training Accuracy:  0.96391\n",
      "Epoch:  892  Training Loss:  1.42445  Training Accuracy:  0.963969\n",
      "Epoch:  893  Training Loss:  1.41763  Training Accuracy:  0.964202\n",
      "Epoch:  894  Training Loss:  1.41505  Training Accuracy:  0.96426\n",
      "Epoch:  895  Training Loss:  1.41074  Training Accuracy:  0.964435\n",
      "Epoch:  896  Training Loss:  1.40799  Training Accuracy:  0.964493\n",
      "Epoch:  897  Training Loss:  1.40636  Training Accuracy:  0.964493\n",
      "Epoch:  898  Training Loss:  1.40057  Training Accuracy:  0.964552\n",
      "Epoch:  899  Training Loss:  1.3966  Training Accuracy:  0.964727\n",
      "Epoch:  900  Training Loss:  1.394  Training Accuracy:  0.96496\n",
      "Epoch:  901  Training Loss:  1.38974  Training Accuracy:  0.965251\n",
      "Epoch:  902  Training Loss:  1.38646  Training Accuracy:  0.96531\n",
      "Epoch:  903  Training Loss:  1.38229  Training Accuracy:  0.965368\n",
      "Epoch:  904  Training Loss:  1.37912  Training Accuracy:  0.965368\n",
      "Epoch:  905  Training Loss:  1.37605  Training Accuracy:  0.965426\n",
      "Epoch:  906  Training Loss:  1.37389  Training Accuracy:  0.965543\n",
      "Epoch:  907  Training Loss:  1.36847  Training Accuracy:  0.965659\n",
      "Epoch:  908  Training Loss:  1.36469  Training Accuracy:  0.965718\n",
      "Epoch:  909  Training Loss:  1.36156  Training Accuracy:  0.965893\n",
      "Epoch:  910  Training Loss:  1.35963  Training Accuracy:  0.965893\n",
      "Epoch:  911  Training Loss:  1.35448  Training Accuracy:  0.965951\n",
      "Epoch:  912  Training Loss:  1.35272  Training Accuracy:  0.965951\n",
      "Epoch:  913  Training Loss:  1.34765  Training Accuracy:  0.966009\n",
      "Epoch:  914  Training Loss:  1.34683  Training Accuracy:  0.965951\n",
      "Epoch:  915  Training Loss:  1.34235  Training Accuracy:  0.966126\n",
      "Epoch:  916  Training Loss:  1.33843  Training Accuracy:  0.966126\n",
      "Epoch:  917  Training Loss:  1.3359  Training Accuracy:  0.966184\n",
      "Epoch:  918  Training Loss:  1.33394  Training Accuracy:  0.966301\n",
      "Epoch:  919  Training Loss:  1.33005  Training Accuracy:  0.966301\n",
      "Epoch:  920  Training Loss:  1.32687  Training Accuracy:  0.966301\n",
      "Epoch:  921  Training Loss:  1.32301  Training Accuracy:  0.966417\n",
      "Epoch:  922  Training Loss:  1.31984  Training Accuracy:  0.966592\n",
      "Epoch:  923  Training Loss:  1.317  Training Accuracy:  0.966592\n",
      "Epoch:  924  Training Loss:  1.31519  Training Accuracy:  0.966651\n",
      "Epoch:  925  Training Loss:  1.31036  Training Accuracy:  0.966825\n",
      "Epoch:  926  Training Loss:  1.30584  Training Accuracy:  0.966884\n",
      "Epoch:  927  Training Loss:  1.30429  Training Accuracy:  0.967\n",
      "Epoch:  928  Training Loss:  1.30276  Training Accuracy:  0.967175\n",
      "Epoch:  929  Training Loss:  1.29759  Training Accuracy:  0.967234\n",
      "Epoch:  930  Training Loss:  1.29489  Training Accuracy:  0.967408\n",
      "Epoch:  931  Training Loss:  1.2921  Training Accuracy:  0.967525\n",
      "Epoch:  932  Training Loss:  1.28872  Training Accuracy:  0.9677\n",
      "Epoch:  933  Training Loss:  1.28478  Training Accuracy:  0.9677\n",
      "Epoch:  934  Training Loss:  1.28238  Training Accuracy:  0.9677\n",
      "Epoch:  935  Training Loss:  1.27888  Training Accuracy:  0.967758\n",
      "Epoch:  936  Training Loss:  1.27612  Training Accuracy:  0.967875\n",
      "Epoch:  937  Training Loss:  1.27362  Training Accuracy:  0.967991\n",
      "Epoch:  938  Training Loss:  1.26934  Training Accuracy:  0.968108\n",
      "Epoch:  939  Training Loss:  1.26638  Training Accuracy:  0.968166\n",
      "Epoch:  940  Training Loss:  1.26369  Training Accuracy:  0.968225\n",
      "Epoch:  941  Training Loss:  1.25878  Training Accuracy:  0.9684\n",
      "Epoch:  942  Training Loss:  1.25776  Training Accuracy:  0.968458\n",
      "Epoch:  943  Training Loss:  1.25572  Training Accuracy:  0.968516\n",
      "Epoch:  944  Training Loss:  1.24972  Training Accuracy:  0.968516\n",
      "Epoch:  945  Training Loss:  1.24688  Training Accuracy:  0.968575\n",
      "Epoch:  946  Training Loss:  1.24515  Training Accuracy:  0.968633\n",
      "Epoch:  947  Training Loss:  1.2415  Training Accuracy:  0.968749\n",
      "Epoch:  948  Training Loss:  1.24041  Training Accuracy:  0.968749\n",
      "Epoch:  949  Training Loss:  1.23671  Training Accuracy:  0.968808\n",
      "Epoch:  950  Training Loss:  1.23303  Training Accuracy:  0.968866\n",
      "Epoch:  951  Training Loss:  1.22922  Training Accuracy:  0.968983\n",
      "Epoch:  952  Training Loss:  1.22809  Training Accuracy:  0.969041\n",
      "Epoch:  953  Training Loss:  1.22267  Training Accuracy:  0.969041\n",
      "Epoch:  954  Training Loss:  1.22118  Training Accuracy:  0.969099\n",
      "Epoch:  955  Training Loss:  1.21723  Training Accuracy:  0.969099\n",
      "Epoch:  956  Training Loss:  1.21584  Training Accuracy:  0.969158\n",
      "Epoch:  957  Training Loss:  1.21148  Training Accuracy:  0.969158\n",
      "Epoch:  958  Training Loss:  1.20911  Training Accuracy:  0.969157\n",
      "Epoch:  959  Training Loss:  1.20494  Training Accuracy:  0.969216\n",
      "Epoch:  960  Training Loss:  1.2024  Training Accuracy:  0.969274\n",
      "Epoch:  961  Training Loss:  1.19994  Training Accuracy:  0.969332\n",
      "Epoch:  962  Training Loss:  1.19775  Training Accuracy:  0.969332\n",
      "Epoch:  963  Training Loss:  1.19231  Training Accuracy:  0.969449\n",
      "Epoch:  964  Training Loss:  1.19047  Training Accuracy:  0.969507\n",
      "Epoch:  965  Training Loss:  1.18751  Training Accuracy:  0.969507\n",
      "Epoch:  966  Training Loss:  1.18513  Training Accuracy:  0.969507\n",
      "Epoch:  967  Training Loss:  1.18027  Training Accuracy:  0.969624\n",
      "Epoch:  968  Training Loss:  1.1789  Training Accuracy:  0.969624\n",
      "Epoch:  969  Training Loss:  1.1749  Training Accuracy:  0.969624\n",
      "Epoch:  970  Training Loss:  1.17201  Training Accuracy:  0.969741\n",
      "Epoch:  971  Training Loss:  1.16988  Training Accuracy:  0.969741\n",
      "Epoch:  972  Training Loss:  1.1659  Training Accuracy:  0.969799\n",
      "Epoch:  973  Training Loss:  1.16266  Training Accuracy:  0.969799\n",
      "Epoch:  974  Training Loss:  1.16148  Training Accuracy:  0.969915\n",
      "Epoch:  975  Training Loss:  1.15794  Training Accuracy:  0.970032\n",
      "Epoch:  976  Training Loss:  1.15532  Training Accuracy:  0.97009\n",
      "Epoch:  977  Training Loss:  1.15199  Training Accuracy:  0.970382\n",
      "Epoch:  978  Training Loss:  1.15016  Training Accuracy:  0.970499\n",
      "Epoch:  979  Training Loss:  1.14616  Training Accuracy:  0.970498\n",
      "Epoch:  980  Training Loss:  1.14522  Training Accuracy:  0.970499\n",
      "Epoch:  981  Training Loss:  1.14108  Training Accuracy:  0.970615\n",
      "Epoch:  982  Training Loss:  1.13731  Training Accuracy:  0.970615\n",
      "Epoch:  983  Training Loss:  1.13698  Training Accuracy:  0.970673\n",
      "Epoch:  984  Training Loss:  1.13292  Training Accuracy:  0.970673\n",
      "Epoch:  985  Training Loss:  1.1314  Training Accuracy:  0.970732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  986  Training Loss:  1.12806  Training Accuracy:  0.970732\n",
      "Epoch:  987  Training Loss:  1.12433  Training Accuracy:  0.970848\n",
      "Epoch:  988  Training Loss:  1.12293  Training Accuracy:  0.970907\n",
      "Epoch:  989  Training Loss:  1.12028  Training Accuracy:  0.970907\n",
      "Epoch:  990  Training Loss:  1.11675  Training Accuracy:  0.970965\n",
      "Epoch:  991  Training Loss:  1.11516  Training Accuracy:  0.970907\n",
      "Epoch:  992  Training Loss:  1.11202  Training Accuracy:  0.970965\n",
      "Epoch:  993  Training Loss:  1.10915  Training Accuracy:  0.971023\n",
      "Epoch:  994  Training Loss:  1.10629  Training Accuracy:  0.97114\n",
      "Epoch:  995  Training Loss:  1.1042  Training Accuracy:  0.971256\n",
      "Epoch:  996  Training Loss:  1.10095  Training Accuracy:  0.971256\n",
      "Epoch:  997  Training Loss:  1.09878  Training Accuracy:  0.971256\n",
      "Epoch:  998  Training Loss:  1.09539  Training Accuracy:  0.971373\n",
      "Epoch:  999  Training Loss:  1.09224  Training Accuracy:  0.971373\n",
      "Testing Accuracy: 0.860847\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  226.271  Training Accuracy:  0.0359788\n",
      "Epoch:  1  Training Loss:  188.439  Training Accuracy:  0.0416226\n",
      "Epoch:  2  Training Loss:  175.691  Training Accuracy:  0.0491476\n",
      "Epoch:  3  Training Loss:  178.367  Training Accuracy:  0.0636684\n",
      "Epoch:  4  Training Loss:  186.929  Training Accuracy:  0.0777778\n",
      "Epoch:  5  Training Loss:  193.83  Training Accuracy:  0.0907113\n",
      "Epoch:  6  Training Loss:  198.51  Training Accuracy:  0.10194\n",
      "Epoch:  7  Training Loss:  201.726  Training Accuracy:  0.110817\n",
      "Epoch:  8  Training Loss:  204.102  Training Accuracy:  0.12087\n",
      "Epoch:  9  Training Loss:  205.972  Training Accuracy:  0.130923\n",
      "Epoch:  10  Training Loss:  207.515  Training Accuracy:  0.144092\n",
      "Epoch:  11  Training Loss:  208.879  Training Accuracy:  0.154909\n",
      "Epoch:  12  Training Loss:  210.121  Training Accuracy:  0.164374\n",
      "Epoch:  13  Training Loss:  211.259  Training Accuracy:  0.175955\n",
      "Epoch:  14  Training Loss:  212.313  Training Accuracy:  0.18448\n",
      "Epoch:  15  Training Loss:  213.301  Training Accuracy:  0.191652\n",
      "Epoch:  16  Training Loss:  214.254  Training Accuracy:  0.200176\n",
      "Epoch:  17  Training Loss:  215.133  Training Accuracy:  0.210464\n",
      "Epoch:  18  Training Loss:  215.942  Training Accuracy:  0.218989\n",
      "Epoch:  19  Training Loss:  216.694  Training Accuracy:  0.227219\n",
      "Epoch:  20  Training Loss:  217.345  Training Accuracy:  0.234156\n",
      "Epoch:  21  Training Loss:  217.953  Training Accuracy:  0.240741\n",
      "Epoch:  22  Training Loss:  218.472  Training Accuracy:  0.246796\n",
      "Epoch:  23  Training Loss:  218.97  Training Accuracy:  0.251969\n",
      "Epoch:  24  Training Loss:  219.374  Training Accuracy:  0.257496\n",
      "Epoch:  25  Training Loss:  219.715  Training Accuracy:  0.263374\n",
      "Epoch:  26  Training Loss:  220.001  Training Accuracy:  0.270194\n",
      "Epoch:  27  Training Loss:  220.197  Training Accuracy:  0.276132\n",
      "Epoch:  28  Training Loss:  220.368  Training Accuracy:  0.281717\n",
      "Epoch:  29  Training Loss:  220.501  Training Accuracy:  0.285538\n",
      "Epoch:  30  Training Loss:  220.534  Training Accuracy:  0.290006\n",
      "Epoch:  31  Training Loss:  220.518  Training Accuracy:  0.293827\n",
      "Epoch:  32  Training Loss:  220.495  Training Accuracy:  0.298942\n",
      "Epoch:  33  Training Loss:  220.411  Training Accuracy:  0.304585\n",
      "Epoch:  34  Training Loss:  220.27  Training Accuracy:  0.308936\n",
      "Epoch:  35  Training Loss:  220.081  Training Accuracy:  0.313169\n",
      "Epoch:  36  Training Loss:  219.881  Training Accuracy:  0.316637\n",
      "Epoch:  37  Training Loss:  219.686  Training Accuracy:  0.320282\n",
      "Epoch:  38  Training Loss:  219.427  Training Accuracy:  0.325044\n",
      "Epoch:  39  Training Loss:  219.177  Training Accuracy:  0.328865\n",
      "Epoch:  40  Training Loss:  218.895  Training Accuracy:  0.333157\n",
      "Epoch:  41  Training Loss:  218.592  Training Accuracy:  0.33639\n",
      "Epoch:  42  Training Loss:  218.289  Training Accuracy:  0.339741\n",
      "Epoch:  43  Training Loss:  217.957  Training Accuracy:  0.343327\n",
      "Epoch:  44  Training Loss:  217.633  Training Accuracy:  0.346267\n",
      "Epoch:  45  Training Loss:  217.231  Training Accuracy:  0.350147\n",
      "Epoch:  46  Training Loss:  216.909  Training Accuracy:  0.353204\n",
      "Epoch:  47  Training Loss:  216.553  Training Accuracy:  0.357084\n",
      "Epoch:  48  Training Loss:  216.159  Training Accuracy:  0.359788\n",
      "Epoch:  49  Training Loss:  215.835  Training Accuracy:  0.362728\n",
      "Epoch:  50  Training Loss:  215.48  Training Accuracy:  0.366079\n",
      "Epoch:  51  Training Loss:  215.082  Training Accuracy:  0.370547\n",
      "Epoch:  52  Training Loss:  214.727  Training Accuracy:  0.373721\n",
      "Epoch:  53  Training Loss:  214.314  Training Accuracy:  0.376543\n",
      "Epoch:  54  Training Loss:  213.973  Training Accuracy:  0.379012\n",
      "Epoch:  55  Training Loss:  213.614  Training Accuracy:  0.382304\n",
      "Epoch:  56  Training Loss:  213.266  Training Accuracy:  0.385655\n",
      "Epoch:  57  Training Loss:  212.928  Training Accuracy:  0.388536\n",
      "Epoch:  58  Training Loss:  212.596  Training Accuracy:  0.391064\n",
      "Epoch:  59  Training Loss:  212.269  Training Accuracy:  0.394062\n",
      "Epoch:  60  Training Loss:  211.929  Training Accuracy:  0.396355\n",
      "Epoch:  61  Training Loss:  211.61  Training Accuracy:  0.4\n",
      "Epoch:  62  Training Loss:  211.279  Training Accuracy:  0.40241\n",
      "Epoch:  63  Training Loss:  210.994  Training Accuracy:  0.405056\n",
      "Epoch:  64  Training Loss:  210.715  Training Accuracy:  0.407466\n",
      "Epoch:  65  Training Loss:  210.432  Training Accuracy:  0.41017\n",
      "Epoch:  66  Training Loss:  210.162  Training Accuracy:  0.412816\n",
      "Epoch:  67  Training Loss:  209.88  Training Accuracy:  0.41505\n",
      "Epoch:  68  Training Loss:  209.683  Training Accuracy:  0.417754\n",
      "Epoch:  69  Training Loss:  209.409  Training Accuracy:  0.420223\n",
      "Epoch:  70  Training Loss:  209.197  Training Accuracy:  0.423221\n",
      "Epoch:  71  Training Loss:  208.979  Training Accuracy:  0.425044\n",
      "Epoch:  72  Training Loss:  208.754  Training Accuracy:  0.427396\n",
      "Epoch:  73  Training Loss:  208.549  Training Accuracy:  0.429747\n",
      "Epoch:  74  Training Loss:  208.348  Training Accuracy:  0.431746\n",
      "Epoch:  75  Training Loss:  208.116  Training Accuracy:  0.434274\n",
      "Epoch:  76  Training Loss:  207.952  Training Accuracy:  0.436802\n",
      "Epoch:  77  Training Loss:  207.73  Training Accuracy:  0.438624\n",
      "Epoch:  78  Training Loss:  207.565  Training Accuracy:  0.441446\n",
      "Epoch:  79  Training Loss:  207.39  Training Accuracy:  0.443562\n",
      "Epoch:  80  Training Loss:  207.206  Training Accuracy:  0.445209\n",
      "Epoch:  81  Training Loss:  207.072  Training Accuracy:  0.44803\n",
      "Epoch:  82  Training Loss:  206.879  Training Accuracy:  0.451087\n",
      "Epoch:  83  Training Loss:  206.739  Training Accuracy:  0.453557\n",
      "Epoch:  84  Training Loss:  206.556  Training Accuracy:  0.456143\n",
      "Epoch:  85  Training Loss:  206.427  Training Accuracy:  0.459142\n",
      "Epoch:  86  Training Loss:  206.29  Training Accuracy:  0.461611\n",
      "Epoch:  87  Training Loss:  206.192  Training Accuracy:  0.463551\n",
      "Epoch:  88  Training Loss:  206.068  Training Accuracy:  0.46602\n",
      "Epoch:  89  Training Loss:  206.001  Training Accuracy:  0.468489\n",
      "Epoch:  90  Training Loss:  205.862  Training Accuracy:  0.470605\n",
      "Epoch:  91  Training Loss:  205.781  Training Accuracy:  0.472369\n",
      "Epoch:  92  Training Loss:  205.723  Training Accuracy:  0.474427\n",
      "Epoch:  93  Training Loss:  205.612  Training Accuracy:  0.476896\n",
      "Epoch:  94  Training Loss:  205.546  Training Accuracy:  0.478718\n",
      "Epoch:  95  Training Loss:  205.452  Training Accuracy:  0.480776\n",
      "Epoch:  96  Training Loss:  205.404  Training Accuracy:  0.483657\n",
      "Epoch:  97  Training Loss:  205.293  Training Accuracy:  0.486714\n",
      "Epoch:  98  Training Loss:  205.28  Training Accuracy:  0.4903\n",
      "Epoch:  99  Training Loss:  205.187  Training Accuracy:  0.492769\n",
      "Epoch:  100  Training Loss:  205.117  Training Accuracy:  0.495297\n",
      "Epoch:  101  Training Loss:  205.076  Training Accuracy:  0.498001\n",
      "Epoch:  102  Training Loss:  205.048  Training Accuracy:  0.500059\n",
      "Epoch:  103  Training Loss:  204.964  Training Accuracy:  0.502116\n",
      "Epoch:  104  Training Loss:  204.983  Training Accuracy:  0.503939\n",
      "Epoch:  105  Training Loss:  204.902  Training Accuracy:  0.505761\n",
      "Epoch:  106  Training Loss:  204.897  Training Accuracy:  0.507701\n",
      "Epoch:  107  Training Loss:  204.854  Training Accuracy:  0.509347\n",
      "Epoch:  108  Training Loss:  204.887  Training Accuracy:  0.511581\n",
      "Epoch:  109  Training Loss:  204.862  Training Accuracy:  0.513756\n",
      "Epoch:  110  Training Loss:  204.88  Training Accuracy:  0.516461\n",
      "Epoch:  111  Training Loss:  204.893  Training Accuracy:  0.517872\n",
      "Epoch:  112  Training Loss:  204.854  Training Accuracy:  0.519871\n",
      "Epoch:  113  Training Loss:  204.891  Training Accuracy:  0.521869\n",
      "Epoch:  114  Training Loss:  204.913  Training Accuracy:  0.523751\n",
      "Epoch:  115  Training Loss:  204.896  Training Accuracy:  0.52622\n",
      "Epoch:  116  Training Loss:  204.934  Training Accuracy:  0.528042\n",
      "Epoch:  117  Training Loss:  204.93  Training Accuracy:  0.529982\n",
      "Epoch:  118  Training Loss:  204.919  Training Accuracy:  0.53157\n",
      "Epoch:  119  Training Loss:  204.968  Training Accuracy:  0.532981\n",
      "Epoch:  120  Training Loss:  204.995  Training Accuracy:  0.535156\n",
      "Epoch:  121  Training Loss:  205.054  Training Accuracy:  0.536978\n",
      "Epoch:  122  Training Loss:  205.07  Training Accuracy:  0.538859\n",
      "Epoch:  123  Training Loss:  205.126  Training Accuracy:  0.540858\n",
      "Epoch:  124  Training Loss:  205.159  Training Accuracy:  0.542857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  205.191  Training Accuracy:  0.544386\n",
      "Epoch:  126  Training Loss:  205.229  Training Accuracy:  0.545797\n",
      "Epoch:  127  Training Loss:  205.294  Training Accuracy:  0.547795\n",
      "Epoch:  128  Training Loss:  205.318  Training Accuracy:  0.549677\n",
      "Epoch:  129  Training Loss:  205.404  Training Accuracy:  0.55097\n",
      "Epoch:  130  Training Loss:  205.41  Training Accuracy:  0.55244\n",
      "Epoch:  131  Training Loss:  205.522  Training Accuracy:  0.554615\n",
      "Epoch:  132  Training Loss:  205.564  Training Accuracy:  0.556143\n",
      "Epoch:  133  Training Loss:  205.635  Training Accuracy:  0.558201\n",
      "Epoch:  134  Training Loss:  205.687  Training Accuracy:  0.559377\n",
      "Epoch:  135  Training Loss:  205.759  Training Accuracy:  0.561552\n",
      "Epoch:  136  Training Loss:  205.812  Training Accuracy:  0.563257\n",
      "Epoch:  137  Training Loss:  205.835  Training Accuracy:  0.56455\n",
      "Epoch:  138  Training Loss:  205.902  Training Accuracy:  0.566255\n",
      "Epoch:  139  Training Loss:  205.937  Training Accuracy:  0.568254\n",
      "Epoch:  140  Training Loss:  205.962  Training Accuracy:  0.56943\n",
      "Epoch:  141  Training Loss:  206.03  Training Accuracy:  0.57137\n",
      "Epoch:  142  Training Loss:  206.069  Training Accuracy:  0.57331\n",
      "Epoch:  143  Training Loss:  206.128  Training Accuracy:  0.57425\n",
      "Epoch:  144  Training Loss:  206.148  Training Accuracy:  0.576132\n",
      "Epoch:  145  Training Loss:  206.25  Training Accuracy:  0.577425\n",
      "Epoch:  146  Training Loss:  206.279  Training Accuracy:  0.579071\n",
      "Epoch:  147  Training Loss:  206.325  Training Accuracy:  0.580894\n",
      "Epoch:  148  Training Loss:  206.404  Training Accuracy:  0.582363\n",
      "Epoch:  149  Training Loss:  206.454  Training Accuracy:  0.584127\n",
      "Epoch:  150  Training Loss:  206.513  Training Accuracy:  0.585597\n",
      "Epoch:  151  Training Loss:  206.563  Training Accuracy:  0.587302\n",
      "Epoch:  152  Training Loss:  206.626  Training Accuracy:  0.588242\n",
      "Epoch:  153  Training Loss:  206.713  Training Accuracy:  0.589535\n",
      "Epoch:  154  Training Loss:  206.79  Training Accuracy:  0.591123\n",
      "Epoch:  155  Training Loss:  206.856  Training Accuracy:  0.592651\n",
      "Epoch:  156  Training Loss:  206.885  Training Accuracy:  0.594004\n",
      "Epoch:  157  Training Loss:  207.0  Training Accuracy:  0.59512\n",
      "Epoch:  158  Training Loss:  207.063  Training Accuracy:  0.596355\n",
      "Epoch:  159  Training Loss:  207.122  Training Accuracy:  0.597648\n",
      "Epoch:  160  Training Loss:  207.199  Training Accuracy:  0.598824\n",
      "Epoch:  161  Training Loss:  207.239  Training Accuracy:  0.59953\n",
      "Epoch:  162  Training Loss:  207.316  Training Accuracy:  0.600647\n",
      "Epoch:  163  Training Loss:  207.387  Training Accuracy:  0.601705\n",
      "Epoch:  164  Training Loss:  207.391  Training Accuracy:  0.602939\n",
      "Epoch:  165  Training Loss:  207.474  Training Accuracy:  0.604174\n",
      "Epoch:  166  Training Loss:  207.516  Training Accuracy:  0.605291\n",
      "Epoch:  167  Training Loss:  207.557  Training Accuracy:  0.606114\n",
      "Epoch:  168  Training Loss:  207.593  Training Accuracy:  0.607584\n",
      "Epoch:  169  Training Loss:  207.608  Training Accuracy:  0.609112\n",
      "Epoch:  170  Training Loss:  207.65  Training Accuracy:  0.610406\n",
      "Epoch:  171  Training Loss:  207.708  Training Accuracy:  0.611523\n",
      "Epoch:  172  Training Loss:  207.753  Training Accuracy:  0.612581\n",
      "Epoch:  173  Training Loss:  207.751  Training Accuracy:  0.613757\n",
      "Epoch:  174  Training Loss:  207.818  Training Accuracy:  0.615461\n",
      "Epoch:  175  Training Loss:  207.872  Training Accuracy:  0.616696\n",
      "Epoch:  176  Training Loss:  207.899  Training Accuracy:  0.618636\n",
      "Epoch:  177  Training Loss:  207.952  Training Accuracy:  0.619635\n",
      "Epoch:  178  Training Loss:  207.96  Training Accuracy:  0.620753\n",
      "Epoch:  179  Training Loss:  207.991  Training Accuracy:  0.622516\n",
      "Epoch:  180  Training Loss:  208.013  Training Accuracy:  0.624162\n",
      "Epoch:  181  Training Loss:  208.072  Training Accuracy:  0.62522\n",
      "Epoch:  182  Training Loss:  208.099  Training Accuracy:  0.62669\n",
      "Epoch:  183  Training Loss:  208.105  Training Accuracy:  0.627454\n",
      "Epoch:  184  Training Loss:  208.114  Training Accuracy:  0.628689\n",
      "Epoch:  185  Training Loss:  208.138  Training Accuracy:  0.630218\n",
      "Epoch:  186  Training Loss:  208.17  Training Accuracy:  0.631393\n",
      "Epoch:  187  Training Loss:  208.201  Training Accuracy:  0.631922\n",
      "Epoch:  188  Training Loss:  208.217  Training Accuracy:  0.633039\n",
      "Epoch:  189  Training Loss:  208.241  Training Accuracy:  0.63398\n",
      "Epoch:  190  Training Loss:  208.219  Training Accuracy:  0.635273\n",
      "Epoch:  191  Training Loss:  208.277  Training Accuracy:  0.63639\n",
      "Epoch:  192  Training Loss:  208.253  Training Accuracy:  0.63739\n",
      "Epoch:  193  Training Loss:  208.299  Training Accuracy:  0.638272\n",
      "Epoch:  194  Training Loss:  208.31  Training Accuracy:  0.639095\n",
      "Epoch:  195  Training Loss:  208.331  Training Accuracy:  0.639977\n",
      "Epoch:  196  Training Loss:  208.343  Training Accuracy:  0.640741\n",
      "Epoch:  197  Training Loss:  208.374  Training Accuracy:  0.641799\n",
      "Epoch:  198  Training Loss:  208.363  Training Accuracy:  0.642563\n",
      "Epoch:  199  Training Loss:  208.402  Training Accuracy:  0.643563\n",
      "Epoch:  200  Training Loss:  208.399  Training Accuracy:  0.644503\n",
      "Epoch:  201  Training Loss:  208.419  Training Accuracy:  0.645503\n",
      "Epoch:  202  Training Loss:  208.406  Training Accuracy:  0.646679\n",
      "Epoch:  203  Training Loss:  208.413  Training Accuracy:  0.64756\n",
      "Epoch:  204  Training Loss:  208.377  Training Accuracy:  0.648442\n",
      "Epoch:  205  Training Loss:  208.394  Training Accuracy:  0.649736\n",
      "Epoch:  206  Training Loss:  208.37  Training Accuracy:  0.650853\n",
      "Epoch:  207  Training Loss:  208.37  Training Accuracy:  0.651617\n",
      "Epoch:  208  Training Loss:  208.323  Training Accuracy:  0.652499\n",
      "Epoch:  209  Training Loss:  208.324  Training Accuracy:  0.653792\n",
      "Epoch:  210  Training Loss:  208.28  Training Accuracy:  0.655027\n",
      "Epoch:  211  Training Loss:  208.264  Training Accuracy:  0.656143\n",
      "Epoch:  212  Training Loss:  208.235  Training Accuracy:  0.657261\n",
      "Epoch:  213  Training Loss:  208.199  Training Accuracy:  0.657966\n",
      "Epoch:  214  Training Loss:  208.149  Training Accuracy:  0.658789\n",
      "Epoch:  215  Training Loss:  208.111  Training Accuracy:  0.659847\n",
      "Epoch:  216  Training Loss:  208.093  Training Accuracy:  0.660729\n",
      "Epoch:  217  Training Loss:  208.04  Training Accuracy:  0.661317\n",
      "Epoch:  218  Training Loss:  208.002  Training Accuracy:  0.662493\n",
      "Epoch:  219  Training Loss:  207.967  Training Accuracy:  0.663316\n",
      "Epoch:  220  Training Loss:  207.886  Training Accuracy:  0.663962\n",
      "Epoch:  221  Training Loss:  207.856  Training Accuracy:  0.665256\n",
      "Epoch:  222  Training Loss:  207.798  Training Accuracy:  0.666138\n",
      "Epoch:  223  Training Loss:  207.736  Training Accuracy:  0.666843\n",
      "Epoch:  224  Training Loss:  207.721  Training Accuracy:  0.66796\n",
      "Epoch:  225  Training Loss:  207.661  Training Accuracy:  0.668901\n",
      "Epoch:  226  Training Loss:  207.618  Training Accuracy:  0.66943\n",
      "Epoch:  227  Training Loss:  207.559  Training Accuracy:  0.67037\n",
      "Epoch:  228  Training Loss:  207.475  Training Accuracy:  0.671546\n",
      "Epoch:  229  Training Loss:  207.446  Training Accuracy:  0.672546\n",
      "Epoch:  230  Training Loss:  207.382  Training Accuracy:  0.673486\n",
      "Epoch:  231  Training Loss:  207.334  Training Accuracy:  0.674251\n",
      "Epoch:  232  Training Loss:  207.263  Training Accuracy:  0.674897\n",
      "Epoch:  233  Training Loss:  207.211  Training Accuracy:  0.676191\n",
      "Epoch:  234  Training Loss:  207.128  Training Accuracy:  0.677014\n",
      "Epoch:  235  Training Loss:  207.082  Training Accuracy:  0.677484\n",
      "Epoch:  236  Training Loss:  207.006  Training Accuracy:  0.678248\n",
      "Epoch:  237  Training Loss:  206.911  Training Accuracy:  0.679071\n",
      "Epoch:  238  Training Loss:  206.849  Training Accuracy:  0.679483\n",
      "Epoch:  239  Training Loss:  206.783  Training Accuracy:  0.680188\n",
      "Epoch:  240  Training Loss:  206.71  Training Accuracy:  0.6806\n",
      "Epoch:  241  Training Loss:  206.633  Training Accuracy:  0.681482\n",
      "Epoch:  242  Training Loss:  206.55  Training Accuracy:  0.682363\n",
      "Epoch:  243  Training Loss:  206.497  Training Accuracy:  0.682834\n",
      "Epoch:  244  Training Loss:  206.441  Training Accuracy:  0.68401\n",
      "Epoch:  245  Training Loss:  206.332  Training Accuracy:  0.684891\n",
      "Epoch:  246  Training Loss:  206.277  Training Accuracy:  0.686008\n",
      "Epoch:  247  Training Loss:  206.177  Training Accuracy:  0.686714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  206.099  Training Accuracy:  0.687243\n",
      "Epoch:  249  Training Loss:  206.048  Training Accuracy:  0.688007\n",
      "Epoch:  250  Training Loss:  205.976  Training Accuracy:  0.688478\n",
      "Epoch:  251  Training Loss:  205.893  Training Accuracy:  0.689183\n",
      "Epoch:  252  Training Loss:  205.799  Training Accuracy:  0.689418\n",
      "Epoch:  253  Training Loss:  205.736  Training Accuracy:  0.690417\n",
      "Epoch:  254  Training Loss:  205.672  Training Accuracy:  0.691241\n",
      "Epoch:  255  Training Loss:  205.56  Training Accuracy:  0.692064\n",
      "Epoch:  256  Training Loss:  205.502  Training Accuracy:  0.693004\n",
      "Epoch:  257  Training Loss:  205.388  Training Accuracy:  0.694063\n",
      "Epoch:  258  Training Loss:  205.312  Training Accuracy:  0.694592\n",
      "Epoch:  259  Training Loss:  205.2  Training Accuracy:  0.695121\n",
      "Epoch:  260  Training Loss:  205.109  Training Accuracy:  0.695709\n",
      "Epoch:  261  Training Loss:  204.994  Training Accuracy:  0.696414\n",
      "Epoch:  262  Training Loss:  204.901  Training Accuracy:  0.697178\n",
      "Epoch:  263  Training Loss:  204.743  Training Accuracy:  0.69806\n",
      "Epoch:  264  Training Loss:  204.653  Training Accuracy:  0.698766\n",
      "Epoch:  265  Training Loss:  204.527  Training Accuracy:  0.699589\n",
      "Epoch:  266  Training Loss:  204.393  Training Accuracy:  0.700176\n",
      "Epoch:  267  Training Loss:  204.281  Training Accuracy:  0.700882\n",
      "Epoch:  268  Training Loss:  204.176  Training Accuracy:  0.701235\n",
      "Epoch:  269  Training Loss:  204.043  Training Accuracy:  0.701999\n",
      "Epoch:  270  Training Loss:  203.923  Training Accuracy:  0.702528\n",
      "Epoch:  271  Training Loss:  203.773  Training Accuracy:  0.703116\n",
      "Epoch:  272  Training Loss:  203.638  Training Accuracy:  0.703645\n",
      "Epoch:  273  Training Loss:  203.45  Training Accuracy:  0.704409\n",
      "Epoch:  274  Training Loss:  203.316  Training Accuracy:  0.705585\n",
      "Epoch:  275  Training Loss:  203.185  Training Accuracy:  0.705997\n",
      "Epoch:  276  Training Loss:  203.037  Training Accuracy:  0.70682\n",
      "Epoch:  277  Training Loss:  202.865  Training Accuracy:  0.707466\n",
      "Epoch:  278  Training Loss:  202.737  Training Accuracy:  0.708231\n",
      "Epoch:  279  Training Loss:  202.588  Training Accuracy:  0.708818\n",
      "Epoch:  280  Training Loss:  202.412  Training Accuracy:  0.709877\n",
      "Epoch:  281  Training Loss:  202.319  Training Accuracy:  0.710465\n",
      "Epoch:  282  Training Loss:  202.128  Training Accuracy:  0.710759\n",
      "Epoch:  283  Training Loss:  201.99  Training Accuracy:  0.711346\n",
      "Epoch:  284  Training Loss:  201.832  Training Accuracy:  0.712405\n",
      "Epoch:  285  Training Loss:  201.674  Training Accuracy:  0.713228\n",
      "Epoch:  286  Training Loss:  201.517  Training Accuracy:  0.713874\n",
      "Epoch:  287  Training Loss:  201.374  Training Accuracy:  0.714697\n",
      "Epoch:  288  Training Loss:  201.227  Training Accuracy:  0.715521\n",
      "Epoch:  289  Training Loss:  201.088  Training Accuracy:  0.716226\n",
      "Epoch:  290  Training Loss:  200.931  Training Accuracy:  0.716873\n",
      "Epoch:  291  Training Loss:  200.814  Training Accuracy:  0.71746\n",
      "Epoch:  292  Training Loss:  200.636  Training Accuracy:  0.71799\n",
      "Epoch:  293  Training Loss:  200.485  Training Accuracy:  0.718754\n",
      "Epoch:  294  Training Loss:  200.355  Training Accuracy:  0.719459\n",
      "Epoch:  295  Training Loss:  200.17  Training Accuracy:  0.720165\n",
      "Epoch:  296  Training Loss:  200.025  Training Accuracy:  0.720694\n",
      "Epoch:  297  Training Loss:  199.856  Training Accuracy:  0.721341\n",
      "Epoch:  298  Training Loss:  199.672  Training Accuracy:  0.721987\n",
      "Epoch:  299  Training Loss:  199.53  Training Accuracy:  0.722634\n",
      "Epoch:  300  Training Loss:  199.363  Training Accuracy:  0.723163\n",
      "Epoch:  301  Training Loss:  199.181  Training Accuracy:  0.724045\n",
      "Epoch:  302  Training Loss:  199.065  Training Accuracy:  0.724927\n",
      "Epoch:  303  Training Loss:  198.862  Training Accuracy:  0.725691\n",
      "Epoch:  304  Training Loss:  198.696  Training Accuracy:  0.726749\n",
      "Epoch:  305  Training Loss:  198.546  Training Accuracy:  0.727513\n",
      "Epoch:  306  Training Loss:  198.364  Training Accuracy:  0.727984\n",
      "Epoch:  307  Training Loss:  198.165  Training Accuracy:  0.728454\n",
      "Epoch:  308  Training Loss:  198.009  Training Accuracy:  0.728983\n",
      "Epoch:  309  Training Loss:  197.807  Training Accuracy:  0.729395\n",
      "Epoch:  310  Training Loss:  197.666  Training Accuracy:  0.729806\n",
      "Epoch:  311  Training Loss:  197.461  Training Accuracy:  0.730394\n",
      "Epoch:  312  Training Loss:  197.299  Training Accuracy:  0.730982\n",
      "Epoch:  313  Training Loss:  197.102  Training Accuracy:  0.731276\n",
      "Epoch:  314  Training Loss:  196.912  Training Accuracy:  0.732393\n",
      "Epoch:  315  Training Loss:  196.715  Training Accuracy:  0.73304\n",
      "Epoch:  316  Training Loss:  196.528  Training Accuracy:  0.733863\n",
      "Epoch:  317  Training Loss:  196.337  Training Accuracy:  0.734686\n",
      "Epoch:  318  Training Loss:  196.146  Training Accuracy:  0.735391\n",
      "Epoch:  319  Training Loss:  195.931  Training Accuracy:  0.736097\n",
      "Epoch:  320  Training Loss:  195.732  Training Accuracy:  0.736861\n",
      "Epoch:  321  Training Loss:  195.563  Training Accuracy:  0.737155\n",
      "Epoch:  322  Training Loss:  195.325  Training Accuracy:  0.737625\n",
      "Epoch:  323  Training Loss:  195.155  Training Accuracy:  0.737978\n",
      "Epoch:  324  Training Loss:  194.921  Training Accuracy:  0.738448\n",
      "Epoch:  325  Training Loss:  194.734  Training Accuracy:  0.738919\n",
      "Epoch:  326  Training Loss:  194.533  Training Accuracy:  0.739448\n",
      "Epoch:  327  Training Loss:  194.343  Training Accuracy:  0.740094\n",
      "Epoch:  328  Training Loss:  194.152  Training Accuracy:  0.740859\n",
      "Epoch:  329  Training Loss:  193.974  Training Accuracy:  0.74127\n",
      "Epoch:  330  Training Loss:  193.735  Training Accuracy:  0.741858\n",
      "Epoch:  331  Training Loss:  193.585  Training Accuracy:  0.742269\n",
      "Epoch:  332  Training Loss:  193.346  Training Accuracy:  0.742799\n",
      "Epoch:  333  Training Loss:  193.135  Training Accuracy:  0.743445\n",
      "Epoch:  334  Training Loss:  192.924  Training Accuracy:  0.744092\n",
      "Epoch:  335  Training Loss:  192.732  Training Accuracy:  0.744503\n",
      "Epoch:  336  Training Loss:  192.489  Training Accuracy:  0.745268\n",
      "Epoch:  337  Training Loss:  192.292  Training Accuracy:  0.745679\n",
      "Epoch:  338  Training Loss:  192.047  Training Accuracy:  0.746208\n",
      "Epoch:  339  Training Loss:  191.873  Training Accuracy:  0.746385\n",
      "Epoch:  340  Training Loss:  191.641  Training Accuracy:  0.746737\n",
      "Epoch:  341  Training Loss:  191.462  Training Accuracy:  0.747384\n",
      "Epoch:  342  Training Loss:  191.232  Training Accuracy:  0.747854\n",
      "Epoch:  343  Training Loss:  191.027  Training Accuracy:  0.748207\n",
      "Epoch:  344  Training Loss:  190.783  Training Accuracy:  0.748971\n",
      "Epoch:  345  Training Loss:  190.557  Training Accuracy:  0.749618\n",
      "Epoch:  346  Training Loss:  190.344  Training Accuracy:  0.750265\n",
      "Epoch:  347  Training Loss:  190.163  Training Accuracy:  0.751205\n",
      "Epoch:  348  Training Loss:  189.93  Training Accuracy:  0.751793\n",
      "Epoch:  349  Training Loss:  189.685  Training Accuracy:  0.752264\n",
      "Epoch:  350  Training Loss:  189.435  Training Accuracy:  0.752675\n",
      "Epoch:  351  Training Loss:  189.223  Training Accuracy:  0.752969\n",
      "Epoch:  352  Training Loss:  189.021  Training Accuracy:  0.753439\n",
      "Epoch:  353  Training Loss:  188.742  Training Accuracy:  0.753675\n",
      "Epoch:  354  Training Loss:  188.506  Training Accuracy:  0.754086\n",
      "Epoch:  355  Training Loss:  188.328  Training Accuracy:  0.75485\n",
      "Epoch:  356  Training Loss:  188.094  Training Accuracy:  0.755321\n",
      "Epoch:  357  Training Loss:  187.866  Training Accuracy:  0.755791\n",
      "Epoch:  358  Training Loss:  187.653  Training Accuracy:  0.756438\n",
      "Epoch:  359  Training Loss:  187.486  Training Accuracy:  0.757084\n",
      "Epoch:  360  Training Loss:  187.209  Training Accuracy:  0.757319\n",
      "Epoch:  361  Training Loss:  187.0  Training Accuracy:  0.757966\n",
      "Epoch:  362  Training Loss:  186.765  Training Accuracy:  0.758554\n",
      "Epoch:  363  Training Loss:  186.553  Training Accuracy:  0.759024\n",
      "Epoch:  364  Training Loss:  186.329  Training Accuracy:  0.75926\n",
      "Epoch:  365  Training Loss:  186.131  Training Accuracy:  0.759789\n",
      "Epoch:  366  Training Loss:  185.886  Training Accuracy:  0.760435\n",
      "Epoch:  367  Training Loss:  185.674  Training Accuracy:  0.760906\n",
      "Epoch:  368  Training Loss:  185.422  Training Accuracy:  0.761141\n",
      "Epoch:  369  Training Loss:  185.236  Training Accuracy:  0.761787\n",
      "Epoch:  370  Training Loss:  184.953  Training Accuracy:  0.762552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  184.757  Training Accuracy:  0.762963\n",
      "Epoch:  372  Training Loss:  184.482  Training Accuracy:  0.763492\n",
      "Epoch:  373  Training Loss:  184.256  Training Accuracy:  0.764021\n",
      "Epoch:  374  Training Loss:  184.001  Training Accuracy:  0.764257\n",
      "Epoch:  375  Training Loss:  183.804  Training Accuracy:  0.764962\n",
      "Epoch:  376  Training Loss:  183.536  Training Accuracy:  0.765491\n",
      "Epoch:  377  Training Loss:  183.343  Training Accuracy:  0.76602\n",
      "Epoch:  378  Training Loss:  183.02  Training Accuracy:  0.766432\n",
      "Epoch:  379  Training Loss:  182.799  Training Accuracy:  0.767078\n",
      "Epoch:  380  Training Loss:  182.544  Training Accuracy:  0.767784\n",
      "Epoch:  381  Training Loss:  182.356  Training Accuracy:  0.768489\n",
      "Epoch:  382  Training Loss:  182.08  Training Accuracy:  0.768725\n",
      "Epoch:  383  Training Loss:  181.875  Training Accuracy:  0.769077\n",
      "Epoch:  384  Training Loss:  181.595  Training Accuracy:  0.769489\n",
      "Epoch:  385  Training Loss:  181.417  Training Accuracy:  0.770312\n",
      "Epoch:  386  Training Loss:  181.142  Training Accuracy:  0.771017\n",
      "Epoch:  387  Training Loss:  180.929  Training Accuracy:  0.771546\n",
      "Epoch:  388  Training Loss:  180.68  Training Accuracy:  0.771782\n",
      "Epoch:  389  Training Loss:  180.468  Training Accuracy:  0.772193\n",
      "Epoch:  390  Training Loss:  180.159  Training Accuracy:  0.772428\n",
      "Epoch:  391  Training Loss:  179.975  Training Accuracy:  0.77284\n",
      "Epoch:  392  Training Loss:  179.687  Training Accuracy:  0.773604\n",
      "Epoch:  393  Training Loss:  179.457  Training Accuracy:  0.774309\n",
      "Epoch:  394  Training Loss:  179.244  Training Accuracy:  0.774545\n",
      "Epoch:  395  Training Loss:  178.993  Training Accuracy:  0.775074\n",
      "Epoch:  396  Training Loss:  178.747  Training Accuracy:  0.775485\n",
      "Epoch:  397  Training Loss:  178.503  Training Accuracy:  0.776014\n",
      "Epoch:  398  Training Loss:  178.269  Training Accuracy:  0.776426\n",
      "Epoch:  399  Training Loss:  178.054  Training Accuracy:  0.776661\n",
      "Epoch:  400  Training Loss:  177.824  Training Accuracy:  0.776955\n",
      "Epoch:  401  Training Loss:  177.592  Training Accuracy:  0.777366\n",
      "Epoch:  402  Training Loss:  177.355  Training Accuracy:  0.777719\n",
      "Epoch:  403  Training Loss:  177.093  Training Accuracy:  0.778307\n",
      "Epoch:  404  Training Loss:  176.842  Training Accuracy:  0.778954\n",
      "Epoch:  405  Training Loss:  176.597  Training Accuracy:  0.779483\n",
      "Epoch:  406  Training Loss:  176.342  Training Accuracy:  0.779953\n",
      "Epoch:  407  Training Loss:  176.107  Training Accuracy:  0.780365\n",
      "Epoch:  408  Training Loss:  175.846  Training Accuracy:  0.780659\n",
      "Epoch:  409  Training Loss:  175.692  Training Accuracy:  0.781011\n",
      "Epoch:  410  Training Loss:  175.341  Training Accuracy:  0.781247\n",
      "Epoch:  411  Training Loss:  175.169  Training Accuracy:  0.782011\n",
      "Epoch:  412  Training Loss:  174.892  Training Accuracy:  0.782305\n",
      "Epoch:  413  Training Loss:  174.683  Training Accuracy:  0.782775\n",
      "Epoch:  414  Training Loss:  174.462  Training Accuracy:  0.783128\n",
      "Epoch:  415  Training Loss:  174.198  Training Accuracy:  0.783892\n",
      "Epoch:  416  Training Loss:  174.01  Training Accuracy:  0.784362\n",
      "Epoch:  417  Training Loss:  173.765  Training Accuracy:  0.784891\n",
      "Epoch:  418  Training Loss:  173.53  Training Accuracy:  0.785185\n",
      "Epoch:  419  Training Loss:  173.24  Training Accuracy:  0.785597\n",
      "Epoch:  420  Training Loss:  173.02  Training Accuracy:  0.786009\n",
      "Epoch:  421  Training Loss:  172.788  Training Accuracy:  0.786655\n",
      "Epoch:  422  Training Loss:  172.508  Training Accuracy:  0.787243\n",
      "Epoch:  423  Training Loss:  172.3  Training Accuracy:  0.787772\n",
      "Epoch:  424  Training Loss:  172.062  Training Accuracy:  0.788536\n",
      "Epoch:  425  Training Loss:  171.799  Training Accuracy:  0.789183\n",
      "Epoch:  426  Training Loss:  171.585  Training Accuracy:  0.78983\n",
      "Epoch:  427  Training Loss:  171.324  Training Accuracy:  0.7903\n",
      "Epoch:  428  Training Loss:  171.081  Training Accuracy:  0.790829\n",
      "Epoch:  429  Training Loss:  170.881  Training Accuracy:  0.791182\n",
      "Epoch:  430  Training Loss:  170.658  Training Accuracy:  0.791887\n",
      "Epoch:  431  Training Loss:  170.355  Training Accuracy:  0.792417\n",
      "Epoch:  432  Training Loss:  170.127  Training Accuracy:  0.792652\n",
      "Epoch:  433  Training Loss:  169.872  Training Accuracy:  0.793063\n",
      "Epoch:  434  Training Loss:  169.609  Training Accuracy:  0.793416\n",
      "Epoch:  435  Training Loss:  169.387  Training Accuracy:  0.794004\n",
      "Epoch:  436  Training Loss:  169.135  Training Accuracy:  0.794357\n",
      "Epoch:  437  Training Loss:  168.884  Training Accuracy:  0.79465\n",
      "Epoch:  438  Training Loss:  168.696  Training Accuracy:  0.79518\n",
      "Epoch:  439  Training Loss:  168.412  Training Accuracy:  0.795415\n",
      "Epoch:  440  Training Loss:  168.123  Training Accuracy:  0.796061\n",
      "Epoch:  441  Training Loss:  167.911  Training Accuracy:  0.796355\n",
      "Epoch:  442  Training Loss:  167.67  Training Accuracy:  0.796708\n",
      "Epoch:  443  Training Loss:  167.456  Training Accuracy:  0.79712\n",
      "Epoch:  444  Training Loss:  167.158  Training Accuracy:  0.797531\n",
      "Epoch:  445  Training Loss:  167.001  Training Accuracy:  0.79806\n",
      "Epoch:  446  Training Loss:  166.739  Training Accuracy:  0.798354\n",
      "Epoch:  447  Training Loss:  166.499  Training Accuracy:  0.798766\n",
      "Epoch:  448  Training Loss:  166.187  Training Accuracy:  0.798942\n",
      "Epoch:  449  Training Loss:  166.005  Training Accuracy:  0.799001\n",
      "Epoch:  450  Training Loss:  165.713  Training Accuracy:  0.799412\n",
      "Epoch:  451  Training Loss:  165.484  Training Accuracy:  0.800118\n",
      "Epoch:  452  Training Loss:  165.219  Training Accuracy:  0.800353\n",
      "Epoch:  453  Training Loss:  164.958  Training Accuracy:  0.800882\n",
      "Epoch:  454  Training Loss:  164.718  Training Accuracy:  0.801117\n",
      "Epoch:  455  Training Loss:  164.512  Training Accuracy:  0.801176\n",
      "Epoch:  456  Training Loss:  164.22  Training Accuracy:  0.80147\n",
      "Epoch:  457  Training Loss:  164.021  Training Accuracy:  0.802234\n",
      "Epoch:  458  Training Loss:  163.712  Training Accuracy:  0.802587\n",
      "Epoch:  459  Training Loss:  163.52  Training Accuracy:  0.80294\n",
      "Epoch:  460  Training Loss:  163.24  Training Accuracy:  0.803234\n",
      "Epoch:  461  Training Loss:  163.016  Training Accuracy:  0.80388\n",
      "Epoch:  462  Training Loss:  162.757  Training Accuracy:  0.803998\n",
      "Epoch:  463  Training Loss:  162.541  Training Accuracy:  0.80441\n",
      "Epoch:  464  Training Loss:  162.277  Training Accuracy:  0.804939\n",
      "Epoch:  465  Training Loss:  162.026  Training Accuracy:  0.805115\n",
      "Epoch:  466  Training Loss:  161.778  Training Accuracy:  0.805409\n",
      "Epoch:  467  Training Loss:  161.607  Training Accuracy:  0.805644\n",
      "Epoch:  468  Training Loss:  161.312  Training Accuracy:  0.805879\n",
      "Epoch:  469  Training Loss:  161.102  Training Accuracy:  0.806349\n",
      "Epoch:  470  Training Loss:  160.837  Training Accuracy:  0.806526\n",
      "Epoch:  471  Training Loss:  160.652  Training Accuracy:  0.806937\n",
      "Epoch:  472  Training Loss:  160.377  Training Accuracy:  0.807408\n",
      "Epoch:  473  Training Loss:  160.175  Training Accuracy:  0.80776\n",
      "Epoch:  474  Training Loss:  159.942  Training Accuracy:  0.808054\n",
      "Epoch:  475  Training Loss:  159.703  Training Accuracy:  0.808583\n",
      "Epoch:  476  Training Loss:  159.459  Training Accuracy:  0.808877\n",
      "Epoch:  477  Training Loss:  159.241  Training Accuracy:  0.80923\n",
      "Epoch:  478  Training Loss:  159.029  Training Accuracy:  0.809642\n",
      "Epoch:  479  Training Loss:  158.787  Training Accuracy:  0.809818\n",
      "Epoch:  480  Training Loss:  158.559  Training Accuracy:  0.810524\n",
      "Epoch:  481  Training Loss:  158.393  Training Accuracy:  0.811111\n",
      "Epoch:  482  Training Loss:  158.069  Training Accuracy:  0.811347\n",
      "Epoch:  483  Training Loss:  157.881  Training Accuracy:  0.811699\n",
      "Epoch:  484  Training Loss:  157.635  Training Accuracy:  0.811993\n",
      "Epoch:  485  Training Loss:  157.405  Training Accuracy:  0.812405\n",
      "Epoch:  486  Training Loss:  157.153  Training Accuracy:  0.81264\n",
      "Epoch:  487  Training Loss:  156.94  Training Accuracy:  0.812875\n",
      "Epoch:  488  Training Loss:  156.687  Training Accuracy:  0.813228\n",
      "Epoch:  489  Training Loss:  156.513  Training Accuracy:  0.813639\n",
      "Epoch:  490  Training Loss:  156.192  Training Accuracy:  0.813816\n",
      "Epoch:  491  Training Loss:  156.015  Training Accuracy:  0.814286\n",
      "Epoch:  492  Training Loss:  155.768  Training Accuracy:  0.814521\n",
      "Epoch:  493  Training Loss:  155.552  Training Accuracy:  0.814756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  155.31  Training Accuracy:  0.815109\n",
      "Epoch:  495  Training Loss:  155.143  Training Accuracy:  0.815697\n",
      "Epoch:  496  Training Loss:  154.83  Training Accuracy:  0.816167\n",
      "Epoch:  497  Training Loss:  154.632  Training Accuracy:  0.816638\n",
      "Epoch:  498  Training Loss:  154.376  Training Accuracy:  0.816814\n",
      "Epoch:  499  Training Loss:  154.157  Training Accuracy:  0.816931\n",
      "Epoch:  500  Training Loss:  153.895  Training Accuracy:  0.817343\n",
      "Epoch:  501  Training Loss:  153.693  Training Accuracy:  0.817402\n",
      "Epoch:  502  Training Loss:  153.425  Training Accuracy:  0.817755\n",
      "Epoch:  503  Training Loss:  153.206  Training Accuracy:  0.81799\n",
      "Epoch:  504  Training Loss:  152.979  Training Accuracy:  0.81846\n",
      "Epoch:  505  Training Loss:  152.739  Training Accuracy:  0.818872\n",
      "Epoch:  506  Training Loss:  152.491  Training Accuracy:  0.819283\n",
      "Epoch:  507  Training Loss:  152.279  Training Accuracy:  0.819636\n",
      "Epoch:  508  Training Loss:  152.031  Training Accuracy:  0.819989\n",
      "Epoch:  509  Training Loss:  151.805  Training Accuracy:  0.820224\n",
      "Epoch:  510  Training Loss:  151.594  Training Accuracy:  0.820694\n",
      "Epoch:  511  Training Loss:  151.349  Training Accuracy:  0.821106\n",
      "Epoch:  512  Training Loss:  151.076  Training Accuracy:  0.821635\n",
      "Epoch:  513  Training Loss:  150.882  Training Accuracy:  0.821752\n",
      "Epoch:  514  Training Loss:  150.613  Training Accuracy:  0.82187\n",
      "Epoch:  515  Training Loss:  150.383  Training Accuracy:  0.822164\n",
      "Epoch:  516  Training Loss:  150.169  Training Accuracy:  0.822517\n",
      "Epoch:  517  Training Loss:  149.908  Training Accuracy:  0.823046\n",
      "Epoch:  518  Training Loss:  149.655  Training Accuracy:  0.823398\n",
      "Epoch:  519  Training Loss:  149.449  Training Accuracy:  0.823692\n",
      "Epoch:  520  Training Loss:  149.196  Training Accuracy:  0.823986\n",
      "Epoch:  521  Training Loss:  148.974  Training Accuracy:  0.824221\n",
      "Epoch:  522  Training Loss:  148.761  Training Accuracy:  0.824515\n",
      "Epoch:  523  Training Loss:  148.564  Training Accuracy:  0.824692\n",
      "Epoch:  524  Training Loss:  148.277  Training Accuracy:  0.824809\n",
      "Epoch:  525  Training Loss:  148.019  Training Accuracy:  0.825044\n",
      "Epoch:  526  Training Loss:  147.795  Training Accuracy:  0.825809\n",
      "Epoch:  527  Training Loss:  147.555  Training Accuracy:  0.826044\n",
      "Epoch:  528  Training Loss:  147.328  Training Accuracy:  0.826573\n",
      "Epoch:  529  Training Loss:  147.136  Training Accuracy:  0.826749\n",
      "Epoch:  530  Training Loss:  146.872  Training Accuracy:  0.827043\n",
      "Epoch:  531  Training Loss:  146.63  Training Accuracy:  0.827161\n",
      "Epoch:  532  Training Loss:  146.427  Training Accuracy:  0.827572\n",
      "Epoch:  533  Training Loss:  146.182  Training Accuracy:  0.827925\n",
      "Epoch:  534  Training Loss:  145.977  Training Accuracy:  0.828219\n",
      "Epoch:  535  Training Loss:  145.767  Training Accuracy:  0.828631\n",
      "Epoch:  536  Training Loss:  145.512  Training Accuracy:  0.829218\n",
      "Epoch:  537  Training Loss:  145.275  Training Accuracy:  0.82963\n",
      "Epoch:  538  Training Loss:  145.035  Training Accuracy:  0.830041\n",
      "Epoch:  539  Training Loss:  144.829  Training Accuracy:  0.830571\n",
      "Epoch:  540  Training Loss:  144.608  Training Accuracy:  0.830864\n",
      "Epoch:  541  Training Loss:  144.421  Training Accuracy:  0.830982\n",
      "Epoch:  542  Training Loss:  144.135  Training Accuracy:  0.831394\n",
      "Epoch:  543  Training Loss:  143.928  Training Accuracy:  0.83157\n",
      "Epoch:  544  Training Loss:  143.687  Training Accuracy:  0.831629\n",
      "Epoch:  545  Training Loss:  143.474  Training Accuracy:  0.831805\n",
      "Epoch:  546  Training Loss:  143.252  Training Accuracy:  0.832158\n",
      "Epoch:  547  Training Loss:  143.037  Training Accuracy:  0.832334\n",
      "Epoch:  548  Training Loss:  142.773  Training Accuracy:  0.832334\n",
      "Epoch:  549  Training Loss:  142.581  Training Accuracy:  0.832569\n",
      "Epoch:  550  Training Loss:  142.332  Training Accuracy:  0.832863\n",
      "Epoch:  551  Training Loss:  142.089  Training Accuracy:  0.83304\n",
      "Epoch:  552  Training Loss:  141.898  Training Accuracy:  0.833157\n",
      "Epoch:  553  Training Loss:  141.649  Training Accuracy:  0.833451\n",
      "Epoch:  554  Training Loss:  141.402  Training Accuracy:  0.83398\n",
      "Epoch:  555  Training Loss:  141.174  Training Accuracy:  0.834274\n",
      "Epoch:  556  Training Loss:  140.957  Training Accuracy:  0.834686\n",
      "Epoch:  557  Training Loss:  140.716  Training Accuracy:  0.834745\n",
      "Epoch:  558  Training Loss:  140.471  Training Accuracy:  0.835156\n",
      "Epoch:  559  Training Loss:  140.237  Training Accuracy:  0.835156\n",
      "Epoch:  560  Training Loss:  140.042  Training Accuracy:  0.83545\n",
      "Epoch:  561  Training Loss:  139.827  Training Accuracy:  0.835979\n",
      "Epoch:  562  Training Loss:  139.577  Training Accuracy:  0.836391\n",
      "Epoch:  563  Training Loss:  139.389  Training Accuracy:  0.836979\n",
      "Epoch:  564  Training Loss:  139.131  Training Accuracy:  0.837214\n",
      "Epoch:  565  Training Loss:  138.947  Training Accuracy:  0.837566\n",
      "Epoch:  566  Training Loss:  138.697  Training Accuracy:  0.83786\n",
      "Epoch:  567  Training Loss:  138.471  Training Accuracy:  0.837919\n",
      "Epoch:  568  Training Loss:  138.264  Training Accuracy:  0.838272\n",
      "Epoch:  569  Training Loss:  138.059  Training Accuracy:  0.838625\n",
      "Epoch:  570  Training Loss:  137.828  Training Accuracy:  0.838801\n",
      "Epoch:  571  Training Loss:  137.638  Training Accuracy:  0.838977\n",
      "Epoch:  572  Training Loss:  137.388  Training Accuracy:  0.839389\n",
      "Epoch:  573  Training Loss:  137.186  Training Accuracy:  0.839624\n",
      "Epoch:  574  Training Loss:  136.983  Training Accuracy:  0.839977\n",
      "Epoch:  575  Training Loss:  136.73  Training Accuracy:  0.840565\n",
      "Epoch:  576  Training Loss:  136.513  Training Accuracy:  0.840741\n",
      "Epoch:  577  Training Loss:  136.29  Training Accuracy:  0.841153\n",
      "Epoch:  578  Training Loss:  136.077  Training Accuracy:  0.841505\n",
      "Epoch:  579  Training Loss:  135.907  Training Accuracy:  0.841682\n",
      "Epoch:  580  Training Loss:  135.639  Training Accuracy:  0.841917\n",
      "Epoch:  581  Training Loss:  135.455  Training Accuracy:  0.842035\n",
      "Epoch:  582  Training Loss:  135.202  Training Accuracy:  0.842328\n",
      "Epoch:  583  Training Loss:  134.984  Training Accuracy:  0.842857\n",
      "Epoch:  584  Training Loss:  134.738  Training Accuracy:  0.84321\n",
      "Epoch:  585  Training Loss:  134.545  Training Accuracy:  0.843387\n",
      "Epoch:  586  Training Loss:  134.314  Training Accuracy:  0.843681\n",
      "Epoch:  587  Training Loss:  134.111  Training Accuracy:  0.84421\n",
      "Epoch:  588  Training Loss:  133.856  Training Accuracy:  0.844562\n",
      "Epoch:  589  Training Loss:  133.594  Training Accuracy:  0.844856\n",
      "Epoch:  590  Training Loss:  133.397  Training Accuracy:  0.845033\n",
      "Epoch:  591  Training Loss:  133.188  Training Accuracy:  0.845444\n",
      "Epoch:  592  Training Loss:  132.975  Training Accuracy:  0.845621\n",
      "Epoch:  593  Training Loss:  132.752  Training Accuracy:  0.845915\n",
      "Epoch:  594  Training Loss:  132.571  Training Accuracy:  0.846326\n",
      "Epoch:  595  Training Loss:  132.325  Training Accuracy:  0.846502\n",
      "Epoch:  596  Training Loss:  132.058  Training Accuracy:  0.846796\n",
      "Epoch:  597  Training Loss:  131.895  Training Accuracy:  0.846973\n",
      "Epoch:  598  Training Loss:  131.65  Training Accuracy:  0.847384\n",
      "Epoch:  599  Training Loss:  131.435  Training Accuracy:  0.847502\n",
      "Epoch:  600  Training Loss:  131.182  Training Accuracy:  0.847678\n",
      "Epoch:  601  Training Loss:  131.038  Training Accuracy:  0.847796\n",
      "Epoch:  602  Training Loss:  130.764  Training Accuracy:  0.84809\n",
      "Epoch:  603  Training Loss:  130.58  Training Accuracy:  0.848325\n",
      "Epoch:  604  Training Loss:  130.384  Training Accuracy:  0.848795\n",
      "Epoch:  605  Training Loss:  130.108  Training Accuracy:  0.849148\n",
      "Epoch:  606  Training Loss:  129.896  Training Accuracy:  0.849265\n",
      "Epoch:  607  Training Loss:  129.731  Training Accuracy:  0.849677\n",
      "Epoch:  608  Training Loss:  129.465  Training Accuracy:  0.849853\n",
      "Epoch:  609  Training Loss:  129.325  Training Accuracy:  0.850089\n",
      "Epoch:  610  Training Loss:  129.101  Training Accuracy:  0.850324\n",
      "Epoch:  611  Training Loss:  128.862  Training Accuracy:  0.850618\n",
      "Epoch:  612  Training Loss:  128.629  Training Accuracy:  0.85097\n",
      "Epoch:  613  Training Loss:  128.417  Training Accuracy:  0.851206\n",
      "Epoch:  614  Training Loss:  128.211  Training Accuracy:  0.851206\n",
      "Epoch:  615  Training Loss:  128.028  Training Accuracy:  0.851382\n",
      "Epoch:  616  Training Loss:  127.749  Training Accuracy:  0.852205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  127.543  Training Accuracy:  0.852675\n",
      "Epoch:  618  Training Loss:  127.32  Training Accuracy:  0.853146\n",
      "Epoch:  619  Training Loss:  127.2  Training Accuracy:  0.853381\n",
      "Epoch:  620  Training Loss:  126.973  Training Accuracy:  0.853733\n",
      "Epoch:  621  Training Loss:  126.698  Training Accuracy:  0.854027\n",
      "Epoch:  622  Training Loss:  126.493  Training Accuracy:  0.85438\n",
      "Epoch:  623  Training Loss:  126.299  Training Accuracy:  0.854557\n",
      "Epoch:  624  Training Loss:  126.075  Training Accuracy:  0.854909\n",
      "Epoch:  625  Training Loss:  125.951  Training Accuracy:  0.855262\n",
      "Epoch:  626  Training Loss:  125.69  Training Accuracy:  0.855438\n",
      "Epoch:  627  Training Loss:  125.5  Training Accuracy:  0.855791\n",
      "Epoch:  628  Training Loss:  125.262  Training Accuracy:  0.855909\n",
      "Epoch:  629  Training Loss:  125.065  Training Accuracy:  0.856085\n",
      "Epoch:  630  Training Loss:  124.879  Training Accuracy:  0.856261\n",
      "Epoch:  631  Training Loss:  124.696  Training Accuracy:  0.856614\n",
      "Epoch:  632  Training Loss:  124.462  Training Accuracy:  0.856849\n",
      "Epoch:  633  Training Loss:  124.273  Training Accuracy:  0.857143\n",
      "Epoch:  634  Training Loss:  124.071  Training Accuracy:  0.857437\n",
      "Epoch:  635  Training Loss:  123.847  Training Accuracy:  0.857614\n",
      "Epoch:  636  Training Loss:  123.654  Training Accuracy:  0.857966\n",
      "Epoch:  637  Training Loss:  123.479  Training Accuracy:  0.858143\n",
      "Epoch:  638  Training Loss:  123.233  Training Accuracy:  0.858495\n",
      "Epoch:  639  Training Loss:  122.993  Training Accuracy:  0.858672\n",
      "Epoch:  640  Training Loss:  122.79  Training Accuracy:  0.859083\n",
      "Epoch:  641  Training Loss:  122.613  Training Accuracy:  0.859495\n",
      "Epoch:  642  Training Loss:  122.396  Training Accuracy:  0.859789\n",
      "Epoch:  643  Training Loss:  122.226  Training Accuracy:  0.8602\n",
      "Epoch:  644  Training Loss:  121.981  Training Accuracy:  0.860553\n",
      "Epoch:  645  Training Loss:  121.761  Training Accuracy:  0.860612\n",
      "Epoch:  646  Training Loss:  121.524  Training Accuracy:  0.861023\n",
      "Epoch:  647  Training Loss:  121.347  Training Accuracy:  0.861611\n",
      "Epoch:  648  Training Loss:  121.088  Training Accuracy:  0.861846\n",
      "Epoch:  649  Training Loss:  120.947  Training Accuracy:  0.862082\n",
      "Epoch:  650  Training Loss:  120.649  Training Accuracy:  0.862434\n",
      "Epoch:  651  Training Loss:  120.5  Training Accuracy:  0.862611\n",
      "Epoch:  652  Training Loss:  120.241  Training Accuracy:  0.862905\n",
      "Epoch:  653  Training Loss:  120.111  Training Accuracy:  0.863316\n",
      "Epoch:  654  Training Loss:  119.875  Training Accuracy:  0.863551\n",
      "Epoch:  655  Training Loss:  119.692  Training Accuracy:  0.863845\n",
      "Epoch:  656  Training Loss:  119.422  Training Accuracy:  0.863963\n",
      "Epoch:  657  Training Loss:  119.218  Training Accuracy:  0.864374\n",
      "Epoch:  658  Training Loss:  118.997  Training Accuracy:  0.864374\n",
      "Epoch:  659  Training Loss:  118.779  Training Accuracy:  0.864668\n",
      "Epoch:  660  Training Loss:  118.556  Training Accuracy:  0.864845\n",
      "Epoch:  661  Training Loss:  118.361  Training Accuracy:  0.865491\n",
      "Epoch:  662  Training Loss:  118.116  Training Accuracy:  0.865609\n",
      "Epoch:  663  Training Loss:  117.947  Training Accuracy:  0.86602\n",
      "Epoch:  664  Training Loss:  117.707  Training Accuracy:  0.866197\n",
      "Epoch:  665  Training Loss:  117.532  Training Accuracy:  0.866138\n",
      "Epoch:  666  Training Loss:  117.302  Training Accuracy:  0.866314\n",
      "Epoch:  667  Training Loss:  117.118  Training Accuracy:  0.866491\n",
      "Epoch:  668  Training Loss:  116.884  Training Accuracy:  0.866608\n",
      "Epoch:  669  Training Loss:  116.704  Training Accuracy:  0.866843\n",
      "Epoch:  670  Training Loss:  116.624  Training Accuracy:  0.86702\n",
      "Epoch:  671  Training Loss:  116.275  Training Accuracy:  0.867196\n",
      "Epoch:  672  Training Loss:  116.099  Training Accuracy:  0.867608\n",
      "Epoch:  673  Training Loss:  115.903  Training Accuracy:  0.86796\n",
      "Epoch:  674  Training Loss:  115.731  Training Accuracy:  0.868607\n",
      "Epoch:  675  Training Loss:  115.489  Training Accuracy:  0.868666\n",
      "Epoch:  676  Training Loss:  115.284  Training Accuracy:  0.869077\n",
      "Epoch:  677  Training Loss:  115.106  Training Accuracy:  0.869371\n",
      "Epoch:  678  Training Loss:  114.914  Training Accuracy:  0.870018\n",
      "Epoch:  679  Training Loss:  114.649  Training Accuracy:  0.869842\n",
      "Epoch:  680  Training Loss:  114.548  Training Accuracy:  0.870371\n",
      "Epoch:  681  Training Loss:  114.308  Training Accuracy:  0.870606\n",
      "Epoch:  682  Training Loss:  114.088  Training Accuracy:  0.870959\n",
      "Epoch:  683  Training Loss:  113.885  Training Accuracy:  0.87137\n",
      "Epoch:  684  Training Loss:  113.685  Training Accuracy:  0.871547\n",
      "Epoch:  685  Training Loss:  113.483  Training Accuracy:  0.871664\n",
      "Epoch:  686  Training Loss:  113.278  Training Accuracy:  0.872076\n",
      "Epoch:  687  Training Loss:  113.04  Training Accuracy:  0.872193\n",
      "Epoch:  688  Training Loss:  112.833  Training Accuracy:  0.872076\n",
      "Epoch:  689  Training Loss:  112.585  Training Accuracy:  0.872487\n",
      "Epoch:  690  Training Loss:  112.394  Training Accuracy:  0.872899\n",
      "Epoch:  691  Training Loss:  112.176  Training Accuracy:  0.873016\n",
      "Epoch:  692  Training Loss:  111.962  Training Accuracy:  0.873369\n",
      "Epoch:  693  Training Loss:  111.754  Training Accuracy:  0.873663\n",
      "Epoch:  694  Training Loss:  111.531  Training Accuracy:  0.873898\n",
      "Epoch:  695  Training Loss:  111.315  Training Accuracy:  0.873898\n",
      "Epoch:  696  Training Loss:  111.147  Training Accuracy:  0.874133\n",
      "Epoch:  697  Training Loss:  110.924  Training Accuracy:  0.874251\n",
      "Epoch:  698  Training Loss:  110.7  Training Accuracy:  0.874545\n",
      "Epoch:  699  Training Loss:  110.483  Training Accuracy:  0.875015\n",
      "Epoch:  700  Training Loss:  110.237  Training Accuracy:  0.87478\n",
      "Epoch:  701  Training Loss:  110.077  Training Accuracy:  0.875427\n",
      "Epoch:  702  Training Loss:  109.842  Training Accuracy:  0.875603\n",
      "Epoch:  703  Training Loss:  109.643  Training Accuracy:  0.875956\n",
      "Epoch:  704  Training Loss:  109.494  Training Accuracy:  0.876191\n",
      "Epoch:  705  Training Loss:  109.24  Training Accuracy:  0.876367\n",
      "Epoch:  706  Training Loss:  109.053  Training Accuracy:  0.876544\n",
      "Epoch:  707  Training Loss:  108.861  Training Accuracy:  0.876544\n",
      "Epoch:  708  Training Loss:  108.622  Training Accuracy:  0.876837\n",
      "Epoch:  709  Training Loss:  108.403  Training Accuracy:  0.877073\n",
      "Epoch:  710  Training Loss:  108.212  Training Accuracy:  0.877484\n",
      "Epoch:  711  Training Loss:  107.985  Training Accuracy:  0.877425\n",
      "Epoch:  712  Training Loss:  107.772  Training Accuracy:  0.877837\n",
      "Epoch:  713  Training Loss:  107.575  Training Accuracy:  0.877778\n",
      "Epoch:  714  Training Loss:  107.355  Training Accuracy:  0.878072\n",
      "Epoch:  715  Training Loss:  107.104  Training Accuracy:  0.878778\n",
      "Epoch:  716  Training Loss:  106.92  Training Accuracy:  0.878836\n",
      "Epoch:  717  Training Loss:  106.678  Training Accuracy:  0.879365\n",
      "Epoch:  718  Training Loss:  106.477  Training Accuracy:  0.879601\n",
      "Epoch:  719  Training Loss:  106.289  Training Accuracy:  0.879836\n",
      "Epoch:  720  Training Loss:  106.071  Training Accuracy:  0.880012\n",
      "Epoch:  721  Training Loss:  105.851  Training Accuracy:  0.880189\n",
      "Epoch:  722  Training Loss:  105.675  Training Accuracy:  0.880482\n",
      "Epoch:  723  Training Loss:  105.421  Training Accuracy:  0.88107\n",
      "Epoch:  724  Training Loss:  105.272  Training Accuracy:  0.881306\n",
      "Epoch:  725  Training Loss:  105.021  Training Accuracy:  0.881541\n",
      "Epoch:  726  Training Loss:  104.81  Training Accuracy:  0.881835\n",
      "Epoch:  727  Training Loss:  104.598  Training Accuracy:  0.88207\n",
      "Epoch:  728  Training Loss:  104.359  Training Accuracy:  0.882364\n",
      "Epoch:  729  Training Loss:  104.131  Training Accuracy:  0.882599\n",
      "Epoch:  730  Training Loss:  103.94  Training Accuracy:  0.882775\n",
      "Epoch:  731  Training Loss:  103.704  Training Accuracy:  0.882952\n",
      "Epoch:  732  Training Loss:  103.521  Training Accuracy:  0.882952\n",
      "Epoch:  733  Training Loss:  103.27  Training Accuracy:  0.882952\n",
      "Epoch:  734  Training Loss:  103.114  Training Accuracy:  0.883304\n",
      "Epoch:  735  Training Loss:  102.918  Training Accuracy:  0.883363\n",
      "Epoch:  736  Training Loss:  102.678  Training Accuracy:  0.88354\n",
      "Epoch:  737  Training Loss:  102.485  Training Accuracy:  0.883833\n",
      "Epoch:  738  Training Loss:  102.292  Training Accuracy:  0.884363\n",
      "Epoch:  739  Training Loss:  102.005  Training Accuracy:  0.884539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  740  Training Loss:  101.836  Training Accuracy:  0.884715\n",
      "Epoch:  741  Training Loss:  101.676  Training Accuracy:  0.884951\n",
      "Epoch:  742  Training Loss:  101.399  Training Accuracy:  0.885068\n",
      "Epoch:  743  Training Loss:  101.219  Training Accuracy:  0.885186\n",
      "Epoch:  744  Training Loss:  101.003  Training Accuracy:  0.885362\n",
      "Epoch:  745  Training Loss:  100.856  Training Accuracy:  0.885715\n",
      "Epoch:  746  Training Loss:  100.578  Training Accuracy:  0.885891\n",
      "Epoch:  747  Training Loss:  100.397  Training Accuracy:  0.88642\n",
      "Epoch:  748  Training Loss:  100.188  Training Accuracy:  0.886479\n",
      "Epoch:  749  Training Loss:  99.9651  Training Accuracy:  0.886597\n",
      "Epoch:  750  Training Loss:  99.8347  Training Accuracy:  0.887008\n",
      "Epoch:  751  Training Loss:  99.5657  Training Accuracy:  0.887302\n",
      "Epoch:  752  Training Loss:  99.4173  Training Accuracy:  0.887361\n",
      "Epoch:  753  Training Loss:  99.1963  Training Accuracy:  0.887655\n",
      "Epoch:  754  Training Loss:  98.9886  Training Accuracy:  0.887772\n",
      "Epoch:  755  Training Loss:  98.7919  Training Accuracy:  0.888184\n",
      "Epoch:  756  Training Loss:  98.6259  Training Accuracy:  0.888301\n",
      "Epoch:  757  Training Loss:  98.4051  Training Accuracy:  0.888595\n",
      "Epoch:  758  Training Loss:  98.1944  Training Accuracy:  0.888713\n",
      "Epoch:  759  Training Loss:  98.0673  Training Accuracy:  0.888889\n",
      "Epoch:  760  Training Loss:  97.8066  Training Accuracy:  0.889066\n",
      "Epoch:  761  Training Loss:  97.5912  Training Accuracy:  0.889066\n",
      "Epoch:  762  Training Loss:  97.4552  Training Accuracy:  0.889125\n",
      "Epoch:  763  Training Loss:  97.2014  Training Accuracy:  0.889242\n",
      "Epoch:  764  Training Loss:  97.051  Training Accuracy:  0.889477\n",
      "Epoch:  765  Training Loss:  96.8762  Training Accuracy:  0.889536\n",
      "Epoch:  766  Training Loss:  96.6428  Training Accuracy:  0.889654\n",
      "Epoch:  767  Training Loss:  96.4576  Training Accuracy:  0.890065\n",
      "Epoch:  768  Training Loss:  96.1965  Training Accuracy:  0.890065\n",
      "Epoch:  769  Training Loss:  96.0551  Training Accuracy:  0.890242\n",
      "Epoch:  770  Training Loss:  95.9116  Training Accuracy:  0.890418\n",
      "Epoch:  771  Training Loss:  95.6639  Training Accuracy:  0.890712\n",
      "Epoch:  772  Training Loss:  95.4742  Training Accuracy:  0.890653\n",
      "Epoch:  773  Training Loss:  95.358  Training Accuracy:  0.890947\n",
      "Epoch:  774  Training Loss:  95.0998  Training Accuracy:  0.891123\n",
      "Epoch:  775  Training Loss:  94.9154  Training Accuracy:  0.891182\n",
      "Epoch:  776  Training Loss:  94.6919  Training Accuracy:  0.891535\n",
      "Epoch:  777  Training Loss:  94.5  Training Accuracy:  0.891476\n",
      "Epoch:  778  Training Loss:  94.2467  Training Accuracy:  0.891888\n",
      "Epoch:  779  Training Loss:  94.0911  Training Accuracy:  0.891946\n",
      "Epoch:  780  Training Loss:  93.8216  Training Accuracy:  0.89224\n",
      "Epoch:  781  Training Loss:  93.6655  Training Accuracy:  0.892475\n",
      "Epoch:  782  Training Loss:  93.486  Training Accuracy:  0.892652\n",
      "Epoch:  783  Training Loss:  93.3153  Training Accuracy:  0.892769\n",
      "Epoch:  784  Training Loss:  93.1109  Training Accuracy:  0.892946\n",
      "Epoch:  785  Training Loss:  92.8603  Training Accuracy:  0.893181\n",
      "Epoch:  786  Training Loss:  92.6957  Training Accuracy:  0.893769\n",
      "Epoch:  787  Training Loss:  92.5256  Training Accuracy:  0.894004\n",
      "Epoch:  788  Training Loss:  92.3143  Training Accuracy:  0.894298\n",
      "Epoch:  789  Training Loss:  92.1598  Training Accuracy:  0.894298\n",
      "Epoch:  790  Training Loss:  91.9353  Training Accuracy:  0.894592\n",
      "Epoch:  791  Training Loss:  91.7737  Training Accuracy:  0.894592\n",
      "Epoch:  792  Training Loss:  91.5344  Training Accuracy:  0.894945\n",
      "Epoch:  793  Training Loss:  91.3275  Training Accuracy:  0.895062\n",
      "Epoch:  794  Training Loss:  91.1573  Training Accuracy:  0.895239\n",
      "Epoch:  795  Training Loss:  90.9248  Training Accuracy:  0.895239\n",
      "Epoch:  796  Training Loss:  90.8284  Training Accuracy:  0.895474\n",
      "Epoch:  797  Training Loss:  90.5456  Training Accuracy:  0.895709\n",
      "Epoch:  798  Training Loss:  90.4646  Training Accuracy:  0.896238\n",
      "Epoch:  799  Training Loss:  90.1661  Training Accuracy:  0.896356\n",
      "Epoch:  800  Training Loss:  90.009  Training Accuracy:  0.896591\n",
      "Epoch:  801  Training Loss:  89.8721  Training Accuracy:  0.896767\n",
      "Epoch:  802  Training Loss:  89.6139  Training Accuracy:  0.896943\n",
      "Epoch:  803  Training Loss:  89.4594  Training Accuracy:  0.896943\n",
      "Epoch:  804  Training Loss:  89.2801  Training Accuracy:  0.897237\n",
      "Epoch:  805  Training Loss:  89.1111  Training Accuracy:  0.897296\n",
      "Epoch:  806  Training Loss:  88.9268  Training Accuracy:  0.897473\n",
      "Epoch:  807  Training Loss:  88.6904  Training Accuracy:  0.897708\n",
      "Epoch:  808  Training Loss:  88.5244  Training Accuracy:  0.897708\n",
      "Epoch:  809  Training Loss:  88.3198  Training Accuracy:  0.898178\n",
      "Epoch:  810  Training Loss:  88.1223  Training Accuracy:  0.89806\n",
      "Epoch:  811  Training Loss:  88.0082  Training Accuracy:  0.898296\n",
      "Epoch:  812  Training Loss:  87.7373  Training Accuracy:  0.898413\n",
      "Epoch:  813  Training Loss:  87.5413  Training Accuracy:  0.898531\n",
      "Epoch:  814  Training Loss:  87.3786  Training Accuracy:  0.898766\n",
      "Epoch:  815  Training Loss:  87.2168  Training Accuracy:  0.898884\n",
      "Epoch:  816  Training Loss:  86.9451  Training Accuracy:  0.89906\n",
      "Epoch:  817  Training Loss:  86.8442  Training Accuracy:  0.89906\n",
      "Epoch:  818  Training Loss:  86.6194  Training Accuracy:  0.899295\n",
      "Epoch:  819  Training Loss:  86.4814  Training Accuracy:  0.899471\n",
      "Epoch:  820  Training Loss:  86.215  Training Accuracy:  0.899765\n",
      "Epoch:  821  Training Loss:  86.0553  Training Accuracy:  0.9\n",
      "Epoch:  822  Training Loss:  85.8931  Training Accuracy:  0.900353\n",
      "Epoch:  823  Training Loss:  85.7296  Training Accuracy:  0.900588\n",
      "Epoch:  824  Training Loss:  85.5451  Training Accuracy:  0.900765\n",
      "Epoch:  825  Training Loss:  85.3042  Training Accuracy:  0.900824\n",
      "Epoch:  826  Training Loss:  85.0924  Training Accuracy:  0.901\n",
      "Epoch:  827  Training Loss:  84.965  Training Accuracy:  0.901176\n",
      "Epoch:  828  Training Loss:  84.7248  Training Accuracy:  0.901294\n",
      "Epoch:  829  Training Loss:  84.5891  Training Accuracy:  0.901411\n",
      "Epoch:  830  Training Loss:  84.4434  Training Accuracy:  0.90147\n",
      "Epoch:  831  Training Loss:  84.1958  Training Accuracy:  0.901646\n",
      "Epoch:  832  Training Loss:  83.9918  Training Accuracy:  0.901823\n",
      "Epoch:  833  Training Loss:  83.8708  Training Accuracy:  0.902117\n",
      "Epoch:  834  Training Loss:  83.6773  Training Accuracy:  0.902176\n",
      "Epoch:  835  Training Loss:  83.4958  Training Accuracy:  0.902646\n",
      "Epoch:  836  Training Loss:  83.3197  Training Accuracy:  0.902881\n",
      "Epoch:  837  Training Loss:  83.0809  Training Accuracy:  0.903175\n",
      "Epoch:  838  Training Loss:  82.9317  Training Accuracy:  0.903293\n",
      "Epoch:  839  Training Loss:  82.741  Training Accuracy:  0.903469\n",
      "Epoch:  840  Training Loss:  82.5803  Training Accuracy:  0.903645\n",
      "Epoch:  841  Training Loss:  82.4388  Training Accuracy:  0.903763\n",
      "Epoch:  842  Training Loss:  82.2025  Training Accuracy:  0.903881\n",
      "Epoch:  843  Training Loss:  82.0733  Training Accuracy:  0.904292\n",
      "Epoch:  844  Training Loss:  81.8472  Training Accuracy:  0.90441\n",
      "Epoch:  845  Training Loss:  81.7215  Training Accuracy:  0.904586\n",
      "Epoch:  846  Training Loss:  81.5036  Training Accuracy:  0.904998\n",
      "Epoch:  847  Training Loss:  81.3428  Training Accuracy:  0.904939\n",
      "Epoch:  848  Training Loss:  81.1926  Training Accuracy:  0.905233\n",
      "Epoch:  849  Training Loss:  81.0228  Training Accuracy:  0.905233\n",
      "Epoch:  850  Training Loss:  80.8001  Training Accuracy:  0.905527\n",
      "Epoch:  851  Training Loss:  80.6126  Training Accuracy:  0.905703\n",
      "Epoch:  852  Training Loss:  80.4164  Training Accuracy:  0.905997\n",
      "Epoch:  853  Training Loss:  80.2932  Training Accuracy:  0.906408\n",
      "Epoch:  854  Training Loss:  80.0938  Training Accuracy:  0.906879\n",
      "Epoch:  855  Training Loss:  79.9504  Training Accuracy:  0.906938\n",
      "Epoch:  856  Training Loss:  79.7631  Training Accuracy:  0.90729\n",
      "Epoch:  857  Training Loss:  79.5527  Training Accuracy:  0.907878\n",
      "Epoch:  858  Training Loss:  79.3436  Training Accuracy:  0.908113\n",
      "Epoch:  859  Training Loss:  79.2288  Training Accuracy:  0.90829\n",
      "Epoch:  860  Training Loss:  79.0203  Training Accuracy:  0.908407\n",
      "Epoch:  861  Training Loss:  78.8433  Training Accuracy:  0.908584\n",
      "Epoch:  862  Training Loss:  78.6734  Training Accuracy:  0.908642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  863  Training Loss:  78.4934  Training Accuracy:  0.90876\n",
      "Epoch:  864  Training Loss:  78.3014  Training Accuracy:  0.908995\n",
      "Epoch:  865  Training Loss:  78.1169  Training Accuracy:  0.908995\n",
      "Epoch:  866  Training Loss:  77.9496  Training Accuracy:  0.909172\n",
      "Epoch:  867  Training Loss:  77.7836  Training Accuracy:  0.909465\n",
      "Epoch:  868  Training Loss:  77.5889  Training Accuracy:  0.909407\n",
      "Epoch:  869  Training Loss:  77.4545  Training Accuracy:  0.909759\n",
      "Epoch:  870  Training Loss:  77.2755  Training Accuracy:  0.91023\n",
      "Epoch:  871  Training Loss:  77.0354  Training Accuracy:  0.91023\n",
      "Epoch:  872  Training Loss:  76.8343  Training Accuracy:  0.910582\n",
      "Epoch:  873  Training Loss:  76.716  Training Accuracy:  0.9107\n",
      "Epoch:  874  Training Loss:  76.4837  Training Accuracy:  0.910759\n",
      "Epoch:  875  Training Loss:  76.3579  Training Accuracy:  0.910994\n",
      "Epoch:  876  Training Loss:  76.1485  Training Accuracy:  0.911053\n",
      "Epoch:  877  Training Loss:  76.0275  Training Accuracy:  0.91117\n",
      "Epoch:  878  Training Loss:  75.8427  Training Accuracy:  0.911229\n",
      "Epoch:  879  Training Loss:  75.6708  Training Accuracy:  0.91117\n",
      "Epoch:  880  Training Loss:  75.5158  Training Accuracy:  0.911641\n",
      "Epoch:  881  Training Loss:  75.3319  Training Accuracy:  0.911817\n",
      "Epoch:  882  Training Loss:  75.1244  Training Accuracy:  0.911817\n",
      "Epoch:  883  Training Loss:  74.9954  Training Accuracy:  0.912052\n",
      "Epoch:  884  Training Loss:  74.8045  Training Accuracy:  0.911935\n",
      "Epoch:  885  Training Loss:  74.6526  Training Accuracy:  0.912111\n",
      "Epoch:  886  Training Loss:  74.5048  Training Accuracy:  0.912346\n",
      "Epoch:  887  Training Loss:  74.328  Training Accuracy:  0.912523\n",
      "Epoch:  888  Training Loss:  74.1238  Training Accuracy:  0.912581\n",
      "Epoch:  889  Training Loss:  73.9974  Training Accuracy:  0.912699\n",
      "Epoch:  890  Training Loss:  73.8174  Training Accuracy:  0.912934\n",
      "Epoch:  891  Training Loss:  73.7348  Training Accuracy:  0.91311\n",
      "Epoch:  892  Training Loss:  73.4953  Training Accuracy:  0.913228\n",
      "Epoch:  893  Training Loss:  73.3356  Training Accuracy:  0.913287\n",
      "Epoch:  894  Training Loss:  73.1256  Training Accuracy:  0.913639\n",
      "Epoch:  895  Training Loss:  73.0203  Training Accuracy:  0.913757\n",
      "Epoch:  896  Training Loss:  72.8068  Training Accuracy:  0.913934\n",
      "Epoch:  897  Training Loss:  72.6677  Training Accuracy:  0.913816\n",
      "Epoch:  898  Training Loss:  72.5021  Training Accuracy:  0.914169\n",
      "Epoch:  899  Training Loss:  72.3571  Training Accuracy:  0.914227\n",
      "Epoch:  900  Training Loss:  72.1763  Training Accuracy:  0.914404\n",
      "Epoch:  901  Training Loss:  71.9801  Training Accuracy:  0.914521\n",
      "Epoch:  902  Training Loss:  71.8455  Training Accuracy:  0.914815\n",
      "Epoch:  903  Training Loss:  71.6936  Training Accuracy:  0.914815\n",
      "Epoch:  904  Training Loss:  71.5098  Training Accuracy:  0.914933\n",
      "Epoch:  905  Training Loss:  71.3777  Training Accuracy:  0.915168\n",
      "Epoch:  906  Training Loss:  71.1904  Training Accuracy:  0.915227\n",
      "Epoch:  907  Training Loss:  71.0619  Training Accuracy:  0.915344\n",
      "Epoch:  908  Training Loss:  70.8706  Training Accuracy:  0.915344\n",
      "Epoch:  909  Training Loss:  70.76  Training Accuracy:  0.915521\n",
      "Epoch:  910  Training Loss:  70.5634  Training Accuracy:  0.915638\n",
      "Epoch:  911  Training Loss:  70.4426  Training Accuracy:  0.915873\n",
      "Epoch:  912  Training Loss:  70.2232  Training Accuracy:  0.915991\n",
      "Epoch:  913  Training Loss:  70.1541  Training Accuracy:  0.916167\n",
      "Epoch:  914  Training Loss:  69.9316  Training Accuracy:  0.916167\n",
      "Epoch:  915  Training Loss:  69.7951  Training Accuracy:  0.916344\n",
      "Epoch:  916  Training Loss:  69.7195  Training Accuracy:  0.916579\n",
      "Epoch:  917  Training Loss:  69.4986  Training Accuracy:  0.916638\n",
      "Epoch:  918  Training Loss:  69.2861  Training Accuracy:  0.916755\n",
      "Epoch:  919  Training Loss:  69.1814  Training Accuracy:  0.916932\n",
      "Epoch:  920  Training Loss:  69.0288  Training Accuracy:  0.917108\n",
      "Epoch:  921  Training Loss:  68.8626  Training Accuracy:  0.917226\n",
      "Epoch:  922  Training Loss:  68.7241  Training Accuracy:  0.91752\n",
      "Epoch:  923  Training Loss:  68.6104  Training Accuracy:  0.917637\n",
      "Epoch:  924  Training Loss:  68.4621  Training Accuracy:  0.917755\n",
      "Epoch:  925  Training Loss:  68.3963  Training Accuracy:  0.918049\n",
      "Epoch:  926  Training Loss:  68.1444  Training Accuracy:  0.918108\n",
      "Epoch:  927  Training Loss:  68.0236  Training Accuracy:  0.918225\n",
      "Epoch:  928  Training Loss:  67.896  Training Accuracy:  0.918401\n",
      "Epoch:  929  Training Loss:  67.7665  Training Accuracy:  0.918637\n",
      "Epoch:  930  Training Loss:  67.563  Training Accuracy:  0.918872\n",
      "Epoch:  931  Training Loss:  67.4977  Training Accuracy:  0.918695\n",
      "Epoch:  932  Training Loss:  67.3306  Training Accuracy:  0.919048\n",
      "Epoch:  933  Training Loss:  67.2358  Training Accuracy:  0.919166\n",
      "Epoch:  934  Training Loss:  67.0032  Training Accuracy:  0.919342\n",
      "Epoch:  935  Training Loss:  66.8856  Training Accuracy:  0.919577\n",
      "Epoch:  936  Training Loss:  66.7298  Training Accuracy:  0.919695\n",
      "Epoch:  937  Training Loss:  66.6136  Training Accuracy:  0.919812\n",
      "Epoch:  938  Training Loss:  66.4682  Training Accuracy:  0.91993\n",
      "Epoch:  939  Training Loss:  66.3593  Training Accuracy:  0.920106\n",
      "Epoch:  940  Training Loss:  66.1933  Training Accuracy:  0.920106\n",
      "Epoch:  941  Training Loss:  66.0308  Training Accuracy:  0.920224\n",
      "Epoch:  942  Training Loss:  65.926  Training Accuracy:  0.9204\n",
      "Epoch:  943  Training Loss:  65.7751  Training Accuracy:  0.920518\n",
      "Epoch:  944  Training Loss:  65.6498  Training Accuracy:  0.920753\n",
      "Epoch:  945  Training Loss:  65.4926  Training Accuracy:  0.920812\n",
      "Epoch:  946  Training Loss:  65.364  Training Accuracy:  0.920929\n",
      "Epoch:  947  Training Loss:  65.2784  Training Accuracy:  0.921223\n",
      "Epoch:  948  Training Loss:  65.0465  Training Accuracy:  0.921282\n",
      "Epoch:  949  Training Loss:  64.976  Training Accuracy:  0.921282\n",
      "Epoch:  950  Training Loss:  64.852  Training Accuracy:  0.9214\n",
      "Epoch:  951  Training Loss:  64.6656  Training Accuracy:  0.921576\n",
      "Epoch:  952  Training Loss:  64.5429  Training Accuracy:  0.921576\n",
      "Epoch:  953  Training Loss:  64.3693  Training Accuracy:  0.921635\n",
      "Epoch:  954  Training Loss:  64.2597  Training Accuracy:  0.921811\n",
      "Epoch:  955  Training Loss:  64.1967  Training Accuracy:  0.921988\n",
      "Epoch:  956  Training Loss:  63.9867  Training Accuracy:  0.922164\n",
      "Epoch:  957  Training Loss:  63.9183  Training Accuracy:  0.92234\n",
      "Epoch:  958  Training Loss:  63.7006  Training Accuracy:  0.92234\n",
      "Epoch:  959  Training Loss:  63.571  Training Accuracy:  0.922575\n",
      "Epoch:  960  Training Loss:  63.4746  Training Accuracy:  0.922693\n",
      "Epoch:  961  Training Loss:  63.3011  Training Accuracy:  0.922811\n",
      "Epoch:  962  Training Loss:  63.1458  Training Accuracy:  0.923105\n",
      "Epoch:  963  Training Loss:  63.0181  Training Accuracy:  0.923222\n",
      "Epoch:  964  Training Loss:  62.9262  Training Accuracy:  0.923222\n",
      "Epoch:  965  Training Loss:  62.7948  Training Accuracy:  0.923457\n",
      "Epoch:  966  Training Loss:  62.6031  Training Accuracy:  0.923575\n",
      "Epoch:  967  Training Loss:  62.5645  Training Accuracy:  0.923692\n",
      "Epoch:  968  Training Loss:  62.3759  Training Accuracy:  0.923869\n",
      "Epoch:  969  Training Loss:  62.2389  Training Accuracy:  0.923928\n",
      "Epoch:  970  Training Loss:  62.1052  Training Accuracy:  0.924104\n",
      "Epoch:  971  Training Loss:  61.9721  Training Accuracy:  0.924104\n",
      "Epoch:  972  Training Loss:  61.8418  Training Accuracy:  0.924398\n",
      "Epoch:  973  Training Loss:  61.7249  Training Accuracy:  0.924574\n",
      "Epoch:  974  Training Loss:  61.561  Training Accuracy:  0.924927\n",
      "Epoch:  975  Training Loss:  61.4771  Training Accuracy:  0.924986\n",
      "Epoch:  976  Training Loss:  61.2932  Training Accuracy:  0.925221\n",
      "Epoch:  977  Training Loss:  61.2452  Training Accuracy:  0.925221\n",
      "Epoch:  978  Training Loss:  61.0672  Training Accuracy:  0.925397\n",
      "Epoch:  979  Training Loss:  60.9682  Training Accuracy:  0.925456\n",
      "Epoch:  980  Training Loss:  60.7812  Training Accuracy:  0.925691\n",
      "Epoch:  981  Training Loss:  60.7077  Training Accuracy:  0.92575\n",
      "Epoch:  982  Training Loss:  60.5467  Training Accuracy:  0.925926\n",
      "Epoch:  983  Training Loss:  60.4008  Training Accuracy:  0.926103\n",
      "Epoch:  984  Training Loss:  60.2548  Training Accuracy:  0.92622\n",
      "Epoch:  985  Training Loss:  60.1324  Training Accuracy:  0.926338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  986  Training Loss:  59.9703  Training Accuracy:  0.926338\n",
      "Epoch:  987  Training Loss:  59.8794  Training Accuracy:  0.926514\n",
      "Epoch:  988  Training Loss:  59.7302  Training Accuracy:  0.926691\n",
      "Epoch:  989  Training Loss:  59.6288  Training Accuracy:  0.926867\n",
      "Epoch:  990  Training Loss:  59.4526  Training Accuracy:  0.926926\n",
      "Epoch:  991  Training Loss:  59.364  Training Accuracy:  0.927043\n",
      "Epoch:  992  Training Loss:  59.2563  Training Accuracy:  0.927102\n",
      "Epoch:  993  Training Loss:  59.1137  Training Accuracy:  0.927102\n",
      "Epoch:  994  Training Loss:  58.9568  Training Accuracy:  0.927279\n",
      "Epoch:  995  Training Loss:  58.8224  Training Accuracy:  0.927455\n",
      "Epoch:  996  Training Loss:  58.7041  Training Accuracy:  0.927455\n",
      "Epoch:  997  Training Loss:  58.6267  Training Accuracy:  0.92769\n",
      "Epoch:  998  Training Loss:  58.4223  Training Accuracy:  0.927631\n",
      "Epoch:  999  Training Loss:  58.3144  Training Accuracy:  0.927866\n",
      "Testing Accuracy: 0.775734\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  34.5833  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  303.607  Training Accuracy:  0.0575544\n",
      "Epoch:  2  Training Loss:  382.569  Training Accuracy:  0.0664903\n",
      "Epoch:  3  Training Loss:  324.378  Training Accuracy:  0.101411\n",
      "Epoch:  4  Training Loss:  312.735  Training Accuracy:  0.113992\n",
      "Epoch:  5  Training Loss:  298.074  Training Accuracy:  0.13204\n",
      "Epoch:  6  Training Loss:  284.079  Training Accuracy:  0.148266\n",
      "Epoch:  7  Training Loss:  267.657  Training Accuracy:  0.174133\n",
      "Epoch:  8  Training Loss:  256.275  Training Accuracy:  0.205115\n",
      "Epoch:  9  Training Loss:  249.703  Training Accuracy:  0.237566\n",
      "Epoch:  10  Training Loss:  245.477  Training Accuracy:  0.268136\n",
      "Epoch:  11  Training Loss:  241.884  Training Accuracy:  0.29565\n",
      "Epoch:  12  Training Loss:  239.926  Training Accuracy:  0.317578\n",
      "Epoch:  13  Training Loss:  236.04  Training Accuracy:  0.336331\n",
      "Epoch:  14  Training Loss:  235.907  Training Accuracy:  0.353498\n",
      "Epoch:  15  Training Loss:  234.517  Training Accuracy:  0.370958\n",
      "Epoch:  16  Training Loss:  229.562  Training Accuracy:  0.380247\n",
      "Epoch:  17  Training Loss:  229.286  Training Accuracy:  0.391064\n",
      "Epoch:  18  Training Loss:  227.723  Training Accuracy:  0.403174\n",
      "Epoch:  19  Training Loss:  226.883  Training Accuracy:  0.414873\n",
      "Epoch:  20  Training Loss:  225.995  Training Accuracy:  0.425632\n",
      "Epoch:  21  Training Loss:  226.169  Training Accuracy:  0.436567\n",
      "Epoch:  22  Training Loss:  225.328  Training Accuracy:  0.447501\n",
      "Epoch:  23  Training Loss:  225.784  Training Accuracy:  0.45873\n",
      "Epoch:  24  Training Loss:  224.461  Training Accuracy:  0.470547\n",
      "Epoch:  25  Training Loss:  224.501  Training Accuracy:  0.480364\n",
      "Epoch:  26  Training Loss:  221.775  Training Accuracy:  0.489888\n",
      "Epoch:  27  Training Loss:  221.294  Training Accuracy:  0.50194\n",
      "Epoch:  28  Training Loss:  220.045  Training Accuracy:  0.510053\n",
      "Epoch:  29  Training Loss:  217.917  Training Accuracy:  0.51893\n",
      "Epoch:  30  Training Loss:  216.074  Training Accuracy:  0.532863\n",
      "Epoch:  31  Training Loss:  214.898  Training Accuracy:  0.545444\n",
      "Epoch:  32  Training Loss:  211.311  Training Accuracy:  0.55679\n",
      "Epoch:  33  Training Loss:  210.994  Training Accuracy:  0.564903\n",
      "Epoch:  34  Training Loss:  208.889  Training Accuracy:  0.574662\n",
      "Epoch:  35  Training Loss:  208.852  Training Accuracy:  0.581834\n",
      "Epoch:  36  Training Loss:  207.384  Training Accuracy:  0.59224\n",
      "Epoch:  37  Training Loss:  205.493  Training Accuracy:  0.598001\n",
      "Epoch:  38  Training Loss:  203.542  Training Accuracy:  0.605938\n",
      "Epoch:  39  Training Loss:  203.1  Training Accuracy:  0.612169\n",
      "Epoch:  40  Training Loss:  200.486  Training Accuracy:  0.618048\n",
      "Epoch:  41  Training Loss:  199.552  Training Accuracy:  0.624691\n",
      "Epoch:  42  Training Loss:  197.396  Training Accuracy:  0.631746\n",
      "Epoch:  43  Training Loss:  196.958  Training Accuracy:  0.637978\n",
      "Epoch:  44  Training Loss:  195.328  Training Accuracy:  0.643798\n",
      "Epoch:  45  Training Loss:  194.75  Training Accuracy:  0.651029\n",
      "Epoch:  46  Training Loss:  190.997  Training Accuracy:  0.65873\n",
      "Epoch:  47  Training Loss:  191.322  Training Accuracy:  0.663139\n",
      "Epoch:  48  Training Loss:  187.634  Training Accuracy:  0.670312\n",
      "Epoch:  49  Training Loss:  186.65  Training Accuracy:  0.675838\n",
      "Epoch:  50  Training Loss:  184.066  Training Accuracy:  0.681129\n",
      "Epoch:  51  Training Loss:  181.842  Training Accuracy:  0.685832\n",
      "Epoch:  52  Training Loss:  179.381  Training Accuracy:  0.689595\n",
      "Epoch:  53  Training Loss:  176.373  Training Accuracy:  0.694298\n",
      "Epoch:  54  Training Loss:  173.518  Training Accuracy:  0.699059\n",
      "Epoch:  55  Training Loss:  173.262  Training Accuracy:  0.703057\n",
      "Epoch:  56  Training Loss:  171.104  Training Accuracy:  0.708995\n",
      "Epoch:  57  Training Loss:  170.0  Training Accuracy:  0.713345\n",
      "Epoch:  58  Training Loss:  168.449  Training Accuracy:  0.717284\n",
      "Epoch:  59  Training Loss:  168.03  Training Accuracy:  0.721987\n",
      "Epoch:  60  Training Loss:  165.435  Training Accuracy:  0.727337\n",
      "Epoch:  61  Training Loss:  162.976  Training Accuracy:  0.730041\n",
      "Epoch:  62  Training Loss:  162.626  Training Accuracy:  0.735685\n",
      "Epoch:  63  Training Loss:  160.891  Training Accuracy:  0.739624\n",
      "Epoch:  64  Training Loss:  160.101  Training Accuracy:  0.742799\n",
      "Epoch:  65  Training Loss:  158.616  Training Accuracy:  0.747443\n",
      "Epoch:  66  Training Loss:  158.267  Training Accuracy:  0.75097\n",
      "Epoch:  67  Training Loss:  155.373  Training Accuracy:  0.753557\n",
      "Epoch:  68  Training Loss:  154.498  Training Accuracy:  0.756496\n",
      "Epoch:  69  Training Loss:  151.913  Training Accuracy:  0.7602\n",
      "Epoch:  70  Training Loss:  152.22  Training Accuracy:  0.763434\n",
      "Epoch:  71  Training Loss:  150.542  Training Accuracy:  0.767196\n",
      "Epoch:  72  Training Loss:  149.407  Training Accuracy:  0.769077\n",
      "Epoch:  73  Training Loss:  139.577  Training Accuracy:  0.761376\n",
      "Epoch:  74  Training Loss:  148.011  Training Accuracy:  0.773075\n",
      "Epoch:  75  Training Loss:  146.665  Training Accuracy:  0.77819\n",
      "Epoch:  76  Training Loss:  144.723  Training Accuracy:  0.780365\n",
      "Epoch:  77  Training Loss:  142.875  Training Accuracy:  0.784068\n",
      "Epoch:  78  Training Loss:  140.171  Training Accuracy:  0.786949\n",
      "Epoch:  79  Training Loss:  132.299  Training Accuracy:  0.789124\n",
      "Epoch:  80  Training Loss:  136.113  Training Accuracy:  0.791299\n",
      "Epoch:  81  Training Loss:  135.1  Training Accuracy:  0.795474\n",
      "Epoch:  82  Training Loss:  133.264  Training Accuracy:  0.798942\n",
      "Epoch:  83  Training Loss:  131.932  Training Accuracy:  0.801705\n",
      "Epoch:  84  Training Loss:  131.22  Training Accuracy:  0.804645\n",
      "Epoch:  85  Training Loss:  129.457  Training Accuracy:  0.808407\n",
      "Epoch:  86  Training Loss:  127.5  Training Accuracy:  0.810818\n",
      "Epoch:  87  Training Loss:  125.887  Training Accuracy:  0.814227\n",
      "Epoch:  88  Training Loss:  123.499  Training Accuracy:  0.817167\n",
      "Epoch:  89  Training Loss:  121.124  Training Accuracy:  0.818989\n",
      "Epoch:  90  Training Loss:  119.268  Training Accuracy:  0.821047\n",
      "Epoch:  91  Training Loss:  118.059  Training Accuracy:  0.824515\n",
      "Epoch:  92  Training Loss:  116.156  Training Accuracy:  0.826103\n",
      "Epoch:  93  Training Loss:  113.558  Training Accuracy:  0.830335\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  298.418  Training Accuracy:  0.0366843\n",
      "Epoch:  1  Training Loss:  165.103  Training Accuracy:  0.0624927\n",
      "Epoch:  2  Training Loss:  169.218  Training Accuracy:  0.0708995\n",
      "Epoch:  3  Training Loss:  178.531  Training Accuracy:  0.0744856\n",
      "Epoch:  4  Training Loss:  185.864  Training Accuracy:  0.0832452\n",
      "Epoch:  5  Training Loss:  190.933  Training Accuracy:  0.0906526\n",
      "Epoch:  6  Training Loss:  194.388  Training Accuracy:  0.0997061\n",
      "Epoch:  7  Training Loss:  196.754  Training Accuracy:  0.108995\n",
      "Epoch:  8  Training Loss:  198.392  Training Accuracy:  0.117872\n",
      "Epoch:  9  Training Loss:  199.518  Training Accuracy:  0.127278\n",
      "Epoch:  10  Training Loss:  200.29  Training Accuracy:  0.137449\n",
      "Epoch:  11  Training Loss:  200.776  Training Accuracy:  0.147619\n",
      "Epoch:  12  Training Loss:  201.098  Training Accuracy:  0.155967\n",
      "Epoch:  13  Training Loss:  201.266  Training Accuracy:  0.164433\n",
      "Epoch:  14  Training Loss:  201.351  Training Accuracy:  0.173192\n",
      "Epoch:  15  Training Loss:  201.345  Training Accuracy:  0.182481\n",
      "Epoch:  16  Training Loss:  201.282  Training Accuracy:  0.191828\n",
      "Epoch:  17  Training Loss:  201.169  Training Accuracy:  0.20147\n",
      "Epoch:  18  Training Loss:  201.004  Training Accuracy:  0.207701\n",
      "Epoch:  19  Training Loss:  200.811  Training Accuracy:  0.214756\n",
      "Epoch:  20  Training Loss:  200.59  Training Accuracy:  0.221634\n",
      "Epoch:  21  Training Loss:  200.326  Training Accuracy:  0.227748\n",
      "Epoch:  22  Training Loss:  200.084  Training Accuracy:  0.235097\n",
      "Epoch:  23  Training Loss:  199.794  Training Accuracy:  0.241975\n",
      "Epoch:  24  Training Loss:  199.491  Training Accuracy:  0.247619\n",
      "Epoch:  25  Training Loss:  199.223  Training Accuracy:  0.253615\n",
      "Epoch:  26  Training Loss:  198.905  Training Accuracy:  0.260376\n",
      "Epoch:  27  Training Loss:  198.571  Training Accuracy:  0.265844\n",
      "Epoch:  28  Training Loss:  198.295  Training Accuracy:  0.271781\n",
      "Epoch:  29  Training Loss:  197.956  Training Accuracy:  0.277425\n",
      "Epoch:  30  Training Loss:  197.661  Training Accuracy:  0.283539\n",
      "Epoch:  31  Training Loss:  197.33  Training Accuracy:  0.288536\n",
      "Epoch:  32  Training Loss:  197.032  Training Accuracy:  0.293827\n",
      "Epoch:  33  Training Loss:  196.748  Training Accuracy:  0.298354\n",
      "Epoch:  34  Training Loss:  196.44  Training Accuracy:  0.303704\n",
      "Epoch:  35  Training Loss:  196.172  Training Accuracy:  0.308936\n",
      "Epoch:  36  Training Loss:  195.9  Training Accuracy:  0.313463\n",
      "Epoch:  37  Training Loss:  195.626  Training Accuracy:  0.317872\n",
      "Epoch:  38  Training Loss:  195.398  Training Accuracy:  0.322222\n",
      "Epoch:  39  Training Loss:  195.167  Training Accuracy:  0.326161\n",
      "Epoch:  40  Training Loss:  194.972  Training Accuracy:  0.330453\n",
      "Epoch:  41  Training Loss:  194.811  Training Accuracy:  0.33398\n",
      "Epoch:  42  Training Loss:  194.64  Training Accuracy:  0.337566\n",
      "Epoch:  43  Training Loss:  194.485  Training Accuracy:  0.340447\n",
      "Epoch:  44  Training Loss:  194.316  Training Accuracy:  0.344033\n",
      "Epoch:  45  Training Loss:  194.233  Training Accuracy:  0.347854\n",
      "Epoch:  46  Training Loss:  194.131  Training Accuracy:  0.350735\n",
      "Epoch:  47  Training Loss:  194.046  Training Accuracy:  0.354615\n",
      "Epoch:  48  Training Loss:  193.982  Training Accuracy:  0.357554\n",
      "Epoch:  49  Training Loss:  193.93  Training Accuracy:  0.359965\n",
      "Epoch:  50  Training Loss:  193.843  Training Accuracy:  0.362493\n",
      "Epoch:  51  Training Loss:  193.831  Training Accuracy:  0.365667\n",
      "Epoch:  52  Training Loss:  193.8  Training Accuracy:  0.369077\n",
      "Epoch:  53  Training Loss:  193.778  Training Accuracy:  0.37231\n",
      "Epoch:  54  Training Loss:  193.755  Training Accuracy:  0.37572\n",
      "Epoch:  55  Training Loss:  193.77  Training Accuracy:  0.377778\n",
      "Epoch:  56  Training Loss:  193.778  Training Accuracy:  0.380541\n",
      "Epoch:  57  Training Loss:  193.796  Training Accuracy:  0.383774\n",
      "Epoch:  58  Training Loss:  193.839  Training Accuracy:  0.386243\n",
      "Epoch:  59  Training Loss:  193.887  Training Accuracy:  0.389771\n",
      "Epoch:  60  Training Loss:  193.939  Training Accuracy:  0.392299\n",
      "Epoch:  61  Training Loss:  193.996  Training Accuracy:  0.394768\n",
      "Epoch:  62  Training Loss:  194.104  Training Accuracy:  0.39759\n",
      "Epoch:  63  Training Loss:  194.199  Training Accuracy:  0.399588\n",
      "Epoch:  64  Training Loss:  194.281  Training Accuracy:  0.402469\n",
      "Epoch:  65  Training Loss:  194.385  Training Accuracy:  0.405115\n",
      "Epoch:  66  Training Loss:  194.522  Training Accuracy:  0.408818\n",
      "Epoch:  67  Training Loss:  194.654  Training Accuracy:  0.412287\n",
      "Epoch:  68  Training Loss:  194.775  Training Accuracy:  0.415226\n",
      "Epoch:  69  Training Loss:  194.948  Training Accuracy:  0.417813\n",
      "Epoch:  70  Training Loss:  195.078  Training Accuracy:  0.420811\n",
      "Epoch:  71  Training Loss:  195.273  Training Accuracy:  0.424632\n",
      "Epoch:  72  Training Loss:  195.411  Training Accuracy:  0.42716\n",
      "Epoch:  73  Training Loss:  195.585  Training Accuracy:  0.429218\n",
      "Epoch:  74  Training Loss:  195.761  Training Accuracy:  0.43157\n",
      "Epoch:  75  Training Loss:  195.932  Training Accuracy:  0.433803\n",
      "Epoch:  76  Training Loss:  196.15  Training Accuracy:  0.436214\n",
      "Epoch:  77  Training Loss:  196.332  Training Accuracy:  0.438742\n",
      "Epoch:  78  Training Loss:  196.501  Training Accuracy:  0.441387\n",
      "Epoch:  79  Training Loss:  196.722  Training Accuracy:  0.443974\n",
      "Epoch:  80  Training Loss:  196.909  Training Accuracy:  0.446443\n",
      "Epoch:  81  Training Loss:  197.069  Training Accuracy:  0.448501\n",
      "Epoch:  82  Training Loss:  197.285  Training Accuracy:  0.450558\n",
      "Epoch:  83  Training Loss:  197.432  Training Accuracy:  0.453145\n",
      "Epoch:  84  Training Loss:  197.598  Training Accuracy:  0.455673\n",
      "Epoch:  85  Training Loss:  197.787  Training Accuracy:  0.458377\n",
      "Epoch:  86  Training Loss:  197.957  Training Accuracy:  0.460788\n",
      "Epoch:  87  Training Loss:  198.103  Training Accuracy:  0.463374\n",
      "Epoch:  88  Training Loss:  198.268  Training Accuracy:  0.466314\n",
      "Epoch:  89  Training Loss:  198.398  Training Accuracy:  0.468548\n",
      "Epoch:  90  Training Loss:  198.556  Training Accuracy:  0.470723\n",
      "Epoch:  91  Training Loss:  198.727  Training Accuracy:  0.472663\n",
      "Epoch:  92  Training Loss:  198.866  Training Accuracy:  0.474721\n",
      "Epoch:  93  Training Loss:  199.021  Training Accuracy:  0.477072\n",
      "Epoch:  94  Training Loss:  199.149  Training Accuracy:  0.4796\n",
      "Epoch:  95  Training Loss:  199.31  Training Accuracy:  0.48201\n",
      "Epoch:  96  Training Loss:  199.435  Training Accuracy:  0.484715\n",
      "Epoch:  97  Training Loss:  199.574  Training Accuracy:  0.487537\n",
      "Epoch:  98  Training Loss:  199.718  Training Accuracy:  0.489594\n",
      "Epoch:  99  Training Loss:  199.838  Training Accuracy:  0.493474\n",
      "Epoch:  100  Training Loss:  199.962  Training Accuracy:  0.495708\n",
      "Epoch:  101  Training Loss:  200.121  Training Accuracy:  0.497354\n",
      "Epoch:  102  Training Loss:  200.209  Training Accuracy:  0.499588\n",
      "Epoch:  103  Training Loss:  200.327  Training Accuracy:  0.501293\n",
      "Epoch:  104  Training Loss:  200.469  Training Accuracy:  0.503116\n",
      "Epoch:  105  Training Loss:  200.564  Training Accuracy:  0.504821\n",
      "Epoch:  106  Training Loss:  200.643  Training Accuracy:  0.507231\n",
      "Epoch:  107  Training Loss:  200.792  Training Accuracy:  0.510229\n",
      "Epoch:  108  Training Loss:  200.85  Training Accuracy:  0.511581\n",
      "Epoch:  109  Training Loss:  200.914  Training Accuracy:  0.513639\n",
      "Epoch:  110  Training Loss:  201.023  Training Accuracy:  0.515755\n",
      "Epoch:  111  Training Loss:  201.092  Training Accuracy:  0.517519\n",
      "Epoch:  112  Training Loss:  201.163  Training Accuracy:  0.519988\n",
      "Epoch:  113  Training Loss:  201.262  Training Accuracy:  0.521928\n",
      "Epoch:  114  Training Loss:  201.307  Training Accuracy:  0.524162\n",
      "Epoch:  115  Training Loss:  201.327  Training Accuracy:  0.525926\n",
      "Epoch:  116  Training Loss:  201.441  Training Accuracy:  0.527866\n",
      "Epoch:  117  Training Loss:  201.448  Training Accuracy:  0.529512\n",
      "Epoch:  118  Training Loss:  201.493  Training Accuracy:  0.530864\n",
      "Epoch:  119  Training Loss:  201.546  Training Accuracy:  0.532745\n",
      "Epoch:  120  Training Loss:  201.553  Training Accuracy:  0.53398\n",
      "Epoch:  121  Training Loss:  201.549  Training Accuracy:  0.535273\n",
      "Epoch:  122  Training Loss:  201.607  Training Accuracy:  0.537096\n",
      "Epoch:  123  Training Loss:  201.628  Training Accuracy:  0.538859\n",
      "Epoch:  124  Training Loss:  201.635  Training Accuracy:  0.540388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  201.686  Training Accuracy:  0.542152\n",
      "Epoch:  126  Training Loss:  201.692  Training Accuracy:  0.543974\n",
      "Epoch:  127  Training Loss:  201.663  Training Accuracy:  0.545326\n",
      "Epoch:  128  Training Loss:  201.712  Training Accuracy:  0.547266\n",
      "Epoch:  129  Training Loss:  201.692  Training Accuracy:  0.548677\n",
      "Epoch:  130  Training Loss:  201.717  Training Accuracy:  0.550382\n",
      "Epoch:  131  Training Loss:  201.682  Training Accuracy:  0.552028\n",
      "Epoch:  132  Training Loss:  201.727  Training Accuracy:  0.553851\n",
      "Epoch:  133  Training Loss:  201.696  Training Accuracy:  0.554909\n",
      "Epoch:  134  Training Loss:  201.717  Training Accuracy:  0.556379\n",
      "Epoch:  135  Training Loss:  201.662  Training Accuracy:  0.557496\n",
      "Epoch:  136  Training Loss:  201.7  Training Accuracy:  0.559318\n",
      "Epoch:  137  Training Loss:  201.663  Training Accuracy:  0.560494\n",
      "Epoch:  138  Training Loss:  201.662  Training Accuracy:  0.561963\n",
      "Epoch:  139  Training Loss:  201.614  Training Accuracy:  0.563668\n",
      "Epoch:  140  Training Loss:  201.647  Training Accuracy:  0.565197\n",
      "Epoch:  141  Training Loss:  201.597  Training Accuracy:  0.566314\n",
      "Epoch:  142  Training Loss:  201.598  Training Accuracy:  0.568783\n",
      "Epoch:  143  Training Loss:  201.538  Training Accuracy:  0.57037\n",
      "Epoch:  144  Training Loss:  201.57  Training Accuracy:  0.572193\n",
      "Epoch:  145  Training Loss:  201.544  Training Accuracy:  0.57378\n",
      "Epoch:  146  Training Loss:  201.52  Training Accuracy:  0.574897\n",
      "Epoch:  147  Training Loss:  201.461  Training Accuracy:  0.57619\n",
      "Epoch:  148  Training Loss:  201.469  Training Accuracy:  0.577778\n",
      "Epoch:  149  Training Loss:  201.422  Training Accuracy:  0.579483\n",
      "Epoch:  150  Training Loss:  201.421  Training Accuracy:  0.581011\n",
      "Epoch:  151  Training Loss:  201.321  Training Accuracy:  0.582775\n",
      "Epoch:  152  Training Loss:  201.316  Training Accuracy:  0.583892\n",
      "Epoch:  153  Training Loss:  201.262  Training Accuracy:  0.585597\n",
      "Epoch:  154  Training Loss:  201.248  Training Accuracy:  0.586949\n",
      "Epoch:  155  Training Loss:  201.151  Training Accuracy:  0.588477\n",
      "Epoch:  156  Training Loss:  201.165  Training Accuracy:  0.589947\n",
      "Epoch:  157  Training Loss:  201.075  Training Accuracy:  0.59177\n",
      "Epoch:  158  Training Loss:  201.042  Training Accuracy:  0.594004\n",
      "Epoch:  159  Training Loss:  200.966  Training Accuracy:  0.595473\n",
      "Epoch:  160  Training Loss:  200.952  Training Accuracy:  0.597648\n",
      "Epoch:  161  Training Loss:  200.877  Training Accuracy:  0.598765\n",
      "Epoch:  162  Training Loss:  200.841  Training Accuracy:  0.599941\n",
      "Epoch:  163  Training Loss:  200.745  Training Accuracy:  0.600999\n",
      "Epoch:  164  Training Loss:  200.719  Training Accuracy:  0.602998\n",
      "Epoch:  165  Training Loss:  200.628  Training Accuracy:  0.604174\n",
      "Epoch:  166  Training Loss:  200.578  Training Accuracy:  0.605644\n",
      "Epoch:  167  Training Loss:  200.48  Training Accuracy:  0.606761\n",
      "Epoch:  168  Training Loss:  200.45  Training Accuracy:  0.608172\n",
      "Epoch:  169  Training Loss:  200.359  Training Accuracy:  0.610406\n",
      "Epoch:  170  Training Loss:  200.283  Training Accuracy:  0.612757\n",
      "Epoch:  171  Training Loss:  200.176  Training Accuracy:  0.613874\n",
      "Epoch:  172  Training Loss:  200.122  Training Accuracy:  0.614697\n",
      "Epoch:  173  Training Loss:  200.053  Training Accuracy:  0.615991\n",
      "Epoch:  174  Training Loss:  199.979  Training Accuracy:  0.617696\n",
      "Epoch:  175  Training Loss:  199.84  Training Accuracy:  0.618342\n",
      "Epoch:  176  Training Loss:  199.802  Training Accuracy:  0.620047\n",
      "Epoch:  177  Training Loss:  199.715  Training Accuracy:  0.621517\n",
      "Epoch:  178  Training Loss:  199.627  Training Accuracy:  0.62234\n",
      "Epoch:  179  Training Loss:  199.513  Training Accuracy:  0.623163\n",
      "Epoch:  180  Training Loss:  199.484  Training Accuracy:  0.625338\n",
      "Epoch:  181  Training Loss:  199.397  Training Accuracy:  0.626455\n",
      "Epoch:  182  Training Loss:  199.304  Training Accuracy:  0.627513\n",
      "Epoch:  183  Training Loss:  199.189  Training Accuracy:  0.628572\n",
      "Epoch:  184  Training Loss:  199.135  Training Accuracy:  0.6301\n",
      "Epoch:  185  Training Loss:  199.032  Training Accuracy:  0.63157\n",
      "Epoch:  186  Training Loss:  198.943  Training Accuracy:  0.632687\n",
      "Epoch:  187  Training Loss:  198.804  Training Accuracy:  0.633627\n",
      "Epoch:  188  Training Loss:  198.76  Training Accuracy:  0.634627\n",
      "Epoch:  189  Training Loss:  198.682  Training Accuracy:  0.635626\n",
      "Epoch:  190  Training Loss:  198.59  Training Accuracy:  0.636802\n",
      "Epoch:  191  Training Loss:  198.467  Training Accuracy:  0.637978\n",
      "Epoch:  192  Training Loss:  198.433  Training Accuracy:  0.639389\n",
      "Epoch:  193  Training Loss:  198.345  Training Accuracy:  0.640506\n",
      "Epoch:  194  Training Loss:  198.243  Training Accuracy:  0.64174\n",
      "Epoch:  195  Training Loss:  198.178  Training Accuracy:  0.642504\n",
      "Epoch:  196  Training Loss:  198.046  Training Accuracy:  0.643798\n",
      "Epoch:  197  Training Loss:  198.011  Training Accuracy:  0.64515\n",
      "Epoch:  198  Training Loss:  197.919  Training Accuracy:  0.646091\n",
      "Epoch:  199  Training Loss:  197.82  Training Accuracy:  0.647384\n",
      "Epoch:  200  Training Loss:  197.693  Training Accuracy:  0.648442\n",
      "Epoch:  201  Training Loss:  197.673  Training Accuracy:  0.649442\n",
      "Epoch:  202  Training Loss:  197.559  Training Accuracy:  0.650382\n",
      "Epoch:  203  Training Loss:  197.499  Training Accuracy:  0.651969\n",
      "Epoch:  204  Training Loss:  197.356  Training Accuracy:  0.652616\n",
      "Epoch:  205  Training Loss:  197.332  Training Accuracy:  0.653204\n",
      "Epoch:  206  Training Loss:  197.224  Training Accuracy:  0.65438\n",
      "Epoch:  207  Training Loss:  197.153  Training Accuracy:  0.655908\n",
      "Epoch:  208  Training Loss:  197.061  Training Accuracy:  0.656908\n",
      "Epoch:  209  Training Loss:  196.983  Training Accuracy:  0.657554\n",
      "Epoch:  210  Training Loss:  196.884  Training Accuracy:  0.658436\n",
      "Epoch:  211  Training Loss:  196.811  Training Accuracy:  0.659906\n",
      "Epoch:  212  Training Loss:  196.704  Training Accuracy:  0.661258\n",
      "Epoch:  213  Training Loss:  196.551  Training Accuracy:  0.662081\n",
      "Epoch:  214  Training Loss:  196.517  Training Accuracy:  0.662963\n",
      "Epoch:  215  Training Loss:  196.399  Training Accuracy:  0.663904\n",
      "Epoch:  216  Training Loss:  196.273  Training Accuracy:  0.664786\n",
      "Epoch:  217  Training Loss:  196.277  Training Accuracy:  0.66555\n",
      "Epoch:  218  Training Loss:  196.117  Training Accuracy:  0.666196\n",
      "Epoch:  219  Training Loss:  196.022  Training Accuracy:  0.667196\n",
      "Epoch:  220  Training Loss:  195.87  Training Accuracy:  0.668195\n",
      "Epoch:  221  Training Loss:  195.827  Training Accuracy:  0.669312\n",
      "Epoch:  222  Training Loss:  195.719  Training Accuracy:  0.670429\n",
      "Epoch:  223  Training Loss:  195.655  Training Accuracy:  0.67137\n",
      "Epoch:  224  Training Loss:  195.534  Training Accuracy:  0.672487\n",
      "Epoch:  225  Training Loss:  195.477  Training Accuracy:  0.673486\n",
      "Epoch:  226  Training Loss:  195.348  Training Accuracy:  0.674251\n",
      "Epoch:  227  Training Loss:  195.295  Training Accuracy:  0.675074\n",
      "Epoch:  228  Training Loss:  195.2  Training Accuracy:  0.675838\n",
      "Epoch:  229  Training Loss:  195.089  Training Accuracy:  0.677425\n",
      "Epoch:  230  Training Loss:  194.983  Training Accuracy:  0.678131\n",
      "Epoch:  231  Training Loss:  194.913  Training Accuracy:  0.679306\n",
      "Epoch:  232  Training Loss:  194.789  Training Accuracy:  0.680188\n",
      "Epoch:  233  Training Loss:  194.706  Training Accuracy:  0.680952\n",
      "Epoch:  234  Training Loss:  194.59  Training Accuracy:  0.681893\n",
      "Epoch:  235  Training Loss:  194.51  Training Accuracy:  0.682599\n",
      "Epoch:  236  Training Loss:  194.447  Training Accuracy:  0.683069\n",
      "Epoch:  237  Training Loss:  194.329  Training Accuracy:  0.683892\n",
      "Epoch:  238  Training Loss:  194.23  Training Accuracy:  0.684597\n",
      "Epoch:  239  Training Loss:  194.169  Training Accuracy:  0.685832\n",
      "Epoch:  240  Training Loss:  194.019  Training Accuracy:  0.686714\n",
      "Epoch:  241  Training Loss:  193.954  Training Accuracy:  0.68789\n",
      "Epoch:  242  Training Loss:  193.833  Training Accuracy:  0.688301\n",
      "Epoch:  243  Training Loss:  193.729  Training Accuracy:  0.689301\n",
      "Epoch:  244  Training Loss:  193.592  Training Accuracy:  0.690065\n",
      "Epoch:  245  Training Loss:  193.536  Training Accuracy:  0.69077\n",
      "Epoch:  246  Training Loss:  193.415  Training Accuracy:  0.691652\n",
      "Epoch:  247  Training Loss:  193.341  Training Accuracy:  0.69224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  193.223  Training Accuracy:  0.692887\n",
      "Epoch:  249  Training Loss:  193.149  Training Accuracy:  0.693592\n",
      "Epoch:  250  Training Loss:  193.02  Training Accuracy:  0.694239\n",
      "Epoch:  251  Training Loss:  192.931  Training Accuracy:  0.695415\n",
      "Epoch:  252  Training Loss:  192.808  Training Accuracy:  0.696179\n",
      "Epoch:  253  Training Loss:  192.719  Training Accuracy:  0.696708\n",
      "Epoch:  254  Training Loss:  192.614  Training Accuracy:  0.69806\n",
      "Epoch:  255  Training Loss:  192.475  Training Accuracy:  0.698766\n",
      "Epoch:  256  Training Loss:  192.368  Training Accuracy:  0.699589\n",
      "Epoch:  257  Training Loss:  192.246  Training Accuracy:  0.700294\n",
      "Epoch:  258  Training Loss:  192.155  Training Accuracy:  0.700823\n",
      "Epoch:  259  Training Loss:  192.034  Training Accuracy:  0.701235\n",
      "Epoch:  260  Training Loss:  191.885  Training Accuracy:  0.701587\n",
      "Epoch:  261  Training Loss:  191.841  Training Accuracy:  0.70194\n",
      "Epoch:  262  Training Loss:  191.685  Training Accuracy:  0.702763\n",
      "Epoch:  263  Training Loss:  191.557  Training Accuracy:  0.703586\n",
      "Epoch:  264  Training Loss:  191.497  Training Accuracy:  0.70435\n",
      "Epoch:  265  Training Loss:  191.386  Training Accuracy:  0.705056\n",
      "Epoch:  266  Training Loss:  191.21  Training Accuracy:  0.705761\n",
      "Epoch:  267  Training Loss:  191.152  Training Accuracy:  0.706937\n",
      "Epoch:  268  Training Loss:  191.079  Training Accuracy:  0.707466\n",
      "Epoch:  269  Training Loss:  190.877  Training Accuracy:  0.707937\n",
      "Epoch:  270  Training Loss:  190.862  Training Accuracy:  0.708936\n",
      "Epoch:  271  Training Loss:  190.699  Training Accuracy:  0.709465\n",
      "Epoch:  272  Training Loss:  190.551  Training Accuracy:  0.710406\n",
      "Epoch:  273  Training Loss:  190.514  Training Accuracy:  0.711229\n",
      "Epoch:  274  Training Loss:  190.374  Training Accuracy:  0.711934\n",
      "Epoch:  275  Training Loss:  190.208  Training Accuracy:  0.712287\n",
      "Epoch:  276  Training Loss:  190.13  Training Accuracy:  0.713051\n",
      "Epoch:  277  Training Loss:  190.052  Training Accuracy:  0.713874\n",
      "Epoch:  278  Training Loss:  189.899  Training Accuracy:  0.714639\n",
      "Epoch:  279  Training Loss:  189.802  Training Accuracy:  0.715697\n",
      "Epoch:  280  Training Loss:  189.707  Training Accuracy:  0.716108\n",
      "Epoch:  281  Training Loss:  189.523  Training Accuracy:  0.716755\n",
      "Epoch:  282  Training Loss:  189.473  Training Accuracy:  0.717402\n",
      "Epoch:  283  Training Loss:  189.311  Training Accuracy:  0.717637\n",
      "Epoch:  284  Training Loss:  189.158  Training Accuracy:  0.718225\n",
      "Epoch:  285  Training Loss:  189.177  Training Accuracy:  0.719165\n",
      "Epoch:  286  Training Loss:  189.007  Training Accuracy:  0.719694\n",
      "Epoch:  287  Training Loss:  188.854  Training Accuracy:  0.720576\n",
      "Epoch:  288  Training Loss:  188.822  Training Accuracy:  0.721164\n",
      "Epoch:  289  Training Loss:  188.684  Training Accuracy:  0.721928\n",
      "Epoch:  290  Training Loss:  188.489  Training Accuracy:  0.722575\n",
      "Epoch:  291  Training Loss:  188.467  Training Accuracy:  0.723339\n",
      "Epoch:  292  Training Loss:  188.318  Training Accuracy:  0.723986\n",
      "Epoch:  293  Training Loss:  188.161  Training Accuracy:  0.724339\n",
      "Epoch:  294  Training Loss:  188.078  Training Accuracy:  0.724868\n",
      "Epoch:  295  Training Loss:  187.925  Training Accuracy:  0.725221\n",
      "Epoch:  296  Training Loss:  187.741  Training Accuracy:  0.725515\n",
      "Epoch:  297  Training Loss:  187.701  Training Accuracy:  0.726102\n",
      "Epoch:  298  Training Loss:  187.579  Training Accuracy:  0.726749\n",
      "Epoch:  299  Training Loss:  187.394  Training Accuracy:  0.727455\n",
      "Epoch:  300  Training Loss:  187.299  Training Accuracy:  0.728513\n",
      "Epoch:  301  Training Loss:  187.155  Training Accuracy:  0.729101\n",
      "Epoch:  302  Training Loss:  187.012  Training Accuracy:  0.729689\n",
      "Epoch:  303  Training Loss:  186.945  Training Accuracy:  0.730159\n",
      "Epoch:  304  Training Loss:  186.753  Training Accuracy:  0.731041\n",
      "Epoch:  305  Training Loss:  186.581  Training Accuracy:  0.731335\n",
      "Epoch:  306  Training Loss:  186.529  Training Accuracy:  0.732334\n",
      "Epoch:  307  Training Loss:  186.345  Training Accuracy:  0.733392\n",
      "Epoch:  308  Training Loss:  186.203  Training Accuracy:  0.73398\n",
      "Epoch:  309  Training Loss:  186.087  Training Accuracy:  0.734921\n",
      "Epoch:  310  Training Loss:  185.978  Training Accuracy:  0.735509\n",
      "Epoch:  311  Training Loss:  185.818  Training Accuracy:  0.736038\n",
      "Epoch:  312  Training Loss:  185.713  Training Accuracy:  0.737508\n",
      "Epoch:  313  Training Loss:  185.577  Training Accuracy:  0.738154\n",
      "Epoch:  314  Training Loss:  185.432  Training Accuracy:  0.738742\n",
      "Epoch:  315  Training Loss:  185.329  Training Accuracy:  0.739154\n",
      "Epoch:  316  Training Loss:  185.205  Training Accuracy:  0.739565\n",
      "Epoch:  317  Training Loss:  185.031  Training Accuracy:  0.740035\n",
      "Epoch:  318  Training Loss:  184.927  Training Accuracy:  0.740858\n",
      "Epoch:  319  Training Loss:  184.774  Training Accuracy:  0.741329\n",
      "Epoch:  320  Training Loss:  184.612  Training Accuracy:  0.74174\n",
      "Epoch:  321  Training Loss:  184.516  Training Accuracy:  0.742387\n",
      "Epoch:  322  Training Loss:  184.383  Training Accuracy:  0.743092\n",
      "Epoch:  323  Training Loss:  184.205  Training Accuracy:  0.743445\n",
      "Epoch:  324  Training Loss:  184.122  Training Accuracy:  0.744092\n",
      "Epoch:  325  Training Loss:  183.955  Training Accuracy:  0.744562\n",
      "Epoch:  326  Training Loss:  183.808  Training Accuracy:  0.745268\n",
      "Epoch:  327  Training Loss:  183.71  Training Accuracy:  0.745797\n",
      "Epoch:  328  Training Loss:  183.566  Training Accuracy:  0.746091\n",
      "Epoch:  329  Training Loss:  183.44  Training Accuracy:  0.74662\n",
      "Epoch:  330  Training Loss:  183.302  Training Accuracy:  0.747384\n",
      "Epoch:  331  Training Loss:  183.177  Training Accuracy:  0.748031\n",
      "Epoch:  332  Training Loss:  183.046  Training Accuracy:  0.748854\n",
      "Epoch:  333  Training Loss:  182.973  Training Accuracy:  0.749265\n",
      "Epoch:  334  Training Loss:  182.796  Training Accuracy:  0.749501\n",
      "Epoch:  335  Training Loss:  182.616  Training Accuracy:  0.750323\n",
      "Epoch:  336  Training Loss:  182.501  Training Accuracy:  0.751147\n",
      "Epoch:  337  Training Loss:  182.399  Training Accuracy:  0.751793\n",
      "Epoch:  338  Training Loss:  182.204  Training Accuracy:  0.752499\n",
      "Epoch:  339  Training Loss:  182.115  Training Accuracy:  0.753087\n",
      "Epoch:  340  Training Loss:  181.949  Training Accuracy:  0.753616\n",
      "Epoch:  341  Training Loss:  181.782  Training Accuracy:  0.754321\n",
      "Epoch:  342  Training Loss:  181.687  Training Accuracy:  0.754968\n",
      "Epoch:  343  Training Loss:  181.568  Training Accuracy:  0.755791\n",
      "Epoch:  344  Training Loss:  181.367  Training Accuracy:  0.755909\n",
      "Epoch:  345  Training Loss:  181.296  Training Accuracy:  0.756908\n",
      "Epoch:  346  Training Loss:  181.127  Training Accuracy:  0.757731\n",
      "Epoch:  347  Training Loss:  181.014  Training Accuracy:  0.758378\n",
      "Epoch:  348  Training Loss:  180.808  Training Accuracy:  0.759201\n",
      "Epoch:  349  Training Loss:  180.707  Training Accuracy:  0.75973\n",
      "Epoch:  350  Training Loss:  180.536  Training Accuracy:  0.7602\n",
      "Epoch:  351  Training Loss:  180.396  Training Accuracy:  0.76067\n",
      "Epoch:  352  Training Loss:  180.284  Training Accuracy:  0.761729\n",
      "Epoch:  353  Training Loss:  180.121  Training Accuracy:  0.762081\n",
      "Epoch:  354  Training Loss:  179.877  Training Accuracy:  0.762904\n",
      "Epoch:  355  Training Loss:  179.858  Training Accuracy:  0.76361\n",
      "Epoch:  356  Training Loss:  179.663  Training Accuracy:  0.764492\n",
      "Epoch:  357  Training Loss:  179.564  Training Accuracy:  0.76508\n",
      "Epoch:  358  Training Loss:  179.349  Training Accuracy:  0.765609\n",
      "Epoch:  359  Training Loss:  179.229  Training Accuracy:  0.766138\n",
      "Epoch:  360  Training Loss:  179.062  Training Accuracy:  0.766491\n",
      "Epoch:  361  Training Loss:  178.926  Training Accuracy:  0.767137\n",
      "Epoch:  362  Training Loss:  178.756  Training Accuracy:  0.767431\n",
      "Epoch:  363  Training Loss:  178.599  Training Accuracy:  0.76796\n",
      "Epoch:  364  Training Loss:  178.362  Training Accuracy:  0.768725\n",
      "Epoch:  365  Training Loss:  178.304  Training Accuracy:  0.769195\n",
      "Epoch:  366  Training Loss:  178.099  Training Accuracy:  0.769724\n",
      "Epoch:  367  Training Loss:  177.982  Training Accuracy:  0.770253\n",
      "Epoch:  368  Training Loss:  177.822  Training Accuracy:  0.770488\n",
      "Epoch:  369  Training Loss:  177.667  Training Accuracy:  0.7709\n",
      "Epoch:  370  Training Loss:  177.499  Training Accuracy:  0.771488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  177.29  Training Accuracy:  0.771958\n",
      "Epoch:  372  Training Loss:  177.17  Training Accuracy:  0.772899\n",
      "Epoch:  373  Training Loss:  176.97  Training Accuracy:  0.773545\n",
      "Epoch:  374  Training Loss:  176.823  Training Accuracy:  0.774251\n",
      "Epoch:  375  Training Loss:  176.621  Training Accuracy:  0.775015\n",
      "Epoch:  376  Training Loss:  176.519  Training Accuracy:  0.775544\n",
      "Epoch:  377  Training Loss:  176.315  Training Accuracy:  0.776014\n",
      "Epoch:  378  Training Loss:  176.18  Training Accuracy:  0.776543\n",
      "Epoch:  379  Training Loss:  175.953  Training Accuracy:  0.776955\n",
      "Epoch:  380  Training Loss:  175.826  Training Accuracy:  0.777367\n",
      "Epoch:  381  Training Loss:  175.67  Training Accuracy:  0.777778\n",
      "Epoch:  382  Training Loss:  175.419  Training Accuracy:  0.77819\n",
      "Epoch:  383  Training Loss:  175.316  Training Accuracy:  0.778836\n",
      "Epoch:  384  Training Loss:  175.111  Training Accuracy:  0.779071\n",
      "Epoch:  385  Training Loss:  174.935  Training Accuracy:  0.779365\n",
      "Epoch:  386  Training Loss:  174.708  Training Accuracy:  0.779777\n",
      "Epoch:  387  Training Loss:  174.613  Training Accuracy:  0.780365\n",
      "Epoch:  388  Training Loss:  174.417  Training Accuracy:  0.780953\n",
      "Epoch:  389  Training Loss:  174.219  Training Accuracy:  0.781423\n",
      "Epoch:  390  Training Loss:  174.074  Training Accuracy:  0.781834\n",
      "Epoch:  391  Training Loss:  173.851  Training Accuracy:  0.781952\n",
      "Epoch:  392  Training Loss:  173.706  Training Accuracy:  0.78254\n",
      "Epoch:  393  Training Loss:  173.523  Training Accuracy:  0.782952\n",
      "Epoch:  394  Training Loss:  173.361  Training Accuracy:  0.783539\n",
      "Epoch:  395  Training Loss:  173.186  Training Accuracy:  0.78401\n",
      "Epoch:  396  Training Loss:  172.985  Training Accuracy:  0.784362\n",
      "Epoch:  397  Training Loss:  172.883  Training Accuracy:  0.785068\n",
      "Epoch:  398  Training Loss:  172.664  Training Accuracy:  0.785538\n",
      "Epoch:  399  Training Loss:  172.55  Training Accuracy:  0.786126\n",
      "Epoch:  400  Training Loss:  172.355  Training Accuracy:  0.786538\n",
      "Epoch:  401  Training Loss:  172.118  Training Accuracy:  0.786832\n",
      "Epoch:  402  Training Loss:  171.997  Training Accuracy:  0.787243\n",
      "Epoch:  403  Training Loss:  171.833  Training Accuracy:  0.78789\n",
      "Epoch:  404  Training Loss:  171.644  Training Accuracy:  0.788654\n",
      "Epoch:  405  Training Loss:  171.407  Training Accuracy:  0.788948\n",
      "Epoch:  406  Training Loss:  171.25  Training Accuracy:  0.789595\n",
      "Epoch:  407  Training Loss:  171.077  Training Accuracy:  0.790183\n",
      "Epoch:  408  Training Loss:  170.862  Training Accuracy:  0.79077\n",
      "Epoch:  409  Training Loss:  170.641  Training Accuracy:  0.791064\n",
      "Epoch:  410  Training Loss:  170.506  Training Accuracy:  0.791652\n",
      "Epoch:  411  Training Loss:  170.283  Training Accuracy:  0.792064\n",
      "Epoch:  412  Training Loss:  170.115  Training Accuracy:  0.792593\n",
      "Epoch:  413  Training Loss:  169.915  Training Accuracy:  0.793004\n",
      "Epoch:  414  Training Loss:  169.71  Training Accuracy:  0.794121\n",
      "Epoch:  415  Training Loss:  169.529  Training Accuracy:  0.794709\n",
      "Epoch:  416  Training Loss:  169.316  Training Accuracy:  0.795297\n",
      "Epoch:  417  Training Loss:  169.135  Training Accuracy:  0.795532\n",
      "Epoch:  418  Training Loss:  168.947  Training Accuracy:  0.795826\n",
      "Epoch:  419  Training Loss:  168.738  Training Accuracy:  0.79612\n",
      "Epoch:  420  Training Loss:  168.653  Training Accuracy:  0.796943\n",
      "Epoch:  421  Training Loss:  168.408  Training Accuracy:  0.797178\n",
      "Epoch:  422  Training Loss:  168.198  Training Accuracy:  0.797531\n",
      "Epoch:  423  Training Loss:  167.983  Training Accuracy:  0.798119\n",
      "Epoch:  424  Training Loss:  167.823  Training Accuracy:  0.798707\n",
      "Epoch:  425  Training Loss:  167.653  Training Accuracy:  0.798942\n",
      "Epoch:  426  Training Loss:  167.433  Training Accuracy:  0.799412\n",
      "Epoch:  427  Training Loss:  167.267  Training Accuracy:  0.799765\n",
      "Epoch:  428  Training Loss:  167.01  Training Accuracy:  0.800294\n",
      "Epoch:  429  Training Loss:  166.855  Training Accuracy:  0.800647\n",
      "Epoch:  430  Training Loss:  166.682  Training Accuracy:  0.800941\n",
      "Epoch:  431  Training Loss:  166.549  Training Accuracy:  0.801529\n",
      "Epoch:  432  Training Loss:  166.331  Training Accuracy:  0.802234\n",
      "Epoch:  433  Training Loss:  166.171  Training Accuracy:  0.802763\n",
      "Epoch:  434  Training Loss:  165.953  Training Accuracy:  0.803234\n",
      "Epoch:  435  Training Loss:  165.782  Training Accuracy:  0.803645\n",
      "Epoch:  436  Training Loss:  165.591  Training Accuracy:  0.804174\n",
      "Epoch:  437  Training Loss:  165.378  Training Accuracy:  0.804939\n",
      "Epoch:  438  Training Loss:  165.242  Training Accuracy:  0.80535\n",
      "Epoch:  439  Training Loss:  165.024  Training Accuracy:  0.805526\n",
      "Epoch:  440  Training Loss:  164.868  Training Accuracy:  0.806467\n",
      "Epoch:  441  Training Loss:  164.708  Training Accuracy:  0.806937\n",
      "Epoch:  442  Training Loss:  164.497  Training Accuracy:  0.80729\n",
      "Epoch:  443  Training Loss:  164.335  Training Accuracy:  0.807466\n",
      "Epoch:  444  Training Loss:  164.102  Training Accuracy:  0.807643\n",
      "Epoch:  445  Training Loss:  163.972  Training Accuracy:  0.807878\n",
      "Epoch:  446  Training Loss:  163.756  Training Accuracy:  0.808466\n",
      "Epoch:  447  Training Loss:  163.555  Training Accuracy:  0.808877\n",
      "Epoch:  448  Training Loss:  163.352  Training Accuracy:  0.808936\n",
      "Epoch:  449  Training Loss:  163.273  Training Accuracy:  0.809465\n",
      "Epoch:  450  Training Loss:  163.001  Training Accuracy:  0.810053\n",
      "Epoch:  451  Training Loss:  162.86  Training Accuracy:  0.810288\n",
      "Epoch:  452  Training Loss:  162.607  Training Accuracy:  0.810817\n",
      "Epoch:  453  Training Loss:  162.438  Training Accuracy:  0.811582\n",
      "Epoch:  454  Training Loss:  162.284  Training Accuracy:  0.811934\n",
      "Epoch:  455  Training Loss:  162.02  Training Accuracy:  0.812228\n",
      "Epoch:  456  Training Loss:  161.905  Training Accuracy:  0.81264\n",
      "Epoch:  457  Training Loss:  161.701  Training Accuracy:  0.813522\n",
      "Epoch:  458  Training Loss:  161.513  Training Accuracy:  0.813933\n",
      "Epoch:  459  Training Loss:  161.281  Training Accuracy:  0.814051\n",
      "Epoch:  460  Training Loss:  161.133  Training Accuracy:  0.814345\n",
      "Epoch:  461  Training Loss:  160.856  Training Accuracy:  0.814933\n",
      "Epoch:  462  Training Loss:  160.752  Training Accuracy:  0.815462\n",
      "Epoch:  463  Training Loss:  160.583  Training Accuracy:  0.815815\n",
      "Epoch:  464  Training Loss:  160.36  Training Accuracy:  0.816344\n",
      "Epoch:  465  Training Loss:  160.204  Training Accuracy:  0.81652\n",
      "Epoch:  466  Training Loss:  159.984  Training Accuracy:  0.816755\n",
      "Epoch:  467  Training Loss:  159.744  Training Accuracy:  0.817108\n",
      "Epoch:  468  Training Loss:  159.586  Training Accuracy:  0.817402\n",
      "Epoch:  469  Training Loss:  159.371  Training Accuracy:  0.817755\n",
      "Epoch:  470  Training Loss:  159.105  Training Accuracy:  0.818284\n",
      "Epoch:  471  Training Loss:  158.944  Training Accuracy:  0.818519\n",
      "Epoch:  472  Training Loss:  158.79  Training Accuracy:  0.818872\n",
      "Epoch:  473  Training Loss:  158.507  Training Accuracy:  0.819401\n",
      "Epoch:  474  Training Loss:  158.377  Training Accuracy:  0.819753\n",
      "Epoch:  475  Training Loss:  158.175  Training Accuracy:  0.819988\n",
      "Epoch:  476  Training Loss:  157.905  Training Accuracy:  0.820224\n",
      "Epoch:  477  Training Loss:  157.725  Training Accuracy:  0.820576\n",
      "Epoch:  478  Training Loss:  157.528  Training Accuracy:  0.820929\n",
      "Epoch:  479  Training Loss:  157.26  Training Accuracy:  0.821576\n",
      "Epoch:  480  Training Loss:  157.174  Training Accuracy:  0.822105\n",
      "Epoch:  481  Training Loss:  156.907  Training Accuracy:  0.822634\n",
      "Epoch:  482  Training Loss:  156.652  Training Accuracy:  0.823281\n",
      "Epoch:  483  Training Loss:  156.475  Training Accuracy:  0.823457\n",
      "Epoch:  484  Training Loss:  156.297  Training Accuracy:  0.823986\n",
      "Epoch:  485  Training Loss:  156.053  Training Accuracy:  0.82428\n",
      "Epoch:  486  Training Loss:  155.859  Training Accuracy:  0.824633\n",
      "Epoch:  487  Training Loss:  155.658  Training Accuracy:  0.82475\n",
      "Epoch:  488  Training Loss:  155.453  Training Accuracy:  0.825456\n",
      "Epoch:  489  Training Loss:  155.259  Training Accuracy:  0.825926\n",
      "Epoch:  490  Training Loss:  155.05  Training Accuracy:  0.826455\n",
      "Epoch:  491  Training Loss:  154.86  Training Accuracy:  0.826808\n",
      "Epoch:  492  Training Loss:  154.655  Training Accuracy:  0.827043\n",
      "Epoch:  493  Training Loss:  154.453  Training Accuracy:  0.827514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  154.205  Training Accuracy:  0.827749\n",
      "Epoch:  495  Training Loss:  154.057  Training Accuracy:  0.828513\n",
      "Epoch:  496  Training Loss:  153.802  Training Accuracy:  0.828866\n",
      "Epoch:  497  Training Loss:  153.547  Training Accuracy:  0.828807\n",
      "Epoch:  498  Training Loss:  153.447  Training Accuracy:  0.829454\n",
      "Epoch:  499  Training Loss:  153.169  Training Accuracy:  0.829865\n",
      "Epoch:  500  Training Loss:  152.961  Training Accuracy:  0.830277\n",
      "Epoch:  501  Training Loss:  152.806  Training Accuracy:  0.830453\n",
      "Epoch:  502  Training Loss:  152.585  Training Accuracy:  0.831217\n",
      "Epoch:  503  Training Loss:  152.405  Training Accuracy:  0.831746\n",
      "Epoch:  504  Training Loss:  152.208  Training Accuracy:  0.831923\n",
      "Epoch:  505  Training Loss:  152.033  Training Accuracy:  0.832628\n",
      "Epoch:  506  Training Loss:  151.759  Training Accuracy:  0.832628\n",
      "Epoch:  507  Training Loss:  151.616  Training Accuracy:  0.833216\n",
      "Epoch:  508  Training Loss:  151.426  Training Accuracy:  0.83351\n",
      "Epoch:  509  Training Loss:  151.182  Training Accuracy:  0.833804\n",
      "Epoch:  510  Training Loss:  151.04  Training Accuracy:  0.83398\n",
      "Epoch:  511  Training Loss:  150.846  Training Accuracy:  0.834274\n",
      "Epoch:  512  Training Loss:  150.616  Training Accuracy:  0.834745\n",
      "Epoch:  513  Training Loss:  150.453  Training Accuracy:  0.835097\n",
      "Epoch:  514  Training Loss:  150.255  Training Accuracy:  0.835391\n",
      "Epoch:  515  Training Loss:  150.013  Training Accuracy:  0.835979\n",
      "Epoch:  516  Training Loss:  149.896  Training Accuracy:  0.836332\n",
      "Epoch:  517  Training Loss:  149.638  Training Accuracy:  0.836861\n",
      "Epoch:  518  Training Loss:  149.492  Training Accuracy:  0.837214\n",
      "Epoch:  519  Training Loss:  149.296  Training Accuracy:  0.837449\n",
      "Epoch:  520  Training Loss:  149.091  Training Accuracy:  0.837684\n",
      "Epoch:  521  Training Loss:  148.87  Training Accuracy:  0.837978\n",
      "Epoch:  522  Training Loss:  148.767  Training Accuracy:  0.838448\n",
      "Epoch:  523  Training Loss:  148.52  Training Accuracy:  0.838801\n",
      "Epoch:  524  Training Loss:  148.362  Training Accuracy:  0.839389\n",
      "Epoch:  525  Training Loss:  148.173  Training Accuracy:  0.839742\n",
      "Epoch:  526  Training Loss:  147.973  Training Accuracy:  0.840153\n",
      "Epoch:  527  Training Loss:  147.742  Training Accuracy:  0.840623\n",
      "Epoch:  528  Training Loss:  147.568  Training Accuracy:  0.840682\n",
      "Epoch:  529  Training Loss:  147.411  Training Accuracy:  0.841329\n",
      "Epoch:  530  Training Loss:  147.164  Training Accuracy:  0.841917\n",
      "Epoch:  531  Training Loss:  147.047  Training Accuracy:  0.842328\n",
      "Epoch:  532  Training Loss:  146.82  Training Accuracy:  0.842681\n",
      "Epoch:  533  Training Loss:  146.6  Training Accuracy:  0.843328\n",
      "Epoch:  534  Training Loss:  146.405  Training Accuracy:  0.843916\n",
      "Epoch:  535  Training Loss:  146.176  Training Accuracy:  0.844151\n",
      "Epoch:  536  Training Loss:  146.036  Training Accuracy:  0.84468\n",
      "Epoch:  537  Training Loss:  145.718  Training Accuracy:  0.844797\n",
      "Epoch:  538  Training Loss:  145.595  Training Accuracy:  0.844915\n",
      "Epoch:  539  Training Loss:  145.347  Training Accuracy:  0.845327\n",
      "Epoch:  540  Training Loss:  145.194  Training Accuracy:  0.845503\n",
      "Epoch:  541  Training Loss:  144.922  Training Accuracy:  0.845621\n",
      "Epoch:  542  Training Loss:  144.796  Training Accuracy:  0.846267\n",
      "Epoch:  543  Training Loss:  144.525  Training Accuracy:  0.846326\n",
      "Epoch:  544  Training Loss:  144.374  Training Accuracy:  0.846738\n",
      "Epoch:  545  Training Loss:  144.116  Training Accuracy:  0.847032\n",
      "Epoch:  546  Training Loss:  143.972  Training Accuracy:  0.847561\n",
      "Epoch:  547  Training Loss:  143.748  Training Accuracy:  0.847502\n",
      "Epoch:  548  Training Loss:  143.519  Training Accuracy:  0.848384\n",
      "Epoch:  549  Training Loss:  143.296  Training Accuracy:  0.84856\n",
      "Epoch:  550  Training Loss:  143.167  Training Accuracy:  0.848854\n",
      "Epoch:  551  Training Loss:  142.85  Training Accuracy:  0.849207\n",
      "Epoch:  552  Training Loss:  142.717  Training Accuracy:  0.849795\n",
      "Epoch:  553  Training Loss:  142.499  Training Accuracy:  0.849795\n",
      "Epoch:  554  Training Loss:  142.274  Training Accuracy:  0.850382\n",
      "Epoch:  555  Training Loss:  142.046  Training Accuracy:  0.850618\n",
      "Epoch:  556  Training Loss:  141.91  Training Accuracy:  0.851147\n",
      "Epoch:  557  Training Loss:  141.574  Training Accuracy:  0.851441\n",
      "Epoch:  558  Training Loss:  141.458  Training Accuracy:  0.851735\n",
      "Epoch:  559  Training Loss:  141.294  Training Accuracy:  0.852205\n",
      "Epoch:  560  Training Loss:  140.997  Training Accuracy:  0.852616\n",
      "Epoch:  561  Training Loss:  140.785  Training Accuracy:  0.85291\n",
      "Epoch:  562  Training Loss:  140.642  Training Accuracy:  0.853146\n",
      "Epoch:  563  Training Loss:  140.372  Training Accuracy:  0.853557\n",
      "Epoch:  564  Training Loss:  140.225  Training Accuracy:  0.853675\n",
      "Epoch:  565  Training Loss:  139.947  Training Accuracy:  0.853792\n",
      "Epoch:  566  Training Loss:  139.778  Training Accuracy:  0.854086\n",
      "Epoch:  567  Training Loss:  139.549  Training Accuracy:  0.854204\n",
      "Epoch:  568  Training Loss:  139.347  Training Accuracy:  0.854615\n",
      "Epoch:  569  Training Loss:  139.093  Training Accuracy:  0.855203\n",
      "Epoch:  570  Training Loss:  138.961  Training Accuracy:  0.855674\n",
      "Epoch:  571  Training Loss:  138.713  Training Accuracy:  0.855909\n",
      "Epoch:  572  Training Loss:  138.473  Training Accuracy:  0.856144\n",
      "Epoch:  573  Training Loss:  138.271  Training Accuracy:  0.856203\n",
      "Epoch:  574  Training Loss:  138.116  Training Accuracy:  0.856673\n",
      "Epoch:  575  Training Loss:  137.84  Training Accuracy:  0.85679\n",
      "Epoch:  576  Training Loss:  137.634  Training Accuracy:  0.857555\n",
      "Epoch:  577  Training Loss:  137.45  Training Accuracy:  0.857555\n",
      "Epoch:  578  Training Loss:  137.188  Training Accuracy:  0.857908\n",
      "Epoch:  579  Training Loss:  137.032  Training Accuracy:  0.858084\n",
      "Epoch:  580  Training Loss:  136.817  Training Accuracy:  0.858378\n",
      "Epoch:  581  Training Loss:  136.522  Training Accuracy:  0.858907\n",
      "Epoch:  582  Training Loss:  136.387  Training Accuracy:  0.859024\n",
      "Epoch:  583  Training Loss:  136.153  Training Accuracy:  0.859554\n",
      "Epoch:  584  Training Loss:  135.94  Training Accuracy:  0.859789\n",
      "Epoch:  585  Training Loss:  135.735  Training Accuracy:  0.860142\n",
      "Epoch:  586  Training Loss:  135.528  Training Accuracy:  0.860435\n",
      "Epoch:  587  Training Loss:  135.275  Training Accuracy:  0.860671\n",
      "Epoch:  588  Training Loss:  135.08  Training Accuracy:  0.860965\n",
      "Epoch:  589  Training Loss:  134.849  Training Accuracy:  0.861376\n",
      "Epoch:  590  Training Loss:  134.658  Training Accuracy:  0.861611\n",
      "Epoch:  591  Training Loss:  134.45  Training Accuracy:  0.861787\n",
      "Epoch:  592  Training Loss:  134.177  Training Accuracy:  0.862023\n",
      "Epoch:  593  Training Loss:  133.992  Training Accuracy:  0.862493\n",
      "Epoch:  594  Training Loss:  133.834  Training Accuracy:  0.862728\n",
      "Epoch:  595  Training Loss:  133.534  Training Accuracy:  0.863022\n",
      "Epoch:  596  Training Loss:  133.395  Training Accuracy:  0.863434\n",
      "Epoch:  597  Training Loss:  133.139  Training Accuracy:  0.864022\n",
      "Epoch:  598  Training Loss:  132.946  Training Accuracy:  0.864257\n",
      "Epoch:  599  Training Loss:  132.767  Training Accuracy:  0.864492\n",
      "Testing Accuracy: 0.749763\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 600\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  632.338  Training Accuracy:  0.0326279\n",
      "Epoch:  1  Training Loss:  517.931  Training Accuracy:  0.0432099\n",
      "Epoch:  2  Training Loss:  485.267  Training Accuracy:  0.0443857\n",
      "Epoch:  3  Training Loss:  455.719  Training Accuracy:  0.0553792\n",
      "Epoch:  4  Training Loss:  439.295  Training Accuracy:  0.0670194\n",
      "Epoch:  5  Training Loss:  431.51  Training Accuracy:  0.0783657\n",
      "Epoch:  6  Training Loss:  428.192  Training Accuracy:  0.0884774\n",
      "Epoch:  7  Training Loss:  427.059  Training Accuracy:  0.0971781\n",
      "Epoch:  8  Training Loss:  426.942  Training Accuracy:  0.105761\n",
      "Epoch:  9  Training Loss:  427.202  Training Accuracy:  0.115461\n",
      "Epoch:  10  Training Loss:  427.59  Training Accuracy:  0.124162\n",
      "Epoch:  11  Training Loss:  427.924  Training Accuracy:  0.133098\n",
      "Epoch:  12  Training Loss:  428.167  Training Accuracy:  0.140917\n",
      "Epoch:  13  Training Loss:  428.306  Training Accuracy:  0.150088\n",
      "Epoch:  14  Training Loss:  428.343  Training Accuracy:  0.159847\n",
      "Epoch:  15  Training Loss:  428.281  Training Accuracy:  0.169312\n",
      "Epoch:  16  Training Loss:  428.137  Training Accuracy:  0.178601\n",
      "Epoch:  17  Training Loss:  427.896  Training Accuracy:  0.187713\n",
      "Epoch:  18  Training Loss:  427.595  Training Accuracy:  0.195414\n",
      "Epoch:  19  Training Loss:  427.248  Training Accuracy:  0.202528\n",
      "Epoch:  20  Training Loss:  426.841  Training Accuracy:  0.210406\n",
      "Epoch:  21  Training Loss:  426.385  Training Accuracy:  0.217519\n",
      "Epoch:  22  Training Loss:  425.903  Training Accuracy:  0.22575\n",
      "Epoch:  23  Training Loss:  425.371  Training Accuracy:  0.232334\n",
      "Epoch:  24  Training Loss:  424.778  Training Accuracy:  0.239095\n",
      "Epoch:  25  Training Loss:  424.194  Training Accuracy:  0.245973\n",
      "Epoch:  26  Training Loss:  423.567  Training Accuracy:  0.251793\n",
      "Epoch:  27  Training Loss:  422.907  Training Accuracy:  0.256379\n",
      "Epoch:  28  Training Loss:  422.256  Training Accuracy:  0.261728\n",
      "Epoch:  29  Training Loss:  421.564  Training Accuracy:  0.267431\n",
      "Epoch:  30  Training Loss:  420.847  Training Accuracy:  0.272546\n",
      "Epoch:  31  Training Loss:  420.107  Training Accuracy:  0.277013\n",
      "Epoch:  32  Training Loss:  419.357  Training Accuracy:  0.281481\n",
      "Epoch:  33  Training Loss:  418.557  Training Accuracy:  0.28642\n",
      "Epoch:  34  Training Loss:  417.727  Training Accuracy:  0.291358\n",
      "Epoch:  35  Training Loss:  416.882  Training Accuracy:  0.295532\n",
      "Epoch:  36  Training Loss:  416.013  Training Accuracy:  0.300294\n",
      "Epoch:  37  Training Loss:  415.14  Training Accuracy:  0.304585\n",
      "Epoch:  38  Training Loss:  414.243  Training Accuracy:  0.308524\n",
      "Epoch:  39  Training Loss:  413.348  Training Accuracy:  0.31211\n",
      "Epoch:  40  Training Loss:  412.422  Training Accuracy:  0.316637\n",
      "Epoch:  41  Training Loss:  411.469  Training Accuracy:  0.320811\n",
      "Epoch:  42  Training Loss:  410.483  Training Accuracy:  0.32428\n",
      "Epoch:  43  Training Loss:  409.489  Training Accuracy:  0.327454\n",
      "Epoch:  44  Training Loss:  408.472  Training Accuracy:  0.331158\n",
      "Epoch:  45  Training Loss:  407.433  Training Accuracy:  0.335097\n",
      "Epoch:  46  Training Loss:  406.354  Training Accuracy:  0.338565\n",
      "Epoch:  47  Training Loss:  405.284  Training Accuracy:  0.342387\n",
      "Epoch:  48  Training Loss:  404.184  Training Accuracy:  0.345385\n",
      "Epoch:  49  Training Loss:  403.055  Training Accuracy:  0.348383\n",
      "Epoch:  50  Training Loss:  401.939  Training Accuracy:  0.351499\n",
      "Epoch:  51  Training Loss:  400.77  Training Accuracy:  0.354556\n",
      "Epoch:  52  Training Loss:  399.623  Training Accuracy:  0.356555\n",
      "Epoch:  53  Training Loss:  398.433  Training Accuracy:  0.359436\n",
      "Epoch:  54  Training Loss:  397.253  Training Accuracy:  0.362669\n",
      "Epoch:  55  Training Loss:  396.046  Training Accuracy:  0.36602\n",
      "Epoch:  56  Training Loss:  394.833  Training Accuracy:  0.368842\n",
      "Epoch:  57  Training Loss:  393.613  Training Accuracy:  0.372369\n",
      "Epoch:  58  Training Loss:  392.374  Training Accuracy:  0.376249\n",
      "Epoch:  59  Training Loss:  391.124  Training Accuracy:  0.379541\n",
      "Epoch:  60  Training Loss:  389.854  Training Accuracy:  0.382128\n",
      "Epoch:  61  Training Loss:  388.558  Training Accuracy:  0.384774\n",
      "Epoch:  62  Training Loss:  387.297  Training Accuracy:  0.387478\n",
      "Epoch:  63  Training Loss:  386.003  Training Accuracy:  0.390359\n",
      "Epoch:  64  Training Loss:  384.684  Training Accuracy:  0.392886\n",
      "Epoch:  65  Training Loss:  383.416  Training Accuracy:  0.395767\n",
      "Epoch:  66  Training Loss:  382.117  Training Accuracy:  0.398177\n",
      "Epoch:  67  Training Loss:  380.839  Training Accuracy:  0.399765\n",
      "Epoch:  68  Training Loss:  379.551  Training Accuracy:  0.402528\n",
      "Epoch:  69  Training Loss:  378.249  Training Accuracy:  0.405056\n",
      "Epoch:  70  Training Loss:  376.984  Training Accuracy:  0.406408\n",
      "Epoch:  71  Training Loss:  375.673  Training Accuracy:  0.407701\n",
      "Epoch:  72  Training Loss:  374.386  Training Accuracy:  0.409524\n",
      "Epoch:  73  Training Loss:  373.112  Training Accuracy:  0.412757\n",
      "Epoch:  74  Training Loss:  371.836  Training Accuracy:  0.41552\n",
      "Epoch:  75  Training Loss:  370.568  Training Accuracy:  0.417637\n",
      "Epoch:  76  Training Loss:  369.3  Training Accuracy:  0.419577\n",
      "Epoch:  77  Training Loss:  368.026  Training Accuracy:  0.422457\n",
      "Epoch:  78  Training Loss:  366.783  Training Accuracy:  0.424515\n",
      "Epoch:  79  Training Loss:  365.52  Training Accuracy:  0.426573\n",
      "Epoch:  80  Training Loss:  364.27  Training Accuracy:  0.428924\n",
      "Epoch:  81  Training Loss:  363.012  Training Accuracy:  0.431511\n",
      "Epoch:  82  Training Loss:  361.778  Training Accuracy:  0.433745\n",
      "Epoch:  83  Training Loss:  360.546  Training Accuracy:  0.436508\n",
      "Epoch:  84  Training Loss:  359.335  Training Accuracy:  0.438213\n",
      "Epoch:  85  Training Loss:  358.127  Training Accuracy:  0.440329\n",
      "Epoch:  86  Training Loss:  356.906  Training Accuracy:  0.441446\n",
      "Epoch:  87  Training Loss:  355.7  Training Accuracy:  0.443856\n",
      "Epoch:  88  Training Loss:  354.492  Training Accuracy:  0.446149\n",
      "Epoch:  89  Training Loss:  353.317  Training Accuracy:  0.448266\n",
      "Epoch:  90  Training Loss:  352.119  Training Accuracy:  0.450735\n",
      "Epoch:  91  Training Loss:  350.966  Training Accuracy:  0.452675\n",
      "Epoch:  92  Training Loss:  349.807  Training Accuracy:  0.454732\n",
      "Epoch:  93  Training Loss:  348.68  Training Accuracy:  0.456672\n",
      "Epoch:  94  Training Loss:  347.555  Training Accuracy:  0.458201\n",
      "Epoch:  95  Training Loss:  346.434  Training Accuracy:  0.460494\n",
      "Epoch:  96  Training Loss:  345.321  Training Accuracy:  0.462669\n",
      "Epoch:  97  Training Loss:  344.214  Training Accuracy:  0.464785\n",
      "Epoch:  98  Training Loss:  343.124  Training Accuracy:  0.466784\n",
      "Epoch:  99  Training Loss:  342.041  Training Accuracy:  0.468783\n",
      "Epoch:  100  Training Loss:  341.001  Training Accuracy:  0.471722\n",
      "Epoch:  101  Training Loss:  339.969  Training Accuracy:  0.474485\n",
      "Epoch:  102  Training Loss:  338.954  Training Accuracy:  0.477425\n",
      "Epoch:  103  Training Loss:  337.924  Training Accuracy:  0.47913\n",
      "Epoch:  104  Training Loss:  336.922  Training Accuracy:  0.481658\n",
      "Epoch:  105  Training Loss:  335.93  Training Accuracy:  0.483657\n",
      "Epoch:  106  Training Loss:  334.95  Training Accuracy:  0.485832\n",
      "Epoch:  107  Training Loss:  333.98  Training Accuracy:  0.487713\n",
      "Epoch:  108  Training Loss:  333.038  Training Accuracy:  0.489535\n",
      "Epoch:  109  Training Loss:  332.092  Training Accuracy:  0.491299\n",
      "Epoch:  110  Training Loss:  331.197  Training Accuracy:  0.493357\n",
      "Epoch:  111  Training Loss:  330.277  Training Accuracy:  0.494709\n",
      "Epoch:  112  Training Loss:  329.345  Training Accuracy:  0.496531\n",
      "Epoch:  113  Training Loss:  328.468  Training Accuracy:  0.498765\n",
      "Epoch:  114  Training Loss:  327.57  Training Accuracy:  0.501117\n",
      "Epoch:  115  Training Loss:  326.698  Training Accuracy:  0.503292\n",
      "Epoch:  116  Training Loss:  325.845  Training Accuracy:  0.505056\n",
      "Epoch:  117  Training Loss:  325.01  Training Accuracy:  0.507055\n",
      "Epoch:  118  Training Loss:  324.17  Training Accuracy:  0.509112\n",
      "Epoch:  119  Training Loss:  323.364  Training Accuracy:  0.510758\n",
      "Epoch:  120  Training Loss:  322.538  Training Accuracy:  0.512287\n",
      "Epoch:  121  Training Loss:  321.722  Training Accuracy:  0.513815\n",
      "Epoch:  122  Training Loss:  320.923  Training Accuracy:  0.515814\n",
      "Epoch:  123  Training Loss:  320.142  Training Accuracy:  0.517343\n",
      "Epoch:  124  Training Loss:  319.34  Training Accuracy:  0.5194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  318.558  Training Accuracy:  0.521517\n",
      "Epoch:  126  Training Loss:  317.794  Training Accuracy:  0.523163\n",
      "Epoch:  127  Training Loss:  317.009  Training Accuracy:  0.524985\n",
      "Epoch:  128  Training Loss:  316.241  Training Accuracy:  0.526572\n",
      "Epoch:  129  Training Loss:  315.487  Training Accuracy:  0.52769\n",
      "Epoch:  130  Training Loss:  314.753  Training Accuracy:  0.528513\n",
      "Epoch:  131  Training Loss:  314.046  Training Accuracy:  0.530394\n",
      "Epoch:  132  Training Loss:  313.299  Training Accuracy:  0.53204\n",
      "Epoch:  133  Training Loss:  312.635  Training Accuracy:  0.533862\n",
      "Epoch:  134  Training Loss:  311.932  Training Accuracy:  0.535567\n",
      "Epoch:  135  Training Loss:  311.282  Training Accuracy:  0.537155\n",
      "Epoch:  136  Training Loss:  310.563  Training Accuracy:  0.538624\n",
      "Epoch:  137  Training Loss:  309.937  Training Accuracy:  0.540211\n",
      "Epoch:  138  Training Loss:  309.28  Training Accuracy:  0.542152\n",
      "Epoch:  139  Training Loss:  308.6  Training Accuracy:  0.543621\n",
      "Epoch:  140  Training Loss:  307.99  Training Accuracy:  0.545444\n",
      "Epoch:  141  Training Loss:  307.348  Training Accuracy:  0.546678\n",
      "Epoch:  142  Training Loss:  306.704  Training Accuracy:  0.548148\n",
      "Epoch:  143  Training Loss:  306.13  Training Accuracy:  0.550323\n",
      "Epoch:  144  Training Loss:  305.491  Training Accuracy:  0.552028\n",
      "Epoch:  145  Training Loss:  304.878  Training Accuracy:  0.553145\n",
      "Epoch:  146  Training Loss:  304.291  Training Accuracy:  0.55438\n",
      "Epoch:  147  Training Loss:  303.722  Training Accuracy:  0.556202\n",
      "Epoch:  148  Training Loss:  303.12  Training Accuracy:  0.55826\n",
      "Epoch:  149  Training Loss:  302.552  Training Accuracy:  0.559612\n",
      "Epoch:  150  Training Loss:  301.96  Training Accuracy:  0.560964\n",
      "Epoch:  151  Training Loss:  301.408  Training Accuracy:  0.562022\n",
      "Epoch:  152  Training Loss:  300.839  Training Accuracy:  0.562963\n",
      "Epoch:  153  Training Loss:  300.312  Training Accuracy:  0.564374\n",
      "Epoch:  154  Training Loss:  299.739  Training Accuracy:  0.565667\n",
      "Epoch:  155  Training Loss:  299.197  Training Accuracy:  0.567431\n",
      "Epoch:  156  Training Loss:  298.643  Training Accuracy:  0.568724\n",
      "Epoch:  157  Training Loss:  298.159  Training Accuracy:  0.570194\n",
      "Epoch:  158  Training Loss:  297.56  Training Accuracy:  0.571487\n",
      "Epoch:  159  Training Loss:  297.082  Training Accuracy:  0.572487\n",
      "Epoch:  160  Training Loss:  296.538  Training Accuracy:  0.573663\n",
      "Epoch:  161  Training Loss:  296.075  Training Accuracy:  0.575426\n",
      "Epoch:  162  Training Loss:  295.537  Training Accuracy:  0.576543\n",
      "Epoch:  163  Training Loss:  295.062  Training Accuracy:  0.577895\n",
      "Epoch:  164  Training Loss:  294.564  Training Accuracy:  0.578954\n",
      "Epoch:  165  Training Loss:  294.094  Training Accuracy:  0.580658\n",
      "Epoch:  166  Training Loss:  293.642  Training Accuracy:  0.581834\n",
      "Epoch:  167  Training Loss:  293.131  Training Accuracy:  0.583715\n",
      "Epoch:  168  Training Loss:  292.653  Training Accuracy:  0.58495\n",
      "Epoch:  169  Training Loss:  292.223  Training Accuracy:  0.58642\n",
      "Epoch:  170  Training Loss:  291.717  Training Accuracy:  0.587184\n",
      "Epoch:  171  Training Loss:  291.288  Training Accuracy:  0.588419\n",
      "Epoch:  172  Training Loss:  290.828  Training Accuracy:  0.589771\n",
      "Epoch:  173  Training Loss:  290.401  Training Accuracy:  0.590829\n",
      "Epoch:  174  Training Loss:  289.942  Training Accuracy:  0.592063\n",
      "Epoch:  175  Training Loss:  289.534  Training Accuracy:  0.59318\n",
      "Epoch:  176  Training Loss:  289.085  Training Accuracy:  0.593827\n",
      "Epoch:  177  Training Loss:  288.669  Training Accuracy:  0.595062\n",
      "Epoch:  178  Training Loss:  288.237  Training Accuracy:  0.595767\n",
      "Epoch:  179  Training Loss:  287.815  Training Accuracy:  0.59759\n",
      "Epoch:  180  Training Loss:  287.417  Training Accuracy:  0.598471\n",
      "Epoch:  181  Training Loss:  287.057  Training Accuracy:  0.599588\n",
      "Epoch:  182  Training Loss:  286.589  Training Accuracy:  0.600764\n",
      "Epoch:  183  Training Loss:  286.202  Training Accuracy:  0.602704\n",
      "Epoch:  184  Training Loss:  285.791  Training Accuracy:  0.603998\n",
      "Epoch:  185  Training Loss:  285.423  Training Accuracy:  0.605173\n",
      "Epoch:  186  Training Loss:  285.047  Training Accuracy:  0.606467\n",
      "Epoch:  187  Training Loss:  284.659  Training Accuracy:  0.607643\n",
      "Epoch:  188  Training Loss:  284.266  Training Accuracy:  0.608877\n",
      "Epoch:  189  Training Loss:  283.892  Training Accuracy:  0.609818\n",
      "Epoch:  190  Training Loss:  283.508  Training Accuracy:  0.611758\n",
      "Epoch:  191  Training Loss:  283.157  Training Accuracy:  0.612934\n",
      "Epoch:  192  Training Loss:  282.789  Training Accuracy:  0.614109\n",
      "Epoch:  193  Training Loss:  282.417  Training Accuracy:  0.615226\n",
      "Epoch:  194  Training Loss:  282.035  Training Accuracy:  0.616108\n",
      "Epoch:  195  Training Loss:  281.689  Training Accuracy:  0.61746\n",
      "Epoch:  196  Training Loss:  281.338  Training Accuracy:  0.61846\n",
      "Epoch:  197  Training Loss:  280.983  Training Accuracy:  0.619459\n",
      "Epoch:  198  Training Loss:  280.603  Training Accuracy:  0.620106\n",
      "Epoch:  199  Training Loss:  280.259  Training Accuracy:  0.620988\n",
      "Epoch:  200  Training Loss:  279.896  Training Accuracy:  0.622163\n",
      "Epoch:  201  Training Loss:  279.545  Training Accuracy:  0.623339\n",
      "Epoch:  202  Training Loss:  279.263  Training Accuracy:  0.624104\n",
      "Epoch:  203  Training Loss:  278.894  Training Accuracy:  0.624691\n",
      "Epoch:  204  Training Loss:  278.548  Training Accuracy:  0.62575\n",
      "Epoch:  205  Training Loss:  278.229  Training Accuracy:  0.626279\n",
      "Epoch:  206  Training Loss:  277.882  Training Accuracy:  0.627454\n",
      "Epoch:  207  Training Loss:  277.575  Training Accuracy:  0.62863\n",
      "Epoch:  208  Training Loss:  277.234  Training Accuracy:  0.630159\n",
      "Epoch:  209  Training Loss:  276.915  Training Accuracy:  0.631217\n",
      "Epoch:  210  Training Loss:  276.575  Training Accuracy:  0.632275\n",
      "Epoch:  211  Training Loss:  276.311  Training Accuracy:  0.632981\n",
      "Epoch:  212  Training Loss:  275.95  Training Accuracy:  0.633921\n",
      "Epoch:  213  Training Loss:  275.631  Training Accuracy:  0.635097\n",
      "Epoch:  214  Training Loss:  275.324  Training Accuracy:  0.635567\n",
      "Epoch:  215  Training Loss:  275.055  Training Accuracy:  0.636273\n",
      "Epoch:  216  Training Loss:  274.698  Training Accuracy:  0.637155\n",
      "Epoch:  217  Training Loss:  274.432  Training Accuracy:  0.638095\n",
      "Epoch:  218  Training Loss:  274.112  Training Accuracy:  0.638624\n",
      "Epoch:  219  Training Loss:  273.848  Training Accuracy:  0.64027\n",
      "Epoch:  220  Training Loss:  273.513  Training Accuracy:  0.640976\n",
      "Epoch:  221  Training Loss:  273.225  Training Accuracy:  0.641564\n",
      "Epoch:  222  Training Loss:  272.93  Training Accuracy:  0.642328\n",
      "Epoch:  223  Training Loss:  272.689  Training Accuracy:  0.643327\n",
      "Epoch:  224  Training Loss:  272.365  Training Accuracy:  0.643915\n",
      "Epoch:  225  Training Loss:  272.093  Training Accuracy:  0.644856\n",
      "Epoch:  226  Training Loss:  271.814  Training Accuracy:  0.645738\n",
      "Epoch:  227  Training Loss:  271.509  Training Accuracy:  0.646502\n",
      "Epoch:  228  Training Loss:  271.261  Training Accuracy:  0.647443\n",
      "Epoch:  229  Training Loss:  270.95  Training Accuracy:  0.648619\n",
      "Epoch:  230  Training Loss:  270.699  Training Accuracy:  0.649618\n",
      "Epoch:  231  Training Loss:  270.458  Training Accuracy:  0.650676\n",
      "Epoch:  232  Training Loss:  270.117  Training Accuracy:  0.651264\n",
      "Epoch:  233  Training Loss:  269.847  Training Accuracy:  0.652322\n",
      "Epoch:  234  Training Loss:  269.628  Training Accuracy:  0.653498\n",
      "Epoch:  235  Training Loss:  269.331  Training Accuracy:  0.654439\n",
      "Epoch:  236  Training Loss:  269.058  Training Accuracy:  0.655732\n",
      "Epoch:  237  Training Loss:  268.776  Training Accuracy:  0.656437\n",
      "Epoch:  238  Training Loss:  268.537  Training Accuracy:  0.656908\n",
      "Epoch:  239  Training Loss:  268.274  Training Accuracy:  0.657966\n",
      "Epoch:  240  Training Loss:  268.004  Training Accuracy:  0.658789\n",
      "Epoch:  241  Training Loss:  267.713  Training Accuracy:  0.659436\n",
      "Epoch:  242  Training Loss:  267.448  Training Accuracy:  0.660024\n",
      "Epoch:  243  Training Loss:  267.21  Training Accuracy:  0.660435\n",
      "Epoch:  244  Training Loss:  266.948  Training Accuracy:  0.661317\n",
      "Epoch:  245  Training Loss:  266.676  Training Accuracy:  0.662258\n",
      "Epoch:  246  Training Loss:  266.443  Training Accuracy:  0.663081\n",
      "Epoch:  247  Training Loss:  266.138  Training Accuracy:  0.66361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  265.896  Training Accuracy:  0.664198\n",
      "Epoch:  249  Training Loss:  265.653  Training Accuracy:  0.665256\n",
      "Epoch:  250  Training Loss:  265.389  Training Accuracy:  0.666255\n",
      "Epoch:  251  Training Loss:  265.128  Training Accuracy:  0.666961\n",
      "Epoch:  252  Training Loss:  264.889  Training Accuracy:  0.668372\n",
      "Epoch:  253  Training Loss:  264.663  Training Accuracy:  0.669018\n",
      "Epoch:  254  Training Loss:  264.418  Training Accuracy:  0.669841\n",
      "Epoch:  255  Training Loss:  264.115  Training Accuracy:  0.67037\n",
      "Epoch:  256  Training Loss:  263.922  Training Accuracy:  0.671487\n",
      "Epoch:  257  Training Loss:  263.658  Training Accuracy:  0.67231\n",
      "Epoch:  258  Training Loss:  263.448  Training Accuracy:  0.673075\n",
      "Epoch:  259  Training Loss:  263.219  Training Accuracy:  0.673545\n",
      "Epoch:  260  Training Loss:  262.968  Training Accuracy:  0.674486\n",
      "Epoch:  261  Training Loss:  262.682  Training Accuracy:  0.675015\n",
      "Epoch:  262  Training Loss:  262.478  Training Accuracy:  0.675838\n",
      "Epoch:  263  Training Loss:  262.234  Training Accuracy:  0.676837\n",
      "Epoch:  264  Training Loss:  261.993  Training Accuracy:  0.677072\n",
      "Epoch:  265  Training Loss:  261.745  Training Accuracy:  0.678013\n",
      "Epoch:  266  Training Loss:  261.508  Training Accuracy:  0.678895\n",
      "Epoch:  267  Training Loss:  261.215  Training Accuracy:  0.679777\n",
      "Epoch:  268  Training Loss:  261.04  Training Accuracy:  0.680482\n",
      "Epoch:  269  Training Loss:  260.788  Training Accuracy:  0.681423\n",
      "Epoch:  270  Training Loss:  260.535  Training Accuracy:  0.682305\n",
      "Epoch:  271  Training Loss:  260.329  Training Accuracy:  0.682657\n",
      "Epoch:  272  Training Loss:  260.076  Training Accuracy:  0.683363\n",
      "Epoch:  273  Training Loss:  259.862  Training Accuracy:  0.683951\n",
      "Epoch:  274  Training Loss:  259.62  Training Accuracy:  0.684362\n",
      "Epoch:  275  Training Loss:  259.334  Training Accuracy:  0.68495\n",
      "Epoch:  276  Training Loss:  259.136  Training Accuracy:  0.686008\n",
      "Epoch:  277  Training Loss:  258.937  Training Accuracy:  0.686244\n",
      "Epoch:  278  Training Loss:  258.631  Training Accuracy:  0.68689\n",
      "Epoch:  279  Training Loss:  258.413  Training Accuracy:  0.68789\n",
      "Epoch:  280  Training Loss:  258.19  Training Accuracy:  0.688654\n",
      "Epoch:  281  Training Loss:  257.946  Training Accuracy:  0.689242\n",
      "Epoch:  282  Training Loss:  257.739  Training Accuracy:  0.68983\n",
      "Epoch:  283  Training Loss:  257.507  Training Accuracy:  0.690065\n",
      "Epoch:  284  Training Loss:  257.25  Training Accuracy:  0.690418\n",
      "Epoch:  285  Training Loss:  256.996  Training Accuracy:  0.690947\n",
      "Epoch:  286  Training Loss:  256.842  Training Accuracy:  0.691652\n",
      "Epoch:  287  Training Loss:  256.547  Training Accuracy:  0.692299\n",
      "Epoch:  288  Training Loss:  256.35  Training Accuracy:  0.692828\n",
      "Epoch:  289  Training Loss:  256.139  Training Accuracy:  0.693239\n",
      "Epoch:  290  Training Loss:  255.9  Training Accuracy:  0.694004\n",
      "Epoch:  291  Training Loss:  255.693  Training Accuracy:  0.694298\n",
      "Epoch:  292  Training Loss:  255.511  Training Accuracy:  0.694827\n",
      "Epoch:  293  Training Loss:  255.247  Training Accuracy:  0.695062\n",
      "Epoch:  294  Training Loss:  255.055  Training Accuracy:  0.695415\n",
      "Epoch:  295  Training Loss:  254.819  Training Accuracy:  0.695944\n",
      "Epoch:  296  Training Loss:  254.581  Training Accuracy:  0.696355\n",
      "Epoch:  297  Training Loss:  254.38  Training Accuracy:  0.696767\n",
      "Epoch:  298  Training Loss:  254.23  Training Accuracy:  0.697061\n",
      "Epoch:  299  Training Loss:  253.917  Training Accuracy:  0.697472\n",
      "Epoch:  300  Training Loss:  253.727  Training Accuracy:  0.69853\n",
      "Epoch:  301  Training Loss:  253.491  Training Accuracy:  0.699118\n",
      "Epoch:  302  Training Loss:  253.266  Training Accuracy:  0.699941\n",
      "Epoch:  303  Training Loss:  253.039  Training Accuracy:  0.700647\n",
      "Epoch:  304  Training Loss:  252.886  Training Accuracy:  0.701235\n",
      "Epoch:  305  Training Loss:  252.612  Training Accuracy:  0.701529\n",
      "Epoch:  306  Training Loss:  252.398  Training Accuracy:  0.702058\n",
      "Epoch:  307  Training Loss:  252.198  Training Accuracy:  0.702763\n",
      "Epoch:  308  Training Loss:  251.969  Training Accuracy:  0.703351\n",
      "Epoch:  309  Training Loss:  251.754  Training Accuracy:  0.703998\n",
      "Epoch:  310  Training Loss:  251.546  Training Accuracy:  0.704586\n",
      "Epoch:  311  Training Loss:  251.328  Training Accuracy:  0.705174\n",
      "Epoch:  312  Training Loss:  251.084  Training Accuracy:  0.705467\n",
      "Epoch:  313  Training Loss:  250.894  Training Accuracy:  0.706114\n",
      "Epoch:  314  Training Loss:  250.671  Training Accuracy:  0.706643\n",
      "Epoch:  315  Training Loss:  250.442  Training Accuracy:  0.707172\n",
      "Epoch:  316  Training Loss:  250.24  Training Accuracy:  0.707466\n",
      "Epoch:  317  Training Loss:  249.963  Training Accuracy:  0.708113\n",
      "Epoch:  318  Training Loss:  249.773  Training Accuracy:  0.70876\n",
      "Epoch:  319  Training Loss:  249.59  Training Accuracy:  0.709171\n",
      "Epoch:  320  Training Loss:  249.31  Training Accuracy:  0.709994\n",
      "Epoch:  321  Training Loss:  249.104  Training Accuracy:  0.710347\n",
      "Epoch:  322  Training Loss:  248.903  Training Accuracy:  0.710876\n",
      "Epoch:  323  Training Loss:  248.649  Training Accuracy:  0.71117\n",
      "Epoch:  324  Training Loss:  248.448  Training Accuracy:  0.71164\n",
      "Epoch:  325  Training Loss:  248.204  Training Accuracy:  0.712111\n",
      "Epoch:  326  Training Loss:  247.97  Training Accuracy:  0.712581\n",
      "Epoch:  327  Training Loss:  247.731  Training Accuracy:  0.712934\n",
      "Epoch:  328  Training Loss:  247.576  Training Accuracy:  0.713286\n",
      "Epoch:  329  Training Loss:  247.28  Training Accuracy:  0.713757\n",
      "Epoch:  330  Training Loss:  247.094  Training Accuracy:  0.714639\n",
      "Epoch:  331  Training Loss:  246.831  Training Accuracy:  0.715168\n",
      "Epoch:  332  Training Loss:  246.667  Training Accuracy:  0.715873\n",
      "Epoch:  333  Training Loss:  246.365  Training Accuracy:  0.716579\n",
      "Epoch:  334  Training Loss:  246.218  Training Accuracy:  0.716931\n",
      "Epoch:  335  Training Loss:  245.914  Training Accuracy:  0.717754\n",
      "Epoch:  336  Training Loss:  245.774  Training Accuracy:  0.718636\n",
      "Epoch:  337  Training Loss:  245.495  Training Accuracy:  0.719342\n",
      "Epoch:  338  Training Loss:  245.283  Training Accuracy:  0.719871\n",
      "Epoch:  339  Training Loss:  245.015  Training Accuracy:  0.720047\n",
      "Epoch:  340  Training Loss:  244.883  Training Accuracy:  0.720576\n",
      "Epoch:  341  Training Loss:  244.552  Training Accuracy:  0.721105\n",
      "Epoch:  342  Training Loss:  244.393  Training Accuracy:  0.721576\n",
      "Epoch:  343  Training Loss:  244.112  Training Accuracy:  0.722222\n",
      "Epoch:  344  Training Loss:  243.973  Training Accuracy:  0.722751\n",
      "Epoch:  345  Training Loss:  243.716  Training Accuracy:  0.723045\n",
      "Epoch:  346  Training Loss:  243.499  Training Accuracy:  0.723104\n",
      "Epoch:  347  Training Loss:  243.275  Training Accuracy:  0.723457\n",
      "Epoch:  348  Training Loss:  243.08  Training Accuracy:  0.724045\n",
      "Epoch:  349  Training Loss:  242.83  Training Accuracy:  0.724692\n",
      "Epoch:  350  Training Loss:  242.599  Training Accuracy:  0.725103\n",
      "Epoch:  351  Training Loss:  242.398  Training Accuracy:  0.725632\n",
      "Epoch:  352  Training Loss:  242.182  Training Accuracy:  0.726161\n",
      "Epoch:  353  Training Loss:  241.955  Training Accuracy:  0.726338\n",
      "Epoch:  354  Training Loss:  241.72  Training Accuracy:  0.726749\n",
      "Epoch:  355  Training Loss:  241.475  Training Accuracy:  0.727102\n",
      "Epoch:  356  Training Loss:  241.275  Training Accuracy:  0.727749\n",
      "Epoch:  357  Training Loss:  241.029  Training Accuracy:  0.727866\n",
      "Epoch:  358  Training Loss:  240.792  Training Accuracy:  0.728336\n",
      "Epoch:  359  Training Loss:  240.574  Training Accuracy:  0.728454\n",
      "Epoch:  360  Training Loss:  240.36  Training Accuracy:  0.729042\n",
      "Epoch:  361  Training Loss:  240.145  Training Accuracy:  0.729453\n",
      "Epoch:  362  Training Loss:  239.875  Training Accuracy:  0.730159\n",
      "Epoch:  363  Training Loss:  239.693  Training Accuracy:  0.730453\n",
      "Epoch:  364  Training Loss:  239.469  Training Accuracy:  0.731041\n",
      "Epoch:  365  Training Loss:  239.243  Training Accuracy:  0.731335\n",
      "Epoch:  366  Training Loss:  238.981  Training Accuracy:  0.731629\n",
      "Epoch:  367  Training Loss:  238.752  Training Accuracy:  0.732452\n",
      "Epoch:  368  Training Loss:  238.528  Training Accuracy:  0.732804\n",
      "Epoch:  369  Training Loss:  238.296  Training Accuracy:  0.73351\n",
      "Epoch:  370  Training Loss:  238.042  Training Accuracy:  0.734039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  237.847  Training Accuracy:  0.734333\n",
      "Epoch:  372  Training Loss:  237.59  Training Accuracy:  0.734451\n",
      "Epoch:  373  Training Loss:  237.327  Training Accuracy:  0.734921\n",
      "Epoch:  374  Training Loss:  237.133  Training Accuracy:  0.735274\n",
      "Epoch:  375  Training Loss:  236.866  Training Accuracy:  0.735509\n",
      "Epoch:  376  Training Loss:  236.636  Training Accuracy:  0.736038\n",
      "Epoch:  377  Training Loss:  236.4  Training Accuracy:  0.736449\n",
      "Epoch:  378  Training Loss:  236.168  Training Accuracy:  0.73692\n",
      "Epoch:  379  Training Loss:  235.927  Training Accuracy:  0.737684\n",
      "Epoch:  380  Training Loss:  235.719  Training Accuracy:  0.738389\n",
      "Epoch:  381  Training Loss:  235.436  Training Accuracy:  0.73886\n",
      "Epoch:  382  Training Loss:  235.233  Training Accuracy:  0.739389\n",
      "Epoch:  383  Training Loss:  234.949  Training Accuracy:  0.739977\n",
      "Epoch:  384  Training Loss:  234.748  Training Accuracy:  0.740212\n",
      "Epoch:  385  Training Loss:  234.496  Training Accuracy:  0.740623\n",
      "Epoch:  386  Training Loss:  234.302  Training Accuracy:  0.741094\n",
      "Epoch:  387  Training Loss:  233.994  Training Accuracy:  0.741623\n",
      "Epoch:  388  Training Loss:  233.816  Training Accuracy:  0.74174\n",
      "Epoch:  389  Training Loss:  233.538  Training Accuracy:  0.742269\n",
      "Epoch:  390  Training Loss:  233.353  Training Accuracy:  0.742857\n",
      "Epoch:  391  Training Loss:  233.026  Training Accuracy:  0.743269\n",
      "Epoch:  392  Training Loss:  232.84  Training Accuracy:  0.743857\n",
      "Epoch:  393  Training Loss:  232.603  Training Accuracy:  0.744033\n",
      "Epoch:  394  Training Loss:  232.374  Training Accuracy:  0.744503\n",
      "Epoch:  395  Training Loss:  232.085  Training Accuracy:  0.744915\n",
      "Epoch:  396  Training Loss:  231.898  Training Accuracy:  0.745385\n",
      "Epoch:  397  Training Loss:  231.621  Training Accuracy:  0.746032\n",
      "Epoch:  398  Training Loss:  231.393  Training Accuracy:  0.746091\n",
      "Epoch:  399  Training Loss:  231.156  Training Accuracy:  0.74662\n",
      "Epoch:  400  Training Loss:  230.896  Training Accuracy:  0.746914\n",
      "Epoch:  401  Training Loss:  230.66  Training Accuracy:  0.747443\n",
      "Epoch:  402  Training Loss:  230.386  Training Accuracy:  0.747854\n",
      "Epoch:  403  Training Loss:  230.16  Training Accuracy:  0.74809\n",
      "Epoch:  404  Training Loss:  229.926  Training Accuracy:  0.748266\n",
      "Epoch:  405  Training Loss:  229.678  Training Accuracy:  0.748854\n",
      "Epoch:  406  Training Loss:  229.42  Training Accuracy:  0.749148\n",
      "Epoch:  407  Training Loss:  229.179  Training Accuracy:  0.749677\n",
      "Epoch:  408  Training Loss:  228.913  Training Accuracy:  0.750088\n",
      "Epoch:  409  Training Loss:  228.728  Training Accuracy:  0.750265\n",
      "Epoch:  410  Training Loss:  228.444  Training Accuracy:  0.750559\n",
      "Epoch:  411  Training Loss:  228.206  Training Accuracy:  0.751264\n",
      "Epoch:  412  Training Loss:  227.952  Training Accuracy:  0.751852\n",
      "Epoch:  413  Training Loss:  227.76  Training Accuracy:  0.75244\n",
      "Epoch:  414  Training Loss:  227.494  Training Accuracy:  0.752675\n",
      "Epoch:  415  Training Loss:  227.255  Training Accuracy:  0.753028\n",
      "Epoch:  416  Training Loss:  226.997  Training Accuracy:  0.753616\n",
      "Epoch:  417  Training Loss:  226.777  Training Accuracy:  0.754086\n",
      "Epoch:  418  Training Loss:  226.52  Training Accuracy:  0.75438\n",
      "Epoch:  419  Training Loss:  226.265  Training Accuracy:  0.754615\n",
      "Epoch:  420  Training Loss:  225.995  Training Accuracy:  0.754968\n",
      "Epoch:  421  Training Loss:  225.808  Training Accuracy:  0.755262\n",
      "Epoch:  422  Training Loss:  225.54  Training Accuracy:  0.755732\n",
      "Epoch:  423  Training Loss:  225.277  Training Accuracy:  0.756144\n",
      "Epoch:  424  Training Loss:  225.031  Training Accuracy:  0.756438\n",
      "Epoch:  425  Training Loss:  224.802  Training Accuracy:  0.756732\n",
      "Epoch:  426  Training Loss:  224.556  Training Accuracy:  0.757202\n",
      "Epoch:  427  Training Loss:  224.278  Training Accuracy:  0.757378\n",
      "Epoch:  428  Training Loss:  224.057  Training Accuracy:  0.757849\n",
      "Epoch:  429  Training Loss:  223.782  Training Accuracy:  0.758142\n",
      "Epoch:  430  Training Loss:  223.574  Training Accuracy:  0.758201\n",
      "Epoch:  431  Training Loss:  223.283  Training Accuracy:  0.75873\n",
      "Epoch:  432  Training Loss:  223.084  Training Accuracy:  0.75926\n",
      "Epoch:  433  Training Loss:  222.786  Training Accuracy:  0.759436\n",
      "Epoch:  434  Training Loss:  222.582  Training Accuracy:  0.759612\n",
      "Epoch:  435  Training Loss:  222.301  Training Accuracy:  0.760024\n",
      "Epoch:  436  Training Loss:  222.103  Training Accuracy:  0.760376\n",
      "Epoch:  437  Training Loss:  221.805  Training Accuracy:  0.760435\n",
      "Epoch:  438  Training Loss:  221.625  Training Accuracy:  0.760906\n",
      "Epoch:  439  Training Loss:  221.337  Training Accuracy:  0.761082\n",
      "Epoch:  440  Training Loss:  221.144  Training Accuracy:  0.761258\n",
      "Epoch:  441  Training Loss:  220.856  Training Accuracy:  0.761435\n",
      "Epoch:  442  Training Loss:  220.656  Training Accuracy:  0.762023\n",
      "Epoch:  443  Training Loss:  220.367  Training Accuracy:  0.762375\n",
      "Epoch:  444  Training Loss:  220.176  Training Accuracy:  0.762787\n",
      "Epoch:  445  Training Loss:  219.866  Training Accuracy:  0.762963\n",
      "Epoch:  446  Training Loss:  219.683  Training Accuracy:  0.763551\n",
      "Epoch:  447  Training Loss:  219.435  Training Accuracy:  0.76361\n",
      "Epoch:  448  Training Loss:  219.185  Training Accuracy:  0.763786\n",
      "Epoch:  449  Training Loss:  218.954  Training Accuracy:  0.764139\n",
      "Epoch:  450  Training Loss:  218.705  Training Accuracy:  0.764315\n",
      "Epoch:  451  Training Loss:  218.452  Training Accuracy:  0.764433\n",
      "Epoch:  452  Training Loss:  218.213  Training Accuracy:  0.764903\n",
      "Epoch:  453  Training Loss:  218.002  Training Accuracy:  0.76508\n",
      "Epoch:  454  Training Loss:  217.731  Training Accuracy:  0.76555\n",
      "Epoch:  455  Training Loss:  217.477  Training Accuracy:  0.765667\n",
      "Epoch:  456  Training Loss:  217.285  Training Accuracy:  0.76602\n",
      "Epoch:  457  Training Loss:  216.994  Training Accuracy:  0.766432\n",
      "Epoch:  458  Training Loss:  216.788  Training Accuracy:  0.76702\n",
      "Epoch:  459  Training Loss:  216.539  Training Accuracy:  0.767137\n",
      "Epoch:  460  Training Loss:  216.275  Training Accuracy:  0.767608\n",
      "Epoch:  461  Training Loss:  216.022  Training Accuracy:  0.767843\n",
      "Epoch:  462  Training Loss:  215.771  Training Accuracy:  0.768137\n",
      "Epoch:  463  Training Loss:  215.559  Training Accuracy:  0.768372\n",
      "Epoch:  464  Training Loss:  215.312  Training Accuracy:  0.768548\n",
      "Epoch:  465  Training Loss:  215.059  Training Accuracy:  0.768548\n",
      "Epoch:  466  Training Loss:  214.825  Training Accuracy:  0.768901\n",
      "Epoch:  467  Training Loss:  214.556  Training Accuracy:  0.769136\n",
      "Epoch:  468  Training Loss:  214.344  Training Accuracy:  0.769548\n",
      "Epoch:  469  Training Loss:  214.11  Training Accuracy:  0.769842\n",
      "Epoch:  470  Training Loss:  213.819  Training Accuracy:  0.770194\n",
      "Epoch:  471  Training Loss:  213.585  Training Accuracy:  0.770606\n",
      "Epoch:  472  Training Loss:  213.386  Training Accuracy:  0.7709\n",
      "Epoch:  473  Training Loss:  213.093  Training Accuracy:  0.771311\n",
      "Epoch:  474  Training Loss:  212.886  Training Accuracy:  0.771488\n",
      "Epoch:  475  Training Loss:  212.651  Training Accuracy:  0.77184\n",
      "Epoch:  476  Training Loss:  212.394  Training Accuracy:  0.772076\n",
      "Epoch:  477  Training Loss:  212.138  Training Accuracy:  0.772252\n",
      "Epoch:  478  Training Loss:  211.928  Training Accuracy:  0.772722\n",
      "Epoch:  479  Training Loss:  211.683  Training Accuracy:  0.77284\n",
      "Epoch:  480  Training Loss:  211.427  Training Accuracy:  0.772899\n",
      "Epoch:  481  Training Loss:  211.194  Training Accuracy:  0.773193\n",
      "Epoch:  482  Training Loss:  210.96  Training Accuracy:  0.77331\n",
      "Epoch:  483  Training Loss:  210.723  Training Accuracy:  0.773957\n",
      "Epoch:  484  Training Loss:  210.488  Training Accuracy:  0.774251\n",
      "Epoch:  485  Training Loss:  210.227  Training Accuracy:  0.774368\n",
      "Epoch:  486  Training Loss:  210.025  Training Accuracy:  0.77478\n",
      "Epoch:  487  Training Loss:  209.789  Training Accuracy:  0.775074\n",
      "Epoch:  488  Training Loss:  209.536  Training Accuracy:  0.775309\n",
      "Epoch:  489  Training Loss:  209.31  Training Accuracy:  0.775779\n",
      "Epoch:  490  Training Loss:  209.052  Training Accuracy:  0.776014\n",
      "Epoch:  491  Training Loss:  208.816  Training Accuracy:  0.776249\n",
      "Epoch:  492  Training Loss:  208.571  Training Accuracy:  0.776602\n",
      "Epoch:  493  Training Loss:  208.362  Training Accuracy:  0.777014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  208.096  Training Accuracy:  0.777308\n",
      "Epoch:  495  Training Loss:  207.88  Training Accuracy:  0.777367\n",
      "Epoch:  496  Training Loss:  207.648  Training Accuracy:  0.777896\n",
      "Epoch:  497  Training Loss:  207.404  Training Accuracy:  0.778131\n",
      "Epoch:  498  Training Loss:  207.139  Training Accuracy:  0.778719\n",
      "Epoch:  499  Training Loss:  206.953  Training Accuracy:  0.778954\n",
      "Epoch:  500  Training Loss:  206.689  Training Accuracy:  0.779307\n",
      "Epoch:  501  Training Loss:  206.499  Training Accuracy:  0.779483\n",
      "Epoch:  502  Training Loss:  206.225  Training Accuracy:  0.779777\n",
      "Epoch:  503  Training Loss:  205.978  Training Accuracy:  0.780071\n",
      "Epoch:  504  Training Loss:  205.813  Training Accuracy:  0.780424\n",
      "Epoch:  505  Training Loss:  205.533  Training Accuracy:  0.780659\n",
      "Epoch:  506  Training Loss:  205.302  Training Accuracy:  0.781129\n",
      "Epoch:  507  Training Loss:  205.12  Training Accuracy:  0.781482\n",
      "Epoch:  508  Training Loss:  204.883  Training Accuracy:  0.781541\n",
      "Epoch:  509  Training Loss:  204.643  Training Accuracy:  0.781952\n",
      "Epoch:  510  Training Loss:  204.403  Training Accuracy:  0.782128\n",
      "Epoch:  511  Training Loss:  204.207  Training Accuracy:  0.782364\n",
      "Epoch:  512  Training Loss:  203.947  Training Accuracy:  0.782599\n",
      "Epoch:  513  Training Loss:  203.713  Training Accuracy:  0.782834\n",
      "Epoch:  514  Training Loss:  203.546  Training Accuracy:  0.782893\n",
      "Epoch:  515  Training Loss:  203.261  Training Accuracy:  0.783245\n",
      "Epoch:  516  Training Loss:  203.026  Training Accuracy:  0.783539\n",
      "Epoch:  517  Training Loss:  202.803  Training Accuracy:  0.783657\n",
      "Epoch:  518  Training Loss:  202.539  Training Accuracy:  0.78401\n",
      "Epoch:  519  Training Loss:  202.338  Training Accuracy:  0.784127\n",
      "Epoch:  520  Training Loss:  202.103  Training Accuracy:  0.784362\n",
      "Epoch:  521  Training Loss:  201.86  Training Accuracy:  0.785009\n",
      "Epoch:  522  Training Loss:  201.627  Training Accuracy:  0.785127\n",
      "Epoch:  523  Training Loss:  201.439  Training Accuracy:  0.785597\n",
      "Epoch:  524  Training Loss:  201.173  Training Accuracy:  0.785891\n",
      "Epoch:  525  Training Loss:  200.964  Training Accuracy:  0.786067\n",
      "Epoch:  526  Training Loss:  200.706  Training Accuracy:  0.786361\n",
      "Epoch:  527  Training Loss:  200.513  Training Accuracy:  0.786773\n",
      "Epoch:  528  Training Loss:  200.25  Training Accuracy:  0.786832\n",
      "Epoch:  529  Training Loss:  200.048  Training Accuracy:  0.787067\n",
      "Epoch:  530  Training Loss:  199.807  Training Accuracy:  0.787831\n",
      "Epoch:  531  Training Loss:  199.604  Training Accuracy:  0.788007\n",
      "Epoch:  532  Training Loss:  199.367  Training Accuracy:  0.788243\n",
      "Epoch:  533  Training Loss:  199.107  Training Accuracy:  0.788419\n",
      "Epoch:  534  Training Loss:  198.893  Training Accuracy:  0.788713\n",
      "Epoch:  535  Training Loss:  198.669  Training Accuracy:  0.789007\n",
      "Epoch:  536  Training Loss:  198.429  Training Accuracy:  0.789477\n",
      "Epoch:  537  Training Loss:  198.198  Training Accuracy:  0.789947\n",
      "Epoch:  538  Training Loss:  197.953  Training Accuracy:  0.790065\n",
      "Epoch:  539  Training Loss:  197.75  Training Accuracy:  0.7903\n",
      "Epoch:  540  Training Loss:  197.458  Training Accuracy:  0.790418\n",
      "Epoch:  541  Training Loss:  197.298  Training Accuracy:  0.790712\n",
      "Epoch:  542  Training Loss:  197.016  Training Accuracy:  0.790829\n",
      "Epoch:  543  Training Loss:  196.853  Training Accuracy:  0.790947\n",
      "Epoch:  544  Training Loss:  196.569  Training Accuracy:  0.790947\n",
      "Epoch:  545  Training Loss:  196.351  Training Accuracy:  0.791358\n",
      "Epoch:  546  Training Loss:  196.151  Training Accuracy:  0.791358\n",
      "Epoch:  547  Training Loss:  195.92  Training Accuracy:  0.791711\n",
      "Epoch:  548  Training Loss:  195.67  Training Accuracy:  0.791946\n",
      "Epoch:  549  Training Loss:  195.486  Training Accuracy:  0.792593\n",
      "Epoch:  550  Training Loss:  195.222  Training Accuracy:  0.79324\n",
      "Epoch:  551  Training Loss:  195.009  Training Accuracy:  0.793475\n",
      "Epoch:  552  Training Loss:  194.817  Training Accuracy:  0.793651\n",
      "Epoch:  553  Training Loss:  194.547  Training Accuracy:  0.793769\n",
      "Epoch:  554  Training Loss:  194.363  Training Accuracy:  0.79418\n",
      "Epoch:  555  Training Loss:  194.133  Training Accuracy:  0.794651\n",
      "Epoch:  556  Training Loss:  193.919  Training Accuracy:  0.795121\n",
      "Epoch:  557  Training Loss:  193.664  Training Accuracy:  0.795356\n",
      "Epoch:  558  Training Loss:  193.44  Training Accuracy:  0.795532\n",
      "Epoch:  559  Training Loss:  193.223  Training Accuracy:  0.795826\n",
      "Epoch:  560  Training Loss:  192.996  Training Accuracy:  0.795885\n",
      "Epoch:  561  Training Loss:  192.785  Training Accuracy:  0.796179\n",
      "Epoch:  562  Training Loss:  192.584  Training Accuracy:  0.796355\n",
      "Epoch:  563  Training Loss:  192.313  Training Accuracy:  0.796708\n",
      "Epoch:  564  Training Loss:  192.122  Training Accuracy:  0.797061\n",
      "Epoch:  565  Training Loss:  191.883  Training Accuracy:  0.797355\n",
      "Epoch:  566  Training Loss:  191.658  Training Accuracy:  0.79759\n",
      "Epoch:  567  Training Loss:  191.429  Training Accuracy:  0.797825\n",
      "Epoch:  568  Training Loss:  191.195  Training Accuracy:  0.79806\n",
      "Epoch:  569  Training Loss:  191.001  Training Accuracy:  0.798178\n",
      "Epoch:  570  Training Loss:  190.744  Training Accuracy:  0.798472\n",
      "Epoch:  571  Training Loss:  190.53  Training Accuracy:  0.798589\n",
      "Epoch:  572  Training Loss:  190.297  Training Accuracy:  0.798648\n",
      "Epoch:  573  Training Loss:  190.087  Training Accuracy:  0.798766\n",
      "Epoch:  574  Training Loss:  189.852  Training Accuracy:  0.798942\n",
      "Epoch:  575  Training Loss:  189.606  Training Accuracy:  0.799295\n",
      "Epoch:  576  Training Loss:  189.384  Training Accuracy:  0.799648\n",
      "Epoch:  577  Training Loss:  189.148  Training Accuracy:  0.800059\n",
      "Epoch:  578  Training Loss:  188.944  Training Accuracy:  0.800177\n",
      "Epoch:  579  Training Loss:  188.689  Training Accuracy:  0.800647\n",
      "Epoch:  580  Training Loss:  188.449  Training Accuracy:  0.800588\n",
      "Epoch:  581  Training Loss:  188.243  Training Accuracy:  0.801059\n",
      "Epoch:  582  Training Loss:  188.014  Training Accuracy:  0.801294\n",
      "Epoch:  583  Training Loss:  187.795  Training Accuracy:  0.801529\n",
      "Epoch:  584  Training Loss:  187.533  Training Accuracy:  0.801588\n",
      "Epoch:  585  Training Loss:  187.327  Training Accuracy:  0.80194\n",
      "Epoch:  586  Training Loss:  187.069  Training Accuracy:  0.802117\n",
      "Epoch:  587  Training Loss:  186.876  Training Accuracy:  0.802411\n",
      "Epoch:  588  Training Loss:  186.623  Training Accuracy:  0.802469\n",
      "Epoch:  589  Training Loss:  186.42  Training Accuracy:  0.802705\n",
      "Epoch:  590  Training Loss:  186.167  Training Accuracy:  0.802999\n",
      "Epoch:  591  Training Loss:  185.925  Training Accuracy:  0.803469\n",
      "Epoch:  592  Training Loss:  185.703  Training Accuracy:  0.803528\n",
      "Epoch:  593  Training Loss:  185.485  Training Accuracy:  0.803645\n",
      "Epoch:  594  Training Loss:  185.227  Training Accuracy:  0.803822\n",
      "Epoch:  595  Training Loss:  185.016  Training Accuracy:  0.803939\n",
      "Epoch:  596  Training Loss:  184.769  Training Accuracy:  0.804292\n",
      "Epoch:  597  Training Loss:  184.55  Training Accuracy:  0.804468\n",
      "Epoch:  598  Training Loss:  184.303  Training Accuracy:  0.804821\n",
      "Epoch:  599  Training Loss:  184.081  Training Accuracy:  0.804997\n",
      "Epoch:  600  Training Loss:  183.884  Training Accuracy:  0.805174\n",
      "Epoch:  601  Training Loss:  183.625  Training Accuracy:  0.805585\n",
      "Epoch:  602  Training Loss:  183.402  Training Accuracy:  0.805879\n",
      "Epoch:  603  Training Loss:  183.191  Training Accuracy:  0.806114\n",
      "Epoch:  604  Training Loss:  182.922  Training Accuracy:  0.806173\n",
      "Epoch:  605  Training Loss:  182.726  Training Accuracy:  0.806408\n",
      "Epoch:  606  Training Loss:  182.496  Training Accuracy:  0.806643\n",
      "Epoch:  607  Training Loss:  182.279  Training Accuracy:  0.806761\n",
      "Epoch:  608  Training Loss:  182.022  Training Accuracy:  0.80682\n",
      "Epoch:  609  Training Loss:  181.83  Training Accuracy:  0.806996\n",
      "Epoch:  610  Training Loss:  181.578  Training Accuracy:  0.807114\n",
      "Epoch:  611  Training Loss:  181.405  Training Accuracy:  0.807349\n",
      "Epoch:  612  Training Loss:  181.141  Training Accuracy:  0.807643\n",
      "Epoch:  613  Training Loss:  180.91  Training Accuracy:  0.807878\n",
      "Epoch:  614  Training Loss:  180.692  Training Accuracy:  0.807996\n",
      "Epoch:  615  Training Loss:  180.462  Training Accuracy:  0.808348\n",
      "Epoch:  616  Training Loss:  180.256  Training Accuracy:  0.808525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  180.009  Training Accuracy:  0.808642\n",
      "Epoch:  618  Training Loss:  179.799  Training Accuracy:  0.80876\n",
      "Epoch:  619  Training Loss:  179.57  Training Accuracy:  0.808877\n",
      "Epoch:  620  Training Loss:  179.353  Training Accuracy:  0.809054\n",
      "Epoch:  621  Training Loss:  179.106  Training Accuracy:  0.809348\n",
      "Epoch:  622  Training Loss:  178.898  Training Accuracy:  0.809583\n",
      "Epoch:  623  Training Loss:  178.705  Training Accuracy:  0.810288\n",
      "Epoch:  624  Training Loss:  178.444  Training Accuracy:  0.81023\n",
      "Epoch:  625  Training Loss:  178.231  Training Accuracy:  0.810406\n",
      "Epoch:  626  Training Loss:  178.002  Training Accuracy:  0.810641\n",
      "Epoch:  627  Training Loss:  177.808  Training Accuracy:  0.810994\n",
      "Epoch:  628  Training Loss:  177.563  Training Accuracy:  0.811405\n",
      "Epoch:  629  Training Loss:  177.359  Training Accuracy:  0.81164\n",
      "Epoch:  630  Training Loss:  177.127  Training Accuracy:  0.811817\n",
      "Epoch:  631  Training Loss:  176.965  Training Accuracy:  0.812287\n",
      "Epoch:  632  Training Loss:  176.706  Training Accuracy:  0.812464\n",
      "Epoch:  633  Training Loss:  176.507  Training Accuracy:  0.812699\n",
      "Epoch:  634  Training Loss:  176.27  Training Accuracy:  0.812757\n",
      "Epoch:  635  Training Loss:  176.072  Training Accuracy:  0.812934\n",
      "Epoch:  636  Training Loss:  175.847  Training Accuracy:  0.81311\n",
      "Epoch:  637  Training Loss:  175.69  Training Accuracy:  0.812993\n",
      "Epoch:  638  Training Loss:  175.405  Training Accuracy:  0.81311\n",
      "Epoch:  639  Training Loss:  175.249  Training Accuracy:  0.813345\n",
      "Epoch:  640  Training Loss:  174.954  Training Accuracy:  0.813698\n",
      "Epoch:  641  Training Loss:  174.801  Training Accuracy:  0.813757\n",
      "Epoch:  642  Training Loss:  174.558  Training Accuracy:  0.814227\n",
      "Epoch:  643  Training Loss:  174.384  Training Accuracy:  0.814462\n",
      "Epoch:  644  Training Loss:  174.135  Training Accuracy:  0.814639\n",
      "Epoch:  645  Training Loss:  173.954  Training Accuracy:  0.814992\n",
      "Epoch:  646  Training Loss:  173.694  Training Accuracy:  0.815285\n",
      "Epoch:  647  Training Loss:  173.512  Training Accuracy:  0.815521\n",
      "Epoch:  648  Training Loss:  173.296  Training Accuracy:  0.815697\n",
      "Epoch:  649  Training Loss:  173.082  Training Accuracy:  0.815932\n",
      "Epoch:  650  Training Loss:  172.851  Training Accuracy:  0.81605\n",
      "Epoch:  651  Training Loss:  172.656  Training Accuracy:  0.816344\n",
      "Epoch:  652  Training Loss:  172.411  Training Accuracy:  0.816638\n",
      "Epoch:  653  Training Loss:  172.255  Training Accuracy:  0.81699\n",
      "Epoch:  654  Training Loss:  171.986  Training Accuracy:  0.817167\n",
      "Epoch:  655  Training Loss:  171.817  Training Accuracy:  0.817402\n",
      "Epoch:  656  Training Loss:  171.551  Training Accuracy:  0.817755\n",
      "Epoch:  657  Training Loss:  171.388  Training Accuracy:  0.818048\n",
      "Epoch:  658  Training Loss:  171.155  Training Accuracy:  0.818284\n",
      "Epoch:  659  Training Loss:  170.948  Training Accuracy:  0.818343\n",
      "Epoch:  660  Training Loss:  170.702  Training Accuracy:  0.818519\n",
      "Epoch:  661  Training Loss:  170.551  Training Accuracy:  0.818695\n",
      "Epoch:  662  Training Loss:  170.312  Training Accuracy:  0.818871\n",
      "Epoch:  663  Training Loss:  170.087  Training Accuracy:  0.819166\n",
      "Epoch:  664  Training Loss:  169.857  Training Accuracy:  0.819283\n",
      "Epoch:  665  Training Loss:  169.674  Training Accuracy:  0.819636\n",
      "Epoch:  666  Training Loss:  169.428  Training Accuracy:  0.819636\n",
      "Epoch:  667  Training Loss:  169.254  Training Accuracy:  0.820047\n",
      "Epoch:  668  Training Loss:  168.998  Training Accuracy:  0.820224\n",
      "Epoch:  669  Training Loss:  168.806  Training Accuracy:  0.820753\n",
      "Epoch:  670  Training Loss:  168.573  Training Accuracy:  0.820929\n",
      "Epoch:  671  Training Loss:  168.408  Training Accuracy:  0.821223\n",
      "Epoch:  672  Training Loss:  168.128  Training Accuracy:  0.821458\n",
      "Epoch:  673  Training Loss:  167.951  Training Accuracy:  0.821517\n",
      "Epoch:  674  Training Loss:  167.715  Training Accuracy:  0.821929\n",
      "Epoch:  675  Training Loss:  167.528  Training Accuracy:  0.822105\n",
      "Epoch:  676  Training Loss:  167.301  Training Accuracy:  0.822281\n",
      "Epoch:  677  Training Loss:  167.134  Training Accuracy:  0.822575\n",
      "Epoch:  678  Training Loss:  166.851  Training Accuracy:  0.822752\n",
      "Epoch:  679  Training Loss:  166.69  Training Accuracy:  0.823163\n",
      "Epoch:  680  Training Loss:  166.456  Training Accuracy:  0.823457\n",
      "Epoch:  681  Training Loss:  166.265  Training Accuracy:  0.823751\n",
      "Epoch:  682  Training Loss:  166.039  Training Accuracy:  0.824163\n",
      "Epoch:  683  Training Loss:  165.848  Training Accuracy:  0.824574\n",
      "Epoch:  684  Training Loss:  165.616  Training Accuracy:  0.825103\n",
      "Epoch:  685  Training Loss:  165.445  Training Accuracy:  0.825515\n",
      "Epoch:  686  Training Loss:  165.213  Training Accuracy:  0.825574\n",
      "Epoch:  687  Training Loss:  164.995  Training Accuracy:  0.825691\n",
      "Epoch:  688  Training Loss:  164.806  Training Accuracy:  0.826161\n",
      "Epoch:  689  Training Loss:  164.623  Training Accuracy:  0.826397\n",
      "Epoch:  690  Training Loss:  164.351  Training Accuracy:  0.826749\n",
      "Epoch:  691  Training Loss:  164.239  Training Accuracy:  0.826808\n",
      "Epoch:  692  Training Loss:  163.993  Training Accuracy:  0.82722\n",
      "Epoch:  693  Training Loss:  163.819  Training Accuracy:  0.827337\n",
      "Epoch:  694  Training Loss:  163.58  Training Accuracy:  0.827455\n",
      "Epoch:  695  Training Loss:  163.418  Training Accuracy:  0.828219\n",
      "Epoch:  696  Training Loss:  163.16  Training Accuracy:  0.828807\n",
      "Epoch:  697  Training Loss:  162.984  Training Accuracy:  0.82916\n",
      "Epoch:  698  Training Loss:  162.746  Training Accuracy:  0.829277\n",
      "Epoch:  699  Training Loss:  162.585  Training Accuracy:  0.829689\n",
      "Testing Accuracy: 0.751387\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 700\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
