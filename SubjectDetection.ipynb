{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "#create\n",
    "def plot_subject(subject,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(subject)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"user-id\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = read_data('actitracker_raw2.txt')\n",
    "dataset = dataset.replace(\";\",\"\",regex=True)#.replace(';',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-223c9458af1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-57cc2f865b90>\u001b[0m in \u001b[0;36mfeature_normalize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6340\u001b[0m                                       skipna=skipna)\n\u001b[1;32m   6341\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m-> 6342\u001b[0;31m                             numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   6343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   2379\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x-axis</th>\n",
       "      <th>y-axis</th>\n",
       "      <th>z-axis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.198203</td>\n",
       "      <td>0.804142</td>\n",
       "      <td>0.50395286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>0.635039</td>\n",
       "      <td>0.594170</td>\n",
       "      <td>0.95342433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.537639</td>\n",
       "      <td>-0.08172209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.186271</td>\n",
       "      <td>1.666240</td>\n",
       "      <td>3.0237172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-0.269795</td>\n",
       "      <td>0.719346</td>\n",
       "      <td>7.205164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106442306000</td>\n",
       "      <td>0.104071</td>\n",
       "      <td>-1.444986</td>\n",
       "      <td>-6.510526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106542312000</td>\n",
       "      <td>-0.186271</td>\n",
       "      <td>0.491202</td>\n",
       "      <td>5.706926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106652389000</td>\n",
       "      <td>-0.170362</td>\n",
       "      <td>0.991906</td>\n",
       "      <td>7.0553403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106762313000</td>\n",
       "      <td>-1.327754</td>\n",
       "      <td>0.616378</td>\n",
       "      <td>5.134871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106872299000</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>-0.871599</td>\n",
       "      <td>1.6480621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106982315000</td>\n",
       "      <td>-1.293948</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>2.7240696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107092330000</td>\n",
       "      <td>0.110037</td>\n",
       "      <td>-0.217454</td>\n",
       "      <td>2.982856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107202316000</td>\n",
       "      <td>-0.371216</td>\n",
       "      <td>-1.517668</td>\n",
       "      <td>-0.29964766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107312332000</td>\n",
       "      <td>-0.991673</td>\n",
       "      <td>-0.059975</td>\n",
       "      <td>-8.158588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107422348000</td>\n",
       "      <td>0.754358</td>\n",
       "      <td>1.593558</td>\n",
       "      <td>8.539958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107522293000</td>\n",
       "      <td>0.819984</td>\n",
       "      <td>-0.633361</td>\n",
       "      <td>2.9147544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107632339000</td>\n",
       "      <td>-0.325477</td>\n",
       "      <td>0.156054</td>\n",
       "      <td>-1.4573772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107742355000</td>\n",
       "      <td>0.418277</td>\n",
       "      <td>0.939413</td>\n",
       "      <td>9.425281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107852340000</td>\n",
       "      <td>-0.393091</td>\n",
       "      <td>-1.921461</td>\n",
       "      <td>-10.18802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107962326000</td>\n",
       "      <td>0.306913</td>\n",
       "      <td>0.456880</td>\n",
       "      <td>-9.724928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108062271000</td>\n",
       "      <td>0.424243</td>\n",
       "      <td>0.951527</td>\n",
       "      <td>1.5390993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108172348000</td>\n",
       "      <td>-0.170362</td>\n",
       "      <td>-0.502129</td>\n",
       "      <td>3.718355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108272262000</td>\n",
       "      <td>-0.432864</td>\n",
       "      <td>-0.825163</td>\n",
       "      <td>0.08172209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108382370000</td>\n",
       "      <td>-0.617808</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>6.510526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108492294000</td>\n",
       "      <td>-0.214113</td>\n",
       "      <td>-1.564104</td>\n",
       "      <td>-4.630918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108602371000</td>\n",
       "      <td>-0.023202</td>\n",
       "      <td>0.531582</td>\n",
       "      <td>13.525005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108702285000</td>\n",
       "      <td>0.736460</td>\n",
       "      <td>1.236201</td>\n",
       "      <td>6.1700177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108812332000</td>\n",
       "      <td>-1.361561</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>4.0180025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108922378000</td>\n",
       "      <td>-0.291670</td>\n",
       "      <td>-0.893808</td>\n",
       "      <td>2.3699405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49109022293000</td>\n",
       "      <td>-0.766956</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>4.7126403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098174</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622091524000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.263769</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098175</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622131471000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098176</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622171541000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098177</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622211580000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.286004</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098178</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622291475000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098179</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622331483000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098180</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622371522000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.244499</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098181</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622451479000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098182</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622491487000</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098183</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622531465000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098184</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622571443000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.244499</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098185</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622611635000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098186</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622691469000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098187</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622731477000</td>\n",
       "      <td>1.183686</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098188</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622771486000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098189</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622851472000</td>\n",
       "      <td>1.144265</td>\n",
       "      <td>-1.297862</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098190</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622891511000</td>\n",
       "      <td>1.110683</td>\n",
       "      <td>-1.320097</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098191</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622931490000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098192</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622971498000</td>\n",
       "      <td>1.250849</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098193</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623051485000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098194</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623091524000</td>\n",
       "      <td>1.150105</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098195</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623131471000</td>\n",
       "      <td>1.167626</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098196</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623172578000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.263769</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098197</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623251466000</td>\n",
       "      <td>1.233328</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098198</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623291475000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.297862</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098199</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623331483000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.308238</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098200</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623371431000</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098201</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623411592000</td>\n",
       "      <td>1.228948</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098202</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623491487000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098203</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623531465000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1098204 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user-id activity        timestamp    x-axis    y-axis       z-axis\n",
       "0             33  Jogging   49105962326000 -0.198203  0.804142   0.50395286\n",
       "1             33  Jogging   49106062271000  0.635039  0.594170   0.95342433\n",
       "2             33  Jogging   49106112167000  0.619130  0.537639  -0.08172209\n",
       "3             33  Jogging   49106222305000 -0.186271  1.666240    3.0237172\n",
       "4             33  Jogging   49106332290000 -0.269795  0.719346     7.205164\n",
       "5             33  Jogging   49106442306000  0.104071 -1.444986    -6.510526\n",
       "6             33  Jogging   49106542312000 -0.186271  0.491202     5.706926\n",
       "7             33  Jogging   49106652389000 -0.170362  0.991906    7.0553403\n",
       "8             33  Jogging   49106762313000 -1.327754  0.616378     5.134871\n",
       "9             33  Jogging   49106872299000  0.042423 -0.871599    1.6480621\n",
       "10            33  Jogging   49106982315000 -1.293948  1.825739    2.7240696\n",
       "11            33  Jogging   49107092330000  0.110037 -0.217454     2.982856\n",
       "12            33  Jogging   49107202316000 -0.371216 -1.517668  -0.29964766\n",
       "13            33  Jogging   49107312332000 -0.991673 -0.059975    -8.158588\n",
       "14            33  Jogging   49107422348000  0.754358  1.593558     8.539958\n",
       "15            33  Jogging   49107522293000  0.819984 -0.633361    2.9147544\n",
       "16            33  Jogging   49107632339000 -0.325477  0.156054   -1.4573772\n",
       "17            33  Jogging   49107742355000  0.418277  0.939413     9.425281\n",
       "18            33  Jogging   49107852340000 -0.393091 -1.921461    -10.18802\n",
       "19            33  Jogging   49107962326000  0.306913  0.456880    -9.724928\n",
       "20            33  Jogging   49108062271000  0.424243  0.951527    1.5390993\n",
       "21            33  Jogging   49108172348000 -0.170362 -0.502129     3.718355\n",
       "22            33  Jogging   49108272262000 -0.432864 -0.825163   0.08172209\n",
       "23            33  Jogging   49108382370000 -0.617808  1.825739     6.510526\n",
       "24            33  Jogging   49108492294000 -0.214113 -1.564104    -4.630918\n",
       "25            33  Jogging   49108602371000 -0.023202  0.531582    13.525005\n",
       "26            33  Jogging   49108702285000  0.736460  1.236201    6.1700177\n",
       "27            33  Jogging   49108812332000 -1.361561  0.002613    4.0180025\n",
       "28            33  Jogging   49108922378000 -0.291670 -0.893808    2.3699405\n",
       "29            33  Jogging   49109022293000 -0.766956  1.825739    4.7126403\n",
       "...          ...      ...              ...       ...       ...          ...\n",
       "1098174       19  Sitting  131622091524000  1.205587 -1.263769         2.22\n",
       "1098175       19  Sitting  131622131471000  1.211427 -1.269698         2.26\n",
       "1098176       19  Sitting  131622171541000  1.205587 -1.272663         2.18\n",
       "1098177       19  Sitting  131622211580000  1.195367 -1.286004         2.26\n",
       "1098178       19  Sitting  131622291475000  1.199747 -1.280074         2.41\n",
       "1098179       19  Sitting  131622331483000  1.205587 -1.269698          2.3\n",
       "1098180       19  Sitting  131622371522000  1.205587 -1.244499         2.26\n",
       "1098181       19  Sitting  131622451479000  1.211427 -1.241534         2.34\n",
       "1098182       19  Sitting  131622491487000  1.223108 -1.241534         2.41\n",
       "1098183       19  Sitting  131622531465000  1.199747 -1.241534         2.37\n",
       "1098184       19  Sitting  131622571443000  1.199747 -1.244499         2.37\n",
       "1098185       19  Sitting  131622611635000  1.205587 -1.257840         2.45\n",
       "1098186       19  Sitting  131622691469000  1.205587 -1.257840         2.45\n",
       "1098187       19  Sitting  131622731477000  1.183686 -1.272663         2.53\n",
       "1098188       19  Sitting  131622771486000  1.211427 -1.280074          2.6\n",
       "1098189       19  Sitting  131622851472000  1.144265 -1.297862         2.56\n",
       "1098190       19  Sitting  131622891511000  1.110683 -1.320097         2.11\n",
       "1098191       19  Sitting  131622931490000  1.211427 -1.291933          2.3\n",
       "1098192       19  Sitting  131622971498000  1.250849 -1.291933         2.26\n",
       "1098193       19  Sitting  131623051485000  1.195367 -1.257840         2.26\n",
       "1098194       19  Sitting  131623091524000  1.150105 -1.269698         2.49\n",
       "1098195       19  Sitting  131623131471000  1.167626 -1.269698         2.37\n",
       "1098196       19  Sitting  131623172578000  1.195367 -1.263769         2.18\n",
       "1098197       19  Sitting  131623251466000  1.233328 -1.280074         1.95\n",
       "1098198       19  Sitting  131623291475000  1.217268 -1.297862          1.8\n",
       "1098199       19  Sitting  131623331483000  1.217268 -1.308238         1.69\n",
       "1098200       19  Sitting  131623371431000  1.223108 -1.291933         1.73\n",
       "1098201       19  Sitting  131623411592000  1.228948 -1.280074         1.69\n",
       "1098202       19  Sitting  131623491487000  1.217268 -1.291933         1.73\n",
       "1098203       19  Sitting  131623531465000  1.199747 -1.272663         1.61\n",
       "\n",
       "[1098204 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-169114f33f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user-id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user-id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplot_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fb8ab1dffe65>\u001b[0m in \u001b[0;36mplot_subject\u001b[0;34m(subject, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb8ab1dffe65>\u001b[0m in \u001b[0;36mplot_axis\u001b[0;34m(ax, x, y, title)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6360\u001b[0m                                       skipna=skipna, ddof=ddof)\n\u001b[1;32m   6361\u001b[0m         return self._reduce(f, name, axis=axis, numeric_only=numeric_only,\n\u001b[0;32m-> 6362\u001b[0;31m                             skipna=skipna, ddof=ddof)\n\u001b[0m\u001b[1;32m   6363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   2379\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mbottleneck_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAJCCAYAAABj8z68AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d16e350b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#modify taerget(user-id)\n",
    "for subject in np.unique(dataset[\"user-id\"]):\n",
    "    subset = dataset[dataset[\"user-id\"] == subject][:40]\n",
    "    plot_subject(subject,subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-967f72759e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#saved data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_x.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_y.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_x.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_y.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#saved data\n",
    "np.save('train_x.npy',train_x)\n",
    "np.save('train_y.npy',train_y)\n",
    "np.save('test_x.npy',test_x)\n",
    "np.save('test_y.npy',test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.load('train_x.npy')\n",
    "train_y = np.load('train_y.npy')\n",
    "test_x = np.load('test_x.npy')\n",
    "test_y = np.load('test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  88.5429  Training Accuracy:  0.0408583\n",
      "Epoch:  1  Training Loss:  84.0675  Training Accuracy:  0.0412111\n",
      "Epoch:  2  Training Loss:  79.2168  Training Accuracy:  0.0443269\n",
      "Epoch:  3  Training Loss:  75.8388  Training Accuracy:  0.0513228\n",
      "Epoch:  4  Training Loss:  73.3623  Training Accuracy:  0.0567901\n",
      "Epoch:  5  Training Loss:  71.4037  Training Accuracy:  0.0624927\n",
      "Epoch:  6  Training Loss:  69.4983  Training Accuracy:  0.0652557\n",
      "Epoch:  7  Training Loss:  67.7493  Training Accuracy:  0.068254\n",
      "Epoch:  8  Training Loss:  65.8863  Training Accuracy:  0.0714874\n",
      "Epoch:  9  Training Loss:  64.1148  Training Accuracy:  0.0745444\n",
      "Epoch:  10  Training Loss:  62.3745  Training Accuracy:  0.0787184\n",
      "Epoch:  11  Training Loss:  60.9252  Training Accuracy:  0.0843621\n",
      "Epoch:  12  Training Loss:  59.6367  Training Accuracy:  0.0906526\n",
      "Epoch:  13  Training Loss:  58.4354  Training Accuracy:  0.0975309\n",
      "Epoch:  14  Training Loss:  57.272  Training Accuracy:  0.105291\n",
      "Epoch:  15  Training Loss:  56.0355  Training Accuracy:  0.112581\n",
      "Epoch:  16  Training Loss:  54.9509  Training Accuracy:  0.120694\n",
      "Epoch:  17  Training Loss:  53.9343  Training Accuracy:  0.128807\n",
      "Epoch:  18  Training Loss:  53.0399  Training Accuracy:  0.138624\n",
      "Epoch:  19  Training Loss:  52.1665  Training Accuracy:  0.148442\n",
      "Epoch:  20  Training Loss:  51.3271  Training Accuracy:  0.156966\n",
      "Epoch:  21  Training Loss:  50.5417  Training Accuracy:  0.16602\n",
      "Epoch:  22  Training Loss:  49.7004  Training Accuracy:  0.173721\n",
      "Epoch:  23  Training Loss:  48.8309  Training Accuracy:  0.180717\n",
      "Epoch:  24  Training Loss:  48.0256  Training Accuracy:  0.188066\n",
      "Epoch:  25  Training Loss:  47.2688  Training Accuracy:  0.194239\n",
      "Epoch:  26  Training Loss:  46.5634  Training Accuracy:  0.201117\n",
      "Epoch:  27  Training Loss:  45.9045  Training Accuracy:  0.207995\n",
      "Epoch:  28  Training Loss:  45.1591  Training Accuracy:  0.213227\n",
      "Epoch:  29  Training Loss:  44.4276  Training Accuracy:  0.220165\n",
      "Epoch:  30  Training Loss:  43.7253  Training Accuracy:  0.22575\n",
      "Epoch:  31  Training Loss:  43.0202  Training Accuracy:  0.230217\n",
      "Epoch:  32  Training Loss:  42.3147  Training Accuracy:  0.235509\n",
      "Epoch:  33  Training Loss:  41.6067  Training Accuracy:  0.240799\n",
      "Epoch:  34  Training Loss:  40.8785  Training Accuracy:  0.246149\n",
      "Epoch:  35  Training Loss:  40.1429  Training Accuracy:  0.250617\n",
      "Epoch:  36  Training Loss:  39.3938  Training Accuracy:  0.255144\n",
      "Epoch:  37  Training Loss:  38.6097  Training Accuracy:  0.260494\n",
      "Epoch:  38  Training Loss:  37.8304  Training Accuracy:  0.264785\n",
      "Epoch:  39  Training Loss:  37.0427  Training Accuracy:  0.269077\n",
      "Epoch:  40  Training Loss:  36.2721  Training Accuracy:  0.273956\n",
      "Epoch:  41  Training Loss:  35.5214  Training Accuracy:  0.278542\n",
      "Epoch:  42  Training Loss:  34.766  Training Accuracy:  0.281834\n",
      "Epoch:  43  Training Loss:  34.0286  Training Accuracy:  0.285773\n",
      "Epoch:  44  Training Loss:  33.2907  Training Accuracy:  0.289888\n",
      "Epoch:  45  Training Loss:  32.5608  Training Accuracy:  0.294533\n",
      "Epoch:  46  Training Loss:  31.8167  Training Accuracy:  0.297884\n",
      "Epoch:  47  Training Loss:  31.0745  Training Accuracy:  0.301587\n",
      "Epoch:  48  Training Loss:  30.351  Training Accuracy:  0.30435\n",
      "Epoch:  49  Training Loss:  29.6392  Training Accuracy:  0.307643\n",
      "Epoch:  50  Training Loss:  28.9297  Training Accuracy:  0.31017\n",
      "Epoch:  51  Training Loss:  28.2086  Training Accuracy:  0.314286\n",
      "Epoch:  52  Training Loss:  27.506  Training Accuracy:  0.317872\n",
      "Epoch:  53  Training Loss:  26.8116  Training Accuracy:  0.320576\n",
      "Epoch:  54  Training Loss:  26.1221  Training Accuracy:  0.324045\n",
      "Epoch:  55  Training Loss:  25.4473  Training Accuracy:  0.327043\n",
      "Epoch:  56  Training Loss:  24.7829  Training Accuracy:  0.330923\n",
      "Epoch:  57  Training Loss:  24.1232  Training Accuracy:  0.334803\n",
      "Epoch:  58  Training Loss:  23.4699  Training Accuracy:  0.33833\n",
      "Epoch:  59  Training Loss:  22.8092  Training Accuracy:  0.341681\n",
      "Epoch:  60  Training Loss:  22.1784  Training Accuracy:  0.345032\n",
      "Epoch:  61  Training Loss:  21.5578  Training Accuracy:  0.348148\n",
      "Epoch:  62  Training Loss:  20.9489  Training Accuracy:  0.351088\n",
      "Epoch:  63  Training Loss:  20.3525  Training Accuracy:  0.353557\n",
      "Epoch:  64  Training Loss:  19.7582  Training Accuracy:  0.356731\n",
      "Epoch:  65  Training Loss:  19.1715  Training Accuracy:  0.360023\n",
      "Epoch:  66  Training Loss:  18.591  Training Accuracy:  0.362963\n",
      "Epoch:  67  Training Loss:  18.0248  Training Accuracy:  0.365667\n",
      "Epoch:  68  Training Loss:  17.4808  Training Accuracy:  0.368313\n",
      "Epoch:  69  Training Loss:  16.9446  Training Accuracy:  0.370958\n",
      "Epoch:  70  Training Loss:  16.4182  Training Accuracy:  0.374838\n",
      "Epoch:  71  Training Loss:  15.9085  Training Accuracy:  0.377366\n",
      "Epoch:  72  Training Loss:  15.4081  Training Accuracy:  0.381011\n",
      "Epoch:  73  Training Loss:  14.9147  Training Accuracy:  0.383833\n",
      "Epoch:  74  Training Loss:  14.4437  Training Accuracy:  0.386478\n",
      "Epoch:  75  Training Loss:  13.9835  Training Accuracy:  0.389065\n",
      "Epoch:  76  Training Loss:  13.532  Training Accuracy:  0.391593\n",
      "Epoch:  77  Training Loss:  13.0965  Training Accuracy:  0.395003\n",
      "Epoch:  78  Training Loss:  12.6734  Training Accuracy:  0.399177\n",
      "Epoch:  79  Training Loss:  12.2578  Training Accuracy:  0.401822\n",
      "Epoch:  80  Training Loss:  11.8559  Training Accuracy:  0.404409\n",
      "Epoch:  81  Training Loss:  11.4715  Training Accuracy:  0.407584\n",
      "Epoch:  82  Training Loss:  11.0985  Training Accuracy:  0.409876\n",
      "Epoch:  83  Training Loss:  10.7327  Training Accuracy:  0.413051\n",
      "Epoch:  84  Training Loss:  10.3849  Training Accuracy:  0.415403\n",
      "Epoch:  85  Training Loss:  10.0412  Training Accuracy:  0.417813\n",
      "Epoch:  86  Training Loss:  9.70936  Training Accuracy:  0.420988\n",
      "Epoch:  87  Training Loss:  9.38283  Training Accuracy:  0.424162\n",
      "Epoch:  88  Training Loss:  9.06779  Training Accuracy:  0.426866\n",
      "Epoch:  89  Training Loss:  8.76683  Training Accuracy:  0.429453\n",
      "Epoch:  90  Training Loss:  8.47924  Training Accuracy:  0.431863\n",
      "Epoch:  91  Training Loss:  8.20096  Training Accuracy:  0.434156\n",
      "Epoch:  92  Training Loss:  7.93293  Training Accuracy:  0.43639\n",
      "Epoch:  93  Training Loss:  7.68195  Training Accuracy:  0.438565\n",
      "Epoch:  94  Training Loss:  7.43544  Training Accuracy:  0.440212\n",
      "Epoch:  95  Training Loss:  7.20097  Training Accuracy:  0.442857\n",
      "Epoch:  96  Training Loss:  6.97199  Training Accuracy:  0.444797\n",
      "Epoch:  97  Training Loss:  6.75321  Training Accuracy:  0.44662\n",
      "Epoch:  98  Training Loss:  6.54008  Training Accuracy:  0.448324\n",
      "Epoch:  99  Training Loss:  6.33143  Training Accuracy:  0.450088\n",
      "Epoch:  100  Training Loss:  6.13088  Training Accuracy:  0.452498\n",
      "Epoch:  101  Training Loss:  5.9406  Training Accuracy:  0.454909\n",
      "Epoch:  102  Training Loss:  5.75488  Training Accuracy:  0.45726\n",
      "Epoch:  103  Training Loss:  5.57492  Training Accuracy:  0.459318\n",
      "Epoch:  104  Training Loss:  5.40288  Training Accuracy:  0.460905\n",
      "Epoch:  105  Training Loss:  5.23794  Training Accuracy:  0.463022\n",
      "Epoch:  106  Training Loss:  5.08062  Training Accuracy:  0.464844\n",
      "Epoch:  107  Training Loss:  4.93239  Training Accuracy:  0.467019\n",
      "Epoch:  108  Training Loss:  4.7883  Training Accuracy:  0.468724\n",
      "Epoch:  109  Training Loss:  4.64897  Training Accuracy:  0.471134\n",
      "Epoch:  110  Training Loss:  4.51556  Training Accuracy:  0.472487\n",
      "Epoch:  111  Training Loss:  4.38822  Training Accuracy:  0.473545\n",
      "Epoch:  112  Training Loss:  4.26413  Training Accuracy:  0.475367\n",
      "Epoch:  113  Training Loss:  4.14457  Training Accuracy:  0.477307\n",
      "Epoch:  114  Training Loss:  4.02974  Training Accuracy:  0.479424\n",
      "Epoch:  115  Training Loss:  3.92053  Training Accuracy:  0.48154\n",
      "Epoch:  116  Training Loss:  3.81333  Training Accuracy:  0.48348\n",
      "Epoch:  117  Training Loss:  3.71138  Training Accuracy:  0.48495\n",
      "Epoch:  118  Training Loss:  3.61491  Training Accuracy:  0.48642\n",
      "Epoch:  119  Training Loss:  3.52272  Training Accuracy:  0.488066\n",
      "Epoch:  120  Training Loss:  3.43357  Training Accuracy:  0.489829\n",
      "Epoch:  121  Training Loss:  3.35016  Training Accuracy:  0.491593\n",
      "Epoch:  122  Training Loss:  3.26726  Training Accuracy:  0.493357\n",
      "Epoch:  123  Training Loss:  3.18661  Training Accuracy:  0.494768\n",
      "Epoch:  124  Training Loss:  3.10848  Training Accuracy:  0.496708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  3.03307  Training Accuracy:  0.498413\n",
      "Epoch:  126  Training Loss:  2.96266  Training Accuracy:  0.500353\n",
      "Epoch:  127  Training Loss:  2.89437  Training Accuracy:  0.501881\n",
      "Epoch:  128  Training Loss:  2.82923  Training Accuracy:  0.503998\n",
      "Epoch:  129  Training Loss:  2.76539  Training Accuracy:  0.50582\n",
      "Epoch:  130  Training Loss:  2.70614  Training Accuracy:  0.507407\n",
      "Epoch:  131  Training Loss:  2.6497  Training Accuracy:  0.5097\n",
      "Epoch:  132  Training Loss:  2.59534  Training Accuracy:  0.511934\n",
      "Epoch:  133  Training Loss:  2.5427  Training Accuracy:  0.513874\n",
      "Epoch:  134  Training Loss:  2.49206  Training Accuracy:  0.516167\n",
      "Epoch:  135  Training Loss:  2.44423  Training Accuracy:  0.517931\n",
      "Epoch:  136  Training Loss:  2.39724  Training Accuracy:  0.520106\n",
      "Epoch:  137  Training Loss:  2.35184  Training Accuracy:  0.521399\n",
      "Epoch:  138  Training Loss:  2.30872  Training Accuracy:  0.523045\n",
      "Epoch:  139  Training Loss:  2.26542  Training Accuracy:  0.52428\n",
      "Epoch:  140  Training Loss:  2.22497  Training Accuracy:  0.525749\n",
      "Epoch:  141  Training Loss:  2.18707  Training Accuracy:  0.527219\n",
      "Epoch:  142  Training Loss:  2.1474  Training Accuracy:  0.528865\n",
      "Epoch:  143  Training Loss:  2.11036  Training Accuracy:  0.530923\n",
      "Epoch:  144  Training Loss:  2.07511  Training Accuracy:  0.53251\n",
      "Epoch:  145  Training Loss:  2.04248  Training Accuracy:  0.534921\n",
      "Epoch:  146  Training Loss:  2.00985  Training Accuracy:  0.536273\n",
      "Epoch:  147  Training Loss:  1.9802  Training Accuracy:  0.537801\n",
      "Epoch:  148  Training Loss:  1.9516  Training Accuracy:  0.538977\n",
      "Epoch:  149  Training Loss:  1.92238  Training Accuracy:  0.540917\n",
      "Epoch:  150  Training Loss:  1.89593  Training Accuracy:  0.542563\n",
      "Epoch:  151  Training Loss:  1.87018  Training Accuracy:  0.544209\n",
      "Epoch:  152  Training Loss:  1.84469  Training Accuracy:  0.546267\n",
      "Epoch:  153  Training Loss:  1.82074  Training Accuracy:  0.548736\n",
      "Epoch:  154  Training Loss:  1.79825  Training Accuracy:  0.550382\n",
      "Epoch:  155  Training Loss:  1.77638  Training Accuracy:  0.551969\n",
      "Epoch:  156  Training Loss:  1.75474  Training Accuracy:  0.553733\n",
      "Epoch:  157  Training Loss:  1.73483  Training Accuracy:  0.555203\n",
      "Epoch:  158  Training Loss:  1.71548  Training Accuracy:  0.556908\n",
      "Epoch:  159  Training Loss:  1.69704  Training Accuracy:  0.559436\n",
      "Epoch:  160  Training Loss:  1.67976  Training Accuracy:  0.560905\n",
      "Epoch:  161  Training Loss:  1.66344  Training Accuracy:  0.562316\n",
      "Epoch:  162  Training Loss:  1.64788  Training Accuracy:  0.564021\n",
      "Epoch:  163  Training Loss:  1.63393  Training Accuracy:  0.565667\n",
      "Epoch:  164  Training Loss:  1.62063  Training Accuracy:  0.567372\n",
      "Epoch:  165  Training Loss:  1.60785  Training Accuracy:  0.568783\n",
      "Epoch:  166  Training Loss:  1.59531  Training Accuracy:  0.570547\n",
      "Epoch:  167  Training Loss:  1.58322  Training Accuracy:  0.571722\n",
      "Epoch:  168  Training Loss:  1.57194  Training Accuracy:  0.573663\n",
      "Epoch:  169  Training Loss:  1.55995  Training Accuracy:  0.57525\n",
      "Epoch:  170  Training Loss:  1.54897  Training Accuracy:  0.57719\n",
      "Epoch:  171  Training Loss:  1.53779  Training Accuracy:  0.578366\n",
      "Epoch:  172  Training Loss:  1.52777  Training Accuracy:  0.580247\n",
      "Epoch:  173  Training Loss:  1.51714  Training Accuracy:  0.581599\n",
      "Epoch:  174  Training Loss:  1.50733  Training Accuracy:  0.582834\n",
      "Epoch:  175  Training Loss:  1.49707  Training Accuracy:  0.583539\n",
      "Epoch:  176  Training Loss:  1.48792  Training Accuracy:  0.585009\n",
      "Epoch:  177  Training Loss:  1.47807  Training Accuracy:  0.587184\n",
      "Epoch:  178  Training Loss:  1.46945  Training Accuracy:  0.588712\n",
      "Epoch:  179  Training Loss:  1.46003  Training Accuracy:  0.590359\n",
      "Epoch:  180  Training Loss:  1.45083  Training Accuracy:  0.591711\n",
      "Epoch:  181  Training Loss:  1.44156  Training Accuracy:  0.593063\n",
      "Epoch:  182  Training Loss:  1.43128  Training Accuracy:  0.594709\n",
      "Epoch:  183  Training Loss:  1.42135  Training Accuracy:  0.595885\n",
      "Epoch:  184  Training Loss:  1.41212  Training Accuracy:  0.597119\n",
      "Epoch:  185  Training Loss:  1.4045  Training Accuracy:  0.59853\n",
      "Epoch:  186  Training Loss:  1.39671  Training Accuracy:  0.600176\n",
      "Epoch:  187  Training Loss:  1.38928  Training Accuracy:  0.601587\n",
      "Epoch:  188  Training Loss:  1.38163  Training Accuracy:  0.602528\n",
      "Epoch:  189  Training Loss:  1.37482  Training Accuracy:  0.603998\n",
      "Epoch:  190  Training Loss:  1.36749  Training Accuracy:  0.605526\n",
      "Epoch:  191  Training Loss:  1.35957  Training Accuracy:  0.606408\n",
      "Epoch:  192  Training Loss:  1.35282  Training Accuracy:  0.607643\n",
      "Epoch:  193  Training Loss:  1.345  Training Accuracy:  0.609347\n",
      "Epoch:  194  Training Loss:  1.33775  Training Accuracy:  0.610582\n",
      "Epoch:  195  Training Loss:  1.32998  Training Accuracy:  0.611699\n",
      "Epoch:  196  Training Loss:  1.3211  Training Accuracy:  0.613227\n",
      "Epoch:  197  Training Loss:  1.31312  Training Accuracy:  0.614638\n",
      "Epoch:  198  Training Loss:  1.30588  Training Accuracy:  0.616343\n",
      "Epoch:  199  Training Loss:  1.29836  Training Accuracy:  0.618283\n",
      "Epoch:  200  Training Loss:  1.29143  Training Accuracy:  0.619929\n",
      "Epoch:  201  Training Loss:  1.2841  Training Accuracy:  0.62187\n",
      "Epoch:  202  Training Loss:  1.27696  Training Accuracy:  0.622928\n",
      "Epoch:  203  Training Loss:  1.26988  Training Accuracy:  0.624221\n",
      "Epoch:  204  Training Loss:  1.26268  Training Accuracy:  0.624985\n",
      "Epoch:  205  Training Loss:  1.25511  Training Accuracy:  0.626925\n",
      "Epoch:  206  Training Loss:  1.24888  Training Accuracy:  0.628042\n",
      "Epoch:  207  Training Loss:  1.24274  Training Accuracy:  0.629453\n",
      "Epoch:  208  Training Loss:  1.23497  Training Accuracy:  0.630747\n",
      "Epoch:  209  Training Loss:  1.22848  Training Accuracy:  0.631864\n",
      "Epoch:  210  Training Loss:  1.22149  Training Accuracy:  0.633098\n",
      "Epoch:  211  Training Loss:  1.21457  Training Accuracy:  0.634333\n",
      "Epoch:  212  Training Loss:  1.20791  Training Accuracy:  0.635802\n",
      "Epoch:  213  Training Loss:  1.20181  Training Accuracy:  0.636978\n",
      "Epoch:  214  Training Loss:  1.19587  Training Accuracy:  0.638448\n",
      "Epoch:  215  Training Loss:  1.18966  Training Accuracy:  0.639683\n",
      "Epoch:  216  Training Loss:  1.18445  Training Accuracy:  0.641211\n",
      "Epoch:  217  Training Loss:  1.17903  Training Accuracy:  0.642446\n",
      "Epoch:  218  Training Loss:  1.17408  Training Accuracy:  0.643563\n",
      "Epoch:  219  Training Loss:  1.16946  Training Accuracy:  0.644915\n",
      "Epoch:  220  Training Loss:  1.16494  Training Accuracy:  0.646091\n",
      "Epoch:  221  Training Loss:  1.16036  Training Accuracy:  0.647502\n",
      "Epoch:  222  Training Loss:  1.15591  Training Accuracy:  0.648677\n",
      "Epoch:  223  Training Loss:  1.15149  Training Accuracy:  0.649383\n",
      "Epoch:  224  Training Loss:  1.1478  Training Accuracy:  0.650735\n",
      "Epoch:  225  Training Loss:  1.14337  Training Accuracy:  0.652028\n",
      "Epoch:  226  Training Loss:  1.13969  Training Accuracy:  0.65338\n",
      "Epoch:  227  Training Loss:  1.13605  Training Accuracy:  0.654968\n",
      "Epoch:  228  Training Loss:  1.13108  Training Accuracy:  0.656026\n",
      "Epoch:  229  Training Loss:  1.1262  Training Accuracy:  0.657496\n",
      "Epoch:  230  Training Loss:  1.12155  Training Accuracy:  0.659259\n",
      "Epoch:  231  Training Loss:  1.11686  Training Accuracy:  0.660847\n",
      "Epoch:  232  Training Loss:  1.11255  Training Accuracy:  0.662316\n",
      "Epoch:  233  Training Loss:  1.10838  Training Accuracy:  0.663845\n",
      "Epoch:  234  Training Loss:  1.10363  Training Accuracy:  0.664844\n",
      "Epoch:  235  Training Loss:  1.09936  Training Accuracy:  0.665844\n",
      "Epoch:  236  Training Loss:  1.09482  Training Accuracy:  0.667078\n",
      "Epoch:  237  Training Loss:  1.0898  Training Accuracy:  0.668783\n",
      "Epoch:  238  Training Loss:  1.08485  Training Accuracy:  0.669724\n",
      "Epoch:  239  Training Loss:  1.07967  Training Accuracy:  0.670958\n",
      "Epoch:  240  Training Loss:  1.07516  Training Accuracy:  0.672075\n",
      "Epoch:  241  Training Loss:  1.07062  Training Accuracy:  0.67331\n",
      "Epoch:  242  Training Loss:  1.06558  Training Accuracy:  0.674838\n",
      "Epoch:  243  Training Loss:  1.06083  Training Accuracy:  0.676014\n",
      "Epoch:  244  Training Loss:  1.056  Training Accuracy:  0.677484\n",
      "Epoch:  245  Training Loss:  1.05088  Training Accuracy:  0.678777\n",
      "Epoch:  246  Training Loss:  1.04584  Training Accuracy:  0.680658\n",
      "Epoch:  247  Training Loss:  1.04078  Training Accuracy:  0.68154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  1.03657  Training Accuracy:  0.683186\n",
      "Epoch:  249  Training Loss:  1.03207  Training Accuracy:  0.68448\n",
      "Epoch:  250  Training Loss:  1.02813  Training Accuracy:  0.686126\n",
      "Epoch:  251  Training Loss:  1.02339  Training Accuracy:  0.687478\n",
      "Epoch:  252  Training Loss:  1.01863  Training Accuracy:  0.688889\n",
      "Epoch:  253  Training Loss:  1.01395  Training Accuracy:  0.690124\n",
      "Epoch:  254  Training Loss:  1.00943  Training Accuracy:  0.691123\n",
      "Epoch:  255  Training Loss:  1.00522  Training Accuracy:  0.691828\n",
      "Epoch:  256  Training Loss:  1.00046  Training Accuracy:  0.692828\n",
      "Epoch:  257  Training Loss:  0.995941  Training Accuracy:  0.694356\n",
      "Epoch:  258  Training Loss:  0.991043  Training Accuracy:  0.695826\n",
      "Epoch:  259  Training Loss:  0.986677  Training Accuracy:  0.696943\n",
      "Epoch:  260  Training Loss:  0.981976  Training Accuracy:  0.698648\n",
      "Epoch:  261  Training Loss:  0.97736  Training Accuracy:  0.699706\n",
      "Epoch:  262  Training Loss:  0.972673  Training Accuracy:  0.700941\n",
      "Epoch:  263  Training Loss:  0.967957  Training Accuracy:  0.701587\n",
      "Epoch:  264  Training Loss:  0.963956  Training Accuracy:  0.70294\n",
      "Epoch:  265  Training Loss:  0.959304  Training Accuracy:  0.703998\n",
      "Epoch:  266  Training Loss:  0.95489  Training Accuracy:  0.704527\n",
      "Epoch:  267  Training Loss:  0.950779  Training Accuracy:  0.70582\n",
      "Epoch:  268  Training Loss:  0.946631  Training Accuracy:  0.707055\n",
      "Epoch:  269  Training Loss:  0.942289  Training Accuracy:  0.708054\n",
      "Epoch:  270  Training Loss:  0.938447  Training Accuracy:  0.709171\n",
      "Epoch:  271  Training Loss:  0.934279  Training Accuracy:  0.710112\n",
      "Epoch:  272  Training Loss:  0.929771  Training Accuracy:  0.711758\n",
      "Epoch:  273  Training Loss:  0.925349  Training Accuracy:  0.712875\n",
      "Epoch:  274  Training Loss:  0.921523  Training Accuracy:  0.714168\n",
      "Epoch:  275  Training Loss:  0.91745  Training Accuracy:  0.715462\n",
      "Epoch:  276  Training Loss:  0.913945  Training Accuracy:  0.716167\n",
      "Epoch:  277  Training Loss:  0.910428  Training Accuracy:  0.717049\n",
      "Epoch:  278  Training Loss:  0.906683  Training Accuracy:  0.717931\n",
      "Epoch:  279  Training Loss:  0.903241  Training Accuracy:  0.719165\n",
      "Epoch:  280  Training Loss:  0.899675  Training Accuracy:  0.7204\n",
      "Epoch:  281  Training Loss:  0.895627  Training Accuracy:  0.721282\n",
      "Epoch:  282  Training Loss:  0.891725  Training Accuracy:  0.722399\n",
      "Epoch:  283  Training Loss:  0.887199  Training Accuracy:  0.723339\n",
      "Epoch:  284  Training Loss:  0.882776  Training Accuracy:  0.724515\n",
      "Epoch:  285  Training Loss:  0.879098  Training Accuracy:  0.725162\n",
      "Epoch:  286  Training Loss:  0.875313  Training Accuracy:  0.726338\n",
      "Epoch:  287  Training Loss:  0.870982  Training Accuracy:  0.726925\n",
      "Epoch:  288  Training Loss:  0.866936  Training Accuracy:  0.727749\n",
      "Epoch:  289  Training Loss:  0.863352  Training Accuracy:  0.728983\n",
      "Epoch:  290  Training Loss:  0.859378  Training Accuracy:  0.730041\n",
      "Epoch:  291  Training Loss:  0.85507  Training Accuracy:  0.730982\n",
      "Epoch:  292  Training Loss:  0.851318  Training Accuracy:  0.731746\n",
      "Epoch:  293  Training Loss:  0.847681  Training Accuracy:  0.732628\n",
      "Epoch:  294  Training Loss:  0.843658  Training Accuracy:  0.733569\n",
      "Epoch:  295  Training Loss:  0.839851  Training Accuracy:  0.734686\n",
      "Epoch:  296  Training Loss:  0.836056  Training Accuracy:  0.735274\n",
      "Epoch:  297  Training Loss:  0.831459  Training Accuracy:  0.736449\n",
      "Epoch:  298  Training Loss:  0.828047  Training Accuracy:  0.737625\n",
      "Epoch:  299  Training Loss:  0.823435  Training Accuracy:  0.738448\n",
      "Testing Accuracy: 0.704991\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "        with tf.name_scope('summary'):\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter('./logs', session.graph)\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  94.7185  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  89.7656  Training Accuracy:  0.0409759\n",
      "Epoch:  2  Training Loss:  86.1089  Training Accuracy:  0.0412698\n",
      "Epoch:  3  Training Loss:  82.8551  Training Accuracy:  0.0453263\n",
      "Epoch:  4  Training Loss:  80.3039  Training Accuracy:  0.048736\n",
      "Epoch:  5  Training Loss:  78.0848  Training Accuracy:  0.0512052\n",
      "Epoch:  6  Training Loss:  76.2207  Training Accuracy:  0.0557319\n",
      "Epoch:  7  Training Loss:  74.4234  Training Accuracy:  0.0597296\n",
      "Epoch:  8  Training Loss:  72.8387  Training Accuracy:  0.0638448\n",
      "Epoch:  9  Training Loss:  71.2813  Training Accuracy:  0.0693122\n",
      "Epoch:  10  Training Loss:  69.8515  Training Accuracy:  0.0742504\n",
      "Epoch:  11  Training Loss:  68.554  Training Accuracy:  0.0800705\n",
      "Epoch:  12  Training Loss:  67.3346  Training Accuracy:  0.0854791\n",
      "Epoch:  13  Training Loss:  66.1287  Training Accuracy:  0.0914168\n",
      "Epoch:  14  Training Loss:  65.0155  Training Accuracy:  0.097766\n",
      "Epoch:  15  Training Loss:  63.9877  Training Accuracy:  0.106937\n",
      "Epoch:  16  Training Loss:  62.9669  Training Accuracy:  0.115403\n",
      "Epoch:  17  Training Loss:  61.9652  Training Accuracy:  0.123633\n",
      "Epoch:  18  Training Loss:  60.9955  Training Accuracy:  0.132216\n",
      "Epoch:  19  Training Loss:  60.0434  Training Accuracy:  0.140976\n",
      "Epoch:  20  Training Loss:  59.0733  Training Accuracy:  0.148677\n",
      "Epoch:  21  Training Loss:  58.1685  Training Accuracy:  0.15726\n",
      "Epoch:  22  Training Loss:  57.2762  Training Accuracy:  0.16455\n",
      "Epoch:  23  Training Loss:  56.4281  Training Accuracy:  0.171723\n",
      "Epoch:  24  Training Loss:  55.6365  Training Accuracy:  0.178424\n",
      "Epoch:  25  Training Loss:  54.7979  Training Accuracy:  0.183833\n",
      "Epoch:  26  Training Loss:  53.9876  Training Accuracy:  0.189418\n",
      "Epoch:  27  Training Loss:  53.1743  Training Accuracy:  0.195708\n",
      "Epoch:  28  Training Loss:  52.3465  Training Accuracy:  0.200411\n",
      "Epoch:  29  Training Loss:  51.565  Training Accuracy:  0.205585\n",
      "Epoch:  30  Training Loss:  50.766  Training Accuracy:  0.210406\n",
      "Epoch:  31  Training Loss:  49.9679  Training Accuracy:  0.215226\n",
      "Epoch:  32  Training Loss:  49.1871  Training Accuracy:  0.219871\n",
      "Epoch:  33  Training Loss:  48.4045  Training Accuracy:  0.224397\n",
      "Epoch:  34  Training Loss:  47.6021  Training Accuracy:  0.228807\n",
      "Epoch:  35  Training Loss:  46.7902  Training Accuracy:  0.233392\n",
      "Epoch:  36  Training Loss:  45.966  Training Accuracy:  0.237684\n",
      "Epoch:  37  Training Loss:  45.2127  Training Accuracy:  0.242975\n",
      "Epoch:  38  Training Loss:  44.4601  Training Accuracy:  0.246737\n",
      "Epoch:  39  Training Loss:  43.6421  Training Accuracy:  0.251264\n",
      "Epoch:  40  Training Loss:  42.7994  Training Accuracy:  0.256085\n",
      "Epoch:  41  Training Loss:  41.9449  Training Accuracy:  0.260082\n",
      "Epoch:  42  Training Loss:  41.0988  Training Accuracy:  0.26361\n",
      "Epoch:  43  Training Loss:  40.2514  Training Accuracy:  0.267431\n",
      "Epoch:  44  Training Loss:  39.3922  Training Accuracy:  0.271781\n",
      "Epoch:  45  Training Loss:  38.5712  Training Accuracy:  0.276778\n",
      "Epoch:  46  Training Loss:  37.7321  Training Accuracy:  0.280482\n",
      "Epoch:  47  Training Loss:  36.8869  Training Accuracy:  0.284597\n",
      "Epoch:  48  Training Loss:  36.0375  Training Accuracy:  0.288889\n",
      "Epoch:  49  Training Loss:  35.2052  Training Accuracy:  0.292593\n",
      "Epoch:  50  Training Loss:  34.3783  Training Accuracy:  0.29612\n",
      "Epoch:  51  Training Loss:  33.5448  Training Accuracy:  0.299824\n",
      "Epoch:  52  Training Loss:  32.7167  Training Accuracy:  0.303468\n",
      "Epoch:  53  Training Loss:  31.8879  Training Accuracy:  0.307701\n",
      "Epoch:  54  Training Loss:  31.0581  Training Accuracy:  0.310876\n",
      "Epoch:  55  Training Loss:  30.2211  Training Accuracy:  0.314874\n",
      "Epoch:  56  Training Loss:  29.4111  Training Accuracy:  0.318577\n",
      "Epoch:  57  Training Loss:  28.5931  Training Accuracy:  0.32281\n",
      "Epoch:  58  Training Loss:  27.7824  Training Accuracy:  0.326102\n",
      "Epoch:  59  Training Loss:  27.0014  Training Accuracy:  0.330335\n",
      "Epoch:  60  Training Loss:  26.1992  Training Accuracy:  0.33351\n",
      "Epoch:  61  Training Loss:  25.4336  Training Accuracy:  0.336802\n",
      "Epoch:  62  Training Loss:  24.6539  Training Accuracy:  0.339976\n",
      "Epoch:  63  Training Loss:  23.9042  Training Accuracy:  0.342916\n",
      "Epoch:  64  Training Loss:  23.1483  Training Accuracy:  0.346502\n",
      "Epoch:  65  Training Loss:  22.4412  Training Accuracy:  0.348677\n",
      "Epoch:  66  Training Loss:  21.723  Training Accuracy:  0.352205\n",
      "Epoch:  67  Training Loss:  21.0225  Training Accuracy:  0.355379\n",
      "Epoch:  68  Training Loss:  20.3516  Training Accuracy:  0.359024\n",
      "Epoch:  69  Training Loss:  19.7032  Training Accuracy:  0.362022\n",
      "Epoch:  70  Training Loss:  19.06  Training Accuracy:  0.365491\n",
      "Epoch:  71  Training Loss:  18.4401  Training Accuracy:  0.368254\n",
      "Epoch:  72  Training Loss:  17.8298  Training Accuracy:  0.371252\n",
      "Epoch:  73  Training Loss:  17.2251  Training Accuracy:  0.374427\n",
      "Epoch:  74  Training Loss:  16.6496  Training Accuracy:  0.377895\n",
      "Epoch:  75  Training Loss:  16.0848  Training Accuracy:  0.380012\n",
      "Epoch:  76  Training Loss:  15.5334  Training Accuracy:  0.382775\n",
      "Epoch:  77  Training Loss:  15.0037  Training Accuracy:  0.386067\n",
      "Epoch:  78  Training Loss:  14.4868  Training Accuracy:  0.389065\n",
      "Epoch:  79  Training Loss:  13.9847  Training Accuracy:  0.391711\n",
      "Epoch:  80  Training Loss:  13.5053  Training Accuracy:  0.394885\n",
      "Epoch:  81  Training Loss:  13.0343  Training Accuracy:  0.398119\n",
      "Epoch:  82  Training Loss:  12.5786  Training Accuracy:  0.400882\n",
      "Epoch:  83  Training Loss:  12.1438  Training Accuracy:  0.403116\n",
      "Epoch:  84  Training Loss:  11.7119  Training Accuracy:  0.40582\n",
      "Epoch:  85  Training Loss:  11.2985  Training Accuracy:  0.408818\n",
      "Epoch:  86  Training Loss:  10.8988  Training Accuracy:  0.411581\n",
      "Epoch:  87  Training Loss:  10.5106  Training Accuracy:  0.414462\n",
      "Epoch:  88  Training Loss:  10.1332  Training Accuracy:  0.416637\n",
      "Epoch:  89  Training Loss:  9.77022  Training Accuracy:  0.418636\n",
      "Epoch:  90  Training Loss:  9.41872  Training Accuracy:  0.420988\n",
      "Epoch:  91  Training Loss:  9.08052  Training Accuracy:  0.423515\n",
      "Epoch:  92  Training Loss:  8.75135  Training Accuracy:  0.426279\n",
      "Epoch:  93  Training Loss:  8.4506  Training Accuracy:  0.428924\n",
      "Epoch:  94  Training Loss:  8.15697  Training Accuracy:  0.431334\n",
      "Epoch:  95  Training Loss:  7.87957  Training Accuracy:  0.433804\n",
      "Epoch:  96  Training Loss:  7.61339  Training Accuracy:  0.436743\n",
      "Epoch:  97  Training Loss:  7.35472  Training Accuracy:  0.439388\n",
      "Epoch:  98  Training Loss:  7.11018  Training Accuracy:  0.441799\n",
      "Epoch:  99  Training Loss:  6.87857  Training Accuracy:  0.444092\n",
      "Epoch:  100  Training Loss:  6.65645  Training Accuracy:  0.447266\n",
      "Epoch:  101  Training Loss:  6.43662  Training Accuracy:  0.449089\n",
      "Epoch:  102  Training Loss:  6.23559  Training Accuracy:  0.451381\n",
      "Epoch:  103  Training Loss:  6.03682  Training Accuracy:  0.453792\n",
      "Epoch:  104  Training Loss:  5.84557  Training Accuracy:  0.456261\n",
      "Epoch:  105  Training Loss:  5.66009  Training Accuracy:  0.458319\n",
      "Epoch:  106  Training Loss:  5.48404  Training Accuracy:  0.460259\n",
      "Epoch:  107  Training Loss:  5.32296  Training Accuracy:  0.461728\n",
      "Epoch:  108  Training Loss:  5.16217  Training Accuracy:  0.463845\n",
      "Epoch:  109  Training Loss:  5.00761  Training Accuracy:  0.46555\n",
      "Epoch:  110  Training Loss:  4.86287  Training Accuracy:  0.468313\n",
      "Epoch:  111  Training Loss:  4.72423  Training Accuracy:  0.469841\n",
      "Epoch:  112  Training Loss:  4.59552  Training Accuracy:  0.472545\n",
      "Epoch:  113  Training Loss:  4.46671  Training Accuracy:  0.474662\n",
      "Epoch:  114  Training Loss:  4.34601  Training Accuracy:  0.476719\n",
      "Epoch:  115  Training Loss:  4.22496  Training Accuracy:  0.478366\n",
      "Epoch:  116  Training Loss:  4.11016  Training Accuracy:  0.4796\n",
      "Epoch:  117  Training Loss:  3.99949  Training Accuracy:  0.481834\n",
      "Epoch:  118  Training Loss:  3.89351  Training Accuracy:  0.484127\n",
      "Epoch:  119  Training Loss:  3.79508  Training Accuracy:  0.487184\n",
      "Epoch:  120  Training Loss:  3.70117  Training Accuracy:  0.488712\n",
      "Epoch:  121  Training Loss:  3.60904  Training Accuracy:  0.491182\n",
      "Epoch:  122  Training Loss:  3.52444  Training Accuracy:  0.493122\n",
      "Epoch:  123  Training Loss:  3.44099  Training Accuracy:  0.49565\n",
      "Epoch:  124  Training Loss:  3.35933  Training Accuracy:  0.497766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  3.28099  Training Accuracy:  0.499765\n",
      "Epoch:  126  Training Loss:  3.20701  Training Accuracy:  0.501176\n",
      "Epoch:  127  Training Loss:  3.13561  Training Accuracy:  0.502881\n",
      "Epoch:  128  Training Loss:  3.06874  Training Accuracy:  0.50435\n",
      "Epoch:  129  Training Loss:  3.00505  Training Accuracy:  0.505996\n",
      "Epoch:  130  Training Loss:  2.94202  Training Accuracy:  0.508465\n",
      "Epoch:  131  Training Loss:  2.88199  Training Accuracy:  0.510347\n",
      "Epoch:  132  Training Loss:  2.8237  Training Accuracy:  0.511993\n",
      "Epoch:  133  Training Loss:  2.76567  Training Accuracy:  0.513815\n",
      "Epoch:  134  Training Loss:  2.70935  Training Accuracy:  0.515109\n",
      "Epoch:  135  Training Loss:  2.65338  Training Accuracy:  0.517401\n",
      "Epoch:  136  Training Loss:  2.6015  Training Accuracy:  0.519283\n",
      "Epoch:  137  Training Loss:  2.5503  Training Accuracy:  0.52134\n",
      "Epoch:  138  Training Loss:  2.50351  Training Accuracy:  0.52281\n",
      "Epoch:  139  Training Loss:  2.457  Training Accuracy:  0.524515\n",
      "Epoch:  140  Training Loss:  2.41325  Training Accuracy:  0.526631\n",
      "Epoch:  141  Training Loss:  2.37019  Training Accuracy:  0.528571\n",
      "Epoch:  142  Training Loss:  2.32983  Training Accuracy:  0.529865\n",
      "Epoch:  143  Training Loss:  2.29036  Training Accuracy:  0.53104\n",
      "Epoch:  144  Training Loss:  2.25141  Training Accuracy:  0.532628\n",
      "Epoch:  145  Training Loss:  2.21363  Training Accuracy:  0.53445\n",
      "Epoch:  146  Training Loss:  2.177  Training Accuracy:  0.536743\n",
      "Epoch:  147  Training Loss:  2.14161  Training Accuracy:  0.537919\n",
      "Epoch:  148  Training Loss:  2.10765  Training Accuracy:  0.538977\n",
      "Epoch:  149  Training Loss:  2.07442  Training Accuracy:  0.540976\n",
      "Epoch:  150  Training Loss:  2.04229  Training Accuracy:  0.542504\n",
      "Epoch:  151  Training Loss:  2.00992  Training Accuracy:  0.544033\n",
      "Epoch:  152  Training Loss:  1.97936  Training Accuracy:  0.545444\n",
      "Epoch:  153  Training Loss:  1.94945  Training Accuracy:  0.547737\n",
      "Epoch:  154  Training Loss:  1.91899  Training Accuracy:  0.548971\n",
      "Epoch:  155  Training Loss:  1.88867  Training Accuracy:  0.550441\n",
      "Epoch:  156  Training Loss:  1.85833  Training Accuracy:  0.551499\n",
      "Epoch:  157  Training Loss:  1.82904  Training Accuracy:  0.553263\n",
      "Epoch:  158  Training Loss:  1.80017  Training Accuracy:  0.555261\n",
      "Epoch:  159  Training Loss:  1.77171  Training Accuracy:  0.55679\n",
      "Epoch:  160  Training Loss:  1.74395  Training Accuracy:  0.558848\n",
      "Epoch:  161  Training Loss:  1.71733  Training Accuracy:  0.560317\n",
      "Epoch:  162  Training Loss:  1.69153  Training Accuracy:  0.562199\n",
      "Epoch:  163  Training Loss:  1.66579  Training Accuracy:  0.563668\n",
      "Epoch:  164  Training Loss:  1.6418  Training Accuracy:  0.565608\n",
      "Epoch:  165  Training Loss:  1.61801  Training Accuracy:  0.567431\n",
      "Epoch:  166  Training Loss:  1.59437  Training Accuracy:  0.569018\n",
      "Epoch:  167  Training Loss:  1.57146  Training Accuracy:  0.570782\n",
      "Epoch:  168  Training Loss:  1.54929  Training Accuracy:  0.57231\n",
      "Epoch:  169  Training Loss:  1.52673  Training Accuracy:  0.573956\n",
      "Epoch:  170  Training Loss:  1.50505  Training Accuracy:  0.575955\n",
      "Epoch:  171  Training Loss:  1.48238  Training Accuracy:  0.577484\n",
      "Epoch:  172  Training Loss:  1.46056  Training Accuracy:  0.579424\n",
      "Epoch:  173  Training Loss:  1.44036  Training Accuracy:  0.580658\n",
      "Epoch:  174  Training Loss:  1.42026  Training Accuracy:  0.581952\n",
      "Epoch:  175  Training Loss:  1.40096  Training Accuracy:  0.583363\n",
      "Epoch:  176  Training Loss:  1.38226  Training Accuracy:  0.585303\n",
      "Epoch:  177  Training Loss:  1.36454  Training Accuracy:  0.587125\n",
      "Epoch:  178  Training Loss:  1.34752  Training Accuracy:  0.58883\n",
      "Epoch:  179  Training Loss:  1.33004  Training Accuracy:  0.591064\n",
      "Epoch:  180  Training Loss:  1.31399  Training Accuracy:  0.59271\n",
      "Epoch:  181  Training Loss:  1.29781  Training Accuracy:  0.594768\n",
      "Epoch:  182  Training Loss:  1.2825  Training Accuracy:  0.596414\n",
      "Epoch:  183  Training Loss:  1.2663  Training Accuracy:  0.59806\n",
      "Epoch:  184  Training Loss:  1.2504  Training Accuracy:  0.599765\n",
      "Epoch:  185  Training Loss:  1.23559  Training Accuracy:  0.601293\n",
      "Epoch:  186  Training Loss:  1.22048  Training Accuracy:  0.602645\n",
      "Epoch:  187  Training Loss:  1.20583  Training Accuracy:  0.604115\n",
      "Epoch:  188  Training Loss:  1.19126  Training Accuracy:  0.605997\n",
      "Epoch:  189  Training Loss:  1.17757  Training Accuracy:  0.606996\n",
      "Epoch:  190  Training Loss:  1.16412  Training Accuracy:  0.608583\n",
      "Epoch:  191  Training Loss:  1.15082  Training Accuracy:  0.609877\n",
      "Epoch:  192  Training Loss:  1.13676  Training Accuracy:  0.611287\n",
      "Epoch:  193  Training Loss:  1.12321  Training Accuracy:  0.612875\n",
      "Epoch:  194  Training Loss:  1.11027  Training Accuracy:  0.613874\n",
      "Epoch:  195  Training Loss:  1.09815  Training Accuracy:  0.615638\n",
      "Epoch:  196  Training Loss:  1.08572  Training Accuracy:  0.617284\n",
      "Epoch:  197  Training Loss:  1.07324  Training Accuracy:  0.618813\n",
      "Epoch:  198  Training Loss:  1.06211  Training Accuracy:  0.620282\n",
      "Epoch:  199  Training Loss:  1.0502  Training Accuracy:  0.622516\n",
      "Epoch:  200  Training Loss:  1.03809  Training Accuracy:  0.624045\n",
      "Epoch:  201  Training Loss:  1.02639  Training Accuracy:  0.625632\n",
      "Epoch:  202  Training Loss:  1.0148  Training Accuracy:  0.626808\n",
      "Epoch:  203  Training Loss:  1.00396  Training Accuracy:  0.628983\n",
      "Epoch:  204  Training Loss:  0.994101  Training Accuracy:  0.630394\n",
      "Epoch:  205  Training Loss:  0.984174  Training Accuracy:  0.631864\n",
      "Epoch:  206  Training Loss:  0.97513  Training Accuracy:  0.633275\n",
      "Epoch:  207  Training Loss:  0.965425  Training Accuracy:  0.634509\n",
      "Epoch:  208  Training Loss:  0.956628  Training Accuracy:  0.635685\n",
      "Epoch:  209  Training Loss:  0.947719  Training Accuracy:  0.636978\n",
      "Epoch:  210  Training Loss:  0.938946  Training Accuracy:  0.638389\n",
      "Epoch:  211  Training Loss:  0.928734  Training Accuracy:  0.639683\n",
      "Epoch:  212  Training Loss:  0.919101  Training Accuracy:  0.64174\n",
      "Epoch:  213  Training Loss:  0.910004  Training Accuracy:  0.643034\n",
      "Epoch:  214  Training Loss:  0.902455  Training Accuracy:  0.644915\n",
      "Epoch:  215  Training Loss:  0.894265  Training Accuracy:  0.646091\n",
      "Epoch:  216  Training Loss:  0.885874  Training Accuracy:  0.647913\n",
      "Epoch:  217  Training Loss:  0.878069  Training Accuracy:  0.648971\n",
      "Epoch:  218  Training Loss:  0.869739  Training Accuracy:  0.6505\n",
      "Epoch:  219  Training Loss:  0.861816  Training Accuracy:  0.651911\n",
      "Epoch:  220  Training Loss:  0.853819  Training Accuracy:  0.653322\n",
      "Epoch:  221  Training Loss:  0.846225  Training Accuracy:  0.654674\n",
      "Epoch:  222  Training Loss:  0.837806  Training Accuracy:  0.65585\n",
      "Epoch:  223  Training Loss:  0.829599  Training Accuracy:  0.657378\n",
      "Epoch:  224  Training Loss:  0.821718  Training Accuracy:  0.658965\n",
      "Epoch:  225  Training Loss:  0.814434  Training Accuracy:  0.660729\n",
      "Epoch:  226  Training Loss:  0.807704  Training Accuracy:  0.661846\n",
      "Epoch:  227  Training Loss:  0.800982  Training Accuracy:  0.663727\n",
      "Epoch:  228  Training Loss:  0.794308  Training Accuracy:  0.665021\n",
      "Epoch:  229  Training Loss:  0.787464  Training Accuracy:  0.666255\n",
      "Epoch:  230  Training Loss:  0.780447  Training Accuracy:  0.667372\n",
      "Epoch:  231  Training Loss:  0.772856  Training Accuracy:  0.669018\n",
      "Epoch:  232  Training Loss:  0.766301  Training Accuracy:  0.6709\n",
      "Epoch:  233  Training Loss:  0.759423  Training Accuracy:  0.672134\n",
      "Epoch:  234  Training Loss:  0.752573  Training Accuracy:  0.673251\n",
      "Epoch:  235  Training Loss:  0.746021  Training Accuracy:  0.674662\n",
      "Epoch:  236  Training Loss:  0.739592  Training Accuracy:  0.675838\n",
      "Epoch:  237  Training Loss:  0.732295  Training Accuracy:  0.676955\n",
      "Epoch:  238  Training Loss:  0.725322  Training Accuracy:  0.677895\n",
      "Epoch:  239  Training Loss:  0.718046  Training Accuracy:  0.679306\n",
      "Epoch:  240  Training Loss:  0.711462  Training Accuracy:  0.680306\n",
      "Epoch:  241  Training Loss:  0.70475  Training Accuracy:  0.681364\n",
      "Epoch:  242  Training Loss:  0.698046  Training Accuracy:  0.682481\n",
      "Epoch:  243  Training Loss:  0.69172  Training Accuracy:  0.683245\n",
      "Epoch:  244  Training Loss:  0.686055  Training Accuracy:  0.68448\n",
      "Epoch:  245  Training Loss:  0.680327  Training Accuracy:  0.685597\n",
      "Epoch:  246  Training Loss:  0.674496  Training Accuracy:  0.686831\n",
      "Epoch:  247  Training Loss:  0.668745  Training Accuracy:  0.687596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  0.662955  Training Accuracy:  0.688536\n",
      "Epoch:  249  Training Loss:  0.657779  Training Accuracy:  0.689771\n",
      "Epoch:  250  Training Loss:  0.652206  Training Accuracy:  0.691182\n",
      "Epoch:  251  Training Loss:  0.647171  Training Accuracy:  0.692064\n",
      "Epoch:  252  Training Loss:  0.642601  Training Accuracy:  0.693298\n",
      "Epoch:  253  Training Loss:  0.6379  Training Accuracy:  0.694709\n",
      "Epoch:  254  Training Loss:  0.633172  Training Accuracy:  0.696296\n",
      "Epoch:  255  Training Loss:  0.629059  Training Accuracy:  0.697531\n",
      "Epoch:  256  Training Loss:  0.62469  Training Accuracy:  0.698824\n",
      "Epoch:  257  Training Loss:  0.620309  Training Accuracy:  0.700412\n",
      "Epoch:  258  Training Loss:  0.615856  Training Accuracy:  0.701294\n",
      "Epoch:  259  Training Loss:  0.611411  Training Accuracy:  0.702175\n",
      "Epoch:  260  Training Loss:  0.60747  Training Accuracy:  0.703292\n",
      "Epoch:  261  Training Loss:  0.6033  Training Accuracy:  0.704351\n",
      "Epoch:  262  Training Loss:  0.600098  Training Accuracy:  0.705174\n",
      "Epoch:  263  Training Loss:  0.596633  Training Accuracy:  0.705703\n",
      "Epoch:  264  Training Loss:  0.593737  Training Accuracy:  0.707055\n",
      "Epoch:  265  Training Loss:  0.590219  Training Accuracy:  0.708995\n",
      "Epoch:  266  Training Loss:  0.586945  Training Accuracy:  0.710288\n",
      "Epoch:  267  Training Loss:  0.582877  Training Accuracy:  0.711699\n",
      "Epoch:  268  Training Loss:  0.579584  Training Accuracy:  0.712934\n",
      "Epoch:  269  Training Loss:  0.575657  Training Accuracy:  0.714521\n",
      "Epoch:  270  Training Loss:  0.572619  Training Accuracy:  0.71552\n",
      "Epoch:  271  Training Loss:  0.569487  Training Accuracy:  0.716167\n",
      "Epoch:  272  Training Loss:  0.566495  Training Accuracy:  0.717108\n",
      "Epoch:  273  Training Loss:  0.563181  Training Accuracy:  0.718107\n",
      "Epoch:  274  Training Loss:  0.559716  Training Accuracy:  0.719224\n",
      "Epoch:  275  Training Loss:  0.555999  Training Accuracy:  0.720341\n",
      "Epoch:  276  Training Loss:  0.552305  Training Accuracy:  0.721576\n",
      "Epoch:  277  Training Loss:  0.548864  Training Accuracy:  0.722281\n",
      "Epoch:  278  Training Loss:  0.545308  Training Accuracy:  0.723222\n",
      "Epoch:  279  Training Loss:  0.541744  Training Accuracy:  0.724162\n",
      "Epoch:  280  Training Loss:  0.539158  Training Accuracy:  0.725456\n",
      "Epoch:  281  Training Loss:  0.536391  Training Accuracy:  0.726632\n",
      "Epoch:  282  Training Loss:  0.533542  Training Accuracy:  0.727925\n",
      "Epoch:  283  Training Loss:  0.530823  Training Accuracy:  0.729218\n",
      "Epoch:  284  Training Loss:  0.528381  Training Accuracy:  0.729689\n",
      "Epoch:  285  Training Loss:  0.52559  Training Accuracy:  0.73057\n",
      "Epoch:  286  Training Loss:  0.523112  Training Accuracy:  0.732099\n",
      "Epoch:  287  Training Loss:  0.520659  Training Accuracy:  0.733216\n",
      "Epoch:  288  Training Loss:  0.518694  Training Accuracy:  0.73445\n",
      "Epoch:  289  Training Loss:  0.516867  Training Accuracy:  0.735568\n",
      "Epoch:  290  Training Loss:  0.514276  Training Accuracy:  0.736685\n",
      "Epoch:  291  Training Loss:  0.51188  Training Accuracy:  0.738037\n",
      "Epoch:  292  Training Loss:  0.509388  Training Accuracy:  0.739036\n",
      "Epoch:  293  Training Loss:  0.50717  Training Accuracy:  0.739683\n",
      "Epoch:  294  Training Loss:  0.504607  Training Accuracy:  0.740506\n",
      "Epoch:  295  Training Loss:  0.5018  Training Accuracy:  0.741564\n",
      "Epoch:  296  Training Loss:  0.498869  Training Accuracy:  0.742681\n",
      "Epoch:  297  Training Loss:  0.495913  Training Accuracy:  0.744209\n",
      "Epoch:  298  Training Loss:  0.493393  Training Accuracy:  0.745562\n",
      "Epoch:  299  Training Loss:  0.49078  Training Accuracy:  0.746385\n",
      "Epoch:  300  Training Loss:  0.488499  Training Accuracy:  0.747208\n",
      "Epoch:  301  Training Loss:  0.485828  Training Accuracy:  0.748031\n",
      "Epoch:  302  Training Loss:  0.48354  Training Accuracy:  0.749089\n",
      "Epoch:  303  Training Loss:  0.480961  Training Accuracy:  0.7505\n",
      "Epoch:  304  Training Loss:  0.478869  Training Accuracy:  0.751558\n",
      "Epoch:  305  Training Loss:  0.476373  Training Accuracy:  0.753498\n",
      "Epoch:  306  Training Loss:  0.474355  Training Accuracy:  0.754498\n",
      "Epoch:  307  Training Loss:  0.472302  Training Accuracy:  0.755203\n",
      "Epoch:  308  Training Loss:  0.47011  Training Accuracy:  0.75632\n",
      "Epoch:  309  Training Loss:  0.467841  Training Accuracy:  0.757261\n",
      "Epoch:  310  Training Loss:  0.465532  Training Accuracy:  0.757907\n",
      "Epoch:  311  Training Loss:  0.463571  Training Accuracy:  0.759024\n",
      "Epoch:  312  Training Loss:  0.461196  Training Accuracy:  0.760141\n",
      "Epoch:  313  Training Loss:  0.459227  Training Accuracy:  0.760964\n",
      "Epoch:  314  Training Loss:  0.457103  Training Accuracy:  0.762023\n",
      "Epoch:  315  Training Loss:  0.455109  Training Accuracy:  0.762787\n",
      "Epoch:  316  Training Loss:  0.452836  Training Accuracy:  0.763669\n",
      "Epoch:  317  Training Loss:  0.450731  Training Accuracy:  0.764374\n",
      "Epoch:  318  Training Loss:  0.448888  Training Accuracy:  0.765256\n",
      "Epoch:  319  Training Loss:  0.447134  Training Accuracy:  0.765961\n",
      "Epoch:  320  Training Loss:  0.4452  Training Accuracy:  0.766667\n",
      "Epoch:  321  Training Loss:  0.443304  Training Accuracy:  0.767372\n",
      "Epoch:  322  Training Loss:  0.441322  Training Accuracy:  0.768137\n",
      "Epoch:  323  Training Loss:  0.439014  Training Accuracy:  0.76896\n",
      "Epoch:  324  Training Loss:  0.436897  Training Accuracy:  0.769959\n",
      "Epoch:  325  Training Loss:  0.434517  Training Accuracy:  0.770723\n",
      "Epoch:  326  Training Loss:  0.432508  Training Accuracy:  0.771664\n",
      "Epoch:  327  Training Loss:  0.430579  Training Accuracy:  0.772428\n",
      "Epoch:  328  Training Loss:  0.428463  Training Accuracy:  0.773428\n",
      "Epoch:  329  Training Loss:  0.42645  Training Accuracy:  0.774662\n",
      "Epoch:  330  Training Loss:  0.424568  Training Accuracy:  0.775603\n",
      "Epoch:  331  Training Loss:  0.422558  Training Accuracy:  0.776367\n",
      "Epoch:  332  Training Loss:  0.42126  Training Accuracy:  0.777308\n",
      "Epoch:  333  Training Loss:  0.419485  Training Accuracy:  0.778072\n",
      "Epoch:  334  Training Loss:  0.418018  Training Accuracy:  0.778836\n",
      "Epoch:  335  Training Loss:  0.416406  Training Accuracy:  0.779894\n",
      "Epoch:  336  Training Loss:  0.414846  Training Accuracy:  0.780365\n",
      "Epoch:  337  Training Loss:  0.41344  Training Accuracy:  0.78107\n",
      "Epoch:  338  Training Loss:  0.412097  Training Accuracy:  0.781835\n",
      "Epoch:  339  Training Loss:  0.410507  Training Accuracy:  0.782481\n",
      "Epoch:  340  Training Loss:  0.4089  Training Accuracy:  0.783304\n",
      "Epoch:  341  Training Loss:  0.40755  Training Accuracy:  0.783951\n",
      "Epoch:  342  Training Loss:  0.405773  Training Accuracy:  0.784539\n",
      "Epoch:  343  Training Loss:  0.404338  Training Accuracy:  0.785773\n",
      "Epoch:  344  Training Loss:  0.402689  Training Accuracy:  0.786596\n",
      "Epoch:  345  Training Loss:  0.401284  Training Accuracy:  0.787302\n",
      "Epoch:  346  Training Loss:  0.399653  Training Accuracy:  0.788125\n",
      "Epoch:  347  Training Loss:  0.397898  Training Accuracy:  0.788772\n",
      "Epoch:  348  Training Loss:  0.396071  Training Accuracy:  0.789595\n",
      "Epoch:  349  Training Loss:  0.394437  Training Accuracy:  0.790241\n",
      "Epoch:  350  Training Loss:  0.392543  Training Accuracy:  0.791652\n",
      "Epoch:  351  Training Loss:  0.390975  Training Accuracy:  0.792828\n",
      "Epoch:  352  Training Loss:  0.389249  Training Accuracy:  0.793769\n",
      "Epoch:  353  Training Loss:  0.387719  Training Accuracy:  0.794768\n",
      "Epoch:  354  Training Loss:  0.386119  Training Accuracy:  0.79565\n",
      "Epoch:  355  Training Loss:  0.384675  Training Accuracy:  0.796884\n",
      "Epoch:  356  Training Loss:  0.383276  Training Accuracy:  0.797884\n",
      "Epoch:  357  Training Loss:  0.38192  Training Accuracy:  0.798883\n",
      "Epoch:  358  Training Loss:  0.380649  Training Accuracy:  0.79953\n",
      "Epoch:  359  Training Loss:  0.379321  Training Accuracy:  0.800529\n",
      "Epoch:  360  Training Loss:  0.377714  Training Accuracy:  0.801294\n",
      "Epoch:  361  Training Loss:  0.376453  Training Accuracy:  0.802058\n",
      "Epoch:  362  Training Loss:  0.374856  Training Accuracy:  0.802881\n",
      "Epoch:  363  Training Loss:  0.373334  Training Accuracy:  0.80388\n",
      "Epoch:  364  Training Loss:  0.371961  Training Accuracy:  0.804527\n",
      "Epoch:  365  Training Loss:  0.370598  Training Accuracy:  0.805291\n",
      "Epoch:  366  Training Loss:  0.369469  Training Accuracy:  0.806467\n",
      "Epoch:  367  Training Loss:  0.368195  Training Accuracy:  0.807231\n",
      "Epoch:  368  Training Loss:  0.367158  Training Accuracy:  0.807878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  369  Training Loss:  0.366189  Training Accuracy:  0.808348\n",
      "Epoch:  370  Training Loss:  0.365107  Training Accuracy:  0.809465\n",
      "Epoch:  371  Training Loss:  0.363867  Training Accuracy:  0.810171\n",
      "Epoch:  372  Training Loss:  0.362743  Training Accuracy:  0.810935\n",
      "Epoch:  373  Training Loss:  0.361527  Training Accuracy:  0.811405\n",
      "Epoch:  374  Training Loss:  0.360505  Training Accuracy:  0.811934\n",
      "Epoch:  375  Training Loss:  0.359184  Training Accuracy:  0.812875\n",
      "Epoch:  376  Training Loss:  0.358032  Training Accuracy:  0.813992\n",
      "Epoch:  377  Training Loss:  0.357164  Training Accuracy:  0.814874\n",
      "Epoch:  378  Training Loss:  0.356154  Training Accuracy:  0.815462\n",
      "Epoch:  379  Training Loss:  0.355029  Training Accuracy:  0.816579\n",
      "Epoch:  380  Training Loss:  0.35416  Training Accuracy:  0.817284\n",
      "Epoch:  381  Training Loss:  0.353123  Training Accuracy:  0.81799\n",
      "Epoch:  382  Training Loss:  0.35217  Training Accuracy:  0.818695\n",
      "Epoch:  383  Training Loss:  0.351185  Training Accuracy:  0.819518\n",
      "Epoch:  384  Training Loss:  0.349914  Training Accuracy:  0.819988\n",
      "Epoch:  385  Training Loss:  0.348992  Training Accuracy:  0.820635\n",
      "Epoch:  386  Training Loss:  0.347983  Training Accuracy:  0.821164\n",
      "Epoch:  387  Training Loss:  0.347165  Training Accuracy:  0.821752\n",
      "Epoch:  388  Training Loss:  0.345978  Training Accuracy:  0.822575\n",
      "Epoch:  389  Training Loss:  0.345157  Training Accuracy:  0.823457\n",
      "Epoch:  390  Training Loss:  0.344192  Training Accuracy:  0.82428\n",
      "Epoch:  391  Training Loss:  0.343168  Training Accuracy:  0.824633\n",
      "Epoch:  392  Training Loss:  0.342196  Training Accuracy:  0.825397\n",
      "Epoch:  393  Training Loss:  0.341381  Training Accuracy:  0.82622\n",
      "Epoch:  394  Training Loss:  0.340647  Training Accuracy:  0.826808\n",
      "Epoch:  395  Training Loss:  0.339783  Training Accuracy:  0.827396\n",
      "Epoch:  396  Training Loss:  0.338872  Training Accuracy:  0.828043\n",
      "Epoch:  397  Training Loss:  0.338123  Training Accuracy:  0.828337\n",
      "Epoch:  398  Training Loss:  0.337063  Training Accuracy:  0.829101\n",
      "Epoch:  399  Training Loss:  0.336347  Training Accuracy:  0.82963\n",
      "Epoch:  400  Training Loss:  0.335486  Training Accuracy:  0.830218\n",
      "Epoch:  401  Training Loss:  0.334652  Training Accuracy:  0.830923\n",
      "Epoch:  402  Training Loss:  0.333726  Training Accuracy:  0.831805\n",
      "Epoch:  403  Training Loss:  0.332707  Training Accuracy:  0.832981\n",
      "Epoch:  404  Training Loss:  0.331914  Training Accuracy:  0.833745\n",
      "Epoch:  405  Training Loss:  0.330964  Training Accuracy:  0.834157\n",
      "Epoch:  406  Training Loss:  0.329879  Training Accuracy:  0.834686\n",
      "Epoch:  407  Training Loss:  0.328953  Training Accuracy:  0.835333\n",
      "Epoch:  408  Training Loss:  0.328071  Training Accuracy:  0.835744\n",
      "Epoch:  409  Training Loss:  0.327074  Training Accuracy:  0.836156\n",
      "Epoch:  410  Training Loss:  0.326084  Training Accuracy:  0.836508\n",
      "Epoch:  411  Training Loss:  0.325164  Training Accuracy:  0.837331\n",
      "Epoch:  412  Training Loss:  0.324119  Training Accuracy:  0.83786\n",
      "Epoch:  413  Training Loss:  0.323091  Training Accuracy:  0.83839\n",
      "Epoch:  414  Training Loss:  0.322272  Training Accuracy:  0.838977\n",
      "Epoch:  415  Training Loss:  0.321223  Training Accuracy:  0.839448\n",
      "Epoch:  416  Training Loss:  0.320479  Training Accuracy:  0.840153\n",
      "Epoch:  417  Training Loss:  0.319347  Training Accuracy:  0.840565\n",
      "Epoch:  418  Training Loss:  0.318537  Training Accuracy:  0.841329\n",
      "Epoch:  419  Training Loss:  0.31788  Training Accuracy:  0.842152\n",
      "Epoch:  420  Training Loss:  0.316886  Training Accuracy:  0.842681\n",
      "Epoch:  421  Training Loss:  0.315928  Training Accuracy:  0.843093\n",
      "Epoch:  422  Training Loss:  0.315226  Training Accuracy:  0.843974\n",
      "Epoch:  423  Training Loss:  0.314171  Training Accuracy:  0.844797\n",
      "Epoch:  424  Training Loss:  0.31338  Training Accuracy:  0.845503\n",
      "Epoch:  425  Training Loss:  0.312957  Training Accuracy:  0.846091\n",
      "Epoch:  426  Training Loss:  0.312146  Training Accuracy:  0.846561\n",
      "Epoch:  427  Training Loss:  0.311172  Training Accuracy:  0.84709\n",
      "Epoch:  428  Training Loss:  0.310315  Training Accuracy:  0.847384\n",
      "Epoch:  429  Training Loss:  0.309519  Training Accuracy:  0.847972\n",
      "Epoch:  430  Training Loss:  0.308813  Training Accuracy:  0.848325\n",
      "Epoch:  431  Training Loss:  0.307884  Training Accuracy:  0.848795\n",
      "Epoch:  432  Training Loss:  0.3069  Training Accuracy:  0.849501\n",
      "Epoch:  433  Training Loss:  0.306379  Training Accuracy:  0.849912\n",
      "Epoch:  434  Training Loss:  0.305353  Training Accuracy:  0.850383\n",
      "Epoch:  435  Training Loss:  0.304581  Training Accuracy:  0.850735\n",
      "Epoch:  436  Training Loss:  0.303618  Training Accuracy:  0.851147\n",
      "Epoch:  437  Training Loss:  0.302791  Training Accuracy:  0.851735\n",
      "Epoch:  438  Training Loss:  0.302218  Training Accuracy:  0.852087\n",
      "Epoch:  439  Training Loss:  0.301193  Training Accuracy:  0.852499\n",
      "Epoch:  440  Training Loss:  0.300373  Training Accuracy:  0.853087\n",
      "Epoch:  441  Training Loss:  0.299568  Training Accuracy:  0.853675\n",
      "Epoch:  442  Training Loss:  0.29885  Training Accuracy:  0.854145\n",
      "Epoch:  443  Training Loss:  0.298294  Training Accuracy:  0.854615\n",
      "Epoch:  444  Training Loss:  0.297476  Training Accuracy:  0.855321\n",
      "Epoch:  445  Training Loss:  0.296781  Training Accuracy:  0.855497\n",
      "Epoch:  446  Training Loss:  0.296152  Training Accuracy:  0.855968\n",
      "Epoch:  447  Training Loss:  0.295634  Training Accuracy:  0.856144\n",
      "Epoch:  448  Training Loss:  0.295162  Training Accuracy:  0.856497\n",
      "Epoch:  449  Training Loss:  0.294559  Training Accuracy:  0.856967\n",
      "Epoch:  450  Training Loss:  0.294035  Training Accuracy:  0.857672\n",
      "Epoch:  451  Training Loss:  0.293573  Training Accuracy:  0.857966\n",
      "Epoch:  452  Training Loss:  0.292722  Training Accuracy:  0.85826\n",
      "Epoch:  453  Training Loss:  0.292468  Training Accuracy:  0.858437\n",
      "Epoch:  454  Training Loss:  0.29182  Training Accuracy:  0.859142\n",
      "Epoch:  455  Training Loss:  0.291523  Training Accuracy:  0.859612\n",
      "Epoch:  456  Training Loss:  0.29046  Training Accuracy:  0.860083\n",
      "Epoch:  457  Training Loss:  0.289909  Training Accuracy:  0.860612\n",
      "Epoch:  458  Training Loss:  0.289468  Training Accuracy:  0.861141\n",
      "Epoch:  459  Training Loss:  0.28887  Training Accuracy:  0.861376\n",
      "Epoch:  460  Training Loss:  0.288438  Training Accuracy:  0.861787\n",
      "Epoch:  461  Training Loss:  0.287836  Training Accuracy:  0.862258\n",
      "Epoch:  462  Training Loss:  0.287132  Training Accuracy:  0.862669\n",
      "Epoch:  463  Training Loss:  0.286476  Training Accuracy:  0.86314\n",
      "Epoch:  464  Training Loss:  0.285773  Training Accuracy:  0.86361\n",
      "Epoch:  465  Training Loss:  0.285235  Training Accuracy:  0.863963\n",
      "Epoch:  466  Training Loss:  0.284424  Training Accuracy:  0.864198\n",
      "Epoch:  467  Training Loss:  0.283593  Training Accuracy:  0.864551\n",
      "Epoch:  468  Training Loss:  0.282857  Training Accuracy:  0.864727\n",
      "Epoch:  469  Training Loss:  0.282055  Training Accuracy:  0.865433\n",
      "Epoch:  470  Training Loss:  0.281313  Training Accuracy:  0.865609\n",
      "Epoch:  471  Training Loss:  0.280541  Training Accuracy:  0.866079\n",
      "Epoch:  472  Training Loss:  0.279723  Training Accuracy:  0.866549\n",
      "Epoch:  473  Training Loss:  0.279094  Training Accuracy:  0.86702\n",
      "Epoch:  474  Training Loss:  0.278244  Training Accuracy:  0.867549\n",
      "Epoch:  475  Training Loss:  0.277496  Training Accuracy:  0.86796\n",
      "Epoch:  476  Training Loss:  0.276673  Training Accuracy:  0.868725\n",
      "Epoch:  477  Training Loss:  0.27588  Training Accuracy:  0.869313\n",
      "Epoch:  478  Training Loss:  0.27495  Training Accuracy:  0.869489\n",
      "Epoch:  479  Training Loss:  0.274191  Training Accuracy:  0.869665\n",
      "Epoch:  480  Training Loss:  0.27333  Training Accuracy:  0.869724\n",
      "Epoch:  481  Training Loss:  0.27235  Training Accuracy:  0.869959\n",
      "Epoch:  482  Training Loss:  0.271676  Training Accuracy:  0.870253\n",
      "Epoch:  483  Training Loss:  0.270905  Training Accuracy:  0.870488\n",
      "Epoch:  484  Training Loss:  0.269878  Training Accuracy:  0.870488\n",
      "Epoch:  485  Training Loss:  0.269139  Training Accuracy:  0.870782\n",
      "Epoch:  486  Training Loss:  0.268333  Training Accuracy:  0.87137\n",
      "Epoch:  487  Training Loss:  0.267527  Training Accuracy:  0.871664\n",
      "Epoch:  488  Training Loss:  0.266754  Training Accuracy:  0.871723\n",
      "Epoch:  489  Training Loss:  0.265897  Training Accuracy:  0.872076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  490  Training Loss:  0.265198  Training Accuracy:  0.872487\n",
      "Epoch:  491  Training Loss:  0.264323  Training Accuracy:  0.872663\n",
      "Epoch:  492  Training Loss:  0.263636  Training Accuracy:  0.873251\n",
      "Epoch:  493  Training Loss:  0.262735  Training Accuracy:  0.873604\n",
      "Epoch:  494  Training Loss:  0.262173  Training Accuracy:  0.873722\n",
      "Epoch:  495  Training Loss:  0.261363  Training Accuracy:  0.873957\n",
      "Epoch:  496  Training Loss:  0.260503  Training Accuracy:  0.874133\n",
      "Epoch:  497  Training Loss:  0.259938  Training Accuracy:  0.874251\n",
      "Epoch:  498  Training Loss:  0.259101  Training Accuracy:  0.874604\n",
      "Epoch:  499  Training Loss:  0.258635  Training Accuracy:  0.874897\n",
      "Testing Accuracy: 0.802787\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  77.3511  Training Accuracy:  0.0393886\n",
      "Epoch:  1  Training Loss:  80.1859  Training Accuracy:  0.0402116\n",
      "Epoch:  2  Training Loss:  74.5507  Training Accuracy:  0.0409759\n",
      "Epoch:  3  Training Loss:  70.0906  Training Accuracy:  0.041505\n",
      "Epoch:  4  Training Loss:  66.1622  Training Accuracy:  0.0432099\n",
      "Epoch:  5  Training Loss:  63.0256  Training Accuracy:  0.0457378\n",
      "Epoch:  6  Training Loss:  60.3409  Training Accuracy:  0.0497942\n",
      "Epoch:  7  Training Loss:  57.8776  Training Accuracy:  0.0553792\n",
      "Epoch:  8  Training Loss:  55.6271  Training Accuracy:  0.0600823\n",
      "Epoch:  9  Training Loss:  53.4911  Training Accuracy:  0.0642563\n",
      "Epoch:  10  Training Loss:  51.5495  Training Accuracy:  0.0690182\n",
      "Epoch:  11  Training Loss:  49.8849  Training Accuracy:  0.0743092\n",
      "Epoch:  12  Training Loss:  48.4075  Training Accuracy:  0.0817754\n",
      "Epoch:  13  Training Loss:  46.9097  Training Accuracy:  0.087184\n",
      "Epoch:  14  Training Loss:  45.5748  Training Accuracy:  0.0926514\n",
      "Epoch:  15  Training Loss:  44.3528  Training Accuracy:  0.0998824\n",
      "Epoch:  16  Training Loss:  43.2495  Training Accuracy:  0.106408\n",
      "Epoch:  17  Training Loss:  42.241  Training Accuracy:  0.113345\n",
      "Epoch:  18  Training Loss:  41.2554  Training Accuracy:  0.121634\n",
      "Epoch:  19  Training Loss:  40.2262  Training Accuracy:  0.128807\n",
      "Epoch:  20  Training Loss:  39.2687  Training Accuracy:  0.137096\n",
      "Epoch:  21  Training Loss:  38.326  Training Accuracy:  0.14321\n",
      "Epoch:  22  Training Loss:  37.423  Training Accuracy:  0.148736\n",
      "Epoch:  23  Training Loss:  36.5317  Training Accuracy:  0.156026\n",
      "Epoch:  24  Training Loss:  35.6889  Training Accuracy:  0.161552\n",
      "Epoch:  25  Training Loss:  34.856  Training Accuracy:  0.167372\n",
      "Epoch:  26  Training Loss:  34.0242  Training Accuracy:  0.172663\n",
      "Epoch:  27  Training Loss:  33.1908  Training Accuracy:  0.177307\n",
      "Epoch:  28  Training Loss:  32.3641  Training Accuracy:  0.183186\n",
      "Epoch:  29  Training Loss:  31.6152  Training Accuracy:  0.188007\n",
      "Epoch:  30  Training Loss:  30.8485  Training Accuracy:  0.193298\n",
      "Epoch:  31  Training Loss:  30.1021  Training Accuracy:  0.198942\n",
      "Epoch:  32  Training Loss:  29.3663  Training Accuracy:  0.204233\n",
      "Epoch:  33  Training Loss:  28.6525  Training Accuracy:  0.209465\n",
      "Epoch:  34  Training Loss:  27.9502  Training Accuracy:  0.214227\n",
      "Epoch:  35  Training Loss:  27.2587  Training Accuracy:  0.220047\n",
      "Epoch:  36  Training Loss:  26.5797  Training Accuracy:  0.22575\n",
      "Epoch:  37  Training Loss:  25.9131  Training Accuracy:  0.230982\n",
      "Epoch:  38  Training Loss:  25.2665  Training Accuracy:  0.235979\n",
      "Epoch:  39  Training Loss:  24.6309  Training Accuracy:  0.240799\n",
      "Epoch:  40  Training Loss:  24.0594  Training Accuracy:  0.245209\n",
      "Epoch:  41  Training Loss:  23.4795  Training Accuracy:  0.249971\n",
      "Epoch:  42  Training Loss:  22.8688  Training Accuracy:  0.25485\n",
      "Epoch:  43  Training Loss:  22.3029  Training Accuracy:  0.258495\n",
      "Epoch:  44  Training Loss:  21.7452  Training Accuracy:  0.262316\n",
      "Epoch:  45  Training Loss:  21.209  Training Accuracy:  0.265667\n",
      "Epoch:  46  Training Loss:  20.6861  Training Accuracy:  0.269018\n",
      "Epoch:  47  Training Loss:  20.1742  Training Accuracy:  0.273251\n",
      "Epoch:  48  Training Loss:  19.6767  Training Accuracy:  0.276602\n",
      "Epoch:  49  Training Loss:  19.2  Training Accuracy:  0.28007\n",
      "Epoch:  50  Training Loss:  18.7347  Training Accuracy:  0.284538\n",
      "Epoch:  51  Training Loss:  18.2773  Training Accuracy:  0.288595\n",
      "Epoch:  52  Training Loss:  17.8367  Training Accuracy:  0.292299\n",
      "Epoch:  53  Training Loss:  17.4141  Training Accuracy:  0.295591\n",
      "Epoch:  54  Training Loss:  17.0114  Training Accuracy:  0.298589\n",
      "Epoch:  55  Training Loss:  16.6146  Training Accuracy:  0.301646\n",
      "Epoch:  56  Training Loss:  16.2249  Training Accuracy:  0.306349\n",
      "Epoch:  57  Training Loss:  15.849  Training Accuracy:  0.309583\n",
      "Epoch:  58  Training Loss:  15.4752  Training Accuracy:  0.313992\n",
      "Epoch:  59  Training Loss:  15.1329  Training Accuracy:  0.317107\n",
      "Epoch:  60  Training Loss:  14.7861  Training Accuracy:  0.319871\n",
      "Epoch:  61  Training Loss:  14.4703  Training Accuracy:  0.323045\n",
      "Epoch:  62  Training Loss:  14.1953  Training Accuracy:  0.325985\n",
      "Epoch:  63  Training Loss:  13.8837  Training Accuracy:  0.328924\n",
      "Epoch:  64  Training Loss:  13.5594  Training Accuracy:  0.331805\n",
      "Epoch:  65  Training Loss:  13.2454  Training Accuracy:  0.335744\n",
      "Epoch:  66  Training Loss:  12.9426  Training Accuracy:  0.338565\n",
      "Epoch:  67  Training Loss:  12.6428  Training Accuracy:  0.341505\n",
      "Epoch:  68  Training Loss:  12.3539  Training Accuracy:  0.344503\n",
      "Epoch:  69  Training Loss:  12.078  Training Accuracy:  0.346855\n",
      "Epoch:  70  Training Loss:  11.8196  Training Accuracy:  0.3495\n",
      "Epoch:  71  Training Loss:  11.5693  Training Accuracy:  0.352146\n",
      "Epoch:  72  Training Loss:  11.3106  Training Accuracy:  0.354968\n",
      "Epoch:  73  Training Loss:  11.0654  Training Accuracy:  0.357554\n",
      "Epoch:  74  Training Loss:  10.8267  Training Accuracy:  0.36067\n",
      "Epoch:  75  Training Loss:  10.5897  Training Accuracy:  0.363139\n",
      "Epoch:  76  Training Loss:  10.3564  Training Accuracy:  0.365491\n",
      "Epoch:  77  Training Loss:  10.1255  Training Accuracy:  0.367842\n",
      "Epoch:  78  Training Loss:  9.90514  Training Accuracy:  0.370488\n",
      "Epoch:  79  Training Loss:  9.68881  Training Accuracy:  0.372898\n",
      "Epoch:  80  Training Loss:  9.48357  Training Accuracy:  0.37572\n",
      "Epoch:  81  Training Loss:  9.27921  Training Accuracy:  0.378248\n",
      "Epoch:  82  Training Loss:  9.08235  Training Accuracy:  0.381246\n",
      "Epoch:  83  Training Loss:  8.88798  Training Accuracy:  0.384362\n",
      "Epoch:  84  Training Loss:  8.6991  Training Accuracy:  0.388125\n",
      "Epoch:  85  Training Loss:  8.52135  Training Accuracy:  0.39077\n",
      "Epoch:  86  Training Loss:  8.34413  Training Accuracy:  0.393004\n",
      "Epoch:  87  Training Loss:  8.17488  Training Accuracy:  0.395179\n",
      "Epoch:  88  Training Loss:  8.01248  Training Accuracy:  0.398177\n",
      "Epoch:  89  Training Loss:  7.85004  Training Accuracy:  0.400647\n",
      "Epoch:  90  Training Loss:  7.69449  Training Accuracy:  0.403762\n",
      "Epoch:  91  Training Loss:  7.54515  Training Accuracy:  0.406408\n",
      "Epoch:  92  Training Loss:  7.40017  Training Accuracy:  0.409112\n",
      "Epoch:  93  Training Loss:  7.25382  Training Accuracy:  0.411464\n",
      "Epoch:  94  Training Loss:  7.11692  Training Accuracy:  0.414344\n",
      "Epoch:  95  Training Loss:  6.98018  Training Accuracy:  0.41699\n",
      "Epoch:  96  Training Loss:  6.84536  Training Accuracy:  0.419518\n",
      "Epoch:  97  Training Loss:  6.70876  Training Accuracy:  0.422692\n",
      "Epoch:  98  Training Loss:  6.58175  Training Accuracy:  0.425573\n",
      "Epoch:  99  Training Loss:  6.45781  Training Accuracy:  0.428924\n",
      "Epoch:  100  Training Loss:  6.33849  Training Accuracy:  0.432275\n",
      "Epoch:  101  Training Loss:  6.22182  Training Accuracy:  0.435038\n",
      "Epoch:  102  Training Loss:  6.10745  Training Accuracy:  0.43786\n",
      "Epoch:  103  Training Loss:  5.99526  Training Accuracy:  0.44027\n",
      "Epoch:  104  Training Loss:  5.88668  Training Accuracy:  0.443269\n",
      "Epoch:  105  Training Loss:  5.77648  Training Accuracy:  0.445326\n",
      "Epoch:  106  Training Loss:  5.67003  Training Accuracy:  0.447854\n",
      "Epoch:  107  Training Loss:  5.56402  Training Accuracy:  0.450323\n",
      "Epoch:  108  Training Loss:  5.45923  Training Accuracy:  0.452969\n",
      "Epoch:  109  Training Loss:  5.35741  Training Accuracy:  0.455203\n",
      "Epoch:  110  Training Loss:  5.25875  Training Accuracy:  0.457613\n",
      "Epoch:  111  Training Loss:  5.1637  Training Accuracy:  0.460494\n",
      "Epoch:  112  Training Loss:  5.07531  Training Accuracy:  0.462551\n",
      "Epoch:  113  Training Loss:  4.98415  Training Accuracy:  0.46502\n",
      "Epoch:  114  Training Loss:  4.89752  Training Accuracy:  0.467313\n",
      "Epoch:  115  Training Loss:  4.81144  Training Accuracy:  0.468901\n",
      "Epoch:  116  Training Loss:  4.7238  Training Accuracy:  0.47037\n",
      "Epoch:  117  Training Loss:  4.63584  Training Accuracy:  0.471958\n",
      "Epoch:  118  Training Loss:  4.54879  Training Accuracy:  0.474015\n",
      "Epoch:  119  Training Loss:  4.46338  Training Accuracy:  0.476249\n",
      "Epoch:  120  Training Loss:  4.37955  Training Accuracy:  0.478836\n",
      "Epoch:  121  Training Loss:  4.29872  Training Accuracy:  0.480893\n",
      "Epoch:  122  Training Loss:  4.21858  Training Accuracy:  0.483069\n",
      "Epoch:  123  Training Loss:  4.1443  Training Accuracy:  0.484774\n",
      "Epoch:  124  Training Loss:  4.07184  Training Accuracy:  0.487478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  4.00456  Training Accuracy:  0.4893\n",
      "Epoch:  126  Training Loss:  3.93707  Training Accuracy:  0.491475\n",
      "Epoch:  127  Training Loss:  3.874  Training Accuracy:  0.493474\n",
      "Epoch:  128  Training Loss:  3.81678  Training Accuracy:  0.495591\n",
      "Epoch:  129  Training Loss:  3.761  Training Accuracy:  0.497531\n",
      "Epoch:  130  Training Loss:  3.70569  Training Accuracy:  0.499824\n",
      "Epoch:  131  Training Loss:  3.6541  Training Accuracy:  0.501881\n",
      "Epoch:  132  Training Loss:  3.60016  Training Accuracy:  0.504233\n",
      "Epoch:  133  Training Loss:  3.5447  Training Accuracy:  0.507407\n",
      "Epoch:  134  Training Loss:  3.49512  Training Accuracy:  0.510582\n",
      "Epoch:  135  Training Loss:  3.44495  Training Accuracy:  0.513051\n",
      "Epoch:  136  Training Loss:  3.39378  Training Accuracy:  0.515814\n",
      "Epoch:  137  Training Loss:  3.34366  Training Accuracy:  0.518636\n",
      "Epoch:  138  Training Loss:  3.29414  Training Accuracy:  0.520694\n",
      "Epoch:  139  Training Loss:  3.24592  Training Accuracy:  0.523633\n",
      "Epoch:  140  Training Loss:  3.19548  Training Accuracy:  0.525867\n",
      "Epoch:  141  Training Loss:  3.1475  Training Accuracy:  0.528336\n",
      "Epoch:  142  Training Loss:  3.10031  Training Accuracy:  0.530217\n",
      "Epoch:  143  Training Loss:  3.05453  Training Accuracy:  0.531922\n",
      "Epoch:  144  Training Loss:  3.01004  Training Accuracy:  0.534097\n",
      "Epoch:  145  Training Loss:  2.96638  Training Accuracy:  0.535979\n",
      "Epoch:  146  Training Loss:  2.92413  Training Accuracy:  0.538154\n",
      "Epoch:  147  Training Loss:  2.88088  Training Accuracy:  0.539918\n",
      "Epoch:  148  Training Loss:  2.83997  Training Accuracy:  0.541446\n",
      "Epoch:  149  Training Loss:  2.80158  Training Accuracy:  0.543856\n",
      "Epoch:  150  Training Loss:  2.76126  Training Accuracy:  0.546032\n",
      "Epoch:  151  Training Loss:  2.7211  Training Accuracy:  0.547678\n",
      "Epoch:  152  Training Loss:  2.6821  Training Accuracy:  0.549324\n",
      "Epoch:  153  Training Loss:  2.64355  Training Accuracy:  0.551087\n",
      "Epoch:  154  Training Loss:  2.60642  Training Accuracy:  0.55291\n",
      "Epoch:  155  Training Loss:  2.57031  Training Accuracy:  0.554968\n",
      "Epoch:  156  Training Loss:  2.53462  Training Accuracy:  0.556437\n",
      "Epoch:  157  Training Loss:  2.50332  Training Accuracy:  0.557789\n",
      "Epoch:  158  Training Loss:  2.47108  Training Accuracy:  0.559671\n",
      "Epoch:  159  Training Loss:  2.44135  Training Accuracy:  0.56114\n",
      "Epoch:  160  Training Loss:  2.41144  Training Accuracy:  0.563198\n",
      "Epoch:  161  Training Loss:  2.38054  Training Accuracy:  0.564903\n",
      "Epoch:  162  Training Loss:  2.35131  Training Accuracy:  0.565785\n",
      "Epoch:  163  Training Loss:  2.32294  Training Accuracy:  0.567666\n",
      "Epoch:  164  Training Loss:  2.29492  Training Accuracy:  0.568724\n",
      "Epoch:  165  Training Loss:  2.26682  Training Accuracy:  0.569782\n",
      "Epoch:  166  Training Loss:  2.24101  Training Accuracy:  0.571429\n",
      "Epoch:  167  Training Loss:  2.21501  Training Accuracy:  0.57331\n",
      "Epoch:  168  Training Loss:  2.18805  Training Accuracy:  0.575191\n",
      "Epoch:  169  Training Loss:  2.16166  Training Accuracy:  0.576602\n",
      "Epoch:  170  Training Loss:  2.13676  Training Accuracy:  0.577954\n",
      "Epoch:  171  Training Loss:  2.11332  Training Accuracy:  0.579541\n",
      "Epoch:  172  Training Loss:  2.09047  Training Accuracy:  0.581599\n",
      "Epoch:  173  Training Loss:  2.06646  Training Accuracy:  0.582834\n",
      "Epoch:  174  Training Loss:  2.04453  Training Accuracy:  0.583892\n",
      "Epoch:  175  Training Loss:  2.02161  Training Accuracy:  0.585362\n",
      "Epoch:  176  Training Loss:  1.99705  Training Accuracy:  0.587184\n",
      "Epoch:  177  Training Loss:  1.97504  Training Accuracy:  0.588595\n",
      "Epoch:  178  Training Loss:  1.95398  Training Accuracy:  0.590359\n",
      "Epoch:  179  Training Loss:  1.93222  Training Accuracy:  0.591417\n",
      "Epoch:  180  Training Loss:  1.91153  Training Accuracy:  0.593239\n",
      "Epoch:  181  Training Loss:  1.89004  Training Accuracy:  0.594944\n",
      "Epoch:  182  Training Loss:  1.86811  Training Accuracy:  0.596943\n",
      "Epoch:  183  Training Loss:  1.8472  Training Accuracy:  0.598707\n",
      "Epoch:  184  Training Loss:  1.82448  Training Accuracy:  0.600941\n",
      "Epoch:  185  Training Loss:  1.8018  Training Accuracy:  0.602939\n",
      "Epoch:  186  Training Loss:  1.7814  Training Accuracy:  0.604703\n",
      "Epoch:  187  Training Loss:  1.76045  Training Accuracy:  0.606584\n",
      "Epoch:  188  Training Loss:  1.73886  Training Accuracy:  0.607937\n",
      "Epoch:  189  Training Loss:  1.71899  Training Accuracy:  0.609289\n",
      "Epoch:  190  Training Loss:  1.69943  Training Accuracy:  0.610876\n",
      "Epoch:  191  Training Loss:  1.67908  Training Accuracy:  0.612346\n",
      "Epoch:  192  Training Loss:  1.65872  Training Accuracy:  0.614286\n",
      "Epoch:  193  Training Loss:  1.63867  Training Accuracy:  0.615579\n",
      "Epoch:  194  Training Loss:  1.61895  Training Accuracy:  0.617343\n",
      "Epoch:  195  Training Loss:  1.5999  Training Accuracy:  0.618695\n",
      "Epoch:  196  Training Loss:  1.58219  Training Accuracy:  0.620517\n",
      "Epoch:  197  Training Loss:  1.56436  Training Accuracy:  0.621693\n",
      "Epoch:  198  Training Loss:  1.5468  Training Accuracy:  0.623516\n",
      "Epoch:  199  Training Loss:  1.52816  Training Accuracy:  0.625162\n",
      "Epoch:  200  Training Loss:  1.51002  Training Accuracy:  0.62669\n",
      "Epoch:  201  Training Loss:  1.4937  Training Accuracy:  0.628277\n",
      "Epoch:  202  Training Loss:  1.47599  Training Accuracy:  0.629688\n",
      "Epoch:  203  Training Loss:  1.46026  Training Accuracy:  0.630923\n",
      "Epoch:  204  Training Loss:  1.44266  Training Accuracy:  0.632216\n",
      "Epoch:  205  Training Loss:  1.42745  Training Accuracy:  0.63351\n",
      "Epoch:  206  Training Loss:  1.41173  Training Accuracy:  0.634862\n",
      "Epoch:  207  Training Loss:  1.39624  Training Accuracy:  0.636214\n",
      "Epoch:  208  Training Loss:  1.37948  Training Accuracy:  0.637331\n",
      "Epoch:  209  Training Loss:  1.36348  Training Accuracy:  0.638683\n",
      "Epoch:  210  Training Loss:  1.34689  Training Accuracy:  0.640094\n",
      "Epoch:  211  Training Loss:  1.33274  Training Accuracy:  0.641681\n",
      "Epoch:  212  Training Loss:  1.31699  Training Accuracy:  0.64321\n",
      "Epoch:  213  Training Loss:  1.30168  Training Accuracy:  0.644386\n",
      "Epoch:  214  Training Loss:  1.28761  Training Accuracy:  0.645797\n",
      "Epoch:  215  Training Loss:  1.27298  Training Accuracy:  0.646796\n",
      "Epoch:  216  Training Loss:  1.25921  Training Accuracy:  0.648089\n",
      "Epoch:  217  Training Loss:  1.24547  Training Accuracy:  0.648971\n",
      "Epoch:  218  Training Loss:  1.2317  Training Accuracy:  0.6505\n",
      "Epoch:  219  Training Loss:  1.2186  Training Accuracy:  0.651734\n",
      "Epoch:  220  Training Loss:  1.20666  Training Accuracy:  0.65291\n",
      "Epoch:  221  Training Loss:  1.19242  Training Accuracy:  0.654145\n",
      "Epoch:  222  Training Loss:  1.17927  Training Accuracy:  0.655614\n",
      "Epoch:  223  Training Loss:  1.16708  Training Accuracy:  0.657261\n",
      "Epoch:  224  Training Loss:  1.15401  Training Accuracy:  0.658789\n",
      "Epoch:  225  Training Loss:  1.14248  Training Accuracy:  0.660259\n",
      "Epoch:  226  Training Loss:  1.12919  Training Accuracy:  0.661376\n",
      "Epoch:  227  Training Loss:  1.11846  Training Accuracy:  0.662904\n",
      "Epoch:  228  Training Loss:  1.10586  Training Accuracy:  0.664315\n",
      "Epoch:  229  Training Loss:  1.09419  Training Accuracy:  0.66555\n",
      "Epoch:  230  Training Loss:  1.08246  Training Accuracy:  0.666902\n",
      "Epoch:  231  Training Loss:  1.07206  Training Accuracy:  0.668254\n",
      "Epoch:  232  Training Loss:  1.06003  Training Accuracy:  0.669783\n",
      "Epoch:  233  Training Loss:  1.04979  Training Accuracy:  0.671311\n",
      "Epoch:  234  Training Loss:  1.03933  Training Accuracy:  0.673251\n",
      "Epoch:  235  Training Loss:  1.02883  Training Accuracy:  0.674309\n",
      "Epoch:  236  Training Loss:  1.01976  Training Accuracy:  0.675544\n",
      "Epoch:  237  Training Loss:  1.00935  Training Accuracy:  0.677072\n",
      "Epoch:  238  Training Loss:  1.00009  Training Accuracy:  0.678777\n",
      "Epoch:  239  Training Loss:  0.990364  Training Accuracy:  0.680247\n",
      "Epoch:  240  Training Loss:  0.981669  Training Accuracy:  0.681364\n",
      "Epoch:  241  Training Loss:  0.972317  Training Accuracy:  0.682481\n",
      "Epoch:  242  Training Loss:  0.962533  Training Accuracy:  0.683892\n",
      "Epoch:  243  Training Loss:  0.953253  Training Accuracy:  0.685303\n",
      "Epoch:  244  Training Loss:  0.943245  Training Accuracy:  0.686361\n",
      "Epoch:  245  Training Loss:  0.934722  Training Accuracy:  0.687419\n",
      "Epoch:  246  Training Loss:  0.926677  Training Accuracy:  0.68836\n",
      "Epoch:  247  Training Loss:  0.917677  Training Accuracy:  0.689712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  0.908852  Training Accuracy:  0.691005\n",
      "Epoch:  249  Training Loss:  0.899504  Training Accuracy:  0.691887\n",
      "Epoch:  250  Training Loss:  0.890899  Training Accuracy:  0.692828\n",
      "Epoch:  251  Training Loss:  0.881958  Training Accuracy:  0.693945\n",
      "Epoch:  252  Training Loss:  0.872906  Training Accuracy:  0.694827\n",
      "Epoch:  253  Training Loss:  0.863356  Training Accuracy:  0.695826\n",
      "Epoch:  254  Training Loss:  0.854771  Training Accuracy:  0.697061\n",
      "Epoch:  255  Training Loss:  0.845001  Training Accuracy:  0.69806\n",
      "Epoch:  256  Training Loss:  0.836813  Training Accuracy:  0.699001\n",
      "Epoch:  257  Training Loss:  0.828233  Training Accuracy:  0.700059\n",
      "Epoch:  258  Training Loss:  0.819964  Training Accuracy:  0.701235\n",
      "Epoch:  259  Training Loss:  0.811445  Training Accuracy:  0.702411\n",
      "Epoch:  260  Training Loss:  0.803123  Training Accuracy:  0.70388\n",
      "Epoch:  261  Training Loss:  0.795302  Training Accuracy:  0.704997\n",
      "Epoch:  262  Training Loss:  0.787593  Training Accuracy:  0.705997\n",
      "Epoch:  263  Training Loss:  0.779855  Training Accuracy:  0.706878\n",
      "Epoch:  264  Training Loss:  0.772155  Training Accuracy:  0.708113\n",
      "Epoch:  265  Training Loss:  0.764956  Training Accuracy:  0.70923\n",
      "Epoch:  266  Training Loss:  0.756388  Training Accuracy:  0.710112\n",
      "Epoch:  267  Training Loss:  0.748637  Training Accuracy:  0.711111\n",
      "Epoch:  268  Training Loss:  0.741489  Training Accuracy:  0.712228\n",
      "Epoch:  269  Training Loss:  0.733878  Training Accuracy:  0.713522\n",
      "Epoch:  270  Training Loss:  0.726527  Training Accuracy:  0.71411\n",
      "Epoch:  271  Training Loss:  0.719852  Training Accuracy:  0.715226\n",
      "Epoch:  272  Training Loss:  0.712673  Training Accuracy:  0.71605\n",
      "Epoch:  273  Training Loss:  0.705906  Training Accuracy:  0.717519\n",
      "Epoch:  274  Training Loss:  0.698898  Training Accuracy:  0.718342\n",
      "Epoch:  275  Training Loss:  0.692408  Training Accuracy:  0.719459\n",
      "Epoch:  276  Training Loss:  0.686488  Training Accuracy:  0.720459\n",
      "Epoch:  277  Training Loss:  0.679553  Training Accuracy:  0.721282\n",
      "Epoch:  278  Training Loss:  0.672845  Training Accuracy:  0.721693\n",
      "Epoch:  279  Training Loss:  0.665947  Training Accuracy:  0.72234\n",
      "Epoch:  280  Training Loss:  0.65957  Training Accuracy:  0.723339\n",
      "Epoch:  281  Training Loss:  0.653344  Training Accuracy:  0.724456\n",
      "Epoch:  282  Training Loss:  0.647122  Training Accuracy:  0.724868\n",
      "Epoch:  283  Training Loss:  0.641394  Training Accuracy:  0.72575\n",
      "Epoch:  284  Training Loss:  0.635736  Training Accuracy:  0.726632\n",
      "Epoch:  285  Training Loss:  0.629548  Training Accuracy:  0.728101\n",
      "Epoch:  286  Training Loss:  0.6239  Training Accuracy:  0.729336\n",
      "Epoch:  287  Training Loss:  0.618812  Training Accuracy:  0.730218\n",
      "Epoch:  288  Training Loss:  0.61283  Training Accuracy:  0.731393\n",
      "Epoch:  289  Training Loss:  0.607296  Training Accuracy:  0.732217\n",
      "Epoch:  290  Training Loss:  0.601733  Training Accuracy:  0.733921\n",
      "Epoch:  291  Training Loss:  0.596531  Training Accuracy:  0.73545\n",
      "Epoch:  292  Training Loss:  0.59149  Training Accuracy:  0.736449\n",
      "Epoch:  293  Training Loss:  0.586536  Training Accuracy:  0.737625\n",
      "Epoch:  294  Training Loss:  0.581217  Training Accuracy:  0.738742\n",
      "Epoch:  295  Training Loss:  0.576648  Training Accuracy:  0.739212\n",
      "Epoch:  296  Training Loss:  0.571942  Training Accuracy:  0.740094\n",
      "Epoch:  297  Training Loss:  0.56754  Training Accuracy:  0.740858\n",
      "Epoch:  298  Training Loss:  0.562708  Training Accuracy:  0.741682\n",
      "Epoch:  299  Training Loss:  0.558568  Training Accuracy:  0.74274\n",
      "Epoch:  300  Training Loss:  0.553672  Training Accuracy:  0.743974\n",
      "Epoch:  301  Training Loss:  0.549074  Training Accuracy:  0.745032\n",
      "Epoch:  302  Training Loss:  0.544555  Training Accuracy:  0.746326\n",
      "Epoch:  303  Training Loss:  0.540282  Training Accuracy:  0.746855\n",
      "Epoch:  304  Training Loss:  0.535517  Training Accuracy:  0.747972\n",
      "Epoch:  305  Training Loss:  0.531447  Training Accuracy:  0.74903\n",
      "Epoch:  306  Training Loss:  0.52741  Training Accuracy:  0.75003\n",
      "Epoch:  307  Training Loss:  0.523417  Training Accuracy:  0.750676\n",
      "Epoch:  308  Training Loss:  0.519098  Training Accuracy:  0.751558\n",
      "Epoch:  309  Training Loss:  0.514963  Training Accuracy:  0.752793\n",
      "Epoch:  310  Training Loss:  0.510369  Training Accuracy:  0.753733\n",
      "Epoch:  311  Training Loss:  0.50633  Training Accuracy:  0.754321\n",
      "Epoch:  312  Training Loss:  0.502208  Training Accuracy:  0.755203\n",
      "Epoch:  313  Training Loss:  0.498502  Training Accuracy:  0.75632\n",
      "Epoch:  314  Training Loss:  0.494526  Training Accuracy:  0.757437\n",
      "Epoch:  315  Training Loss:  0.491124  Training Accuracy:  0.758613\n",
      "Epoch:  316  Training Loss:  0.487796  Training Accuracy:  0.759612\n",
      "Epoch:  317  Training Loss:  0.484022  Training Accuracy:  0.7602\n",
      "Epoch:  318  Training Loss:  0.480856  Training Accuracy:  0.760788\n",
      "Epoch:  319  Training Loss:  0.477515  Training Accuracy:  0.761729\n",
      "Epoch:  320  Training Loss:  0.474177  Training Accuracy:  0.762375\n",
      "Epoch:  321  Training Loss:  0.471232  Training Accuracy:  0.763022\n",
      "Epoch:  322  Training Loss:  0.468008  Training Accuracy:  0.763786\n",
      "Epoch:  323  Training Loss:  0.464838  Training Accuracy:  0.765197\n",
      "Epoch:  324  Training Loss:  0.46167  Training Accuracy:  0.765844\n",
      "Epoch:  325  Training Loss:  0.458441  Training Accuracy:  0.766314\n",
      "Epoch:  326  Training Loss:  0.455644  Training Accuracy:  0.767137\n",
      "Epoch:  327  Training Loss:  0.45251  Training Accuracy:  0.768254\n",
      "Epoch:  328  Training Loss:  0.449476  Training Accuracy:  0.769136\n",
      "Epoch:  329  Training Loss:  0.446606  Training Accuracy:  0.7699\n",
      "Epoch:  330  Training Loss:  0.443515  Training Accuracy:  0.770606\n",
      "Epoch:  331  Training Loss:  0.440473  Training Accuracy:  0.771076\n",
      "Epoch:  332  Training Loss:  0.437692  Training Accuracy:  0.772075\n",
      "Epoch:  333  Training Loss:  0.434829  Training Accuracy:  0.772899\n",
      "Epoch:  334  Training Loss:  0.43183  Training Accuracy:  0.773839\n",
      "Epoch:  335  Training Loss:  0.429302  Training Accuracy:  0.774545\n",
      "Epoch:  336  Training Loss:  0.426483  Training Accuracy:  0.775132\n",
      "Epoch:  337  Training Loss:  0.423574  Training Accuracy:  0.776073\n",
      "Epoch:  338  Training Loss:  0.420455  Training Accuracy:  0.776837\n",
      "Epoch:  339  Training Loss:  0.417331  Training Accuracy:  0.777249\n",
      "Epoch:  340  Training Loss:  0.414336  Training Accuracy:  0.778013\n",
      "Epoch:  341  Training Loss:  0.411171  Training Accuracy:  0.779013\n",
      "Epoch:  342  Training Loss:  0.408104  Training Accuracy:  0.7796\n",
      "Epoch:  343  Training Loss:  0.405142  Training Accuracy:  0.780306\n",
      "Epoch:  344  Training Loss:  0.402146  Training Accuracy:  0.781188\n",
      "Epoch:  345  Training Loss:  0.399595  Training Accuracy:  0.781717\n",
      "Epoch:  346  Training Loss:  0.396822  Training Accuracy:  0.78301\n",
      "Epoch:  347  Training Loss:  0.394098  Training Accuracy:  0.783833\n",
      "Epoch:  348  Training Loss:  0.391579  Training Accuracy:  0.78448\n",
      "Epoch:  349  Training Loss:  0.389136  Training Accuracy:  0.785479\n",
      "Epoch:  350  Training Loss:  0.386782  Training Accuracy:  0.786302\n",
      "Epoch:  351  Training Loss:  0.384482  Training Accuracy:  0.787419\n",
      "Epoch:  352  Training Loss:  0.381971  Training Accuracy:  0.787831\n",
      "Epoch:  353  Training Loss:  0.379867  Training Accuracy:  0.788419\n",
      "Epoch:  354  Training Loss:  0.377668  Training Accuracy:  0.788889\n",
      "Epoch:  355  Training Loss:  0.37547  Training Accuracy:  0.789536\n",
      "Epoch:  356  Training Loss:  0.373363  Training Accuracy:  0.790241\n",
      "Epoch:  357  Training Loss:  0.371232  Training Accuracy:  0.791064\n",
      "Epoch:  358  Training Loss:  0.369022  Training Accuracy:  0.791652\n",
      "Epoch:  359  Training Loss:  0.366665  Training Accuracy:  0.792534\n",
      "Epoch:  360  Training Loss:  0.364657  Training Accuracy:  0.79324\n",
      "Epoch:  361  Training Loss:  0.362277  Training Accuracy:  0.793827\n",
      "Epoch:  362  Training Loss:  0.360189  Training Accuracy:  0.795003\n",
      "Epoch:  363  Training Loss:  0.35792  Training Accuracy:  0.795532\n",
      "Epoch:  364  Training Loss:  0.355792  Training Accuracy:  0.796003\n",
      "Epoch:  365  Training Loss:  0.353521  Training Accuracy:  0.796473\n",
      "Epoch:  366  Training Loss:  0.351098  Training Accuracy:  0.79759\n",
      "Epoch:  367  Training Loss:  0.348837  Training Accuracy:  0.798883\n",
      "Epoch:  368  Training Loss:  0.346492  Training Accuracy:  0.799765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  369  Training Loss:  0.34422  Training Accuracy:  0.800471\n",
      "Epoch:  370  Training Loss:  0.342125  Training Accuracy:  0.801176\n",
      "Epoch:  371  Training Loss:  0.339892  Training Accuracy:  0.801705\n",
      "Epoch:  372  Training Loss:  0.337611  Training Accuracy:  0.802293\n",
      "Epoch:  373  Training Loss:  0.33561  Training Accuracy:  0.80294\n",
      "Epoch:  374  Training Loss:  0.333539  Training Accuracy:  0.803704\n",
      "Epoch:  375  Training Loss:  0.331504  Training Accuracy:  0.804351\n",
      "Epoch:  376  Training Loss:  0.32936  Training Accuracy:  0.805115\n",
      "Epoch:  377  Training Loss:  0.327201  Training Accuracy:  0.805762\n",
      "Epoch:  378  Training Loss:  0.325002  Training Accuracy:  0.806349\n",
      "Epoch:  379  Training Loss:  0.323177  Training Accuracy:  0.807408\n",
      "Epoch:  380  Training Loss:  0.321089  Training Accuracy:  0.807819\n",
      "Epoch:  381  Training Loss:  0.318774  Training Accuracy:  0.808642\n",
      "Epoch:  382  Training Loss:  0.316691  Training Accuracy:  0.809348\n",
      "Epoch:  383  Training Loss:  0.31461  Training Accuracy:  0.81023\n",
      "Epoch:  384  Training Loss:  0.312553  Training Accuracy:  0.811347\n",
      "Epoch:  385  Training Loss:  0.310847  Training Accuracy:  0.811876\n",
      "Epoch:  386  Training Loss:  0.308839  Training Accuracy:  0.812699\n",
      "Epoch:  387  Training Loss:  0.307046  Training Accuracy:  0.813287\n",
      "Epoch:  388  Training Loss:  0.305094  Training Accuracy:  0.813992\n",
      "Epoch:  389  Training Loss:  0.303172  Training Accuracy:  0.814815\n",
      "Epoch:  390  Training Loss:  0.301563  Training Accuracy:  0.816108\n",
      "Epoch:  391  Training Loss:  0.299684  Training Accuracy:  0.816814\n",
      "Epoch:  392  Training Loss:  0.297939  Training Accuracy:  0.817931\n",
      "Epoch:  393  Training Loss:  0.296437  Training Accuracy:  0.818754\n",
      "Epoch:  394  Training Loss:  0.29481  Training Accuracy:  0.81893\n",
      "Epoch:  395  Training Loss:  0.293199  Training Accuracy:  0.819342\n",
      "Epoch:  396  Training Loss:  0.291691  Training Accuracy:  0.819812\n",
      "Epoch:  397  Training Loss:  0.290173  Training Accuracy:  0.820341\n",
      "Epoch:  398  Training Loss:  0.28885  Training Accuracy:  0.820635\n",
      "Epoch:  399  Training Loss:  0.287158  Training Accuracy:  0.820929\n",
      "Epoch:  400  Training Loss:  0.285819  Training Accuracy:  0.821341\n",
      "Epoch:  401  Training Loss:  0.28416  Training Accuracy:  0.821752\n",
      "Epoch:  402  Training Loss:  0.282904  Training Accuracy:  0.82234\n",
      "Epoch:  403  Training Loss:  0.281474  Training Accuracy:  0.823104\n",
      "Epoch:  404  Training Loss:  0.280048  Training Accuracy:  0.823751\n",
      "Epoch:  405  Training Loss:  0.278726  Training Accuracy:  0.824221\n",
      "Epoch:  406  Training Loss:  0.277323  Training Accuracy:  0.824986\n",
      "Epoch:  407  Training Loss:  0.275931  Training Accuracy:  0.825867\n",
      "Epoch:  408  Training Loss:  0.274604  Training Accuracy:  0.826749\n",
      "Epoch:  409  Training Loss:  0.273193  Training Accuracy:  0.827102\n",
      "Epoch:  410  Training Loss:  0.271895  Training Accuracy:  0.827749\n",
      "Epoch:  411  Training Loss:  0.27043  Training Accuracy:  0.828395\n",
      "Epoch:  412  Training Loss:  0.269061  Training Accuracy:  0.82916\n",
      "Epoch:  413  Training Loss:  0.2677  Training Accuracy:  0.829806\n",
      "Epoch:  414  Training Loss:  0.266385  Training Accuracy:  0.830159\n",
      "Epoch:  415  Training Loss:  0.264914  Training Accuracy:  0.830865\n",
      "Epoch:  416  Training Loss:  0.263341  Training Accuracy:  0.831335\n",
      "Epoch:  417  Training Loss:  0.262052  Training Accuracy:  0.831688\n",
      "Epoch:  418  Training Loss:  0.26066  Training Accuracy:  0.832452\n",
      "Epoch:  419  Training Loss:  0.259218  Training Accuracy:  0.833216\n",
      "Epoch:  420  Training Loss:  0.257822  Training Accuracy:  0.83398\n",
      "Epoch:  421  Training Loss:  0.256751  Training Accuracy:  0.83498\n",
      "Epoch:  422  Training Loss:  0.255573  Training Accuracy:  0.835685\n",
      "Epoch:  423  Training Loss:  0.254328  Training Accuracy:  0.836214\n",
      "Epoch:  424  Training Loss:  0.253131  Training Accuracy:  0.836861\n",
      "Epoch:  425  Training Loss:  0.25182  Training Accuracy:  0.837625\n",
      "Epoch:  426  Training Loss:  0.25083  Training Accuracy:  0.837978\n",
      "Epoch:  427  Training Loss:  0.249687  Training Accuracy:  0.838566\n",
      "Epoch:  428  Training Loss:  0.24858  Training Accuracy:  0.839213\n",
      "Epoch:  429  Training Loss:  0.24732  Training Accuracy:  0.839977\n",
      "Epoch:  430  Training Loss:  0.246173  Training Accuracy:  0.840388\n",
      "Epoch:  431  Training Loss:  0.244913  Training Accuracy:  0.841035\n",
      "Epoch:  432  Training Loss:  0.243666  Training Accuracy:  0.841623\n",
      "Epoch:  433  Training Loss:  0.242579  Training Accuracy:  0.841976\n",
      "Epoch:  434  Training Loss:  0.241367  Training Accuracy:  0.84274\n",
      "Epoch:  435  Training Loss:  0.240413  Training Accuracy:  0.843504\n",
      "Epoch:  436  Training Loss:  0.239135  Training Accuracy:  0.844151\n",
      "Epoch:  437  Training Loss:  0.238048  Training Accuracy:  0.844504\n",
      "Epoch:  438  Training Loss:  0.236945  Training Accuracy:  0.845209\n",
      "Epoch:  439  Training Loss:  0.236023  Training Accuracy:  0.846032\n",
      "Epoch:  440  Training Loss:  0.235004  Training Accuracy:  0.846444\n",
      "Epoch:  441  Training Loss:  0.23396  Training Accuracy:  0.846973\n",
      "Epoch:  442  Training Loss:  0.233029  Training Accuracy:  0.847502\n",
      "Epoch:  443  Training Loss:  0.231898  Training Accuracy:  0.847737\n",
      "Epoch:  444  Training Loss:  0.230845  Training Accuracy:  0.848266\n",
      "Epoch:  445  Training Loss:  0.229724  Training Accuracy:  0.848736\n",
      "Epoch:  446  Training Loss:  0.228631  Training Accuracy:  0.848971\n",
      "Epoch:  447  Training Loss:  0.227516  Training Accuracy:  0.849207\n",
      "Epoch:  448  Training Loss:  0.226434  Training Accuracy:  0.850147\n",
      "Epoch:  449  Training Loss:  0.225195  Training Accuracy:  0.850676\n",
      "Epoch:  450  Training Loss:  0.224132  Training Accuracy:  0.850912\n",
      "Epoch:  451  Training Loss:  0.223147  Training Accuracy:  0.851206\n",
      "Epoch:  452  Training Loss:  0.221977  Training Accuracy:  0.851735\n",
      "Epoch:  453  Training Loss:  0.22115  Training Accuracy:  0.852499\n",
      "Epoch:  454  Training Loss:  0.220041  Training Accuracy:  0.852852\n",
      "Epoch:  455  Training Loss:  0.2191  Training Accuracy:  0.853498\n",
      "Epoch:  456  Training Loss:  0.218144  Training Accuracy:  0.854027\n",
      "Epoch:  457  Training Loss:  0.21724  Training Accuracy:  0.854263\n",
      "Epoch:  458  Training Loss:  0.216252  Training Accuracy:  0.854498\n",
      "Epoch:  459  Training Loss:  0.215318  Training Accuracy:  0.854909\n",
      "Epoch:  460  Training Loss:  0.214272  Training Accuracy:  0.855321\n",
      "Epoch:  461  Training Loss:  0.213397  Training Accuracy:  0.855556\n",
      "Epoch:  462  Training Loss:  0.212504  Training Accuracy:  0.855967\n",
      "Epoch:  463  Training Loss:  0.211627  Training Accuracy:  0.856614\n",
      "Epoch:  464  Training Loss:  0.210671  Training Accuracy:  0.857084\n",
      "Epoch:  465  Training Loss:  0.209879  Training Accuracy:  0.857555\n",
      "Epoch:  466  Training Loss:  0.209  Training Accuracy:  0.858084\n",
      "Epoch:  467  Training Loss:  0.208278  Training Accuracy:  0.858437\n",
      "Epoch:  468  Training Loss:  0.207527  Training Accuracy:  0.858789\n",
      "Epoch:  469  Training Loss:  0.206821  Training Accuracy:  0.859201\n",
      "Epoch:  470  Training Loss:  0.20613  Training Accuracy:  0.859377\n",
      "Epoch:  471  Training Loss:  0.205402  Training Accuracy:  0.859612\n",
      "Epoch:  472  Training Loss:  0.204576  Training Accuracy:  0.860259\n",
      "Epoch:  473  Training Loss:  0.203906  Training Accuracy:  0.860435\n",
      "Epoch:  474  Training Loss:  0.203069  Training Accuracy:  0.861023\n",
      "Epoch:  475  Training Loss:  0.202307  Training Accuracy:  0.861376\n",
      "Epoch:  476  Training Loss:  0.201455  Training Accuracy:  0.86167\n",
      "Epoch:  477  Training Loss:  0.200668  Training Accuracy:  0.861905\n",
      "Epoch:  478  Training Loss:  0.199737  Training Accuracy:  0.862375\n",
      "Epoch:  479  Training Loss:  0.198964  Training Accuracy:  0.862846\n",
      "Epoch:  480  Training Loss:  0.198203  Training Accuracy:  0.863199\n",
      "Epoch:  481  Training Loss:  0.197469  Training Accuracy:  0.863492\n",
      "Epoch:  482  Training Loss:  0.196639  Training Accuracy:  0.864022\n",
      "Epoch:  483  Training Loss:  0.195903  Training Accuracy:  0.864316\n",
      "Epoch:  484  Training Loss:  0.19511  Training Accuracy:  0.864609\n",
      "Epoch:  485  Training Loss:  0.194528  Training Accuracy:  0.865197\n",
      "Epoch:  486  Training Loss:  0.193788  Training Accuracy:  0.865668\n",
      "Epoch:  487  Training Loss:  0.193024  Training Accuracy:  0.865903\n",
      "Epoch:  488  Training Loss:  0.192263  Training Accuracy:  0.866608\n",
      "Epoch:  489  Training Loss:  0.191375  Training Accuracy:  0.866785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  490  Training Loss:  0.190449  Training Accuracy:  0.867137\n",
      "Epoch:  491  Training Loss:  0.189377  Training Accuracy:  0.867373\n",
      "Epoch:  492  Training Loss:  0.188254  Training Accuracy:  0.867667\n",
      "Epoch:  493  Training Loss:  0.187301  Training Accuracy:  0.868137\n",
      "Epoch:  494  Training Loss:  0.186498  Training Accuracy:  0.868607\n",
      "Epoch:  495  Training Loss:  0.185484  Training Accuracy:  0.868783\n",
      "Epoch:  496  Training Loss:  0.184634  Training Accuracy:  0.869077\n",
      "Epoch:  497  Training Loss:  0.183672  Training Accuracy:  0.869606\n",
      "Epoch:  498  Training Loss:  0.182821  Training Accuracy:  0.869842\n",
      "Epoch:  499  Training Loss:  0.182007  Training Accuracy:  0.870312\n",
      "Epoch:  500  Training Loss:  0.181399  Training Accuracy:  0.870606\n",
      "Epoch:  501  Training Loss:  0.180699  Training Accuracy:  0.871076\n",
      "Epoch:  502  Training Loss:  0.180197  Training Accuracy:  0.871311\n",
      "Epoch:  503  Training Loss:  0.179707  Training Accuracy:  0.871429\n",
      "Epoch:  504  Training Loss:  0.179144  Training Accuracy:  0.871605\n",
      "Epoch:  505  Training Loss:  0.178639  Training Accuracy:  0.872076\n",
      "Epoch:  506  Training Loss:  0.17804  Training Accuracy:  0.872546\n",
      "Epoch:  507  Training Loss:  0.177525  Training Accuracy:  0.872957\n",
      "Epoch:  508  Training Loss:  0.176905  Training Accuracy:  0.873251\n",
      "Epoch:  509  Training Loss:  0.17634  Training Accuracy:  0.873487\n",
      "Epoch:  510  Training Loss:  0.175778  Training Accuracy:  0.873722\n",
      "Epoch:  511  Training Loss:  0.175304  Training Accuracy:  0.87431\n",
      "Epoch:  512  Training Loss:  0.174751  Training Accuracy:  0.874486\n",
      "Epoch:  513  Training Loss:  0.174267  Training Accuracy:  0.875074\n",
      "Epoch:  514  Training Loss:  0.17381  Training Accuracy:  0.875544\n",
      "Epoch:  515  Training Loss:  0.173304  Training Accuracy:  0.875897\n",
      "Epoch:  516  Training Loss:  0.172779  Training Accuracy:  0.876544\n",
      "Epoch:  517  Training Loss:  0.172325  Training Accuracy:  0.877014\n",
      "Epoch:  518  Training Loss:  0.17194  Training Accuracy:  0.877249\n",
      "Epoch:  519  Training Loss:  0.171386  Training Accuracy:  0.877602\n",
      "Epoch:  520  Training Loss:  0.171108  Training Accuracy:  0.878013\n",
      "Epoch:  521  Training Loss:  0.170682  Training Accuracy:  0.878425\n",
      "Epoch:  522  Training Loss:  0.17036  Training Accuracy:  0.878778\n",
      "Epoch:  523  Training Loss:  0.170023  Training Accuracy:  0.87913\n",
      "Epoch:  524  Training Loss:  0.169618  Training Accuracy:  0.879542\n",
      "Epoch:  525  Training Loss:  0.169259  Training Accuracy:  0.879718\n",
      "Epoch:  526  Training Loss:  0.168875  Training Accuracy:  0.880012\n",
      "Epoch:  527  Training Loss:  0.168494  Training Accuracy:  0.880306\n",
      "Epoch:  528  Training Loss:  0.168056  Training Accuracy:  0.880659\n",
      "Epoch:  529  Training Loss:  0.167691  Training Accuracy:  0.880953\n",
      "Epoch:  530  Training Loss:  0.167296  Training Accuracy:  0.881305\n",
      "Epoch:  531  Training Loss:  0.166968  Training Accuracy:  0.881717\n",
      "Epoch:  532  Training Loss:  0.166625  Training Accuracy:  0.881952\n",
      "Epoch:  533  Training Loss:  0.166151  Training Accuracy:  0.882364\n",
      "Epoch:  534  Training Loss:  0.165772  Training Accuracy:  0.882893\n",
      "Epoch:  535  Training Loss:  0.16526  Training Accuracy:  0.88354\n",
      "Epoch:  536  Training Loss:  0.164733  Training Accuracy:  0.884245\n",
      "Epoch:  537  Training Loss:  0.164361  Training Accuracy:  0.884715\n",
      "Epoch:  538  Training Loss:  0.163863  Training Accuracy:  0.884892\n",
      "Epoch:  539  Training Loss:  0.163333  Training Accuracy:  0.885127\n",
      "Epoch:  540  Training Loss:  0.162795  Training Accuracy:  0.885421\n",
      "Epoch:  541  Training Loss:  0.162303  Training Accuracy:  0.885715\n",
      "Epoch:  542  Training Loss:  0.161734  Training Accuracy:  0.886067\n",
      "Epoch:  543  Training Loss:  0.161296  Training Accuracy:  0.88642\n",
      "Epoch:  544  Training Loss:  0.160892  Training Accuracy:  0.886714\n",
      "Epoch:  545  Training Loss:  0.160298  Training Accuracy:  0.887067\n",
      "Epoch:  546  Training Loss:  0.159836  Training Accuracy:  0.887361\n",
      "Epoch:  547  Training Loss:  0.159343  Training Accuracy:  0.887596\n",
      "Epoch:  548  Training Loss:  0.158918  Training Accuracy:  0.88789\n",
      "Epoch:  549  Training Loss:  0.158474  Training Accuracy:  0.888125\n",
      "Epoch:  550  Training Loss:  0.158084  Training Accuracy:  0.888184\n",
      "Epoch:  551  Training Loss:  0.157608  Training Accuracy:  0.88836\n",
      "Epoch:  552  Training Loss:  0.157218  Training Accuracy:  0.88836\n",
      "Epoch:  553  Training Loss:  0.156781  Training Accuracy:  0.888713\n",
      "Epoch:  554  Training Loss:  0.15633  Training Accuracy:  0.889066\n",
      "Epoch:  555  Training Loss:  0.156028  Training Accuracy:  0.889301\n",
      "Epoch:  556  Training Loss:  0.155614  Training Accuracy:  0.889654\n",
      "Epoch:  557  Training Loss:  0.155245  Training Accuracy:  0.88983\n",
      "Epoch:  558  Training Loss:  0.15474  Training Accuracy:  0.8903\n",
      "Epoch:  559  Training Loss:  0.154329  Training Accuracy:  0.890535\n",
      "Epoch:  560  Training Loss:  0.153865  Training Accuracy:  0.890829\n",
      "Epoch:  561  Training Loss:  0.153418  Training Accuracy:  0.891006\n",
      "Epoch:  562  Training Loss:  0.153012  Training Accuracy:  0.891241\n",
      "Epoch:  563  Training Loss:  0.15248  Training Accuracy:  0.891476\n",
      "Epoch:  564  Training Loss:  0.152059  Training Accuracy:  0.89177\n",
      "Epoch:  565  Training Loss:  0.151679  Training Accuracy:  0.892417\n",
      "Epoch:  566  Training Loss:  0.151245  Training Accuracy:  0.892593\n",
      "Epoch:  567  Training Loss:  0.150718  Training Accuracy:  0.893063\n",
      "Epoch:  568  Training Loss:  0.150426  Training Accuracy:  0.893122\n",
      "Epoch:  569  Training Loss:  0.149996  Training Accuracy:  0.893357\n",
      "Epoch:  570  Training Loss:  0.149611  Training Accuracy:  0.893592\n",
      "Epoch:  571  Training Loss:  0.149191  Training Accuracy:  0.893945\n",
      "Epoch:  572  Training Loss:  0.148725  Training Accuracy:  0.894298\n",
      "Epoch:  573  Training Loss:  0.148364  Training Accuracy:  0.894886\n",
      "Epoch:  574  Training Loss:  0.147916  Training Accuracy:  0.89518\n",
      "Epoch:  575  Training Loss:  0.147632  Training Accuracy:  0.895532\n",
      "Epoch:  576  Training Loss:  0.14728  Training Accuracy:  0.895885\n",
      "Epoch:  577  Training Loss:  0.146914  Training Accuracy:  0.896179\n",
      "Epoch:  578  Training Loss:  0.146589  Training Accuracy:  0.896414\n",
      "Epoch:  579  Training Loss:  0.146284  Training Accuracy:  0.896885\n",
      "Epoch:  580  Training Loss:  0.145952  Training Accuracy:  0.89712\n",
      "Epoch:  581  Training Loss:  0.145784  Training Accuracy:  0.89759\n",
      "Epoch:  582  Training Loss:  0.145408  Training Accuracy:  0.897943\n",
      "Epoch:  583  Training Loss:  0.145085  Training Accuracy:  0.898472\n",
      "Epoch:  584  Training Loss:  0.14487  Training Accuracy:  0.898825\n",
      "Epoch:  585  Training Loss:  0.144474  Training Accuracy:  0.899589\n",
      "Epoch:  586  Training Loss:  0.144212  Training Accuracy:  0.899883\n",
      "Epoch:  587  Training Loss:  0.143975  Training Accuracy:  0.900177\n",
      "Epoch:  588  Training Loss:  0.143705  Training Accuracy:  0.90053\n",
      "Epoch:  589  Training Loss:  0.143569  Training Accuracy:  0.900882\n",
      "Epoch:  590  Training Loss:  0.143309  Training Accuracy:  0.901059\n",
      "Epoch:  591  Training Loss:  0.142987  Training Accuracy:  0.901411\n",
      "Epoch:  592  Training Loss:  0.142889  Training Accuracy:  0.901705\n",
      "Epoch:  593  Training Loss:  0.142718  Training Accuracy:  0.90194\n",
      "Epoch:  594  Training Loss:  0.142462  Training Accuracy:  0.902234\n",
      "Epoch:  595  Training Loss:  0.142326  Training Accuracy:  0.902587\n",
      "Epoch:  596  Training Loss:  0.142128  Training Accuracy:  0.902764\n",
      "Epoch:  597  Training Loss:  0.141947  Training Accuracy:  0.90294\n",
      "Epoch:  598  Training Loss:  0.141832  Training Accuracy:  0.903351\n",
      "Epoch:  599  Training Loss:  0.141684  Training Accuracy:  0.903587\n",
      "Epoch:  600  Training Loss:  0.141509  Training Accuracy:  0.903998\n",
      "Epoch:  601  Training Loss:  0.141353  Training Accuracy:  0.90441\n",
      "Epoch:  602  Training Loss:  0.141046  Training Accuracy:  0.904762\n",
      "Epoch:  603  Training Loss:  0.140967  Training Accuracy:  0.904939\n",
      "Epoch:  604  Training Loss:  0.140749  Training Accuracy:  0.905233\n",
      "Epoch:  605  Training Loss:  0.140614  Training Accuracy:  0.90535\n",
      "Epoch:  606  Training Loss:  0.14037  Training Accuracy:  0.905468\n",
      "Epoch:  607  Training Loss:  0.140232  Training Accuracy:  0.905585\n",
      "Epoch:  608  Training Loss:  0.140005  Training Accuracy:  0.90582\n",
      "Epoch:  609  Training Loss:  0.139839  Training Accuracy:  0.905938\n",
      "Epoch:  610  Training Loss:  0.139554  Training Accuracy:  0.905997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  611  Training Loss:  0.139407  Training Accuracy:  0.906232\n",
      "Epoch:  612  Training Loss:  0.139136  Training Accuracy:  0.906526\n",
      "Epoch:  613  Training Loss:  0.13907  Training Accuracy:  0.906879\n",
      "Epoch:  614  Training Loss:  0.138763  Training Accuracy:  0.907055\n",
      "Epoch:  615  Training Loss:  0.138666  Training Accuracy:  0.907349\n",
      "Epoch:  616  Training Loss:  0.138443  Training Accuracy:  0.907643\n",
      "Epoch:  617  Training Loss:  0.13834  Training Accuracy:  0.908055\n",
      "Epoch:  618  Training Loss:  0.138064  Training Accuracy:  0.908878\n",
      "Epoch:  619  Training Loss:  0.137936  Training Accuracy:  0.909172\n",
      "Epoch:  620  Training Loss:  0.137752  Training Accuracy:  0.909524\n",
      "Epoch:  621  Training Loss:  0.137662  Training Accuracy:  0.910112\n",
      "Epoch:  622  Training Loss:  0.137331  Training Accuracy:  0.910524\n",
      "Epoch:  623  Training Loss:  0.137156  Training Accuracy:  0.910582\n",
      "Epoch:  624  Training Loss:  0.136936  Training Accuracy:  0.910935\n",
      "Epoch:  625  Training Loss:  0.136744  Training Accuracy:  0.91117\n",
      "Epoch:  626  Training Loss:  0.136577  Training Accuracy:  0.911406\n",
      "Epoch:  627  Training Loss:  0.13648  Training Accuracy:  0.911464\n",
      "Epoch:  628  Training Loss:  0.136242  Training Accuracy:  0.911817\n",
      "Epoch:  629  Training Loss:  0.136114  Training Accuracy:  0.911993\n",
      "Epoch:  630  Training Loss:  0.135897  Training Accuracy:  0.912287\n",
      "Epoch:  631  Training Loss:  0.135682  Training Accuracy:  0.912699\n",
      "Epoch:  632  Training Loss:  0.135565  Training Accuracy:  0.912816\n",
      "Epoch:  633  Training Loss:  0.135414  Training Accuracy:  0.913052\n",
      "Epoch:  634  Training Loss:  0.135298  Training Accuracy:  0.913463\n",
      "Epoch:  635  Training Loss:  0.135063  Training Accuracy:  0.913992\n",
      "Epoch:  636  Training Loss:  0.13494  Training Accuracy:  0.914345\n",
      "Epoch:  637  Training Loss:  0.134863  Training Accuracy:  0.914639\n",
      "Epoch:  638  Training Loss:  0.134651  Training Accuracy:  0.914874\n",
      "Epoch:  639  Training Loss:  0.134453  Training Accuracy:  0.915168\n",
      "Epoch:  640  Training Loss:  0.13446  Training Accuracy:  0.915344\n",
      "Epoch:  641  Training Loss:  0.134255  Training Accuracy:  0.915403\n",
      "Epoch:  642  Training Loss:  0.134098  Training Accuracy:  0.915521\n",
      "Epoch:  643  Training Loss:  0.133963  Training Accuracy:  0.915697\n",
      "Epoch:  644  Training Loss:  0.1338  Training Accuracy:  0.915874\n",
      "Epoch:  645  Training Loss:  0.133649  Training Accuracy:  0.915991\n",
      "Epoch:  646  Training Loss:  0.133494  Training Accuracy:  0.916167\n",
      "Epoch:  647  Training Loss:  0.133375  Training Accuracy:  0.916403\n",
      "Epoch:  648  Training Loss:  0.133166  Training Accuracy:  0.916697\n",
      "Epoch:  649  Training Loss:  0.133167  Training Accuracy:  0.916932\n",
      "Epoch:  650  Training Loss:  0.132918  Training Accuracy:  0.917049\n",
      "Epoch:  651  Training Loss:  0.132814  Training Accuracy:  0.917108\n",
      "Epoch:  652  Training Loss:  0.132663  Training Accuracy:  0.917343\n",
      "Epoch:  653  Training Loss:  0.132526  Training Accuracy:  0.917696\n",
      "Epoch:  654  Training Loss:  0.13238  Training Accuracy:  0.917814\n",
      "Epoch:  655  Training Loss:  0.132294  Training Accuracy:  0.918049\n",
      "Epoch:  656  Training Loss:  0.132167  Training Accuracy:  0.918166\n",
      "Epoch:  657  Training Loss:  0.132075  Training Accuracy:  0.918519\n",
      "Epoch:  658  Training Loss:  0.131937  Training Accuracy:  0.918813\n",
      "Epoch:  659  Training Loss:  0.131778  Training Accuracy:  0.918989\n",
      "Epoch:  660  Training Loss:  0.13167  Training Accuracy:  0.919283\n",
      "Epoch:  661  Training Loss:  0.131595  Training Accuracy:  0.919754\n",
      "Epoch:  662  Training Loss:  0.131439  Training Accuracy:  0.919871\n",
      "Epoch:  663  Training Loss:  0.131284  Training Accuracy:  0.920224\n",
      "Epoch:  664  Training Loss:  0.131113  Training Accuracy:  0.920518\n",
      "Epoch:  665  Training Loss:  0.131016  Training Accuracy:  0.920753\n",
      "Epoch:  666  Training Loss:  0.130858  Training Accuracy:  0.920812\n",
      "Epoch:  667  Training Loss:  0.130738  Training Accuracy:  0.921106\n",
      "Epoch:  668  Training Loss:  0.130444  Training Accuracy:  0.921458\n",
      "Epoch:  669  Training Loss:  0.130331  Training Accuracy:  0.921635\n",
      "Epoch:  670  Training Loss:  0.13007  Training Accuracy:  0.921811\n",
      "Epoch:  671  Training Loss:  0.130032  Training Accuracy:  0.921988\n",
      "Epoch:  672  Training Loss:  0.129809  Training Accuracy:  0.922046\n",
      "Epoch:  673  Training Loss:  0.12966  Training Accuracy:  0.92234\n",
      "Epoch:  674  Training Loss:  0.129499  Training Accuracy:  0.922517\n",
      "Epoch:  675  Training Loss:  0.12936  Training Accuracy:  0.922752\n",
      "Epoch:  676  Training Loss:  0.129183  Training Accuracy:  0.922811\n",
      "Epoch:  677  Training Loss:  0.129183  Training Accuracy:  0.922928\n",
      "Epoch:  678  Training Loss:  0.129008  Training Accuracy:  0.923105\n",
      "Epoch:  679  Training Loss:  0.128869  Training Accuracy:  0.923457\n",
      "Epoch:  680  Training Loss:  0.128736  Training Accuracy:  0.923634\n",
      "Epoch:  681  Training Loss:  0.128596  Training Accuracy:  0.923986\n",
      "Epoch:  682  Training Loss:  0.128436  Training Accuracy:  0.924221\n",
      "Epoch:  683  Training Loss:  0.128335  Training Accuracy:  0.924339\n",
      "Epoch:  684  Training Loss:  0.128078  Training Accuracy:  0.924457\n",
      "Epoch:  685  Training Loss:  0.12798  Training Accuracy:  0.924868\n",
      "Epoch:  686  Training Loss:  0.127917  Training Accuracy:  0.925103\n",
      "Epoch:  687  Training Loss:  0.127748  Training Accuracy:  0.92528\n",
      "Epoch:  688  Training Loss:  0.127708  Training Accuracy:  0.925338\n",
      "Epoch:  689  Training Loss:  0.127549  Training Accuracy:  0.925456\n",
      "Epoch:  690  Training Loss:  0.127442  Training Accuracy:  0.925809\n",
      "Epoch:  691  Training Loss:  0.127279  Training Accuracy:  0.925868\n",
      "Epoch:  692  Training Loss:  0.127204  Training Accuracy:  0.926103\n",
      "Epoch:  693  Training Loss:  0.127073  Training Accuracy:  0.92622\n",
      "Epoch:  694  Training Loss:  0.126914  Training Accuracy:  0.926279\n",
      "Epoch:  695  Training Loss:  0.126821  Training Accuracy:  0.926632\n",
      "Epoch:  696  Training Loss:  0.12662  Training Accuracy:  0.927043\n",
      "Epoch:  697  Training Loss:  0.126623  Training Accuracy:  0.92722\n",
      "Epoch:  698  Training Loss:  0.126441  Training Accuracy:  0.927279\n",
      "Epoch:  699  Training Loss:  0.126378  Training Accuracy:  0.927514\n",
      "Testing Accuracy: 0.841337\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 700\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  111.776  Training Accuracy:  0.040682\n",
      "Epoch:  1  Training Loss:  103.429  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  97.7269  Training Accuracy:  0.0417402\n",
      "Epoch:  3  Training Loss:  92.9377  Training Accuracy:  0.0458554\n",
      "Epoch:  4  Training Loss:  88.5904  Training Accuracy:  0.0480306\n",
      "Epoch:  5  Training Loss:  84.5804  Training Accuracy:  0.0510288\n",
      "Epoch:  6  Training Loss:  81.0006  Training Accuracy:  0.0535567\n",
      "Epoch:  7  Training Loss:  77.8068  Training Accuracy:  0.0560259\n",
      "Epoch:  8  Training Loss:  74.7887  Training Accuracy:  0.0594944\n",
      "Epoch:  9  Training Loss:  71.8854  Training Accuracy:  0.06408\n",
      "Epoch:  10  Training Loss:  69.1493  Training Accuracy:  0.0701352\n",
      "Epoch:  11  Training Loss:  66.5967  Training Accuracy:  0.0758965\n",
      "Epoch:  12  Training Loss:  64.0846  Training Accuracy:  0.0815403\n",
      "Epoch:  13  Training Loss:  61.7395  Training Accuracy:  0.0890065\n",
      "Epoch:  14  Training Loss:  59.6304  Training Accuracy:  0.0961199\n",
      "Epoch:  15  Training Loss:  57.6963  Training Accuracy:  0.102881\n",
      "Epoch:  16  Training Loss:  55.6644  Training Accuracy:  0.109818\n",
      "Epoch:  17  Training Loss:  53.8186  Training Accuracy:  0.118812\n",
      "Epoch:  18  Training Loss:  52.0951  Training Accuracy:  0.128042\n",
      "Epoch:  19  Training Loss:  50.3755  Training Accuracy:  0.135744\n",
      "Epoch:  20  Training Loss:  48.7262  Training Accuracy:  0.144092\n",
      "Epoch:  21  Training Loss:  47.1538  Training Accuracy:  0.150911\n",
      "Epoch:  22  Training Loss:  45.6563  Training Accuracy:  0.158671\n",
      "Epoch:  23  Training Loss:  44.2264  Training Accuracy:  0.164962\n",
      "Epoch:  24  Training Loss:  42.863  Training Accuracy:  0.171605\n",
      "Epoch:  25  Training Loss:  41.564  Training Accuracy:  0.178189\n",
      "Epoch:  26  Training Loss:  40.3241  Training Accuracy:  0.183774\n",
      "Epoch:  27  Training Loss:  39.0993  Training Accuracy:  0.189359\n",
      "Epoch:  28  Training Loss:  37.9119  Training Accuracy:  0.194944\n",
      "Epoch:  29  Training Loss:  36.7052  Training Accuracy:  0.200588\n",
      "Epoch:  30  Training Loss:  35.6448  Training Accuracy:  0.205879\n",
      "Epoch:  31  Training Loss:  34.6  Training Accuracy:  0.210464\n",
      "Epoch:  32  Training Loss:  33.5913  Training Accuracy:  0.214462\n",
      "Epoch:  33  Training Loss:  32.6406  Training Accuracy:  0.219577\n",
      "Epoch:  34  Training Loss:  31.7163  Training Accuracy:  0.223633\n",
      "Epoch:  35  Training Loss:  30.8237  Training Accuracy:  0.227631\n",
      "Epoch:  36  Training Loss:  29.9451  Training Accuracy:  0.231452\n",
      "Epoch:  37  Training Loss:  29.1165  Training Accuracy:  0.236273\n",
      "Epoch:  38  Training Loss:  28.3237  Training Accuracy:  0.240506\n",
      "Epoch:  39  Training Loss:  27.5473  Training Accuracy:  0.244562\n",
      "Epoch:  40  Training Loss:  26.7995  Training Accuracy:  0.247972\n",
      "Epoch:  41  Training Loss:  26.066  Training Accuracy:  0.25144\n",
      "Epoch:  42  Training Loss:  25.3666  Training Accuracy:  0.255673\n",
      "Epoch:  43  Training Loss:  24.7089  Training Accuracy:  0.259083\n",
      "Epoch:  44  Training Loss:  24.0668  Training Accuracy:  0.262434\n",
      "Epoch:  45  Training Loss:  23.4173  Training Accuracy:  0.266902\n",
      "Epoch:  46  Training Loss:  22.7768  Training Accuracy:  0.270664\n",
      "Epoch:  47  Training Loss:  22.1521  Training Accuracy:  0.275073\n",
      "Epoch:  48  Training Loss:  21.572  Training Accuracy:  0.278072\n",
      "Epoch:  49  Training Loss:  20.9909  Training Accuracy:  0.281423\n",
      "Epoch:  50  Training Loss:  20.4262  Training Accuracy:  0.284891\n",
      "Epoch:  51  Training Loss:  19.871  Training Accuracy:  0.288183\n",
      "Epoch:  52  Training Loss:  19.3234  Training Accuracy:  0.292005\n",
      "Epoch:  53  Training Loss:  18.7968  Training Accuracy:  0.29512\n",
      "Epoch:  54  Training Loss:  18.2781  Training Accuracy:  0.298589\n",
      "Epoch:  55  Training Loss:  17.778  Training Accuracy:  0.301764\n",
      "Epoch:  56  Training Loss:  17.2873  Training Accuracy:  0.305115\n",
      "Epoch:  57  Training Loss:  16.8025  Training Accuracy:  0.308466\n",
      "Epoch:  58  Training Loss:  16.3211  Training Accuracy:  0.311229\n",
      "Epoch:  59  Training Loss:  15.8676  Training Accuracy:  0.313463\n",
      "Epoch:  60  Training Loss:  15.422  Training Accuracy:  0.316872\n",
      "Epoch:  61  Training Loss:  14.9725  Training Accuracy:  0.319929\n",
      "Epoch:  62  Training Loss:  14.5339  Training Accuracy:  0.323574\n",
      "Epoch:  63  Training Loss:  14.1198  Training Accuracy:  0.326573\n",
      "Epoch:  64  Training Loss:  13.7134  Training Accuracy:  0.329688\n",
      "Epoch:  65  Training Loss:  13.3222  Training Accuracy:  0.332863\n",
      "Epoch:  66  Training Loss:  12.9307  Training Accuracy:  0.335626\n",
      "Epoch:  67  Training Loss:  12.548  Training Accuracy:  0.338742\n",
      "Epoch:  68  Training Loss:  12.1749  Training Accuracy:  0.341799\n",
      "Epoch:  69  Training Loss:  11.8138  Training Accuracy:  0.345267\n",
      "Epoch:  70  Training Loss:  11.4598  Training Accuracy:  0.347325\n",
      "Epoch:  71  Training Loss:  11.1193  Training Accuracy:  0.349912\n",
      "Epoch:  72  Training Loss:  10.7794  Training Accuracy:  0.352322\n",
      "Epoch:  73  Training Loss:  10.452  Training Accuracy:  0.35485\n",
      "Epoch:  74  Training Loss:  10.1329  Training Accuracy:  0.357789\n",
      "Epoch:  75  Training Loss:  9.82492  Training Accuracy:  0.360846\n",
      "Epoch:  76  Training Loss:  9.52112  Training Accuracy:  0.363668\n",
      "Epoch:  77  Training Loss:  9.22123  Training Accuracy:  0.366961\n",
      "Epoch:  78  Training Loss:  8.92386  Training Accuracy:  0.369782\n",
      "Epoch:  79  Training Loss:  8.63551  Training Accuracy:  0.372957\n",
      "Epoch:  80  Training Loss:  8.35635  Training Accuracy:  0.375838\n",
      "Epoch:  81  Training Loss:  8.08938  Training Accuracy:  0.378424\n",
      "Epoch:  82  Training Loss:  7.8289  Training Accuracy:  0.38107\n",
      "Epoch:  83  Training Loss:  7.57375  Training Accuracy:  0.383539\n",
      "Epoch:  84  Training Loss:  7.32906  Training Accuracy:  0.385655\n",
      "Epoch:  85  Training Loss:  7.09206  Training Accuracy:  0.387948\n",
      "Epoch:  86  Training Loss:  6.86634  Training Accuracy:  0.391064\n",
      "Epoch:  87  Training Loss:  6.64512  Training Accuracy:  0.393709\n",
      "Epoch:  88  Training Loss:  6.43331  Training Accuracy:  0.396767\n",
      "Epoch:  89  Training Loss:  6.22289  Training Accuracy:  0.4\n",
      "Epoch:  90  Training Loss:  6.01536  Training Accuracy:  0.403351\n",
      "Epoch:  91  Training Loss:  5.81578  Training Accuracy:  0.406114\n",
      "Epoch:  92  Training Loss:  5.62788  Training Accuracy:  0.408995\n",
      "Epoch:  93  Training Loss:  5.44724  Training Accuracy:  0.412052\n",
      "Epoch:  94  Training Loss:  5.27537  Training Accuracy:  0.414344\n",
      "Epoch:  95  Training Loss:  5.10182  Training Accuracy:  0.416578\n",
      "Epoch:  96  Training Loss:  4.93414  Training Accuracy:  0.418812\n",
      "Epoch:  97  Training Loss:  4.77873  Training Accuracy:  0.421164\n",
      "Epoch:  98  Training Loss:  4.62579  Training Accuracy:  0.423751\n",
      "Epoch:  99  Training Loss:  4.47652  Training Accuracy:  0.426631\n",
      "Epoch:  100  Training Loss:  4.3319  Training Accuracy:  0.429453\n",
      "Epoch:  101  Training Loss:  4.18953  Training Accuracy:  0.431805\n",
      "Epoch:  102  Training Loss:  4.05409  Training Accuracy:  0.433921\n",
      "Epoch:  103  Training Loss:  3.92399  Training Accuracy:  0.437096\n",
      "Epoch:  104  Training Loss:  3.80207  Training Accuracy:  0.438859\n",
      "Epoch:  105  Training Loss:  3.67995  Training Accuracy:  0.441564\n",
      "Epoch:  106  Training Loss:  3.56339  Training Accuracy:  0.443739\n",
      "Epoch:  107  Training Loss:  3.45119  Training Accuracy:  0.446384\n",
      "Epoch:  108  Training Loss:  3.33956  Training Accuracy:  0.448207\n",
      "Epoch:  109  Training Loss:  3.23107  Training Accuracy:  0.450794\n",
      "Epoch:  110  Training Loss:  3.13  Training Accuracy:  0.453498\n",
      "Epoch:  111  Training Loss:  3.03503  Training Accuracy:  0.455673\n",
      "Epoch:  112  Training Loss:  2.94269  Training Accuracy:  0.457437\n",
      "Epoch:  113  Training Loss:  2.85657  Training Accuracy:  0.459965\n",
      "Epoch:  114  Training Loss:  2.77044  Training Accuracy:  0.46261\n",
      "Epoch:  115  Training Loss:  2.6877  Training Accuracy:  0.464785\n",
      "Epoch:  116  Training Loss:  2.60825  Training Accuracy:  0.467137\n",
      "Epoch:  117  Training Loss:  2.53412  Training Accuracy:  0.469253\n",
      "Epoch:  118  Training Loss:  2.4627  Training Accuracy:  0.471605\n",
      "Epoch:  119  Training Loss:  2.39252  Training Accuracy:  0.47378\n",
      "Epoch:  120  Training Loss:  2.32491  Training Accuracy:  0.476367\n",
      "Epoch:  121  Training Loss:  2.25723  Training Accuracy:  0.478836\n",
      "Epoch:  122  Training Loss:  2.19224  Training Accuracy:  0.480306\n",
      "Epoch:  123  Training Loss:  2.1265  Training Accuracy:  0.481893\n",
      "Epoch:  124  Training Loss:  2.06594  Training Accuracy:  0.484127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  2.0076  Training Accuracy:  0.485361\n",
      "Epoch:  126  Training Loss:  1.95159  Training Accuracy:  0.487478\n",
      "Epoch:  127  Training Loss:  1.89598  Training Accuracy:  0.489124\n",
      "Epoch:  128  Training Loss:  1.84414  Training Accuracy:  0.490358\n",
      "Epoch:  129  Training Loss:  1.79353  Training Accuracy:  0.492299\n",
      "Epoch:  130  Training Loss:  1.74617  Training Accuracy:  0.494533\n",
      "Epoch:  131  Training Loss:  1.69989  Training Accuracy:  0.496179\n",
      "Epoch:  132  Training Loss:  1.6561  Training Accuracy:  0.497883\n",
      "Epoch:  133  Training Loss:  1.61484  Training Accuracy:  0.499824\n",
      "Epoch:  134  Training Loss:  1.5742  Training Accuracy:  0.502175\n",
      "Epoch:  135  Training Loss:  1.54224  Training Accuracy:  0.503586\n",
      "Epoch:  136  Training Loss:  1.51077  Training Accuracy:  0.50582\n",
      "Epoch:  137  Training Loss:  1.48073  Training Accuracy:  0.507995\n",
      "Epoch:  138  Training Loss:  1.4517  Training Accuracy:  0.50923\n",
      "Epoch:  139  Training Loss:  1.4243  Training Accuracy:  0.51117\n",
      "Epoch:  140  Training Loss:  1.39732  Training Accuracy:  0.512522\n",
      "Epoch:  141  Training Loss:  1.37122  Training Accuracy:  0.514344\n",
      "Epoch:  142  Training Loss:  1.34752  Training Accuracy:  0.516402\n",
      "Epoch:  143  Training Loss:  1.32424  Training Accuracy:  0.518518\n",
      "Epoch:  144  Training Loss:  1.30125  Training Accuracy:  0.5204\n",
      "Epoch:  145  Training Loss:  1.2791  Training Accuracy:  0.521634\n",
      "Epoch:  146  Training Loss:  1.25801  Training Accuracy:  0.52281\n",
      "Epoch:  147  Training Loss:  1.23784  Training Accuracy:  0.525162\n",
      "Epoch:  148  Training Loss:  1.21837  Training Accuracy:  0.526279\n",
      "Epoch:  149  Training Loss:  1.20044  Training Accuracy:  0.527572\n",
      "Epoch:  150  Training Loss:  1.18254  Training Accuracy:  0.528983\n",
      "Epoch:  151  Training Loss:  1.1649  Training Accuracy:  0.530688\n",
      "Epoch:  152  Training Loss:  1.14881  Training Accuracy:  0.53251\n",
      "Epoch:  153  Training Loss:  1.13232  Training Accuracy:  0.534156\n",
      "Epoch:  154  Training Loss:  1.11838  Training Accuracy:  0.535685\n",
      "Epoch:  155  Training Loss:  1.10345  Training Accuracy:  0.537448\n",
      "Epoch:  156  Training Loss:  1.08872  Training Accuracy:  0.539212\n",
      "Epoch:  157  Training Loss:  1.07521  Training Accuracy:  0.541329\n",
      "Epoch:  158  Training Loss:  1.06133  Training Accuracy:  0.542446\n",
      "Epoch:  159  Training Loss:  1.04738  Training Accuracy:  0.544092\n",
      "Epoch:  160  Training Loss:  1.03425  Training Accuracy:  0.545914\n",
      "Epoch:  161  Training Loss:  1.02246  Training Accuracy:  0.547207\n",
      "Epoch:  162  Training Loss:  1.00855  Training Accuracy:  0.548677\n",
      "Epoch:  163  Training Loss:  0.997402  Training Accuracy:  0.550735\n",
      "Epoch:  164  Training Loss:  0.984563  Training Accuracy:  0.552616\n",
      "Epoch:  165  Training Loss:  0.971858  Training Accuracy:  0.555026\n",
      "Epoch:  166  Training Loss:  0.959213  Training Accuracy:  0.556849\n",
      "Epoch:  167  Training Loss:  0.947197  Training Accuracy:  0.558319\n",
      "Epoch:  168  Training Loss:  0.936188  Training Accuracy:  0.560317\n",
      "Epoch:  169  Training Loss:  0.923254  Training Accuracy:  0.56214\n",
      "Epoch:  170  Training Loss:  0.912538  Training Accuracy:  0.563904\n",
      "Epoch:  171  Training Loss:  0.901157  Training Accuracy:  0.565961\n",
      "Epoch:  172  Training Loss:  0.889531  Training Accuracy:  0.567431\n",
      "Epoch:  173  Training Loss:  0.87846  Training Accuracy:  0.569018\n",
      "Epoch:  174  Training Loss:  0.867837  Training Accuracy:  0.5699\n",
      "Epoch:  175  Training Loss:  0.857352  Training Accuracy:  0.57137\n",
      "Epoch:  176  Training Loss:  0.84672  Training Accuracy:  0.572957\n",
      "Epoch:  177  Training Loss:  0.836422  Training Accuracy:  0.575309\n",
      "Epoch:  178  Training Loss:  0.826999  Training Accuracy:  0.576661\n",
      "Epoch:  179  Training Loss:  0.817137  Training Accuracy:  0.577954\n",
      "Epoch:  180  Training Loss:  0.806782  Training Accuracy:  0.579777\n",
      "Epoch:  181  Training Loss:  0.79752  Training Accuracy:  0.581129\n",
      "Epoch:  182  Training Loss:  0.788593  Training Accuracy:  0.58254\n",
      "Epoch:  183  Training Loss:  0.779634  Training Accuracy:  0.584303\n",
      "Epoch:  184  Training Loss:  0.771689  Training Accuracy:  0.585832\n",
      "Epoch:  185  Training Loss:  0.764036  Training Accuracy:  0.587654\n",
      "Epoch:  186  Training Loss:  0.755459  Training Accuracy:  0.589242\n",
      "Epoch:  187  Training Loss:  0.747301  Training Accuracy:  0.591005\n",
      "Epoch:  188  Training Loss:  0.739785  Training Accuracy:  0.592887\n",
      "Epoch:  189  Training Loss:  0.732619  Training Accuracy:  0.593945\n",
      "Epoch:  190  Training Loss:  0.725669  Training Accuracy:  0.596002\n",
      "Epoch:  191  Training Loss:  0.718911  Training Accuracy:  0.597648\n",
      "Epoch:  192  Training Loss:  0.712665  Training Accuracy:  0.599118\n",
      "Epoch:  193  Training Loss:  0.705743  Training Accuracy:  0.600529\n",
      "Epoch:  194  Training Loss:  0.699975  Training Accuracy:  0.602234\n",
      "Epoch:  195  Training Loss:  0.693373  Training Accuracy:  0.604115\n",
      "Epoch:  196  Training Loss:  0.687299  Training Accuracy:  0.605761\n",
      "Epoch:  197  Training Loss:  0.680932  Training Accuracy:  0.607701\n",
      "Epoch:  198  Training Loss:  0.674888  Training Accuracy:  0.609641\n",
      "Epoch:  199  Training Loss:  0.668454  Training Accuracy:  0.610876\n",
      "Epoch:  200  Training Loss:  0.662607  Training Accuracy:  0.611934\n",
      "Epoch:  201  Training Loss:  0.656455  Training Accuracy:  0.613521\n",
      "Epoch:  202  Training Loss:  0.650877  Training Accuracy:  0.61458\n",
      "Epoch:  203  Training Loss:  0.644737  Training Accuracy:  0.616226\n",
      "Epoch:  204  Training Loss:  0.639194  Training Accuracy:  0.618225\n",
      "Epoch:  205  Training Loss:  0.633525  Training Accuracy:  0.619577\n",
      "Epoch:  206  Training Loss:  0.627452  Training Accuracy:  0.620929\n",
      "Epoch:  207  Training Loss:  0.621597  Training Accuracy:  0.622281\n",
      "Epoch:  208  Training Loss:  0.616388  Training Accuracy:  0.623986\n",
      "Epoch:  209  Training Loss:  0.610864  Training Accuracy:  0.626573\n",
      "Epoch:  210  Training Loss:  0.605563  Training Accuracy:  0.628336\n",
      "Epoch:  211  Training Loss:  0.600211  Training Accuracy:  0.6301\n",
      "Epoch:  212  Training Loss:  0.594616  Training Accuracy:  0.632393\n",
      "Epoch:  213  Training Loss:  0.58955  Training Accuracy:  0.634098\n",
      "Epoch:  214  Training Loss:  0.58474  Training Accuracy:  0.635803\n",
      "Epoch:  215  Training Loss:  0.579937  Training Accuracy:  0.637449\n",
      "Epoch:  216  Training Loss:  0.575497  Training Accuracy:  0.639153\n",
      "Epoch:  217  Training Loss:  0.571472  Training Accuracy:  0.640623\n",
      "Epoch:  218  Training Loss:  0.56692  Training Accuracy:  0.64174\n",
      "Epoch:  219  Training Loss:  0.562336  Training Accuracy:  0.643034\n",
      "Epoch:  220  Training Loss:  0.558333  Training Accuracy:  0.644738\n",
      "Epoch:  221  Training Loss:  0.553807  Training Accuracy:  0.646385\n",
      "Epoch:  222  Training Loss:  0.549172  Training Accuracy:  0.647678\n",
      "Epoch:  223  Training Loss:  0.544169  Training Accuracy:  0.649206\n",
      "Epoch:  224  Training Loss:  0.539613  Training Accuracy:  0.650265\n",
      "Epoch:  225  Training Loss:  0.534577  Training Accuracy:  0.651676\n",
      "Epoch:  226  Training Loss:  0.530818  Training Accuracy:  0.653674\n",
      "Epoch:  227  Training Loss:  0.527816  Training Accuracy:  0.655027\n",
      "Epoch:  228  Training Loss:  0.524768  Training Accuracy:  0.655967\n",
      "Epoch:  229  Training Loss:  0.521594  Training Accuracy:  0.657554\n",
      "Epoch:  230  Training Loss:  0.518163  Training Accuracy:  0.659142\n",
      "Epoch:  231  Training Loss:  0.515042  Training Accuracy:  0.660612\n",
      "Epoch:  232  Training Loss:  0.5108  Training Accuracy:  0.66214\n",
      "Epoch:  233  Training Loss:  0.507166  Training Accuracy:  0.663316\n",
      "Epoch:  234  Training Loss:  0.503591  Training Accuracy:  0.663786\n",
      "Epoch:  235  Training Loss:  0.499885  Training Accuracy:  0.665138\n",
      "Epoch:  236  Training Loss:  0.496611  Training Accuracy:  0.666667\n",
      "Epoch:  237  Training Loss:  0.492578  Training Accuracy:  0.668078\n",
      "Epoch:  238  Training Loss:  0.489258  Training Accuracy:  0.669606\n",
      "Epoch:  239  Training Loss:  0.485618  Training Accuracy:  0.670312\n",
      "Epoch:  240  Training Loss:  0.482066  Training Accuracy:  0.67184\n",
      "Epoch:  241  Training Loss:  0.478397  Training Accuracy:  0.673663\n",
      "Epoch:  242  Training Loss:  0.474681  Training Accuracy:  0.674838\n",
      "Epoch:  243  Training Loss:  0.47128  Training Accuracy:  0.675897\n",
      "Epoch:  244  Training Loss:  0.467915  Training Accuracy:  0.677308\n",
      "Epoch:  245  Training Loss:  0.464289  Training Accuracy:  0.678483\n",
      "Epoch:  246  Training Loss:  0.46101  Training Accuracy:  0.679659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  247  Training Loss:  0.457332  Training Accuracy:  0.680952\n",
      "Epoch:  248  Training Loss:  0.453658  Training Accuracy:  0.682187\n",
      "Epoch:  249  Training Loss:  0.450299  Training Accuracy:  0.683245\n",
      "Epoch:  250  Training Loss:  0.446972  Training Accuracy:  0.684362\n",
      "Epoch:  251  Training Loss:  0.443608  Training Accuracy:  0.685538\n",
      "Epoch:  252  Training Loss:  0.440405  Training Accuracy:  0.687302\n",
      "Epoch:  253  Training Loss:  0.437225  Training Accuracy:  0.688713\n",
      "Epoch:  254  Training Loss:  0.434004  Training Accuracy:  0.689947\n",
      "Epoch:  255  Training Loss:  0.430412  Training Accuracy:  0.690947\n",
      "Epoch:  256  Training Loss:  0.426952  Training Accuracy:  0.692299\n",
      "Epoch:  257  Training Loss:  0.423443  Training Accuracy:  0.693357\n",
      "Epoch:  258  Training Loss:  0.419812  Training Accuracy:  0.694592\n",
      "Epoch:  259  Training Loss:  0.416219  Training Accuracy:  0.695944\n",
      "Epoch:  260  Training Loss:  0.412923  Training Accuracy:  0.697355\n",
      "Epoch:  261  Training Loss:  0.409491  Training Accuracy:  0.698648\n",
      "Epoch:  262  Training Loss:  0.405893  Training Accuracy:  0.699941\n",
      "Epoch:  263  Training Loss:  0.402428  Training Accuracy:  0.701529\n",
      "Epoch:  264  Training Loss:  0.39888  Training Accuracy:  0.702646\n",
      "Epoch:  265  Training Loss:  0.395301  Training Accuracy:  0.704057\n",
      "Epoch:  266  Training Loss:  0.39197  Training Accuracy:  0.705056\n",
      "Epoch:  267  Training Loss:  0.388477  Training Accuracy:  0.706526\n",
      "Epoch:  268  Training Loss:  0.385179  Training Accuracy:  0.707643\n",
      "Epoch:  269  Training Loss:  0.381857  Training Accuracy:  0.708525\n",
      "Epoch:  270  Training Loss:  0.378511  Training Accuracy:  0.709642\n",
      "Epoch:  271  Training Loss:  0.375299  Training Accuracy:  0.711229\n",
      "Epoch:  272  Training Loss:  0.371983  Training Accuracy:  0.712287\n",
      "Epoch:  273  Training Loss:  0.368669  Training Accuracy:  0.713816\n",
      "Epoch:  274  Training Loss:  0.365703  Training Accuracy:  0.715168\n",
      "Epoch:  275  Training Loss:  0.362754  Training Accuracy:  0.716108\n",
      "Epoch:  276  Training Loss:  0.359712  Training Accuracy:  0.716696\n",
      "Epoch:  277  Training Loss:  0.356744  Training Accuracy:  0.717813\n",
      "Epoch:  278  Training Loss:  0.353708  Training Accuracy:  0.719342\n",
      "Epoch:  279  Training Loss:  0.350862  Training Accuracy:  0.720635\n",
      "Epoch:  280  Training Loss:  0.347813  Training Accuracy:  0.721693\n",
      "Epoch:  281  Training Loss:  0.344796  Training Accuracy:  0.722869\n",
      "Epoch:  282  Training Loss:  0.341667  Training Accuracy:  0.72428\n",
      "Epoch:  283  Training Loss:  0.338967  Training Accuracy:  0.725456\n",
      "Epoch:  284  Training Loss:  0.336138  Training Accuracy:  0.72669\n",
      "Epoch:  285  Training Loss:  0.333001  Training Accuracy:  0.727337\n",
      "Epoch:  286  Training Loss:  0.330139  Training Accuracy:  0.728748\n",
      "Epoch:  287  Training Loss:  0.327287  Training Accuracy:  0.72963\n",
      "Epoch:  288  Training Loss:  0.324404  Training Accuracy:  0.7311\n",
      "Epoch:  289  Training Loss:  0.321637  Training Accuracy:  0.732275\n",
      "Epoch:  290  Training Loss:  0.31882  Training Accuracy:  0.73304\n",
      "Epoch:  291  Training Loss:  0.316362  Training Accuracy:  0.734157\n",
      "Epoch:  292  Training Loss:  0.313605  Training Accuracy:  0.735568\n",
      "Epoch:  293  Training Loss:  0.311377  Training Accuracy:  0.736978\n",
      "Epoch:  294  Training Loss:  0.308981  Training Accuracy:  0.738272\n",
      "Epoch:  295  Training Loss:  0.3065  Training Accuracy:  0.739154\n",
      "Epoch:  296  Training Loss:  0.304221  Training Accuracy:  0.740388\n",
      "Epoch:  297  Training Loss:  0.301847  Training Accuracy:  0.74127\n",
      "Epoch:  298  Training Loss:  0.299502  Training Accuracy:  0.743034\n",
      "Epoch:  299  Training Loss:  0.297347  Training Accuracy:  0.744092\n",
      "Epoch:  300  Training Loss:  0.295307  Training Accuracy:  0.74515\n",
      "Epoch:  301  Training Loss:  0.293185  Training Accuracy:  0.746091\n",
      "Epoch:  302  Training Loss:  0.290997  Training Accuracy:  0.747208\n",
      "Epoch:  303  Training Loss:  0.288858  Training Accuracy:  0.747972\n",
      "Epoch:  304  Training Loss:  0.286582  Training Accuracy:  0.748971\n",
      "Epoch:  305  Training Loss:  0.28418  Training Accuracy:  0.749912\n",
      "Epoch:  306  Training Loss:  0.282156  Training Accuracy:  0.751441\n",
      "Epoch:  307  Training Loss:  0.279843  Training Accuracy:  0.752028\n",
      "Epoch:  308  Training Loss:  0.277724  Training Accuracy:  0.753204\n",
      "Epoch:  309  Training Loss:  0.27558  Training Accuracy:  0.754204\n",
      "Epoch:  310  Training Loss:  0.273439  Training Accuracy:  0.754909\n",
      "Epoch:  311  Training Loss:  0.271238  Training Accuracy:  0.755908\n",
      "Epoch:  312  Training Loss:  0.269052  Training Accuracy:  0.757261\n",
      "Epoch:  313  Training Loss:  0.267042  Training Accuracy:  0.758495\n",
      "Epoch:  314  Training Loss:  0.265034  Training Accuracy:  0.759671\n",
      "Epoch:  315  Training Loss:  0.263293  Training Accuracy:  0.760435\n",
      "Epoch:  316  Training Loss:  0.261553  Training Accuracy:  0.7612\n",
      "Epoch:  317  Training Loss:  0.259497  Training Accuracy:  0.762023\n",
      "Epoch:  318  Training Loss:  0.25764  Training Accuracy:  0.762552\n",
      "Epoch:  319  Training Loss:  0.255564  Training Accuracy:  0.763257\n",
      "Epoch:  320  Training Loss:  0.253611  Training Accuracy:  0.764257\n",
      "Epoch:  321  Training Loss:  0.251654  Training Accuracy:  0.76508\n",
      "Epoch:  322  Training Loss:  0.249843  Training Accuracy:  0.766255\n",
      "Epoch:  323  Training Loss:  0.248116  Training Accuracy:  0.767137\n",
      "Epoch:  324  Training Loss:  0.246223  Training Accuracy:  0.767666\n",
      "Epoch:  325  Training Loss:  0.244561  Training Accuracy:  0.76896\n",
      "Epoch:  326  Training Loss:  0.242737  Training Accuracy:  0.770194\n",
      "Epoch:  327  Training Loss:  0.240957  Training Accuracy:  0.77137\n",
      "Epoch:  328  Training Loss:  0.239187  Training Accuracy:  0.772546\n",
      "Epoch:  329  Training Loss:  0.237395  Training Accuracy:  0.773369\n",
      "Epoch:  330  Training Loss:  0.235834  Training Accuracy:  0.77478\n",
      "Epoch:  331  Training Loss:  0.23438  Training Accuracy:  0.775368\n",
      "Epoch:  332  Training Loss:  0.232897  Training Accuracy:  0.776132\n",
      "Epoch:  333  Training Loss:  0.231375  Training Accuracy:  0.776896\n",
      "Epoch:  334  Training Loss:  0.229887  Training Accuracy:  0.777543\n",
      "Epoch:  335  Training Loss:  0.228498  Training Accuracy:  0.778131\n",
      "Epoch:  336  Training Loss:  0.227211  Training Accuracy:  0.77913\n",
      "Epoch:  337  Training Loss:  0.225711  Training Accuracy:  0.78013\n",
      "Epoch:  338  Training Loss:  0.224262  Training Accuracy:  0.780953\n",
      "Epoch:  339  Training Loss:  0.222814  Training Accuracy:  0.781952\n",
      "Epoch:  340  Training Loss:  0.221446  Training Accuracy:  0.782775\n",
      "Epoch:  341  Training Loss:  0.220054  Training Accuracy:  0.783598\n",
      "Epoch:  342  Training Loss:  0.218765  Training Accuracy:  0.784598\n",
      "Epoch:  343  Training Loss:  0.217399  Training Accuracy:  0.784891\n",
      "Epoch:  344  Training Loss:  0.216019  Training Accuracy:  0.786244\n",
      "Epoch:  345  Training Loss:  0.214726  Training Accuracy:  0.787655\n",
      "Epoch:  346  Training Loss:  0.213323  Training Accuracy:  0.78836\n",
      "Epoch:  347  Training Loss:  0.212026  Training Accuracy:  0.789007\n",
      "Epoch:  348  Training Loss:  0.210752  Training Accuracy:  0.789889\n",
      "Epoch:  349  Training Loss:  0.209465  Training Accuracy:  0.790712\n",
      "Epoch:  350  Training Loss:  0.208262  Training Accuracy:  0.791123\n",
      "Epoch:  351  Training Loss:  0.206993  Training Accuracy:  0.792005\n",
      "Epoch:  352  Training Loss:  0.205801  Training Accuracy:  0.792534\n",
      "Epoch:  353  Training Loss:  0.204552  Training Accuracy:  0.793004\n",
      "Epoch:  354  Training Loss:  0.203415  Training Accuracy:  0.793651\n",
      "Epoch:  355  Training Loss:  0.20221  Training Accuracy:  0.793945\n",
      "Epoch:  356  Training Loss:  0.200964  Training Accuracy:  0.794886\n",
      "Epoch:  357  Training Loss:  0.199692  Training Accuracy:  0.795297\n",
      "Epoch:  358  Training Loss:  0.19853  Training Accuracy:  0.795944\n",
      "Epoch:  359  Training Loss:  0.197334  Training Accuracy:  0.796708\n",
      "Epoch:  360  Training Loss:  0.196251  Training Accuracy:  0.797296\n",
      "Epoch:  361  Training Loss:  0.194966  Training Accuracy:  0.798001\n",
      "Epoch:  362  Training Loss:  0.193723  Training Accuracy:  0.798531\n",
      "Epoch:  363  Training Loss:  0.192566  Training Accuracy:  0.799471\n",
      "Epoch:  364  Training Loss:  0.191493  Training Accuracy:  0.800294\n",
      "Epoch:  365  Training Loss:  0.190171  Training Accuracy:  0.801352\n",
      "Epoch:  366  Training Loss:  0.189136  Training Accuracy:  0.802234\n",
      "Epoch:  367  Training Loss:  0.18792  Training Accuracy:  0.803057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  368  Training Loss:  0.186742  Training Accuracy:  0.803822\n",
      "Epoch:  369  Training Loss:  0.185633  Training Accuracy:  0.804351\n",
      "Epoch:  370  Training Loss:  0.184453  Training Accuracy:  0.805174\n",
      "Epoch:  371  Training Loss:  0.183369  Training Accuracy:  0.805703\n",
      "Epoch:  372  Training Loss:  0.182163  Training Accuracy:  0.806291\n",
      "Epoch:  373  Training Loss:  0.181108  Training Accuracy:  0.806937\n",
      "Epoch:  374  Training Loss:  0.180058  Training Accuracy:  0.807466\n",
      "Epoch:  375  Training Loss:  0.178947  Training Accuracy:  0.808113\n",
      "Epoch:  376  Training Loss:  0.177888  Training Accuracy:  0.808995\n",
      "Epoch:  377  Training Loss:  0.176795  Training Accuracy:  0.810053\n",
      "Epoch:  378  Training Loss:  0.175669  Training Accuracy:  0.810935\n",
      "Epoch:  379  Training Loss:  0.174584  Training Accuracy:  0.811464\n",
      "Epoch:  380  Training Loss:  0.173443  Training Accuracy:  0.811993\n",
      "Epoch:  381  Training Loss:  0.172495  Training Accuracy:  0.812464\n",
      "Epoch:  382  Training Loss:  0.171366  Training Accuracy:  0.813287\n",
      "Epoch:  383  Training Loss:  0.170413  Training Accuracy:  0.813816\n",
      "Epoch:  384  Training Loss:  0.169499  Training Accuracy:  0.814521\n",
      "Epoch:  385  Training Loss:  0.168544  Training Accuracy:  0.815227\n",
      "Epoch:  386  Training Loss:  0.167579  Training Accuracy:  0.815697\n",
      "Epoch:  387  Training Loss:  0.166637  Training Accuracy:  0.816285\n",
      "Epoch:  388  Training Loss:  0.165611  Training Accuracy:  0.817167\n",
      "Epoch:  389  Training Loss:  0.1647  Training Accuracy:  0.817755\n",
      "Epoch:  390  Training Loss:  0.163814  Training Accuracy:  0.818519\n",
      "Epoch:  391  Training Loss:  0.162945  Training Accuracy:  0.818989\n",
      "Epoch:  392  Training Loss:  0.161997  Training Accuracy:  0.819342\n",
      "Epoch:  393  Training Loss:  0.161108  Training Accuracy:  0.819812\n",
      "Epoch:  394  Training Loss:  0.160252  Training Accuracy:  0.820694\n",
      "Epoch:  395  Training Loss:  0.159412  Training Accuracy:  0.821282\n",
      "Epoch:  396  Training Loss:  0.158632  Training Accuracy:  0.822164\n",
      "Epoch:  397  Training Loss:  0.157878  Training Accuracy:  0.823046\n",
      "Epoch:  398  Training Loss:  0.157013  Training Accuracy:  0.823927\n",
      "Epoch:  399  Training Loss:  0.156317  Training Accuracy:  0.824868\n",
      "Epoch:  400  Training Loss:  0.155542  Training Accuracy:  0.825632\n",
      "Epoch:  401  Training Loss:  0.154877  Training Accuracy:  0.826338\n",
      "Epoch:  402  Training Loss:  0.154191  Training Accuracy:  0.827102\n",
      "Epoch:  403  Training Loss:  0.153383  Training Accuracy:  0.827984\n",
      "Epoch:  404  Training Loss:  0.152685  Training Accuracy:  0.829042\n",
      "Epoch:  405  Training Loss:  0.151868  Training Accuracy:  0.829689\n",
      "Epoch:  406  Training Loss:  0.151242  Training Accuracy:  0.830747\n",
      "Epoch:  407  Training Loss:  0.150435  Training Accuracy:  0.831629\n",
      "Epoch:  408  Training Loss:  0.149695  Training Accuracy:  0.832452\n",
      "Epoch:  409  Training Loss:  0.148944  Training Accuracy:  0.832863\n",
      "Epoch:  410  Training Loss:  0.148163  Training Accuracy:  0.833451\n",
      "Epoch:  411  Training Loss:  0.14748  Training Accuracy:  0.834098\n",
      "Epoch:  412  Training Loss:  0.146626  Training Accuracy:  0.834862\n",
      "Epoch:  413  Training Loss:  0.145874  Training Accuracy:  0.835509\n",
      "Epoch:  414  Training Loss:  0.145121  Training Accuracy:  0.836097\n",
      "Epoch:  415  Training Loss:  0.144341  Training Accuracy:  0.836979\n",
      "Epoch:  416  Training Loss:  0.143569  Training Accuracy:  0.837802\n",
      "Epoch:  417  Training Loss:  0.142836  Training Accuracy:  0.838331\n",
      "Epoch:  418  Training Loss:  0.142048  Training Accuracy:  0.839213\n",
      "Epoch:  419  Training Loss:  0.141379  Training Accuracy:  0.8398\n",
      "Epoch:  420  Training Loss:  0.14075  Training Accuracy:  0.84033\n",
      "Epoch:  421  Training Loss:  0.140051  Training Accuracy:  0.840976\n",
      "Epoch:  422  Training Loss:  0.139527  Training Accuracy:  0.84127\n",
      "Epoch:  423  Training Loss:  0.138833  Training Accuracy:  0.841976\n",
      "Epoch:  424  Training Loss:  0.138293  Training Accuracy:  0.842799\n",
      "Epoch:  425  Training Loss:  0.137741  Training Accuracy:  0.843387\n",
      "Epoch:  426  Training Loss:  0.13715  Training Accuracy:  0.843974\n",
      "Epoch:  427  Training Loss:  0.136547  Training Accuracy:  0.844386\n",
      "Epoch:  428  Training Loss:  0.136043  Training Accuracy:  0.845209\n",
      "Epoch:  429  Training Loss:  0.135382  Training Accuracy:  0.845444\n",
      "Epoch:  430  Training Loss:  0.134846  Training Accuracy:  0.845973\n",
      "Epoch:  431  Training Loss:  0.134212  Training Accuracy:  0.84615\n",
      "Epoch:  432  Training Loss:  0.133677  Training Accuracy:  0.846679\n",
      "Epoch:  433  Training Loss:  0.133131  Training Accuracy:  0.847149\n",
      "Epoch:  434  Training Loss:  0.132703  Training Accuracy:  0.847502\n",
      "Epoch:  435  Training Loss:  0.132173  Training Accuracy:  0.848149\n",
      "Epoch:  436  Training Loss:  0.131665  Training Accuracy:  0.848795\n",
      "Epoch:  437  Training Loss:  0.131155  Training Accuracy:  0.849324\n",
      "Epoch:  438  Training Loss:  0.130674  Training Accuracy:  0.849795\n",
      "Epoch:  439  Training Loss:  0.130167  Training Accuracy:  0.850618\n",
      "Epoch:  440  Training Loss:  0.129692  Training Accuracy:  0.851029\n",
      "Epoch:  441  Training Loss:  0.129295  Training Accuracy:  0.851676\n",
      "Epoch:  442  Training Loss:  0.128776  Training Accuracy:  0.852146\n",
      "Epoch:  443  Training Loss:  0.128328  Training Accuracy:  0.852499\n",
      "Epoch:  444  Training Loss:  0.127884  Training Accuracy:  0.852616\n",
      "Epoch:  445  Training Loss:  0.127419  Training Accuracy:  0.853263\n",
      "Epoch:  446  Training Loss:  0.127026  Training Accuracy:  0.853557\n",
      "Epoch:  447  Training Loss:  0.126559  Training Accuracy:  0.85438\n",
      "Epoch:  448  Training Loss:  0.126061  Training Accuracy:  0.855027\n",
      "Epoch:  449  Training Loss:  0.125615  Training Accuracy:  0.85538\n",
      "Epoch:  450  Training Loss:  0.125116  Training Accuracy:  0.856379\n",
      "Epoch:  451  Training Loss:  0.124696  Training Accuracy:  0.857084\n",
      "Epoch:  452  Training Loss:  0.124261  Training Accuracy:  0.857437\n",
      "Epoch:  453  Training Loss:  0.123842  Training Accuracy:  0.858025\n",
      "Epoch:  454  Training Loss:  0.123417  Training Accuracy:  0.858613\n",
      "Epoch:  455  Training Loss:  0.123032  Training Accuracy:  0.859024\n",
      "Epoch:  456  Training Loss:  0.122717  Training Accuracy:  0.859495\n",
      "Epoch:  457  Training Loss:  0.122325  Training Accuracy:  0.860259\n",
      "Epoch:  458  Training Loss:  0.121983  Training Accuracy:  0.860788\n",
      "Epoch:  459  Training Loss:  0.121667  Training Accuracy:  0.861141\n",
      "Epoch:  460  Training Loss:  0.121361  Training Accuracy:  0.861435\n",
      "Epoch:  461  Training Loss:  0.120965  Training Accuracy:  0.862082\n",
      "Epoch:  462  Training Loss:  0.120637  Training Accuracy:  0.863257\n",
      "Epoch:  463  Training Loss:  0.1202  Training Accuracy:  0.86361\n",
      "Epoch:  464  Training Loss:  0.119867  Training Accuracy:  0.864139\n",
      "Epoch:  465  Training Loss:  0.119464  Training Accuracy:  0.864786\n",
      "Epoch:  466  Training Loss:  0.11909  Training Accuracy:  0.865491\n",
      "Epoch:  467  Training Loss:  0.118687  Training Accuracy:  0.865844\n",
      "Epoch:  468  Training Loss:  0.118354  Training Accuracy:  0.866491\n",
      "Epoch:  469  Training Loss:  0.118023  Training Accuracy:  0.866961\n",
      "Epoch:  470  Training Loss:  0.117652  Training Accuracy:  0.867549\n",
      "Epoch:  471  Training Loss:  0.117336  Training Accuracy:  0.867725\n",
      "Epoch:  472  Training Loss:  0.116941  Training Accuracy:  0.868137\n",
      "Epoch:  473  Training Loss:  0.116587  Training Accuracy:  0.868431\n",
      "Epoch:  474  Training Loss:  0.116211  Training Accuracy:  0.868901\n",
      "Epoch:  475  Training Loss:  0.11585  Training Accuracy:  0.869254\n",
      "Epoch:  476  Training Loss:  0.115393  Training Accuracy:  0.869724\n",
      "Epoch:  477  Training Loss:  0.115101  Training Accuracy:  0.870077\n",
      "Epoch:  478  Training Loss:  0.114726  Training Accuracy:  0.87043\n",
      "Epoch:  479  Training Loss:  0.114377  Training Accuracy:  0.8709\n",
      "Epoch:  480  Training Loss:  0.113992  Training Accuracy:  0.871135\n",
      "Epoch:  481  Training Loss:  0.113671  Training Accuracy:  0.871547\n",
      "Epoch:  482  Training Loss:  0.113388  Training Accuracy:  0.871723\n",
      "Epoch:  483  Training Loss:  0.113135  Training Accuracy:  0.872311\n",
      "Epoch:  484  Training Loss:  0.112832  Training Accuracy:  0.87284\n",
      "Epoch:  485  Training Loss:  0.112602  Training Accuracy:  0.87331\n",
      "Epoch:  486  Training Loss:  0.112211  Training Accuracy:  0.873545\n",
      "Epoch:  487  Training Loss:  0.111845  Training Accuracy:  0.874016\n",
      "Epoch:  488  Training Loss:  0.111485  Training Accuracy:  0.87431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  489  Training Loss:  0.111093  Training Accuracy:  0.874427\n",
      "Epoch:  490  Training Loss:  0.110746  Training Accuracy:  0.874839\n",
      "Epoch:  491  Training Loss:  0.110433  Training Accuracy:  0.875309\n",
      "Epoch:  492  Training Loss:  0.110047  Training Accuracy:  0.875603\n",
      "Epoch:  493  Training Loss:  0.109701  Training Accuracy:  0.875956\n",
      "Epoch:  494  Training Loss:  0.109399  Training Accuracy:  0.876367\n",
      "Epoch:  495  Training Loss:  0.109099  Training Accuracy:  0.87719\n",
      "Epoch:  496  Training Loss:  0.108813  Training Accuracy:  0.877308\n",
      "Epoch:  497  Training Loss:  0.108495  Training Accuracy:  0.877719\n",
      "Epoch:  498  Training Loss:  0.108188  Training Accuracy:  0.878072\n",
      "Epoch:  499  Training Loss:  0.107849  Training Accuracy:  0.878484\n",
      "Epoch:  500  Training Loss:  0.107534  Training Accuracy:  0.878836\n",
      "Epoch:  501  Training Loss:  0.107281  Training Accuracy:  0.879424\n",
      "Epoch:  502  Training Loss:  0.106922  Training Accuracy:  0.879542\n",
      "Epoch:  503  Training Loss:  0.106675  Training Accuracy:  0.879895\n",
      "Epoch:  504  Training Loss:  0.106336  Training Accuracy:  0.880306\n",
      "Epoch:  505  Training Loss:  0.105976  Training Accuracy:  0.880541\n",
      "Epoch:  506  Training Loss:  0.105766  Training Accuracy:  0.880953\n",
      "Epoch:  507  Training Loss:  0.105522  Training Accuracy:  0.881247\n",
      "Epoch:  508  Training Loss:  0.105297  Training Accuracy:  0.881717\n",
      "Epoch:  509  Training Loss:  0.105074  Training Accuracy:  0.881893\n",
      "Epoch:  510  Training Loss:  0.104794  Training Accuracy:  0.882305\n",
      "Epoch:  511  Training Loss:  0.104647  Training Accuracy:  0.882599\n",
      "Epoch:  512  Training Loss:  0.104326  Training Accuracy:  0.88301\n",
      "Epoch:  513  Training Loss:  0.104124  Training Accuracy:  0.883187\n",
      "Epoch:  514  Training Loss:  0.103929  Training Accuracy:  0.883775\n",
      "Epoch:  515  Training Loss:  0.103666  Training Accuracy:  0.884186\n",
      "Epoch:  516  Training Loss:  0.103456  Training Accuracy:  0.884598\n",
      "Epoch:  517  Training Loss:  0.103285  Training Accuracy:  0.885009\n",
      "Epoch:  518  Training Loss:  0.103118  Training Accuracy:  0.885186\n",
      "Epoch:  519  Training Loss:  0.102924  Training Accuracy:  0.885421\n",
      "Epoch:  520  Training Loss:  0.102803  Training Accuracy:  0.885715\n",
      "Epoch:  521  Training Loss:  0.102578  Training Accuracy:  0.88595\n",
      "Epoch:  522  Training Loss:  0.102394  Training Accuracy:  0.886185\n",
      "Epoch:  523  Training Loss:  0.102186  Training Accuracy:  0.886714\n",
      "Epoch:  524  Training Loss:  0.101988  Training Accuracy:  0.887067\n",
      "Epoch:  525  Training Loss:  0.101844  Training Accuracy:  0.887361\n",
      "Epoch:  526  Training Loss:  0.101679  Training Accuracy:  0.887596\n",
      "Epoch:  527  Training Loss:  0.101543  Training Accuracy:  0.88789\n",
      "Epoch:  528  Training Loss:  0.101392  Training Accuracy:  0.88836\n",
      "Epoch:  529  Training Loss:  0.10119  Training Accuracy:  0.888772\n",
      "Epoch:  530  Training Loss:  0.101013  Training Accuracy:  0.88936\n",
      "Epoch:  531  Training Loss:  0.100934  Training Accuracy:  0.889595\n",
      "Epoch:  532  Training Loss:  0.100775  Training Accuracy:  0.88983\n",
      "Epoch:  533  Training Loss:  0.100691  Training Accuracy:  0.890065\n",
      "Epoch:  534  Training Loss:  0.100602  Training Accuracy:  0.890359\n",
      "Epoch:  535  Training Loss:  0.100463  Training Accuracy:  0.890477\n",
      "Epoch:  536  Training Loss:  0.100287  Training Accuracy:  0.890771\n",
      "Epoch:  537  Training Loss:  0.100166  Training Accuracy:  0.890947\n",
      "Epoch:  538  Training Loss:  0.0999622  Training Accuracy:  0.891476\n",
      "Epoch:  539  Training Loss:  0.0997987  Training Accuracy:  0.891535\n",
      "Epoch:  540  Training Loss:  0.0996156  Training Accuracy:  0.891888\n",
      "Epoch:  541  Training Loss:  0.099443  Training Accuracy:  0.892182\n",
      "Epoch:  542  Training Loss:  0.099316  Training Accuracy:  0.892652\n",
      "Epoch:  543  Training Loss:  0.0991255  Training Accuracy:  0.893122\n",
      "Epoch:  544  Training Loss:  0.0989793  Training Accuracy:  0.893475\n",
      "Epoch:  545  Training Loss:  0.0988162  Training Accuracy:  0.894004\n",
      "Epoch:  546  Training Loss:  0.0987214  Training Accuracy:  0.89418\n",
      "Epoch:  547  Training Loss:  0.0985867  Training Accuracy:  0.894651\n",
      "Epoch:  548  Training Loss:  0.0984791  Training Accuracy:  0.895003\n",
      "Epoch:  549  Training Loss:  0.098367  Training Accuracy:  0.895474\n",
      "Epoch:  550  Training Loss:  0.0983666  Training Accuracy:  0.89565\n",
      "Epoch:  551  Training Loss:  0.0982364  Training Accuracy:  0.895826\n",
      "Epoch:  552  Training Loss:  0.0981421  Training Accuracy:  0.896062\n",
      "Epoch:  553  Training Loss:  0.0980673  Training Accuracy:  0.896297\n",
      "Epoch:  554  Training Loss:  0.097979  Training Accuracy:  0.896767\n",
      "Epoch:  555  Training Loss:  0.0978813  Training Accuracy:  0.897002\n",
      "Epoch:  556  Training Loss:  0.0978706  Training Accuracy:  0.897473\n",
      "Epoch:  557  Training Loss:  0.0977623  Training Accuracy:  0.898002\n",
      "Epoch:  558  Training Loss:  0.0976686  Training Accuracy:  0.898531\n",
      "Epoch:  559  Training Loss:  0.0975937  Training Accuracy:  0.898648\n",
      "Epoch:  560  Training Loss:  0.0975075  Training Accuracy:  0.898883\n",
      "Epoch:  561  Training Loss:  0.0974132  Training Accuracy:  0.899236\n",
      "Epoch:  562  Training Loss:  0.0973016  Training Accuracy:  0.899648\n",
      "Epoch:  563  Training Loss:  0.0972487  Training Accuracy:  0.899883\n",
      "Epoch:  564  Training Loss:  0.0971836  Training Accuracy:  0.900118\n",
      "Epoch:  565  Training Loss:  0.0971135  Training Accuracy:  0.900353\n",
      "Epoch:  566  Training Loss:  0.0970302  Training Accuracy:  0.90053\n",
      "Epoch:  567  Training Loss:  0.0969833  Training Accuracy:  0.900941\n",
      "Epoch:  568  Training Loss:  0.096974  Training Accuracy:  0.901411\n",
      "Epoch:  569  Training Loss:  0.0969269  Training Accuracy:  0.901588\n",
      "Epoch:  570  Training Loss:  0.0968574  Training Accuracy:  0.901764\n",
      "Epoch:  571  Training Loss:  0.0968442  Training Accuracy:  0.90194\n",
      "Epoch:  572  Training Loss:  0.0968034  Training Accuracy:  0.902411\n",
      "Epoch:  573  Training Loss:  0.0967989  Training Accuracy:  0.90294\n",
      "Epoch:  574  Training Loss:  0.0967587  Training Accuracy:  0.903351\n",
      "Epoch:  575  Training Loss:  0.0967239  Training Accuracy:  0.903645\n",
      "Epoch:  576  Training Loss:  0.0967182  Training Accuracy:  0.904057\n",
      "Epoch:  577  Training Loss:  0.0967517  Training Accuracy:  0.904233\n",
      "Epoch:  578  Training Loss:  0.0968223  Training Accuracy:  0.904233\n",
      "Epoch:  579  Training Loss:  0.0968464  Training Accuracy:  0.904586\n",
      "Epoch:  580  Training Loss:  0.0969316  Training Accuracy:  0.904821\n",
      "Epoch:  581  Training Loss:  0.0969607  Training Accuracy:  0.905056\n",
      "Epoch:  582  Training Loss:  0.097055  Training Accuracy:  0.90535\n",
      "Epoch:  583  Training Loss:  0.0971106  Training Accuracy:  0.905703\n",
      "Epoch:  584  Training Loss:  0.0972466  Training Accuracy:  0.905879\n",
      "Epoch:  585  Training Loss:  0.0973338  Training Accuracy:  0.906115\n",
      "Epoch:  586  Training Loss:  0.0974589  Training Accuracy:  0.906585\n",
      "Epoch:  587  Training Loss:  0.0974939  Training Accuracy:  0.90682\n",
      "Epoch:  588  Training Loss:  0.0976314  Training Accuracy:  0.907055\n",
      "Epoch:  589  Training Loss:  0.0976724  Training Accuracy:  0.907232\n",
      "Epoch:  590  Training Loss:  0.097753  Training Accuracy:  0.907467\n",
      "Epoch:  591  Training Loss:  0.0978267  Training Accuracy:  0.907761\n",
      "Epoch:  592  Training Loss:  0.0979376  Training Accuracy:  0.908055\n",
      "Epoch:  593  Training Loss:  0.0980316  Training Accuracy:  0.908349\n",
      "Epoch:  594  Training Loss:  0.0981397  Training Accuracy:  0.90876\n",
      "Epoch:  595  Training Loss:  0.0983157  Training Accuracy:  0.909113\n",
      "Epoch:  596  Training Loss:  0.0984195  Training Accuracy:  0.909407\n",
      "Epoch:  597  Training Loss:  0.0984936  Training Accuracy:  0.909524\n",
      "Epoch:  598  Training Loss:  0.098676  Training Accuracy:  0.909642\n",
      "Epoch:  599  Training Loss:  0.0989023  Training Accuracy:  0.909936\n",
      "Epoch:  600  Training Loss:  0.0990634  Training Accuracy:  0.910112\n",
      "Epoch:  601  Training Loss:  0.099187  Training Accuracy:  0.910289\n",
      "Epoch:  602  Training Loss:  0.0993301  Training Accuracy:  0.910641\n",
      "Epoch:  603  Training Loss:  0.0994521  Training Accuracy:  0.910935\n",
      "Epoch:  604  Training Loss:  0.099535  Training Accuracy:  0.91117\n",
      "Epoch:  605  Training Loss:  0.0995962  Training Accuracy:  0.911523\n",
      "Epoch:  606  Training Loss:  0.099638  Training Accuracy:  0.911699\n",
      "Epoch:  607  Training Loss:  0.0997688  Training Accuracy:  0.91217\n",
      "Epoch:  608  Training Loss:  0.0998599  Training Accuracy:  0.91264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  609  Training Loss:  0.0999368  Training Accuracy:  0.912699\n",
      "Epoch:  610  Training Loss:  0.100054  Training Accuracy:  0.912875\n",
      "Epoch:  611  Training Loss:  0.100124  Training Accuracy:  0.913228\n",
      "Epoch:  612  Training Loss:  0.1002  Training Accuracy:  0.913581\n",
      "Epoch:  613  Training Loss:  0.100311  Training Accuracy:  0.913816\n",
      "Epoch:  614  Training Loss:  0.10037  Training Accuracy:  0.914051\n",
      "Epoch:  615  Training Loss:  0.100502  Training Accuracy:  0.914345\n",
      "Epoch:  616  Training Loss:  0.10059  Training Accuracy:  0.914756\n",
      "Epoch:  617  Training Loss:  0.100694  Training Accuracy:  0.914933\n",
      "Epoch:  618  Training Loss:  0.100742  Training Accuracy:  0.914992\n",
      "Epoch:  619  Training Loss:  0.100871  Training Accuracy:  0.915227\n",
      "Epoch:  620  Training Loss:  0.100939  Training Accuracy:  0.915344\n",
      "Epoch:  621  Training Loss:  0.100907  Training Accuracy:  0.91558\n",
      "Epoch:  622  Training Loss:  0.101027  Training Accuracy:  0.915697\n",
      "Epoch:  623  Training Loss:  0.101042  Training Accuracy:  0.915697\n",
      "Epoch:  624  Training Loss:  0.101025  Training Accuracy:  0.91605\n",
      "Epoch:  625  Training Loss:  0.101054  Training Accuracy:  0.916461\n",
      "Epoch:  626  Training Loss:  0.100976  Training Accuracy:  0.916697\n",
      "Epoch:  627  Training Loss:  0.100989  Training Accuracy:  0.916873\n",
      "Epoch:  628  Training Loss:  0.101  Training Accuracy:  0.917226\n",
      "Epoch:  629  Training Loss:  0.101064  Training Accuracy:  0.917402\n",
      "Epoch:  630  Training Loss:  0.101069  Training Accuracy:  0.917578\n",
      "Epoch:  631  Training Loss:  0.101062  Training Accuracy:  0.917872\n",
      "Epoch:  632  Training Loss:  0.101076  Training Accuracy:  0.918049\n",
      "Epoch:  633  Training Loss:  0.101066  Training Accuracy:  0.918108\n",
      "Epoch:  634  Training Loss:  0.101104  Training Accuracy:  0.918343\n",
      "Epoch:  635  Training Loss:  0.10104  Training Accuracy:  0.918578\n",
      "Epoch:  636  Training Loss:  0.101139  Training Accuracy:  0.918813\n",
      "Epoch:  637  Training Loss:  0.101136  Training Accuracy:  0.919048\n",
      "Epoch:  638  Training Loss:  0.101212  Training Accuracy:  0.919224\n",
      "Epoch:  639  Training Loss:  0.101135  Training Accuracy:  0.919342\n",
      "Epoch:  640  Training Loss:  0.101052  Training Accuracy:  0.91946\n",
      "Epoch:  641  Training Loss:  0.101013  Training Accuracy:  0.919989\n",
      "Epoch:  642  Training Loss:  0.100871  Training Accuracy:  0.920283\n",
      "Epoch:  643  Training Loss:  0.100859  Training Accuracy:  0.920635\n",
      "Epoch:  644  Training Loss:  0.100768  Training Accuracy:  0.920753\n",
      "Epoch:  645  Training Loss:  0.100652  Training Accuracy:  0.921164\n",
      "Epoch:  646  Training Loss:  0.100595  Training Accuracy:  0.921517\n",
      "Epoch:  647  Training Loss:  0.100528  Training Accuracy:  0.921576\n",
      "Epoch:  648  Training Loss:  0.100426  Training Accuracy:  0.921811\n",
      "Epoch:  649  Training Loss:  0.100305  Training Accuracy:  0.921988\n",
      "Epoch:  650  Training Loss:  0.100244  Training Accuracy:  0.922164\n",
      "Epoch:  651  Training Loss:  0.100156  Training Accuracy:  0.922517\n",
      "Epoch:  652  Training Loss:  0.100066  Training Accuracy:  0.922634\n",
      "Epoch:  653  Training Loss:  0.0999212  Training Accuracy:  0.922869\n",
      "Epoch:  654  Training Loss:  0.099821  Training Accuracy:  0.922987\n",
      "Epoch:  655  Training Loss:  0.099718  Training Accuracy:  0.923046\n",
      "Epoch:  656  Training Loss:  0.0995925  Training Accuracy:  0.923105\n",
      "Epoch:  657  Training Loss:  0.0994863  Training Accuracy:  0.92334\n",
      "Epoch:  658  Training Loss:  0.0994494  Training Accuracy:  0.923575\n",
      "Epoch:  659  Training Loss:  0.0994244  Training Accuracy:  0.923869\n",
      "Epoch:  660  Training Loss:  0.0992749  Training Accuracy:  0.924045\n",
      "Epoch:  661  Training Loss:  0.0992046  Training Accuracy:  0.924339\n",
      "Epoch:  662  Training Loss:  0.0990198  Training Accuracy:  0.924633\n",
      "Epoch:  663  Training Loss:  0.098878  Training Accuracy:  0.924868\n",
      "Epoch:  664  Training Loss:  0.0987652  Training Accuracy:  0.925045\n",
      "Epoch:  665  Training Loss:  0.098601  Training Accuracy:  0.925339\n",
      "Epoch:  666  Training Loss:  0.0985202  Training Accuracy:  0.925456\n",
      "Epoch:  667  Training Loss:  0.098384  Training Accuracy:  0.92575\n",
      "Epoch:  668  Training Loss:  0.0982324  Training Accuracy:  0.92622\n",
      "Epoch:  669  Training Loss:  0.0981141  Training Accuracy:  0.926455\n",
      "Epoch:  670  Training Loss:  0.097979  Training Accuracy:  0.926632\n",
      "Epoch:  671  Training Loss:  0.0978469  Training Accuracy:  0.926867\n",
      "Epoch:  672  Training Loss:  0.0977385  Training Accuracy:  0.926926\n",
      "Epoch:  673  Training Loss:  0.0975946  Training Accuracy:  0.927043\n",
      "Epoch:  674  Training Loss:  0.0974349  Training Accuracy:  0.927161\n",
      "Epoch:  675  Training Loss:  0.097307  Training Accuracy:  0.927279\n",
      "Epoch:  676  Training Loss:  0.0971698  Training Accuracy:  0.927396\n",
      "Epoch:  677  Training Loss:  0.097019  Training Accuracy:  0.927631\n",
      "Epoch:  678  Training Loss:  0.0968707  Training Accuracy:  0.927866\n",
      "Epoch:  679  Training Loss:  0.0967208  Training Accuracy:  0.927925\n",
      "Epoch:  680  Training Loss:  0.0966451  Training Accuracy:  0.928043\n",
      "Epoch:  681  Training Loss:  0.0965479  Training Accuracy:  0.928337\n",
      "Epoch:  682  Training Loss:  0.0963745  Training Accuracy:  0.928454\n",
      "Epoch:  683  Training Loss:  0.0962807  Training Accuracy:  0.928748\n",
      "Epoch:  684  Training Loss:  0.0961719  Training Accuracy:  0.928748\n",
      "Epoch:  685  Training Loss:  0.0960847  Training Accuracy:  0.929101\n",
      "Epoch:  686  Training Loss:  0.0960185  Training Accuracy:  0.929336\n",
      "Epoch:  687  Training Loss:  0.0959606  Training Accuracy:  0.929513\n",
      "Epoch:  688  Training Loss:  0.095919  Training Accuracy:  0.92963\n",
      "Epoch:  689  Training Loss:  0.095857  Training Accuracy:  0.929924\n",
      "Epoch:  690  Training Loss:  0.0958793  Training Accuracy:  0.930336\n",
      "Epoch:  691  Training Loss:  0.0958008  Training Accuracy:  0.930394\n",
      "Epoch:  692  Training Loss:  0.0957586  Training Accuracy:  0.930688\n",
      "Epoch:  693  Training Loss:  0.0956477  Training Accuracy:  0.930747\n",
      "Epoch:  694  Training Loss:  0.0956406  Training Accuracy:  0.930747\n",
      "Epoch:  695  Training Loss:  0.0955491  Training Accuracy:  0.930923\n",
      "Epoch:  696  Training Loss:  0.0955523  Training Accuracy:  0.931511\n",
      "Epoch:  697  Training Loss:  0.0954497  Training Accuracy:  0.931688\n",
      "Epoch:  698  Training Loss:  0.0954424  Training Accuracy:  0.931805\n",
      "Epoch:  699  Training Loss:  0.0954239  Training Accuracy:  0.931923\n",
      "Testing Accuracy: 0.844853\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size =10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 700\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  158.753  Training Accuracy:  0.0444444\n",
      "Epoch:  1  Training Loss:  102.651  Training Accuracy:  0.0409759\n",
      "Epoch:  2  Training Loss:  95.6677  Training Accuracy:  0.0465021\n",
      "Epoch:  3  Training Loss:  90.5245  Training Accuracy:  0.0519106\n",
      "Epoch:  4  Training Loss:  86.3577  Training Accuracy:  0.0579071\n",
      "Epoch:  5  Training Loss:  82.9014  Training Accuracy:  0.0634333\n",
      "Epoch:  6  Training Loss:  80.1226  Training Accuracy:  0.068254\n",
      "Epoch:  7  Training Loss:  77.752  Training Accuracy:  0.0722516\n",
      "Epoch:  8  Training Loss:  75.4969  Training Accuracy:  0.0763081\n",
      "Epoch:  9  Training Loss:  73.4069  Training Accuracy:  0.0820106\n",
      "Epoch:  10  Training Loss:  71.4874  Training Accuracy:  0.0878895\n",
      "Epoch:  11  Training Loss:  69.8364  Training Accuracy:  0.0928865\n",
      "Epoch:  12  Training Loss:  68.2586  Training Accuracy:  0.0998824\n",
      "Epoch:  13  Training Loss:  66.728  Training Accuracy:  0.108054\n",
      "Epoch:  14  Training Loss:  65.2463  Training Accuracy:  0.114815\n",
      "Epoch:  15  Training Loss:  63.7934  Training Accuracy:  0.121752\n",
      "Epoch:  16  Training Loss:  62.39  Training Accuracy:  0.128748\n",
      "Epoch:  17  Training Loss:  61.04  Training Accuracy:  0.136038\n",
      "Epoch:  18  Training Loss:  59.6864  Training Accuracy:  0.141623\n",
      "Epoch:  19  Training Loss:  58.3382  Training Accuracy:  0.148148\n",
      "Epoch:  20  Training Loss:  57.0453  Training Accuracy:  0.155026\n",
      "Epoch:  21  Training Loss:  55.795  Training Accuracy:  0.161317\n",
      "Epoch:  22  Training Loss:  54.5768  Training Accuracy:  0.167549\n",
      "Epoch:  23  Training Loss:  53.3743  Training Accuracy:  0.173663\n",
      "Epoch:  24  Training Loss:  52.204  Training Accuracy:  0.17766\n",
      "Epoch:  25  Training Loss:  51.0444  Training Accuracy:  0.183186\n",
      "Epoch:  26  Training Loss:  49.8909  Training Accuracy:  0.188654\n",
      "Epoch:  27  Training Loss:  48.7512  Training Accuracy:  0.194003\n",
      "Epoch:  28  Training Loss:  47.6339  Training Accuracy:  0.199471\n",
      "Epoch:  29  Training Loss:  46.55  Training Accuracy:  0.204292\n",
      "Epoch:  30  Training Loss:  45.4767  Training Accuracy:  0.209641\n",
      "Epoch:  31  Training Loss:  44.4109  Training Accuracy:  0.215226\n",
      "Epoch:  32  Training Loss:  43.3447  Training Accuracy:  0.220635\n",
      "Epoch:  33  Training Loss:  42.3053  Training Accuracy:  0.225867\n",
      "Epoch:  34  Training Loss:  41.275  Training Accuracy:  0.231217\n",
      "Epoch:  35  Training Loss:  40.2936  Training Accuracy:  0.236449\n",
      "Epoch:  36  Training Loss:  39.3231  Training Accuracy:  0.241681\n",
      "Epoch:  37  Training Loss:  38.352  Training Accuracy:  0.246914\n",
      "Epoch:  38  Training Loss:  37.3718  Training Accuracy:  0.252028\n",
      "Epoch:  39  Training Loss:  36.4157  Training Accuracy:  0.256849\n",
      "Epoch:  40  Training Loss:  35.4592  Training Accuracy:  0.262257\n",
      "Epoch:  41  Training Loss:  34.5036  Training Accuracy:  0.266138\n",
      "Epoch:  42  Training Loss:  33.5659  Training Accuracy:  0.270547\n",
      "Epoch:  43  Training Loss:  32.633  Training Accuracy:  0.274074\n",
      "Epoch:  44  Training Loss:  31.7371  Training Accuracy:  0.277719\n",
      "Epoch:  45  Training Loss:  30.8651  Training Accuracy:  0.281952\n",
      "Epoch:  46  Training Loss:  30.0105  Training Accuracy:  0.285655\n",
      "Epoch:  47  Training Loss:  29.1655  Training Accuracy:  0.290006\n",
      "Epoch:  48  Training Loss:  28.334  Training Accuracy:  0.294297\n",
      "Epoch:  49  Training Loss:  27.5036  Training Accuracy:  0.297648\n",
      "Epoch:  50  Training Loss:  26.683  Training Accuracy:  0.301235\n",
      "Epoch:  51  Training Loss:  25.8842  Training Accuracy:  0.304703\n",
      "Epoch:  52  Training Loss:  25.0918  Training Accuracy:  0.308701\n",
      "Epoch:  53  Training Loss:  24.3083  Training Accuracy:  0.312052\n",
      "Epoch:  54  Training Loss:  23.5404  Training Accuracy:  0.316049\n",
      "Epoch:  55  Training Loss:  22.7978  Training Accuracy:  0.31893\n",
      "Epoch:  56  Training Loss:  22.0672  Training Accuracy:  0.322163\n",
      "Epoch:  57  Training Loss:  21.3465  Training Accuracy:  0.326043\n",
      "Epoch:  58  Training Loss:  20.6596  Training Accuracy:  0.329336\n",
      "Epoch:  59  Training Loss:  20.0109  Training Accuracy:  0.332922\n",
      "Epoch:  60  Training Loss:  19.3781  Training Accuracy:  0.335979\n",
      "Epoch:  61  Training Loss:  18.7453  Training Accuracy:  0.33933\n",
      "Epoch:  62  Training Loss:  18.1382  Training Accuracy:  0.344033\n",
      "Epoch:  63  Training Loss:  17.5421  Training Accuracy:  0.347913\n",
      "Epoch:  64  Training Loss:  16.9496  Training Accuracy:  0.352028\n",
      "Epoch:  65  Training Loss:  16.3909  Training Accuracy:  0.355849\n",
      "Epoch:  66  Training Loss:  15.8444  Training Accuracy:  0.359965\n",
      "Epoch:  67  Training Loss:  15.3108  Training Accuracy:  0.363845\n",
      "Epoch:  68  Training Loss:  14.787  Training Accuracy:  0.367431\n",
      "Epoch:  69  Training Loss:  14.2865  Training Accuracy:  0.370488\n",
      "Epoch:  70  Training Loss:  13.8041  Training Accuracy:  0.373721\n",
      "Epoch:  71  Training Loss:  13.3399  Training Accuracy:  0.376837\n",
      "Epoch:  72  Training Loss:  12.8953  Training Accuracy:  0.380012\n",
      "Epoch:  73  Training Loss:  12.4601  Training Accuracy:  0.383421\n",
      "Epoch:  74  Training Loss:  12.0313  Training Accuracy:  0.38689\n",
      "Epoch:  75  Training Loss:  11.6259  Training Accuracy:  0.389888\n",
      "Epoch:  76  Training Loss:  11.2324  Training Accuracy:  0.392534\n",
      "Epoch:  77  Training Loss:  10.8594  Training Accuracy:  0.395356\n",
      "Epoch:  78  Training Loss:  10.4895  Training Accuracy:  0.398119\n",
      "Epoch:  79  Training Loss:  10.1436  Training Accuracy:  0.400882\n",
      "Epoch:  80  Training Loss:  9.81105  Training Accuracy:  0.404056\n",
      "Epoch:  81  Training Loss:  9.49865  Training Accuracy:  0.406525\n",
      "Epoch:  82  Training Loss:  9.1921  Training Accuracy:  0.408818\n",
      "Epoch:  83  Training Loss:  8.90077  Training Accuracy:  0.411758\n",
      "Epoch:  84  Training Loss:  8.61831  Training Accuracy:  0.414109\n",
      "Epoch:  85  Training Loss:  8.34617  Training Accuracy:  0.417637\n",
      "Epoch:  86  Training Loss:  8.08647  Training Accuracy:  0.420576\n",
      "Epoch:  87  Training Loss:  7.83446  Training Accuracy:  0.422986\n",
      "Epoch:  88  Training Loss:  7.59704  Training Accuracy:  0.425338\n",
      "Epoch:  89  Training Loss:  7.36925  Training Accuracy:  0.427219\n",
      "Epoch:  90  Training Loss:  7.15481  Training Accuracy:  0.430394\n",
      "Epoch:  91  Training Loss:  6.94579  Training Accuracy:  0.432745\n",
      "Epoch:  92  Training Loss:  6.74351  Training Accuracy:  0.435861\n",
      "Epoch:  93  Training Loss:  6.55094  Training Accuracy:  0.439036\n",
      "Epoch:  94  Training Loss:  6.36693  Training Accuracy:  0.441446\n",
      "Epoch:  95  Training Loss:  6.18944  Training Accuracy:  0.443327\n",
      "Epoch:  96  Training Loss:  6.02408  Training Accuracy:  0.444915\n",
      "Epoch:  97  Training Loss:  5.8614  Training Accuracy:  0.446561\n",
      "Epoch:  98  Training Loss:  5.7059  Training Accuracy:  0.44856\n",
      "Epoch:  99  Training Loss:  5.55518  Training Accuracy:  0.4505\n",
      "Epoch:  100  Training Loss:  5.41312  Training Accuracy:  0.45291\n",
      "Epoch:  101  Training Loss:  5.27377  Training Accuracy:  0.455026\n",
      "Epoch:  102  Training Loss:  5.13991  Training Accuracy:  0.457319\n",
      "Epoch:  103  Training Loss:  5.01239  Training Accuracy:  0.459612\n",
      "Epoch:  104  Training Loss:  4.89366  Training Accuracy:  0.461669\n",
      "Epoch:  105  Training Loss:  4.77934  Training Accuracy:  0.463257\n",
      "Epoch:  106  Training Loss:  4.66891  Training Accuracy:  0.46602\n",
      "Epoch:  107  Training Loss:  4.56476  Training Accuracy:  0.467548\n",
      "Epoch:  108  Training Loss:  4.46096  Training Accuracy:  0.469253\n",
      "Epoch:  109  Training Loss:  4.36276  Training Accuracy:  0.470723\n",
      "Epoch:  110  Training Loss:  4.26596  Training Accuracy:  0.472781\n",
      "Epoch:  111  Training Loss:  4.16993  Training Accuracy:  0.474603\n",
      "Epoch:  112  Training Loss:  4.08247  Training Accuracy:  0.47619\n",
      "Epoch:  113  Training Loss:  3.99489  Training Accuracy:  0.47813\n",
      "Epoch:  114  Training Loss:  3.90915  Training Accuracy:  0.480012\n",
      "Epoch:  115  Training Loss:  3.8238  Training Accuracy:  0.481717\n",
      "Epoch:  116  Training Loss:  3.75006  Training Accuracy:  0.483245\n",
      "Epoch:  117  Training Loss:  3.67604  Training Accuracy:  0.485655\n",
      "Epoch:  118  Training Loss:  3.60576  Training Accuracy:  0.487243\n",
      "Epoch:  119  Training Loss:  3.53796  Training Accuracy:  0.489065\n",
      "Epoch:  120  Training Loss:  3.47178  Training Accuracy:  0.490182\n",
      "Epoch:  121  Training Loss:  3.40784  Training Accuracy:  0.491711\n",
      "Epoch:  122  Training Loss:  3.34552  Training Accuracy:  0.49318\n",
      "Epoch:  123  Training Loss:  3.28361  Training Accuracy:  0.495062\n",
      "Epoch:  124  Training Loss:  3.22452  Training Accuracy:  0.497002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  3.16783  Training Accuracy:  0.499706\n",
      "Epoch:  126  Training Loss:  3.11132  Training Accuracy:  0.501705\n",
      "Epoch:  127  Training Loss:  3.05832  Training Accuracy:  0.504174\n",
      "Epoch:  128  Training Loss:  3.0064  Training Accuracy:  0.505996\n",
      "Epoch:  129  Training Loss:  2.9574  Training Accuracy:  0.508113\n",
      "Epoch:  130  Training Loss:  2.90929  Training Accuracy:  0.51017\n",
      "Epoch:  131  Training Loss:  2.86426  Training Accuracy:  0.511875\n",
      "Epoch:  132  Training Loss:  2.82144  Training Accuracy:  0.512698\n",
      "Epoch:  133  Training Loss:  2.78002  Training Accuracy:  0.514815\n",
      "Epoch:  134  Training Loss:  2.73944  Training Accuracy:  0.516578\n",
      "Epoch:  135  Training Loss:  2.69919  Training Accuracy:  0.518166\n",
      "Epoch:  136  Training Loss:  2.66044  Training Accuracy:  0.520223\n",
      "Epoch:  137  Training Loss:  2.62288  Training Accuracy:  0.522457\n",
      "Epoch:  138  Training Loss:  2.58743  Training Accuracy:  0.52475\n",
      "Epoch:  139  Training Loss:  2.55204  Training Accuracy:  0.526514\n",
      "Epoch:  140  Training Loss:  2.517  Training Accuracy:  0.528454\n",
      "Epoch:  141  Training Loss:  2.48292  Training Accuracy:  0.5301\n",
      "Epoch:  142  Training Loss:  2.45175  Training Accuracy:  0.532099\n",
      "Epoch:  143  Training Loss:  2.4208  Training Accuracy:  0.53351\n",
      "Epoch:  144  Training Loss:  2.39081  Training Accuracy:  0.535567\n",
      "Epoch:  145  Training Loss:  2.36112  Training Accuracy:  0.537272\n",
      "Epoch:  146  Training Loss:  2.33154  Training Accuracy:  0.538977\n",
      "Epoch:  147  Training Loss:  2.30249  Training Accuracy:  0.540917\n",
      "Epoch:  148  Training Loss:  2.27463  Training Accuracy:  0.542269\n",
      "Epoch:  149  Training Loss:  2.24574  Training Accuracy:  0.544092\n",
      "Epoch:  150  Training Loss:  2.21903  Training Accuracy:  0.545679\n",
      "Epoch:  151  Training Loss:  2.19616  Training Accuracy:  0.54709\n",
      "Epoch:  152  Training Loss:  2.17139  Training Accuracy:  0.548383\n",
      "Epoch:  153  Training Loss:  2.14678  Training Accuracy:  0.550147\n",
      "Epoch:  154  Training Loss:  2.12156  Training Accuracy:  0.551911\n",
      "Epoch:  155  Training Loss:  2.09777  Training Accuracy:  0.553733\n",
      "Epoch:  156  Training Loss:  2.0736  Training Accuracy:  0.555438\n",
      "Epoch:  157  Training Loss:  2.05077  Training Accuracy:  0.55726\n",
      "Epoch:  158  Training Loss:  2.02633  Training Accuracy:  0.558965\n",
      "Epoch:  159  Training Loss:  2.00397  Training Accuracy:  0.560553\n",
      "Epoch:  160  Training Loss:  1.98192  Training Accuracy:  0.562669\n",
      "Epoch:  161  Training Loss:  1.96066  Training Accuracy:  0.564962\n",
      "Epoch:  162  Training Loss:  1.93871  Training Accuracy:  0.567372\n",
      "Epoch:  163  Training Loss:  1.9168  Training Accuracy:  0.569136\n",
      "Epoch:  164  Training Loss:  1.89587  Training Accuracy:  0.570841\n",
      "Epoch:  165  Training Loss:  1.87517  Training Accuracy:  0.572369\n",
      "Epoch:  166  Training Loss:  1.85412  Training Accuracy:  0.57425\n",
      "Epoch:  167  Training Loss:  1.83388  Training Accuracy:  0.575896\n",
      "Epoch:  168  Training Loss:  1.815  Training Accuracy:  0.577072\n",
      "Epoch:  169  Training Loss:  1.79558  Training Accuracy:  0.579189\n",
      "Epoch:  170  Training Loss:  1.77673  Training Accuracy:  0.580835\n",
      "Epoch:  171  Training Loss:  1.75719  Training Accuracy:  0.581952\n",
      "Epoch:  172  Training Loss:  1.73961  Training Accuracy:  0.583363\n",
      "Epoch:  173  Training Loss:  1.72087  Training Accuracy:  0.584774\n",
      "Epoch:  174  Training Loss:  1.70304  Training Accuracy:  0.585949\n",
      "Epoch:  175  Training Loss:  1.68539  Training Accuracy:  0.587419\n",
      "Epoch:  176  Training Loss:  1.66893  Training Accuracy:  0.588654\n",
      "Epoch:  177  Training Loss:  1.65193  Training Accuracy:  0.590653\n",
      "Epoch:  178  Training Loss:  1.63636  Training Accuracy:  0.591828\n",
      "Epoch:  179  Training Loss:  1.61966  Training Accuracy:  0.593886\n",
      "Epoch:  180  Training Loss:  1.60318  Training Accuracy:  0.595297\n",
      "Epoch:  181  Training Loss:  1.58763  Training Accuracy:  0.596884\n",
      "Epoch:  182  Training Loss:  1.57132  Training Accuracy:  0.598589\n",
      "Epoch:  183  Training Loss:  1.55507  Training Accuracy:  0.600823\n",
      "Epoch:  184  Training Loss:  1.5392  Training Accuracy:  0.601881\n",
      "Epoch:  185  Training Loss:  1.52247  Training Accuracy:  0.603704\n",
      "Epoch:  186  Training Loss:  1.50678  Training Accuracy:  0.605232\n",
      "Epoch:  187  Training Loss:  1.49155  Training Accuracy:  0.606349\n",
      "Epoch:  188  Training Loss:  1.47676  Training Accuracy:  0.60776\n",
      "Epoch:  189  Training Loss:  1.46162  Training Accuracy:  0.60923\n",
      "Epoch:  190  Training Loss:  1.4475  Training Accuracy:  0.610406\n",
      "Epoch:  191  Training Loss:  1.43236  Training Accuracy:  0.611817\n",
      "Epoch:  192  Training Loss:  1.41736  Training Accuracy:  0.612875\n",
      "Epoch:  193  Training Loss:  1.40349  Training Accuracy:  0.614344\n",
      "Epoch:  194  Training Loss:  1.38991  Training Accuracy:  0.615461\n",
      "Epoch:  195  Training Loss:  1.37569  Training Accuracy:  0.616284\n",
      "Epoch:  196  Training Loss:  1.36229  Training Accuracy:  0.617225\n",
      "Epoch:  197  Training Loss:  1.34941  Training Accuracy:  0.618813\n",
      "Epoch:  198  Training Loss:  1.33836  Training Accuracy:  0.62087\n",
      "Epoch:  199  Training Loss:  1.32712  Training Accuracy:  0.62234\n",
      "Epoch:  200  Training Loss:  1.31549  Training Accuracy:  0.624103\n",
      "Epoch:  201  Training Loss:  1.30452  Training Accuracy:  0.625103\n",
      "Epoch:  202  Training Loss:  1.29362  Training Accuracy:  0.626044\n",
      "Epoch:  203  Training Loss:  1.28356  Training Accuracy:  0.627454\n",
      "Epoch:  204  Training Loss:  1.27279  Training Accuracy:  0.628748\n",
      "Epoch:  205  Training Loss:  1.26188  Training Accuracy:  0.6301\n",
      "Epoch:  206  Training Loss:  1.25158  Training Accuracy:  0.631335\n",
      "Epoch:  207  Training Loss:  1.24207  Training Accuracy:  0.632922\n",
      "Epoch:  208  Training Loss:  1.23252  Training Accuracy:  0.634627\n",
      "Epoch:  209  Training Loss:  1.22252  Training Accuracy:  0.635567\n",
      "Epoch:  210  Training Loss:  1.21364  Training Accuracy:  0.636743\n",
      "Epoch:  211  Training Loss:  1.20365  Training Accuracy:  0.638154\n",
      "Epoch:  212  Training Loss:  1.194  Training Accuracy:  0.639683\n",
      "Epoch:  213  Training Loss:  1.18488  Training Accuracy:  0.640506\n",
      "Epoch:  214  Training Loss:  1.17599  Training Accuracy:  0.642034\n",
      "Epoch:  215  Training Loss:  1.16773  Training Accuracy:  0.643563\n",
      "Epoch:  216  Training Loss:  1.15969  Training Accuracy:  0.644856\n",
      "Epoch:  217  Training Loss:  1.15146  Training Accuracy:  0.646384\n",
      "Epoch:  218  Training Loss:  1.14353  Training Accuracy:  0.647913\n",
      "Epoch:  219  Training Loss:  1.13527  Training Accuracy:  0.649853\n",
      "Epoch:  220  Training Loss:  1.12721  Training Accuracy:  0.650911\n",
      "Epoch:  221  Training Loss:  1.11963  Training Accuracy:  0.651969\n",
      "Epoch:  222  Training Loss:  1.1118  Training Accuracy:  0.653028\n",
      "Epoch:  223  Training Loss:  1.1031  Training Accuracy:  0.65438\n",
      "Epoch:  224  Training Loss:  1.09444  Training Accuracy:  0.655497\n",
      "Epoch:  225  Training Loss:  1.08607  Training Accuracy:  0.656849\n",
      "Epoch:  226  Training Loss:  1.07698  Training Accuracy:  0.65826\n",
      "Epoch:  227  Training Loss:  1.06857  Training Accuracy:  0.659201\n",
      "Epoch:  228  Training Loss:  1.05999  Training Accuracy:  0.660553\n",
      "Epoch:  229  Training Loss:  1.05194  Training Accuracy:  0.661787\n",
      "Epoch:  230  Training Loss:  1.04334  Training Accuracy:  0.663198\n",
      "Epoch:  231  Training Loss:  1.03534  Training Accuracy:  0.664727\n",
      "Epoch:  232  Training Loss:  1.02659  Training Accuracy:  0.665844\n",
      "Epoch:  233  Training Loss:  1.01803  Training Accuracy:  0.667431\n",
      "Epoch:  234  Training Loss:  1.00934  Training Accuracy:  0.669077\n",
      "Epoch:  235  Training Loss:  1.0016  Training Accuracy:  0.67037\n",
      "Epoch:  236  Training Loss:  0.993387  Training Accuracy:  0.671723\n",
      "Epoch:  237  Training Loss:  0.985265  Training Accuracy:  0.673016\n",
      "Epoch:  238  Training Loss:  0.977174  Training Accuracy:  0.674192\n",
      "Epoch:  239  Training Loss:  0.96922  Training Accuracy:  0.675779\n",
      "Epoch:  240  Training Loss:  0.961162  Training Accuracy:  0.677719\n",
      "Epoch:  241  Training Loss:  0.953115  Training Accuracy:  0.679071\n",
      "Epoch:  242  Training Loss:  0.945359  Training Accuracy:  0.680012\n",
      "Epoch:  243  Training Loss:  0.938367  Training Accuracy:  0.681423\n",
      "Epoch:  244  Training Loss:  0.930618  Training Accuracy:  0.682305\n",
      "Epoch:  245  Training Loss:  0.922774  Training Accuracy:  0.683598\n",
      "Epoch:  246  Training Loss:  0.915338  Training Accuracy:  0.685009\n",
      "Epoch:  247  Training Loss:  0.907704  Training Accuracy:  0.686126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  0.900521  Training Accuracy:  0.688007\n",
      "Epoch:  249  Training Loss:  0.892703  Training Accuracy:  0.689418\n",
      "Epoch:  250  Training Loss:  0.885262  Training Accuracy:  0.690535\n",
      "Epoch:  251  Training Loss:  0.877591  Training Accuracy:  0.69177\n",
      "Epoch:  252  Training Loss:  0.870547  Training Accuracy:  0.693063\n",
      "Epoch:  253  Training Loss:  0.863198  Training Accuracy:  0.694121\n",
      "Epoch:  254  Training Loss:  0.855844  Training Accuracy:  0.695415\n",
      "Epoch:  255  Training Loss:  0.848337  Training Accuracy:  0.696649\n",
      "Epoch:  256  Training Loss:  0.841759  Training Accuracy:  0.697296\n",
      "Epoch:  257  Training Loss:  0.834826  Training Accuracy:  0.69853\n",
      "Epoch:  258  Training Loss:  0.827897  Training Accuracy:  0.699824\n",
      "Epoch:  259  Training Loss:  0.820956  Training Accuracy:  0.700529\n",
      "Epoch:  260  Training Loss:  0.814424  Training Accuracy:  0.701764\n",
      "Epoch:  261  Training Loss:  0.807743  Training Accuracy:  0.702587\n",
      "Epoch:  262  Training Loss:  0.801378  Training Accuracy:  0.703998\n",
      "Epoch:  263  Training Loss:  0.795256  Training Accuracy:  0.704997\n",
      "Epoch:  264  Training Loss:  0.789312  Training Accuracy:  0.706055\n",
      "Epoch:  265  Training Loss:  0.782797  Training Accuracy:  0.706761\n",
      "Epoch:  266  Training Loss:  0.776347  Training Accuracy:  0.708113\n",
      "Epoch:  267  Training Loss:  0.76975  Training Accuracy:  0.708877\n",
      "Epoch:  268  Training Loss:  0.763931  Training Accuracy:  0.709759\n",
      "Epoch:  269  Training Loss:  0.757299  Training Accuracy:  0.710406\n",
      "Epoch:  270  Training Loss:  0.750999  Training Accuracy:  0.711699\n",
      "Epoch:  271  Training Loss:  0.744209  Training Accuracy:  0.71311\n",
      "Epoch:  272  Training Loss:  0.738477  Training Accuracy:  0.713933\n",
      "Epoch:  273  Training Loss:  0.732528  Training Accuracy:  0.715344\n",
      "Epoch:  274  Training Loss:  0.726508  Training Accuracy:  0.716226\n",
      "Epoch:  275  Training Loss:  0.720042  Training Accuracy:  0.717284\n",
      "Epoch:  276  Training Loss:  0.714005  Training Accuracy:  0.718695\n",
      "Epoch:  277  Training Loss:  0.707665  Training Accuracy:  0.719753\n",
      "Epoch:  278  Training Loss:  0.701554  Training Accuracy:  0.721282\n",
      "Epoch:  279  Training Loss:  0.69519  Training Accuracy:  0.722399\n",
      "Epoch:  280  Training Loss:  0.689218  Training Accuracy:  0.723575\n",
      "Epoch:  281  Training Loss:  0.683102  Training Accuracy:  0.724692\n",
      "Epoch:  282  Training Loss:  0.677917  Training Accuracy:  0.726161\n",
      "Epoch:  283  Training Loss:  0.672099  Training Accuracy:  0.727278\n",
      "Epoch:  284  Training Loss:  0.666825  Training Accuracy:  0.72816\n",
      "Epoch:  285  Training Loss:  0.661514  Training Accuracy:  0.729101\n",
      "Epoch:  286  Training Loss:  0.655666  Training Accuracy:  0.730159\n",
      "Epoch:  287  Training Loss:  0.650538  Training Accuracy:  0.730982\n",
      "Epoch:  288  Training Loss:  0.645172  Training Accuracy:  0.731805\n",
      "Epoch:  289  Training Loss:  0.63946  Training Accuracy:  0.732628\n",
      "Epoch:  290  Training Loss:  0.634377  Training Accuracy:  0.733216\n",
      "Epoch:  291  Training Loss:  0.629027  Training Accuracy:  0.734098\n",
      "Epoch:  292  Training Loss:  0.623814  Training Accuracy:  0.735097\n",
      "Epoch:  293  Training Loss:  0.618748  Training Accuracy:  0.736038\n",
      "Epoch:  294  Training Loss:  0.61375  Training Accuracy:  0.737566\n",
      "Epoch:  295  Training Loss:  0.608807  Training Accuracy:  0.739095\n",
      "Epoch:  296  Training Loss:  0.603973  Training Accuracy:  0.740035\n",
      "Epoch:  297  Training Loss:  0.598741  Training Accuracy:  0.741211\n",
      "Epoch:  298  Training Loss:  0.593767  Training Accuracy:  0.741917\n",
      "Epoch:  299  Training Loss:  0.588377  Training Accuracy:  0.742975\n",
      "Epoch:  300  Training Loss:  0.583838  Training Accuracy:  0.744092\n",
      "Epoch:  301  Training Loss:  0.578645  Training Accuracy:  0.745268\n",
      "Epoch:  302  Training Loss:  0.573942  Training Accuracy:  0.746679\n",
      "Epoch:  303  Training Loss:  0.568736  Training Accuracy:  0.747854\n",
      "Epoch:  304  Training Loss:  0.56394  Training Accuracy:  0.74903\n",
      "Epoch:  305  Training Loss:  0.55942  Training Accuracy:  0.75003\n",
      "Epoch:  306  Training Loss:  0.554954  Training Accuracy:  0.750853\n",
      "Epoch:  307  Training Loss:  0.550919  Training Accuracy:  0.751852\n",
      "Epoch:  308  Training Loss:  0.546599  Training Accuracy:  0.752793\n",
      "Epoch:  309  Training Loss:  0.542251  Training Accuracy:  0.753733\n",
      "Epoch:  310  Training Loss:  0.537677  Training Accuracy:  0.755085\n",
      "Epoch:  311  Training Loss:  0.533473  Training Accuracy:  0.755967\n",
      "Epoch:  312  Training Loss:  0.529156  Training Accuracy:  0.757025\n",
      "Epoch:  313  Training Loss:  0.525194  Training Accuracy:  0.757613\n",
      "Epoch:  314  Training Loss:  0.521263  Training Accuracy:  0.758672\n",
      "Epoch:  315  Training Loss:  0.517247  Training Accuracy:  0.759377\n",
      "Epoch:  316  Training Loss:  0.513186  Training Accuracy:  0.761023\n",
      "Epoch:  317  Training Loss:  0.509606  Training Accuracy:  0.762199\n",
      "Epoch:  318  Training Loss:  0.505685  Training Accuracy:  0.763022\n",
      "Epoch:  319  Training Loss:  0.50228  Training Accuracy:  0.763786\n",
      "Epoch:  320  Training Loss:  0.498841  Training Accuracy:  0.764903\n",
      "Epoch:  321  Training Loss:  0.495739  Training Accuracy:  0.766432\n",
      "Epoch:  322  Training Loss:  0.492547  Training Accuracy:  0.767902\n",
      "Epoch:  323  Training Loss:  0.489566  Training Accuracy:  0.768666\n",
      "Epoch:  324  Training Loss:  0.486721  Training Accuracy:  0.769313\n",
      "Epoch:  325  Training Loss:  0.483537  Training Accuracy:  0.770429\n",
      "Epoch:  326  Training Loss:  0.480459  Training Accuracy:  0.771429\n",
      "Epoch:  327  Training Loss:  0.477538  Training Accuracy:  0.77184\n",
      "Epoch:  328  Training Loss:  0.47448  Training Accuracy:  0.772957\n",
      "Epoch:  329  Training Loss:  0.471563  Training Accuracy:  0.773604\n",
      "Epoch:  330  Training Loss:  0.468583  Training Accuracy:  0.774721\n",
      "Epoch:  331  Training Loss:  0.465619  Training Accuracy:  0.775426\n",
      "Epoch:  332  Training Loss:  0.463063  Training Accuracy:  0.776191\n",
      "Epoch:  333  Training Loss:  0.460184  Training Accuracy:  0.776661\n",
      "Epoch:  334  Training Loss:  0.457576  Training Accuracy:  0.777249\n",
      "Epoch:  335  Training Loss:  0.45517  Training Accuracy:  0.77819\n",
      "Epoch:  336  Training Loss:  0.452646  Training Accuracy:  0.779071\n",
      "Epoch:  337  Training Loss:  0.450123  Training Accuracy:  0.780012\n",
      "Epoch:  338  Training Loss:  0.447765  Training Accuracy:  0.780659\n",
      "Epoch:  339  Training Loss:  0.445164  Training Accuracy:  0.781482\n",
      "Epoch:  340  Training Loss:  0.44253  Training Accuracy:  0.78254\n",
      "Epoch:  341  Training Loss:  0.439769  Training Accuracy:  0.783657\n",
      "Epoch:  342  Training Loss:  0.436924  Training Accuracy:  0.784598\n",
      "Epoch:  343  Training Loss:  0.434458  Training Accuracy:  0.785303\n",
      "Epoch:  344  Training Loss:  0.431931  Training Accuracy:  0.786244\n",
      "Epoch:  345  Training Loss:  0.4294  Training Accuracy:  0.78689\n",
      "Epoch:  346  Training Loss:  0.427049  Training Accuracy:  0.787655\n",
      "Epoch:  347  Training Loss:  0.424511  Training Accuracy:  0.788478\n",
      "Epoch:  348  Training Loss:  0.422163  Training Accuracy:  0.789301\n",
      "Epoch:  349  Training Loss:  0.419913  Training Accuracy:  0.789889\n",
      "Epoch:  350  Training Loss:  0.417518  Training Accuracy:  0.790241\n",
      "Epoch:  351  Training Loss:  0.415442  Training Accuracy:  0.791006\n",
      "Epoch:  352  Training Loss:  0.413304  Training Accuracy:  0.792475\n",
      "Epoch:  353  Training Loss:  0.411336  Training Accuracy:  0.793416\n",
      "Epoch:  354  Training Loss:  0.409088  Training Accuracy:  0.79418\n",
      "Epoch:  355  Training Loss:  0.407358  Training Accuracy:  0.794886\n",
      "Epoch:  356  Training Loss:  0.405386  Training Accuracy:  0.795474\n",
      "Epoch:  357  Training Loss:  0.403503  Training Accuracy:  0.796473\n",
      "Epoch:  358  Training Loss:  0.401742  Training Accuracy:  0.797237\n",
      "Epoch:  359  Training Loss:  0.399882  Training Accuracy:  0.798001\n",
      "Epoch:  360  Training Loss:  0.39813  Training Accuracy:  0.798648\n",
      "Epoch:  361  Training Loss:  0.396341  Training Accuracy:  0.799295\n",
      "Epoch:  362  Training Loss:  0.394454  Training Accuracy:  0.800294\n",
      "Epoch:  363  Training Loss:  0.392621  Training Accuracy:  0.801294\n",
      "Epoch:  364  Training Loss:  0.390746  Training Accuracy:  0.80194\n",
      "Epoch:  365  Training Loss:  0.389241  Training Accuracy:  0.80294\n",
      "Epoch:  366  Training Loss:  0.387371  Training Accuracy:  0.804351\n",
      "Epoch:  367  Training Loss:  0.385382  Training Accuracy:  0.805174\n",
      "Epoch:  368  Training Loss:  0.383627  Training Accuracy:  0.806526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  369  Training Loss:  0.381874  Training Accuracy:  0.807172\n",
      "Epoch:  370  Training Loss:  0.380192  Training Accuracy:  0.808172\n",
      "Epoch:  371  Training Loss:  0.378404  Training Accuracy:  0.809583\n",
      "Epoch:  372  Training Loss:  0.376813  Training Accuracy:  0.810465\n",
      "Epoch:  373  Training Loss:  0.375072  Training Accuracy:  0.811347\n",
      "Epoch:  374  Training Loss:  0.373035  Training Accuracy:  0.812346\n",
      "Epoch:  375  Training Loss:  0.371186  Training Accuracy:  0.813404\n",
      "Epoch:  376  Training Loss:  0.369104  Training Accuracy:  0.81411\n",
      "Epoch:  377  Training Loss:  0.367368  Training Accuracy:  0.814933\n",
      "Epoch:  378  Training Loss:  0.365624  Training Accuracy:  0.815697\n",
      "Epoch:  379  Training Loss:  0.364102  Training Accuracy:  0.816755\n",
      "Epoch:  380  Training Loss:  0.362489  Training Accuracy:  0.817519\n",
      "Epoch:  381  Training Loss:  0.360873  Training Accuracy:  0.818225\n",
      "Epoch:  382  Training Loss:  0.358945  Training Accuracy:  0.818989\n",
      "Epoch:  383  Training Loss:  0.356996  Training Accuracy:  0.819695\n",
      "Epoch:  384  Training Loss:  0.355245  Training Accuracy:  0.820459\n",
      "Epoch:  385  Training Loss:  0.353441  Training Accuracy:  0.820753\n",
      "Epoch:  386  Training Loss:  0.351674  Training Accuracy:  0.821458\n",
      "Epoch:  387  Training Loss:  0.350093  Training Accuracy:  0.821929\n",
      "Epoch:  388  Training Loss:  0.348502  Training Accuracy:  0.82281\n",
      "Epoch:  389  Training Loss:  0.346837  Training Accuracy:  0.823281\n",
      "Epoch:  390  Training Loss:  0.345304  Training Accuracy:  0.823751\n",
      "Epoch:  391  Training Loss:  0.343575  Training Accuracy:  0.824456\n",
      "Epoch:  392  Training Loss:  0.341972  Training Accuracy:  0.825515\n",
      "Epoch:  393  Training Loss:  0.340025  Training Accuracy:  0.825868\n",
      "Epoch:  394  Training Loss:  0.338451  Training Accuracy:  0.826396\n",
      "Epoch:  395  Training Loss:  0.336633  Training Accuracy:  0.826632\n",
      "Epoch:  396  Training Loss:  0.334792  Training Accuracy:  0.827514\n",
      "Epoch:  397  Training Loss:  0.333145  Training Accuracy:  0.827984\n",
      "Epoch:  398  Training Loss:  0.33151  Training Accuracy:  0.828631\n",
      "Epoch:  399  Training Loss:  0.329965  Training Accuracy:  0.829101\n",
      "Epoch:  400  Training Loss:  0.328464  Training Accuracy:  0.829689\n",
      "Epoch:  401  Training Loss:  0.327013  Training Accuracy:  0.830041\n",
      "Epoch:  402  Training Loss:  0.325592  Training Accuracy:  0.830571\n",
      "Epoch:  403  Training Loss:  0.324032  Training Accuracy:  0.831158\n",
      "Epoch:  404  Training Loss:  0.32289  Training Accuracy:  0.831452\n",
      "Epoch:  405  Training Loss:  0.321388  Training Accuracy:  0.83204\n",
      "Epoch:  406  Training Loss:  0.319915  Training Accuracy:  0.832628\n",
      "Epoch:  407  Training Loss:  0.318305  Training Accuracy:  0.833216\n",
      "Epoch:  408  Training Loss:  0.316837  Training Accuracy:  0.833921\n",
      "Epoch:  409  Training Loss:  0.315602  Training Accuracy:  0.834568\n",
      "Epoch:  410  Training Loss:  0.314204  Training Accuracy:  0.835391\n",
      "Epoch:  411  Training Loss:  0.312867  Training Accuracy:  0.83592\n",
      "Epoch:  412  Training Loss:  0.311416  Training Accuracy:  0.836567\n",
      "Epoch:  413  Training Loss:  0.31024  Training Accuracy:  0.837037\n",
      "Epoch:  414  Training Loss:  0.308874  Training Accuracy:  0.837684\n",
      "Epoch:  415  Training Loss:  0.307299  Training Accuracy:  0.838037\n",
      "Epoch:  416  Training Loss:  0.306099  Training Accuracy:  0.838625\n",
      "Epoch:  417  Training Loss:  0.304691  Training Accuracy:  0.839154\n",
      "Epoch:  418  Training Loss:  0.303434  Training Accuracy:  0.840036\n",
      "Epoch:  419  Training Loss:  0.302009  Training Accuracy:  0.840506\n",
      "Epoch:  420  Training Loss:  0.300726  Training Accuracy:  0.841094\n",
      "Epoch:  421  Training Loss:  0.299392  Training Accuracy:  0.841799\n",
      "Epoch:  422  Training Loss:  0.297979  Training Accuracy:  0.842505\n",
      "Epoch:  423  Training Loss:  0.296826  Training Accuracy:  0.84321\n",
      "Epoch:  424  Training Loss:  0.295568  Training Accuracy:  0.843739\n",
      "Epoch:  425  Training Loss:  0.29439  Training Accuracy:  0.844033\n",
      "Epoch:  426  Training Loss:  0.293278  Training Accuracy:  0.844856\n",
      "Epoch:  427  Training Loss:  0.291757  Training Accuracy:  0.845562\n",
      "Epoch:  428  Training Loss:  0.290486  Training Accuracy:  0.846444\n",
      "Epoch:  429  Training Loss:  0.288897  Training Accuracy:  0.846796\n",
      "Epoch:  430  Training Loss:  0.287844  Training Accuracy:  0.847326\n",
      "Epoch:  431  Training Loss:  0.286572  Training Accuracy:  0.847443\n",
      "Epoch:  432  Training Loss:  0.285265  Training Accuracy:  0.848207\n",
      "Epoch:  433  Training Loss:  0.28399  Training Accuracy:  0.848619\n",
      "Epoch:  434  Training Loss:  0.282896  Training Accuracy:  0.849324\n",
      "Epoch:  435  Training Loss:  0.281386  Training Accuracy:  0.849677\n",
      "Epoch:  436  Training Loss:  0.280489  Training Accuracy:  0.850265\n",
      "Epoch:  437  Training Loss:  0.279179  Training Accuracy:  0.850794\n",
      "Epoch:  438  Training Loss:  0.277969  Training Accuracy:  0.851147\n",
      "Epoch:  439  Training Loss:  0.276896  Training Accuracy:  0.851793\n",
      "Epoch:  440  Training Loss:  0.275793  Training Accuracy:  0.852499\n",
      "Epoch:  441  Training Loss:  0.274803  Training Accuracy:  0.853145\n",
      "Epoch:  442  Training Loss:  0.273771  Training Accuracy:  0.853792\n",
      "Epoch:  443  Training Loss:  0.272991  Training Accuracy:  0.854204\n",
      "Epoch:  444  Training Loss:  0.27221  Training Accuracy:  0.854557\n",
      "Epoch:  445  Training Loss:  0.271083  Training Accuracy:  0.855027\n",
      "Epoch:  446  Training Loss:  0.270417  Training Accuracy:  0.855438\n",
      "Epoch:  447  Training Loss:  0.26958  Training Accuracy:  0.855909\n",
      "Epoch:  448  Training Loss:  0.268695  Training Accuracy:  0.855968\n",
      "Epoch:  449  Training Loss:  0.267904  Training Accuracy:  0.856261\n",
      "Epoch:  450  Training Loss:  0.267133  Training Accuracy:  0.856614\n",
      "Epoch:  451  Training Loss:  0.266241  Training Accuracy:  0.85679\n",
      "Epoch:  452  Training Loss:  0.265602  Training Accuracy:  0.857378\n",
      "Epoch:  453  Training Loss:  0.264879  Training Accuracy:  0.85779\n",
      "Epoch:  454  Training Loss:  0.264083  Training Accuracy:  0.858143\n",
      "Epoch:  455  Training Loss:  0.263499  Training Accuracy:  0.858613\n",
      "Epoch:  456  Training Loss:  0.262666  Training Accuracy:  0.859083\n",
      "Epoch:  457  Training Loss:  0.262015  Training Accuracy:  0.859318\n",
      "Epoch:  458  Training Loss:  0.26145  Training Accuracy:  0.859965\n",
      "Epoch:  459  Training Loss:  0.260831  Training Accuracy:  0.860553\n",
      "Epoch:  460  Training Loss:  0.260252  Training Accuracy:  0.860788\n",
      "Epoch:  461  Training Loss:  0.259757  Training Accuracy:  0.861317\n",
      "Epoch:  462  Training Loss:  0.2593  Training Accuracy:  0.861788\n",
      "Epoch:  463  Training Loss:  0.258786  Training Accuracy:  0.861905\n",
      "Epoch:  464  Training Loss:  0.25847  Training Accuracy:  0.86214\n",
      "Epoch:  465  Training Loss:  0.258123  Training Accuracy:  0.862375\n",
      "Epoch:  466  Training Loss:  0.257557  Training Accuracy:  0.862963\n",
      "Epoch:  467  Training Loss:  0.257368  Training Accuracy:  0.863434\n",
      "Epoch:  468  Training Loss:  0.256634  Training Accuracy:  0.863669\n",
      "Epoch:  469  Training Loss:  0.256268  Training Accuracy:  0.864139\n",
      "Epoch:  470  Training Loss:  0.255749  Training Accuracy:  0.864609\n",
      "Epoch:  471  Training Loss:  0.255417  Training Accuracy:  0.864727\n",
      "Epoch:  472  Training Loss:  0.254965  Training Accuracy:  0.865256\n",
      "Epoch:  473  Training Loss:  0.254488  Training Accuracy:  0.865844\n",
      "Epoch:  474  Training Loss:  0.253921  Training Accuracy:  0.866314\n",
      "Epoch:  475  Training Loss:  0.253554  Training Accuracy:  0.866902\n",
      "Epoch:  476  Training Loss:  0.252958  Training Accuracy:  0.867079\n",
      "Epoch:  477  Training Loss:  0.252517  Training Accuracy:  0.867666\n",
      "Epoch:  478  Training Loss:  0.251857  Training Accuracy:  0.868078\n",
      "Epoch:  479  Training Loss:  0.251164  Training Accuracy:  0.868372\n",
      "Epoch:  480  Training Loss:  0.250568  Training Accuracy:  0.868842\n",
      "Epoch:  481  Training Loss:  0.25013  Training Accuracy:  0.869195\n",
      "Epoch:  482  Training Loss:  0.249384  Training Accuracy:  0.869548\n",
      "Epoch:  483  Training Loss:  0.248826  Training Accuracy:  0.869842\n",
      "Epoch:  484  Training Loss:  0.248306  Training Accuracy:  0.870136\n",
      "Epoch:  485  Training Loss:  0.247451  Training Accuracy:  0.870547\n",
      "Epoch:  486  Training Loss:  0.246822  Training Accuracy:  0.870782\n",
      "Epoch:  487  Training Loss:  0.246084  Training Accuracy:  0.871076\n",
      "Epoch:  488  Training Loss:  0.245563  Training Accuracy:  0.871605\n",
      "Epoch:  489  Training Loss:  0.244818  Training Accuracy:  0.87184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  490  Training Loss:  0.244251  Training Accuracy:  0.872135\n",
      "Epoch:  491  Training Loss:  0.243602  Training Accuracy:  0.872487\n",
      "Epoch:  492  Training Loss:  0.243005  Training Accuracy:  0.872664\n",
      "Epoch:  493  Training Loss:  0.242395  Training Accuracy:  0.872899\n",
      "Epoch:  494  Training Loss:  0.241724  Training Accuracy:  0.873016\n",
      "Epoch:  495  Training Loss:  0.24124  Training Accuracy:  0.873075\n",
      "Epoch:  496  Training Loss:  0.240708  Training Accuracy:  0.873134\n",
      "Epoch:  497  Training Loss:  0.240025  Training Accuracy:  0.873545\n",
      "Epoch:  498  Training Loss:  0.239426  Training Accuracy:  0.874016\n",
      "Epoch:  499  Training Loss:  0.238979  Training Accuracy:  0.87431\n",
      "Epoch:  500  Training Loss:  0.23843  Training Accuracy:  0.874604\n",
      "Epoch:  501  Training Loss:  0.237795  Training Accuracy:  0.874898\n",
      "Epoch:  502  Training Loss:  0.237283  Training Accuracy:  0.875074\n",
      "Epoch:  503  Training Loss:  0.236506  Training Accuracy:  0.875368\n",
      "Epoch:  504  Training Loss:  0.235954  Training Accuracy:  0.875662\n",
      "Epoch:  505  Training Loss:  0.235203  Training Accuracy:  0.876073\n",
      "Epoch:  506  Training Loss:  0.234424  Training Accuracy:  0.87625\n",
      "Epoch:  507  Training Loss:  0.23378  Training Accuracy:  0.876955\n",
      "Epoch:  508  Training Loss:  0.232908  Training Accuracy:  0.877249\n",
      "Epoch:  509  Training Loss:  0.232307  Training Accuracy:  0.877661\n",
      "Epoch:  510  Training Loss:  0.231718  Training Accuracy:  0.878248\n",
      "Epoch:  511  Training Loss:  0.230923  Training Accuracy:  0.878601\n",
      "Epoch:  512  Training Loss:  0.23035  Training Accuracy:  0.87913\n",
      "Epoch:  513  Training Loss:  0.229721  Training Accuracy:  0.879718\n",
      "Epoch:  514  Training Loss:  0.228911  Training Accuracy:  0.879836\n",
      "Epoch:  515  Training Loss:  0.228047  Training Accuracy:  0.88013\n",
      "Epoch:  516  Training Loss:  0.227326  Training Accuracy:  0.880365\n",
      "Epoch:  517  Training Loss:  0.226593  Training Accuracy:  0.880659\n",
      "Epoch:  518  Training Loss:  0.225886  Training Accuracy:  0.880718\n",
      "Epoch:  519  Training Loss:  0.225134  Training Accuracy:  0.881364\n",
      "Epoch:  520  Training Loss:  0.224509  Training Accuracy:  0.881599\n",
      "Epoch:  521  Training Loss:  0.223664  Training Accuracy:  0.881717\n",
      "Epoch:  522  Training Loss:  0.223017  Training Accuracy:  0.88207\n",
      "Epoch:  523  Training Loss:  0.222249  Training Accuracy:  0.882423\n",
      "Epoch:  524  Training Loss:  0.221436  Training Accuracy:  0.882716\n",
      "Epoch:  525  Training Loss:  0.220661  Training Accuracy:  0.883128\n",
      "Epoch:  526  Training Loss:  0.219743  Training Accuracy:  0.883481\n",
      "Epoch:  527  Training Loss:  0.218972  Training Accuracy:  0.884127\n",
      "Epoch:  528  Training Loss:  0.218095  Training Accuracy:  0.884774\n",
      "Epoch:  529  Training Loss:  0.217285  Training Accuracy:  0.885068\n",
      "Epoch:  530  Training Loss:  0.216569  Training Accuracy:  0.885244\n",
      "Epoch:  531  Training Loss:  0.215734  Training Accuracy:  0.885891\n",
      "Epoch:  532  Training Loss:  0.214893  Training Accuracy:  0.886126\n",
      "Epoch:  533  Training Loss:  0.214038  Training Accuracy:  0.886538\n",
      "Epoch:  534  Training Loss:  0.213249  Training Accuracy:  0.886773\n",
      "Epoch:  535  Training Loss:  0.212447  Training Accuracy:  0.887008\n",
      "Epoch:  536  Training Loss:  0.211829  Training Accuracy:  0.887243\n",
      "Epoch:  537  Training Loss:  0.211013  Training Accuracy:  0.887478\n",
      "Epoch:  538  Training Loss:  0.210349  Training Accuracy:  0.887537\n",
      "Epoch:  539  Training Loss:  0.209665  Training Accuracy:  0.887772\n",
      "Epoch:  540  Training Loss:  0.209128  Training Accuracy:  0.888007\n",
      "Epoch:  541  Training Loss:  0.208253  Training Accuracy:  0.888125\n",
      "Epoch:  542  Training Loss:  0.207359  Training Accuracy:  0.88836\n",
      "Epoch:  543  Training Loss:  0.206855  Training Accuracy:  0.888772\n",
      "Epoch:  544  Training Loss:  0.206029  Training Accuracy:  0.889007\n",
      "Epoch:  545  Training Loss:  0.205287  Training Accuracy:  0.889418\n",
      "Epoch:  546  Training Loss:  0.204503  Training Accuracy:  0.889654\n",
      "Epoch:  547  Training Loss:  0.203608  Training Accuracy:  0.889947\n",
      "Epoch:  548  Training Loss:  0.202822  Training Accuracy:  0.8903\n",
      "Epoch:  549  Training Loss:  0.201975  Training Accuracy:  0.890947\n",
      "Epoch:  550  Training Loss:  0.201286  Training Accuracy:  0.891123\n",
      "Epoch:  551  Training Loss:  0.200451  Training Accuracy:  0.891594\n",
      "Epoch:  552  Training Loss:  0.199536  Training Accuracy:  0.891946\n",
      "Epoch:  553  Training Loss:  0.198766  Training Accuracy:  0.892475\n",
      "Epoch:  554  Training Loss:  0.19785  Training Accuracy:  0.892769\n",
      "Epoch:  555  Training Loss:  0.197062  Training Accuracy:  0.892887\n",
      "Epoch:  556  Training Loss:  0.196412  Training Accuracy:  0.893122\n",
      "Epoch:  557  Training Loss:  0.195589  Training Accuracy:  0.893416\n",
      "Epoch:  558  Training Loss:  0.194699  Training Accuracy:  0.89371\n",
      "Epoch:  559  Training Loss:  0.193901  Training Accuracy:  0.893945\n",
      "Epoch:  560  Training Loss:  0.192962  Training Accuracy:  0.894239\n",
      "Epoch:  561  Training Loss:  0.192403  Training Accuracy:  0.894474\n",
      "Epoch:  562  Training Loss:  0.191791  Training Accuracy:  0.894768\n",
      "Epoch:  563  Training Loss:  0.190937  Training Accuracy:  0.895239\n",
      "Epoch:  564  Training Loss:  0.190297  Training Accuracy:  0.895591\n",
      "Epoch:  565  Training Loss:  0.189665  Training Accuracy:  0.896179\n",
      "Epoch:  566  Training Loss:  0.189021  Training Accuracy:  0.896414\n",
      "Epoch:  567  Training Loss:  0.188342  Training Accuracy:  0.896708\n",
      "Epoch:  568  Training Loss:  0.18767  Training Accuracy:  0.897179\n",
      "Epoch:  569  Training Loss:  0.186623  Training Accuracy:  0.897355\n",
      "Epoch:  570  Training Loss:  0.185851  Training Accuracy:  0.897531\n",
      "Epoch:  571  Training Loss:  0.185034  Training Accuracy:  0.897649\n",
      "Epoch:  572  Training Loss:  0.184279  Training Accuracy:  0.898296\n",
      "Epoch:  573  Training Loss:  0.183434  Training Accuracy:  0.898472\n",
      "Epoch:  574  Training Loss:  0.182906  Training Accuracy:  0.898884\n",
      "Epoch:  575  Training Loss:  0.182055  Training Accuracy:  0.899001\n",
      "Epoch:  576  Training Loss:  0.181485  Training Accuracy:  0.899295\n",
      "Epoch:  577  Training Loss:  0.180596  Training Accuracy:  0.899648\n",
      "Epoch:  578  Training Loss:  0.179812  Training Accuracy:  0.899824\n",
      "Epoch:  579  Training Loss:  0.179071  Training Accuracy:  0.900118\n",
      "Epoch:  580  Training Loss:  0.178325  Training Accuracy:  0.900412\n",
      "Epoch:  581  Training Loss:  0.177633  Training Accuracy:  0.900588\n",
      "Epoch:  582  Training Loss:  0.176876  Training Accuracy:  0.900824\n",
      "Epoch:  583  Training Loss:  0.17621  Training Accuracy:  0.901\n",
      "Epoch:  584  Training Loss:  0.1755  Training Accuracy:  0.901176\n",
      "Epoch:  585  Training Loss:  0.174866  Training Accuracy:  0.901294\n",
      "Epoch:  586  Training Loss:  0.174098  Training Accuracy:  0.901705\n",
      "Epoch:  587  Training Loss:  0.173564  Training Accuracy:  0.901823\n",
      "Epoch:  588  Training Loss:  0.172907  Training Accuracy:  0.902234\n",
      "Epoch:  589  Training Loss:  0.172454  Training Accuracy:  0.902646\n",
      "Epoch:  590  Training Loss:  0.171843  Training Accuracy:  0.903058\n",
      "Epoch:  591  Training Loss:  0.171333  Training Accuracy:  0.903293\n",
      "Epoch:  592  Training Loss:  0.170669  Training Accuracy:  0.903469\n",
      "Epoch:  593  Training Loss:  0.170131  Training Accuracy:  0.903528\n",
      "Epoch:  594  Training Loss:  0.169587  Training Accuracy:  0.903704\n",
      "Epoch:  595  Training Loss:  0.169112  Training Accuracy:  0.904057\n",
      "Epoch:  596  Training Loss:  0.168422  Training Accuracy:  0.904292\n",
      "Epoch:  597  Training Loss:  0.168031  Training Accuracy:  0.904351\n",
      "Epoch:  598  Training Loss:  0.167283  Training Accuracy:  0.904645\n",
      "Epoch:  599  Training Loss:  0.166778  Training Accuracy:  0.904821\n",
      "Epoch:  600  Training Loss:  0.166191  Training Accuracy:  0.905056\n",
      "Epoch:  601  Training Loss:  0.165639  Training Accuracy:  0.905468\n",
      "Epoch:  602  Training Loss:  0.165039  Training Accuracy:  0.90582\n",
      "Epoch:  603  Training Loss:  0.164397  Training Accuracy:  0.906173\n",
      "Epoch:  604  Training Loss:  0.163607  Training Accuracy:  0.906173\n",
      "Epoch:  605  Training Loss:  0.163026  Training Accuracy:  0.906408\n",
      "Epoch:  606  Training Loss:  0.162269  Training Accuracy:  0.906408\n",
      "Epoch:  607  Training Loss:  0.161696  Training Accuracy:  0.906467\n",
      "Epoch:  608  Training Loss:  0.161009  Training Accuracy:  0.906526\n",
      "Epoch:  609  Training Loss:  0.160149  Training Accuracy:  0.906761\n",
      "Epoch:  610  Training Loss:  0.159643  Training Accuracy:  0.906938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  611  Training Loss:  0.158948  Training Accuracy:  0.907349\n",
      "Epoch:  612  Training Loss:  0.158287  Training Accuracy:  0.907584\n",
      "Epoch:  613  Training Loss:  0.157426  Training Accuracy:  0.907819\n",
      "Epoch:  614  Training Loss:  0.156877  Training Accuracy:  0.908231\n",
      "Epoch:  615  Training Loss:  0.156204  Training Accuracy:  0.908349\n",
      "Epoch:  616  Training Loss:  0.155406  Training Accuracy:  0.908466\n",
      "Epoch:  617  Training Loss:  0.15487  Training Accuracy:  0.908642\n",
      "Epoch:  618  Training Loss:  0.154284  Training Accuracy:  0.908995\n",
      "Epoch:  619  Training Loss:  0.153538  Training Accuracy:  0.909172\n",
      "Epoch:  620  Training Loss:  0.152892  Training Accuracy:  0.909407\n",
      "Epoch:  621  Training Loss:  0.152122  Training Accuracy:  0.909759\n",
      "Epoch:  622  Training Loss:  0.151506  Training Accuracy:  0.910406\n",
      "Epoch:  623  Training Loss:  0.150845  Training Accuracy:  0.910406\n",
      "Epoch:  624  Training Loss:  0.150081  Training Accuracy:  0.910818\n",
      "Epoch:  625  Training Loss:  0.149482  Training Accuracy:  0.910935\n",
      "Epoch:  626  Training Loss:  0.148791  Training Accuracy:  0.91117\n",
      "Epoch:  627  Training Loss:  0.148164  Training Accuracy:  0.911229\n",
      "Epoch:  628  Training Loss:  0.147556  Training Accuracy:  0.911347\n",
      "Epoch:  629  Training Loss:  0.14688  Training Accuracy:  0.911641\n",
      "Epoch:  630  Training Loss:  0.146174  Training Accuracy:  0.911699\n",
      "Epoch:  631  Training Loss:  0.145585  Training Accuracy:  0.911817\n",
      "Epoch:  632  Training Loss:  0.145022  Training Accuracy:  0.912287\n",
      "Epoch:  633  Training Loss:  0.144374  Training Accuracy:  0.912523\n",
      "Epoch:  634  Training Loss:  0.143631  Training Accuracy:  0.912699\n",
      "Epoch:  635  Training Loss:  0.143212  Training Accuracy:  0.912993\n",
      "Epoch:  636  Training Loss:  0.142513  Training Accuracy:  0.913228\n",
      "Epoch:  637  Training Loss:  0.141979  Training Accuracy:  0.913463\n",
      "Epoch:  638  Training Loss:  0.141259  Training Accuracy:  0.913698\n",
      "Epoch:  639  Training Loss:  0.14079  Training Accuracy:  0.914051\n",
      "Epoch:  640  Training Loss:  0.140119  Training Accuracy:  0.914286\n",
      "Epoch:  641  Training Loss:  0.139584  Training Accuracy:  0.914698\n",
      "Epoch:  642  Training Loss:  0.138934  Training Accuracy:  0.914933\n",
      "Epoch:  643  Training Loss:  0.138394  Training Accuracy:  0.915344\n",
      "Epoch:  644  Training Loss:  0.13785  Training Accuracy:  0.915756\n",
      "Epoch:  645  Training Loss:  0.137157  Training Accuracy:  0.915873\n",
      "Epoch:  646  Training Loss:  0.136647  Training Accuracy:  0.916167\n",
      "Epoch:  647  Training Loss:  0.136049  Training Accuracy:  0.916344\n",
      "Epoch:  648  Training Loss:  0.135361  Training Accuracy:  0.916461\n",
      "Epoch:  649  Training Loss:  0.134666  Training Accuracy:  0.916579\n",
      "Epoch:  650  Training Loss:  0.134203  Training Accuracy:  0.916755\n",
      "Epoch:  651  Training Loss:  0.133515  Training Accuracy:  0.917049\n",
      "Epoch:  652  Training Loss:  0.13303  Training Accuracy:  0.917402\n",
      "Epoch:  653  Training Loss:  0.132509  Training Accuracy:  0.917637\n",
      "Epoch:  654  Training Loss:  0.131936  Training Accuracy:  0.917931\n",
      "Epoch:  655  Training Loss:  0.131489  Training Accuracy:  0.918343\n",
      "Epoch:  656  Training Loss:  0.130818  Training Accuracy:  0.918401\n",
      "Epoch:  657  Training Loss:  0.130372  Training Accuracy:  0.918578\n",
      "Epoch:  658  Training Loss:  0.129767  Training Accuracy:  0.918813\n",
      "Epoch:  659  Training Loss:  0.12928  Training Accuracy:  0.918872\n",
      "Epoch:  660  Training Loss:  0.128922  Training Accuracy:  0.919107\n",
      "Epoch:  661  Training Loss:  0.128355  Training Accuracy:  0.919401\n",
      "Epoch:  662  Training Loss:  0.127952  Training Accuracy:  0.919518\n",
      "Epoch:  663  Training Loss:  0.12757  Training Accuracy:  0.919754\n",
      "Epoch:  664  Training Loss:  0.126975  Training Accuracy:  0.919871\n",
      "Epoch:  665  Training Loss:  0.126578  Training Accuracy:  0.91993\n",
      "Epoch:  666  Training Loss:  0.126097  Training Accuracy:  0.920048\n",
      "Epoch:  667  Training Loss:  0.125469  Training Accuracy:  0.920224\n",
      "Epoch:  668  Training Loss:  0.125054  Training Accuracy:  0.920518\n",
      "Epoch:  669  Training Loss:  0.124545  Training Accuracy:  0.920635\n",
      "Epoch:  670  Training Loss:  0.124129  Training Accuracy:  0.921047\n",
      "Epoch:  671  Training Loss:  0.123637  Training Accuracy:  0.921047\n",
      "Epoch:  672  Training Loss:  0.123342  Training Accuracy:  0.921106\n",
      "Epoch:  673  Training Loss:  0.122975  Training Accuracy:  0.9214\n",
      "Epoch:  674  Training Loss:  0.12261  Training Accuracy:  0.921811\n",
      "Epoch:  675  Training Loss:  0.122094  Training Accuracy:  0.921988\n",
      "Epoch:  676  Training Loss:  0.121797  Training Accuracy:  0.922458\n",
      "Epoch:  677  Training Loss:  0.121483  Training Accuracy:  0.922458\n",
      "Epoch:  678  Training Loss:  0.121107  Training Accuracy:  0.922575\n",
      "Epoch:  679  Training Loss:  0.120823  Training Accuracy:  0.922928\n",
      "Epoch:  680  Training Loss:  0.120349  Training Accuracy:  0.923105\n",
      "Epoch:  681  Training Loss:  0.120004  Training Accuracy:  0.923163\n",
      "Epoch:  682  Training Loss:  0.11969  Training Accuracy:  0.92334\n",
      "Epoch:  683  Training Loss:  0.119325  Training Accuracy:  0.923398\n",
      "Epoch:  684  Training Loss:  0.118825  Training Accuracy:  0.923869\n",
      "Epoch:  685  Training Loss:  0.118503  Training Accuracy:  0.924045\n",
      "Epoch:  686  Training Loss:  0.11817  Training Accuracy:  0.924222\n",
      "Epoch:  687  Training Loss:  0.11781  Training Accuracy:  0.924457\n",
      "Epoch:  688  Training Loss:  0.117403  Training Accuracy:  0.924868\n",
      "Epoch:  689  Training Loss:  0.117091  Training Accuracy:  0.924868\n",
      "Epoch:  690  Training Loss:  0.116727  Training Accuracy:  0.924986\n",
      "Epoch:  691  Training Loss:  0.116362  Training Accuracy:  0.925045\n",
      "Epoch:  692  Training Loss:  0.116014  Training Accuracy:  0.925162\n",
      "Epoch:  693  Training Loss:  0.115665  Training Accuracy:  0.925574\n",
      "Epoch:  694  Training Loss:  0.115325  Training Accuracy:  0.925632\n",
      "Epoch:  695  Training Loss:  0.114805  Training Accuracy:  0.925868\n",
      "Epoch:  696  Training Loss:  0.114524  Training Accuracy:  0.926162\n",
      "Epoch:  697  Training Loss:  0.114112  Training Accuracy:  0.926279\n",
      "Epoch:  698  Training Loss:  0.11374  Training Accuracy:  0.926456\n",
      "Epoch:  699  Training Loss:  0.113289  Training Accuracy:  0.926691\n",
      "Epoch:  700  Training Loss:  0.112997  Training Accuracy:  0.926867\n",
      "Epoch:  701  Training Loss:  0.112564  Training Accuracy:  0.927043\n",
      "Epoch:  702  Training Loss:  0.112155  Training Accuracy:  0.927102\n",
      "Epoch:  703  Training Loss:  0.111825  Training Accuracy:  0.927102\n",
      "Epoch:  704  Training Loss:  0.111375  Training Accuracy:  0.927279\n",
      "Epoch:  705  Training Loss:  0.111045  Training Accuracy:  0.927749\n",
      "Epoch:  706  Training Loss:  0.110527  Training Accuracy:  0.928102\n",
      "Epoch:  707  Training Loss:  0.110208  Training Accuracy:  0.928102\n",
      "Epoch:  708  Training Loss:  0.109767  Training Accuracy:  0.928337\n",
      "Epoch:  709  Training Loss:  0.109415  Training Accuracy:  0.928572\n",
      "Epoch:  710  Training Loss:  0.10903  Training Accuracy:  0.928866\n",
      "Epoch:  711  Training Loss:  0.108673  Training Accuracy:  0.928983\n",
      "Epoch:  712  Training Loss:  0.108317  Training Accuracy:  0.929101\n",
      "Epoch:  713  Training Loss:  0.107952  Training Accuracy:  0.929219\n",
      "Epoch:  714  Training Loss:  0.107602  Training Accuracy:  0.929454\n",
      "Epoch:  715  Training Loss:  0.10727  Training Accuracy:  0.92963\n",
      "Epoch:  716  Training Loss:  0.106882  Training Accuracy:  0.929807\n",
      "Epoch:  717  Training Loss:  0.106545  Training Accuracy:  0.9301\n",
      "Epoch:  718  Training Loss:  0.106155  Training Accuracy:  0.930159\n",
      "Epoch:  719  Training Loss:  0.105776  Training Accuracy:  0.930512\n",
      "Epoch:  720  Training Loss:  0.105569  Training Accuracy:  0.930688\n",
      "Epoch:  721  Training Loss:  0.105116  Training Accuracy:  0.930865\n",
      "Epoch:  722  Training Loss:  0.104861  Training Accuracy:  0.931041\n",
      "Epoch:  723  Training Loss:  0.104608  Training Accuracy:  0.931159\n",
      "Epoch:  724  Training Loss:  0.104251  Training Accuracy:  0.931159\n",
      "Epoch:  725  Training Loss:  0.104018  Training Accuracy:  0.931335\n",
      "Epoch:  726  Training Loss:  0.103716  Training Accuracy:  0.931511\n",
      "Epoch:  727  Training Loss:  0.103423  Training Accuracy:  0.931629\n",
      "Epoch:  728  Training Loss:  0.103241  Training Accuracy:  0.931923\n",
      "Epoch:  729  Training Loss:  0.102939  Training Accuracy:  0.932099\n",
      "Epoch:  730  Training Loss:  0.102649  Training Accuracy:  0.932276\n",
      "Epoch:  731  Training Loss:  0.1024  Training Accuracy:  0.932452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  732  Training Loss:  0.102157  Training Accuracy:  0.932452\n",
      "Epoch:  733  Training Loss:  0.101808  Training Accuracy:  0.93257\n",
      "Epoch:  734  Training Loss:  0.101531  Training Accuracy:  0.932746\n",
      "Epoch:  735  Training Loss:  0.101248  Training Accuracy:  0.932922\n",
      "Epoch:  736  Training Loss:  0.100957  Training Accuracy:  0.933393\n",
      "Epoch:  737  Training Loss:  0.100616  Training Accuracy:  0.93351\n",
      "Epoch:  738  Training Loss:  0.100401  Training Accuracy:  0.933569\n",
      "Epoch:  739  Training Loss:  0.100154  Training Accuracy:  0.933687\n",
      "Epoch:  740  Training Loss:  0.099823  Training Accuracy:  0.933804\n",
      "Epoch:  741  Training Loss:  0.0995704  Training Accuracy:  0.933981\n",
      "Epoch:  742  Training Loss:  0.0993773  Training Accuracy:  0.934216\n",
      "Epoch:  743  Training Loss:  0.0990575  Training Accuracy:  0.934451\n",
      "Epoch:  744  Training Loss:  0.0988466  Training Accuracy:  0.934568\n",
      "Epoch:  745  Training Loss:  0.0985969  Training Accuracy:  0.934627\n",
      "Epoch:  746  Training Loss:  0.0983203  Training Accuracy:  0.93498\n",
      "Epoch:  747  Training Loss:  0.0981226  Training Accuracy:  0.935156\n",
      "Epoch:  748  Training Loss:  0.0979191  Training Accuracy:  0.935274\n",
      "Epoch:  749  Training Loss:  0.0975512  Training Accuracy:  0.935333\n",
      "Epoch:  750  Training Loss:  0.0974243  Training Accuracy:  0.935391\n",
      "Epoch:  751  Training Loss:  0.0971593  Training Accuracy:  0.935509\n",
      "Epoch:  752  Training Loss:  0.0969602  Training Accuracy:  0.935744\n",
      "Epoch:  753  Training Loss:  0.0966728  Training Accuracy:  0.935979\n",
      "Epoch:  754  Training Loss:  0.0964565  Training Accuracy:  0.936038\n",
      "Epoch:  755  Training Loss:  0.0962353  Training Accuracy:  0.936332\n",
      "Epoch:  756  Training Loss:  0.0960772  Training Accuracy:  0.936567\n",
      "Epoch:  757  Training Loss:  0.0957721  Training Accuracy:  0.936861\n",
      "Epoch:  758  Training Loss:  0.0956615  Training Accuracy:  0.937214\n",
      "Epoch:  759  Training Loss:  0.0953201  Training Accuracy:  0.937273\n",
      "Epoch:  760  Training Loss:  0.0952019  Training Accuracy:  0.937508\n",
      "Epoch:  761  Training Loss:  0.0948533  Training Accuracy:  0.937684\n",
      "Epoch:  762  Training Loss:  0.0947185  Training Accuracy:  0.937978\n",
      "Epoch:  763  Training Loss:  0.0944088  Training Accuracy:  0.938213\n",
      "Epoch:  764  Training Loss:  0.0942039  Training Accuracy:  0.938331\n",
      "Epoch:  765  Training Loss:  0.0939939  Training Accuracy:  0.938566\n",
      "Epoch:  766  Training Loss:  0.0938046  Training Accuracy:  0.93886\n",
      "Epoch:  767  Training Loss:  0.0936053  Training Accuracy:  0.939213\n",
      "Epoch:  768  Training Loss:  0.0933732  Training Accuracy:  0.93933\n",
      "Epoch:  769  Training Loss:  0.0931721  Training Accuracy:  0.939448\n",
      "Epoch:  770  Training Loss:  0.0928465  Training Accuracy:  0.939565\n",
      "Epoch:  771  Training Loss:  0.0926787  Training Accuracy:  0.939742\n",
      "Epoch:  772  Training Loss:  0.0924754  Training Accuracy:  0.939801\n",
      "Epoch:  773  Training Loss:  0.092195  Training Accuracy:  0.939859\n",
      "Epoch:  774  Training Loss:  0.0919557  Training Accuracy:  0.939918\n",
      "Epoch:  775  Training Loss:  0.0917604  Training Accuracy:  0.940095\n",
      "Epoch:  776  Training Loss:  0.0914227  Training Accuracy:  0.940153\n",
      "Epoch:  777  Training Loss:  0.0912265  Training Accuracy:  0.940271\n",
      "Epoch:  778  Training Loss:  0.0908719  Training Accuracy:  0.940565\n",
      "Epoch:  779  Training Loss:  0.0907045  Training Accuracy:  0.940741\n",
      "Epoch:  780  Training Loss:  0.0903681  Training Accuracy:  0.940859\n",
      "Epoch:  781  Training Loss:  0.0901968  Training Accuracy:  0.941094\n",
      "Epoch:  782  Training Loss:  0.0899891  Training Accuracy:  0.94127\n",
      "Epoch:  783  Training Loss:  0.0897745  Training Accuracy:  0.941447\n",
      "Epoch:  784  Training Loss:  0.0894843  Training Accuracy:  0.941623\n",
      "Epoch:  785  Training Loss:  0.0892395  Training Accuracy:  0.941741\n",
      "Epoch:  786  Training Loss:  0.0889919  Training Accuracy:  0.941858\n",
      "Epoch:  787  Training Loss:  0.0887665  Training Accuracy:  0.942093\n",
      "Epoch:  788  Training Loss:  0.0885007  Training Accuracy:  0.942329\n",
      "Epoch:  789  Training Loss:  0.0882374  Training Accuracy:  0.942622\n",
      "Epoch:  790  Training Loss:  0.0880186  Training Accuracy:  0.942681\n",
      "Epoch:  791  Training Loss:  0.0876407  Training Accuracy:  0.942975\n",
      "Epoch:  792  Training Loss:  0.0874741  Training Accuracy:  0.943093\n",
      "Epoch:  793  Training Loss:  0.0872593  Training Accuracy:  0.943328\n",
      "Epoch:  794  Training Loss:  0.0868661  Training Accuracy:  0.943681\n",
      "Epoch:  795  Training Loss:  0.0867425  Training Accuracy:  0.94374\n",
      "Epoch:  796  Training Loss:  0.0864355  Training Accuracy:  0.943916\n",
      "Epoch:  797  Training Loss:  0.0860604  Training Accuracy:  0.943975\n",
      "Epoch:  798  Training Loss:  0.0859011  Training Accuracy:  0.944033\n",
      "Epoch:  799  Training Loss:  0.0854705  Training Accuracy:  0.944151\n",
      "Epoch:  800  Training Loss:  0.0852621  Training Accuracy:  0.944269\n",
      "Epoch:  801  Training Loss:  0.0850487  Training Accuracy:  0.944386\n",
      "Epoch:  802  Training Loss:  0.0846914  Training Accuracy:  0.944621\n",
      "Epoch:  803  Training Loss:  0.0844744  Training Accuracy:  0.944739\n",
      "Epoch:  804  Training Loss:  0.08419  Training Accuracy:  0.944739\n",
      "Epoch:  805  Training Loss:  0.083843  Training Accuracy:  0.945033\n",
      "Epoch:  806  Training Loss:  0.0836352  Training Accuracy:  0.945033\n",
      "Epoch:  807  Training Loss:  0.0833575  Training Accuracy:  0.94515\n",
      "Epoch:  808  Training Loss:  0.0830166  Training Accuracy:  0.945327\n",
      "Epoch:  809  Training Loss:  0.0828184  Training Accuracy:  0.945562\n",
      "Epoch:  810  Training Loss:  0.0825487  Training Accuracy:  0.945562\n",
      "Epoch:  811  Training Loss:  0.082169  Training Accuracy:  0.945621\n",
      "Epoch:  812  Training Loss:  0.0820176  Training Accuracy:  0.945679\n",
      "Epoch:  813  Training Loss:  0.0817642  Training Accuracy:  0.945738\n",
      "Epoch:  814  Training Loss:  0.081554  Training Accuracy:  0.945797\n",
      "Epoch:  815  Training Loss:  0.0812092  Training Accuracy:  0.945856\n",
      "Epoch:  816  Training Loss:  0.0810011  Training Accuracy:  0.945973\n",
      "Epoch:  817  Training Loss:  0.0807253  Training Accuracy:  0.946091\n",
      "Epoch:  818  Training Loss:  0.0804174  Training Accuracy:  0.946209\n",
      "Epoch:  819  Training Loss:  0.0801878  Training Accuracy:  0.946326\n",
      "Epoch:  820  Training Loss:  0.079895  Training Accuracy:  0.946385\n",
      "Epoch:  821  Training Loss:  0.0795289  Training Accuracy:  0.946444\n",
      "Epoch:  822  Training Loss:  0.0792768  Training Accuracy:  0.946738\n",
      "Epoch:  823  Training Loss:  0.0790242  Training Accuracy:  0.946973\n",
      "Epoch:  824  Training Loss:  0.0787608  Training Accuracy:  0.947091\n",
      "Epoch:  825  Training Loss:  0.0783837  Training Accuracy:  0.947267\n",
      "Epoch:  826  Training Loss:  0.0782017  Training Accuracy:  0.947502\n",
      "Epoch:  827  Training Loss:  0.0778958  Training Accuracy:  0.947914\n",
      "Epoch:  828  Training Loss:  0.0775975  Training Accuracy:  0.94809\n",
      "Epoch:  829  Training Loss:  0.0772326  Training Accuracy:  0.948149\n",
      "Epoch:  830  Training Loss:  0.0769689  Training Accuracy:  0.948149\n",
      "Epoch:  831  Training Loss:  0.0766543  Training Accuracy:  0.948443\n",
      "Epoch:  832  Training Loss:  0.0763762  Training Accuracy:  0.948501\n",
      "Epoch:  833  Training Loss:  0.0759897  Training Accuracy:  0.94856\n",
      "Epoch:  834  Training Loss:  0.0757413  Training Accuracy:  0.948737\n",
      "Epoch:  835  Training Loss:  0.0754694  Training Accuracy:  0.948972\n",
      "Epoch:  836  Training Loss:  0.0751886  Training Accuracy:  0.949089\n",
      "Epoch:  837  Training Loss:  0.074932  Training Accuracy:  0.949207\n",
      "Epoch:  838  Training Loss:  0.0745788  Training Accuracy:  0.949266\n",
      "Epoch:  839  Training Loss:  0.0743417  Training Accuracy:  0.949383\n",
      "Epoch:  840  Training Loss:  0.0740934  Training Accuracy:  0.949618\n",
      "Epoch:  841  Training Loss:  0.0738181  Training Accuracy:  0.949736\n",
      "Epoch:  842  Training Loss:  0.0736103  Training Accuracy:  0.949736\n",
      "Epoch:  843  Training Loss:  0.0733471  Training Accuracy:  0.949971\n",
      "Epoch:  844  Training Loss:  0.0730577  Training Accuracy:  0.950265\n",
      "Epoch:  845  Training Loss:  0.0728125  Training Accuracy:  0.950324\n",
      "Epoch:  846  Training Loss:  0.0725645  Training Accuracy:  0.950383\n",
      "Epoch:  847  Training Loss:  0.0722389  Training Accuracy:  0.950618\n",
      "Epoch:  848  Training Loss:  0.0720154  Training Accuracy:  0.950735\n",
      "Epoch:  849  Training Loss:  0.071794  Training Accuracy:  0.950735\n",
      "Epoch:  850  Training Loss:  0.0715444  Training Accuracy:  0.950971\n",
      "Epoch:  851  Training Loss:  0.0713041  Training Accuracy:  0.951088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  852  Training Loss:  0.071065  Training Accuracy:  0.951206\n",
      "Epoch:  853  Training Loss:  0.0708172  Training Accuracy:  0.951382\n",
      "Epoch:  854  Training Loss:  0.0704973  Training Accuracy:  0.951441\n",
      "Epoch:  855  Training Loss:  0.0702616  Training Accuracy:  0.951558\n",
      "Epoch:  856  Training Loss:  0.0700319  Training Accuracy:  0.951676\n",
      "Epoch:  857  Training Loss:  0.069778  Training Accuracy:  0.951911\n",
      "Epoch:  858  Training Loss:  0.0695703  Training Accuracy:  0.95197\n",
      "Epoch:  859  Training Loss:  0.0693326  Training Accuracy:  0.952029\n",
      "Epoch:  860  Training Loss:  0.0690999  Training Accuracy:  0.952264\n",
      "Epoch:  861  Training Loss:  0.0687975  Training Accuracy:  0.95244\n",
      "Epoch:  862  Training Loss:  0.0685885  Training Accuracy:  0.952499\n",
      "Epoch:  863  Training Loss:  0.0683863  Training Accuracy:  0.952675\n",
      "Epoch:  864  Training Loss:  0.0681565  Training Accuracy:  0.952852\n",
      "Epoch:  865  Training Loss:  0.067971  Training Accuracy:  0.952852\n",
      "Epoch:  866  Training Loss:  0.0677377  Training Accuracy:  0.953087\n",
      "Epoch:  867  Training Loss:  0.0675367  Training Accuracy:  0.953205\n",
      "Epoch:  868  Training Loss:  0.0673096  Training Accuracy:  0.953263\n",
      "Epoch:  869  Training Loss:  0.0671304  Training Accuracy:  0.953263\n",
      "Epoch:  870  Training Loss:  0.0668349  Training Accuracy:  0.953322\n",
      "Epoch:  871  Training Loss:  0.0666545  Training Accuracy:  0.95344\n",
      "Epoch:  872  Training Loss:  0.0664402  Training Accuracy:  0.953498\n",
      "Epoch:  873  Training Loss:  0.0662326  Training Accuracy:  0.953675\n",
      "Epoch:  874  Training Loss:  0.0660335  Training Accuracy:  0.953734\n",
      "Epoch:  875  Training Loss:  0.0658008  Training Accuracy:  0.953851\n",
      "Epoch:  876  Training Loss:  0.0656162  Training Accuracy:  0.954028\n",
      "Epoch:  877  Training Loss:  0.0654082  Training Accuracy:  0.954145\n",
      "Epoch:  878  Training Loss:  0.0651506  Training Accuracy:  0.954322\n",
      "Epoch:  879  Training Loss:  0.0649955  Training Accuracy:  0.95438\n",
      "Epoch:  880  Training Loss:  0.0648079  Training Accuracy:  0.954557\n",
      "Epoch:  881  Training Loss:  0.0646094  Training Accuracy:  0.954615\n",
      "Epoch:  882  Training Loss:  0.0644108  Training Accuracy:  0.954674\n",
      "Epoch:  883  Training Loss:  0.0641925  Training Accuracy:  0.954851\n",
      "Epoch:  884  Training Loss:  0.0639879  Training Accuracy:  0.955027\n",
      "Epoch:  885  Training Loss:  0.0637547  Training Accuracy:  0.955145\n",
      "Epoch:  886  Training Loss:  0.0635808  Training Accuracy:  0.95538\n",
      "Epoch:  887  Training Loss:  0.0633273  Training Accuracy:  0.95538\n",
      "Epoch:  888  Training Loss:  0.063138  Training Accuracy:  0.955497\n",
      "Epoch:  889  Training Loss:  0.0628866  Training Accuracy:  0.955497\n",
      "Epoch:  890  Training Loss:  0.0626834  Training Accuracy:  0.955674\n",
      "Epoch:  891  Training Loss:  0.0625014  Training Accuracy:  0.955674\n",
      "Epoch:  892  Training Loss:  0.0622571  Training Accuracy:  0.955674\n",
      "Epoch:  893  Training Loss:  0.0620608  Training Accuracy:  0.955674\n",
      "Epoch:  894  Training Loss:  0.0618049  Training Accuracy:  0.955791\n",
      "Epoch:  895  Training Loss:  0.0616152  Training Accuracy:  0.955791\n",
      "Epoch:  896  Training Loss:  0.0614759  Training Accuracy:  0.955909\n",
      "Epoch:  897  Training Loss:  0.0611996  Training Accuracy:  0.955909\n",
      "Epoch:  898  Training Loss:  0.0610036  Training Accuracy:  0.956085\n",
      "Epoch:  899  Training Loss:  0.0607971  Training Accuracy:  0.956203\n",
      "Epoch:  900  Training Loss:  0.0605851  Training Accuracy:  0.95632\n",
      "Epoch:  901  Training Loss:  0.0603889  Training Accuracy:  0.956379\n",
      "Epoch:  902  Training Loss:  0.0601765  Training Accuracy:  0.956379\n",
      "Epoch:  903  Training Loss:  0.0599819  Training Accuracy:  0.956438\n",
      "Epoch:  904  Training Loss:  0.0597521  Training Accuracy:  0.956555\n",
      "Epoch:  905  Training Loss:  0.0595547  Training Accuracy:  0.956673\n",
      "Epoch:  906  Training Loss:  0.0594334  Training Accuracy:  0.956791\n",
      "Epoch:  907  Training Loss:  0.0592109  Training Accuracy:  0.956849\n",
      "Epoch:  908  Training Loss:  0.0590079  Training Accuracy:  0.956849\n",
      "Epoch:  909  Training Loss:  0.0588115  Training Accuracy:  0.956849\n",
      "Epoch:  910  Training Loss:  0.0586196  Training Accuracy:  0.957026\n",
      "Epoch:  911  Training Loss:  0.0584141  Training Accuracy:  0.957085\n",
      "Epoch:  912  Training Loss:  0.0582484  Training Accuracy:  0.957261\n",
      "Epoch:  913  Training Loss:  0.058033  Training Accuracy:  0.957437\n",
      "Epoch:  914  Training Loss:  0.0578377  Training Accuracy:  0.957437\n",
      "Epoch:  915  Training Loss:  0.0576378  Training Accuracy:  0.957496\n",
      "Epoch:  916  Training Loss:  0.0574379  Training Accuracy:  0.957672\n",
      "Epoch:  917  Training Loss:  0.0572815  Training Accuracy:  0.957731\n",
      "Epoch:  918  Training Loss:  0.0570829  Training Accuracy:  0.957731\n",
      "Epoch:  919  Training Loss:  0.0568448  Training Accuracy:  0.957849\n",
      "Epoch:  920  Training Loss:  0.0566903  Training Accuracy:  0.957849\n",
      "Epoch:  921  Training Loss:  0.0564614  Training Accuracy:  0.957908\n",
      "Epoch:  922  Training Loss:  0.0562909  Training Accuracy:  0.957966\n",
      "Epoch:  923  Training Loss:  0.0560827  Training Accuracy:  0.958143\n",
      "Epoch:  924  Training Loss:  0.0559057  Training Accuracy:  0.958319\n",
      "Epoch:  925  Training Loss:  0.0556996  Training Accuracy:  0.958495\n",
      "Epoch:  926  Training Loss:  0.0555237  Training Accuracy:  0.958554\n",
      "Epoch:  927  Training Loss:  0.0553407  Training Accuracy:  0.958613\n",
      "Epoch:  928  Training Loss:  0.0551758  Training Accuracy:  0.958672\n",
      "Epoch:  929  Training Loss:  0.0549838  Training Accuracy:  0.958731\n",
      "Epoch:  930  Training Loss:  0.0548215  Training Accuracy:  0.958789\n",
      "Epoch:  931  Training Loss:  0.0546368  Training Accuracy:  0.958848\n",
      "Epoch:  932  Training Loss:  0.05448  Training Accuracy:  0.958907\n",
      "Epoch:  933  Training Loss:  0.0542901  Training Accuracy:  0.958966\n",
      "Epoch:  934  Training Loss:  0.0541167  Training Accuracy:  0.958966\n",
      "Epoch:  935  Training Loss:  0.0539791  Training Accuracy:  0.959142\n",
      "Epoch:  936  Training Loss:  0.0537972  Training Accuracy:  0.95926\n",
      "Epoch:  937  Training Loss:  0.0536201  Training Accuracy:  0.959436\n",
      "Epoch:  938  Training Loss:  0.0534718  Training Accuracy:  0.959554\n",
      "Epoch:  939  Training Loss:  0.0533049  Training Accuracy:  0.959671\n",
      "Epoch:  940  Training Loss:  0.0531551  Training Accuracy:  0.95973\n",
      "Epoch:  941  Training Loss:  0.0529718  Training Accuracy:  0.95973\n",
      "Epoch:  942  Training Loss:  0.0528465  Training Accuracy:  0.959848\n",
      "Epoch:  943  Training Loss:  0.0526531  Training Accuracy:  0.960083\n",
      "Epoch:  944  Training Loss:  0.0524999  Training Accuracy:  0.960083\n",
      "Epoch:  945  Training Loss:  0.0523359  Training Accuracy:  0.960083\n",
      "Epoch:  946  Training Loss:  0.0521688  Training Accuracy:  0.960142\n",
      "Epoch:  947  Training Loss:  0.0520123  Training Accuracy:  0.9602\n",
      "Epoch:  948  Training Loss:  0.051881  Training Accuracy:  0.960318\n",
      "Epoch:  949  Training Loss:  0.0516764  Training Accuracy:  0.960318\n",
      "Epoch:  950  Training Loss:  0.0515625  Training Accuracy:  0.960436\n",
      "Epoch:  951  Training Loss:  0.0513894  Training Accuracy:  0.960436\n",
      "Epoch:  952  Training Loss:  0.0512324  Training Accuracy:  0.960553\n",
      "Epoch:  953  Training Loss:  0.0510916  Training Accuracy:  0.960553\n",
      "Epoch:  954  Training Loss:  0.051001  Training Accuracy:  0.960671\n",
      "Epoch:  955  Training Loss:  0.0508103  Training Accuracy:  0.96073\n",
      "Epoch:  956  Training Loss:  0.0506858  Training Accuracy:  0.960847\n",
      "Epoch:  957  Training Loss:  0.0505308  Training Accuracy:  0.960965\n",
      "Epoch:  958  Training Loss:  0.0504082  Training Accuracy:  0.961023\n",
      "Epoch:  959  Training Loss:  0.0502488  Training Accuracy:  0.961082\n",
      "Epoch:  960  Training Loss:  0.0501132  Training Accuracy:  0.961141\n",
      "Epoch:  961  Training Loss:  0.0499628  Training Accuracy:  0.9612\n",
      "Epoch:  962  Training Loss:  0.0498274  Training Accuracy:  0.961259\n",
      "Epoch:  963  Training Loss:  0.0496856  Training Accuracy:  0.961317\n",
      "Epoch:  964  Training Loss:  0.0495564  Training Accuracy:  0.961435\n",
      "Epoch:  965  Training Loss:  0.0494118  Training Accuracy:  0.961435\n",
      "Epoch:  966  Training Loss:  0.049273  Training Accuracy:  0.961435\n",
      "Epoch:  967  Training Loss:  0.0491631  Training Accuracy:  0.961494\n",
      "Epoch:  968  Training Loss:  0.0490652  Training Accuracy:  0.961553\n",
      "Epoch:  969  Training Loss:  0.0488937  Training Accuracy:  0.961788\n",
      "Epoch:  970  Training Loss:  0.0487752  Training Accuracy:  0.961846\n",
      "Epoch:  971  Training Loss:  0.0486512  Training Accuracy:  0.961964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  972  Training Loss:  0.0485201  Training Accuracy:  0.96214\n",
      "Epoch:  973  Training Loss:  0.0484223  Training Accuracy:  0.96214\n",
      "Epoch:  974  Training Loss:  0.0483054  Training Accuracy:  0.96214\n",
      "Epoch:  975  Training Loss:  0.0481564  Training Accuracy:  0.962258\n",
      "Epoch:  976  Training Loss:  0.0480272  Training Accuracy:  0.962317\n",
      "Epoch:  977  Training Loss:  0.047902  Training Accuracy:  0.962317\n",
      "Epoch:  978  Training Loss:  0.0477786  Training Accuracy:  0.962434\n",
      "Epoch:  979  Training Loss:  0.0476614  Training Accuracy:  0.962493\n",
      "Epoch:  980  Training Loss:  0.0475516  Training Accuracy:  0.962787\n",
      "Epoch:  981  Training Loss:  0.047396  Training Accuracy:  0.962846\n",
      "Epoch:  982  Training Loss:  0.0472948  Training Accuracy:  0.962846\n",
      "Epoch:  983  Training Loss:  0.0471668  Training Accuracy:  0.962964\n",
      "Epoch:  984  Training Loss:  0.047058  Training Accuracy:  0.963022\n",
      "Epoch:  985  Training Loss:  0.0468955  Training Accuracy:  0.96314\n",
      "Epoch:  986  Training Loss:  0.0468117  Training Accuracy:  0.96314\n",
      "Epoch:  987  Training Loss:  0.0466898  Training Accuracy:  0.96314\n",
      "Epoch:  988  Training Loss:  0.046588  Training Accuracy:  0.963316\n",
      "Epoch:  989  Training Loss:  0.0464525  Training Accuracy:  0.963493\n",
      "Epoch:  990  Training Loss:  0.0463475  Training Accuracy:  0.963493\n",
      "Epoch:  991  Training Loss:  0.0462321  Training Accuracy:  0.963551\n",
      "Epoch:  992  Training Loss:  0.0461387  Training Accuracy:  0.963551\n",
      "Epoch:  993  Training Loss:  0.0460308  Training Accuracy:  0.96361\n",
      "Epoch:  994  Training Loss:  0.0459059  Training Accuracy:  0.963845\n",
      "Epoch:  995  Training Loss:  0.0457868  Training Accuracy:  0.963904\n",
      "Epoch:  996  Training Loss:  0.0457019  Training Accuracy:  0.963904\n",
      "Epoch:  997  Training Loss:  0.0456006  Training Accuracy:  0.964022\n",
      "Epoch:  998  Training Loss:  0.0454695  Training Accuracy:  0.964081\n",
      "Epoch:  999  Training Loss:  0.0453813  Training Accuracy:  0.964198\n",
      "Testing Accuracy: 0.860273\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  132.002  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  128.139  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  124.723  Training Accuracy:  0.042328\n",
      "Epoch:  3  Training Loss:  113.589  Training Accuracy:  0.0490888\n",
      "Epoch:  4  Training Loss:  104.427  Training Accuracy:  0.0556731\n",
      "Epoch:  5  Training Loss:  97.9281  Training Accuracy:  0.0599647\n",
      "Epoch:  6  Training Loss:  93.119  Training Accuracy:  0.0643739\n",
      "Epoch:  7  Training Loss:  89.3246  Training Accuracy:  0.067137\n",
      "Epoch:  8  Training Loss:  86.2618  Training Accuracy:  0.0712522\n",
      "Epoch:  9  Training Loss:  83.5915  Training Accuracy:  0.0743092\n",
      "Epoch:  10  Training Loss:  81.1141  Training Accuracy:  0.076602\n",
      "Epoch:  11  Training Loss:  78.6606  Training Accuracy:  0.0803057\n",
      "Epoch:  12  Training Loss:  76.076  Training Accuracy:  0.0848325\n",
      "Epoch:  13  Training Loss:  73.2805  Training Accuracy:  0.0895356\n",
      "Epoch:  14  Training Loss:  70.3549  Training Accuracy:  0.0948266\n",
      "Epoch:  15  Training Loss:  67.3519  Training Accuracy:  0.101293\n",
      "Epoch:  16  Training Loss:  64.3549  Training Accuracy:  0.108936\n",
      "Epoch:  17  Training Loss:  61.4116  Training Accuracy:  0.117519\n",
      "Epoch:  18  Training Loss:  58.5957  Training Accuracy:  0.126279\n",
      "Epoch:  19  Training Loss:  55.957  Training Accuracy:  0.134568\n",
      "Epoch:  20  Training Loss:  53.5047  Training Accuracy:  0.142328\n",
      "Epoch:  21  Training Loss:  51.212  Training Accuracy:  0.150088\n",
      "Epoch:  22  Training Loss:  49.0755  Training Accuracy:  0.158377\n",
      "Epoch:  23  Training Loss:  47.0921  Training Accuracy:  0.167431\n",
      "Epoch:  24  Training Loss:  45.2795  Training Accuracy:  0.17572\n",
      "Epoch:  25  Training Loss:  43.5421  Training Accuracy:  0.18301\n",
      "Epoch:  26  Training Loss:  41.9121  Training Accuracy:  0.190417\n",
      "Epoch:  27  Training Loss:  40.3056  Training Accuracy:  0.197354\n",
      "Epoch:  28  Training Loss:  38.7862  Training Accuracy:  0.205467\n",
      "Epoch:  29  Training Loss:  37.2929  Training Accuracy:  0.211875\n",
      "Epoch:  30  Training Loss:  35.8463  Training Accuracy:  0.218577\n",
      "Epoch:  31  Training Loss:  34.4472  Training Accuracy:  0.225691\n",
      "Epoch:  32  Training Loss:  33.1026  Training Accuracy:  0.232569\n",
      "Epoch:  33  Training Loss:  31.7416  Training Accuracy:  0.23833\n",
      "Epoch:  34  Training Loss:  30.4189  Training Accuracy:  0.245385\n",
      "Epoch:  35  Training Loss:  29.16  Training Accuracy:  0.250617\n",
      "Epoch:  36  Training Loss:  27.9005  Training Accuracy:  0.256143\n",
      "Epoch:  37  Training Loss:  26.6332  Training Accuracy:  0.262493\n",
      "Epoch:  38  Training Loss:  25.37  Training Accuracy:  0.268078\n",
      "Epoch:  39  Training Loss:  24.131  Training Accuracy:  0.273898\n",
      "Epoch:  40  Training Loss:  22.9273  Training Accuracy:  0.2806\n",
      "Epoch:  41  Training Loss:  21.7289  Training Accuracy:  0.286478\n",
      "Epoch:  42  Training Loss:  20.6839  Training Accuracy:  0.291946\n",
      "Epoch:  43  Training Loss:  19.6753  Training Accuracy:  0.297884\n",
      "Epoch:  44  Training Loss:  18.7201  Training Accuracy:  0.302998\n",
      "Epoch:  45  Training Loss:  17.7984  Training Accuracy:  0.307525\n",
      "Epoch:  46  Training Loss:  16.9015  Training Accuracy:  0.31264\n",
      "Epoch:  47  Training Loss:  16.0676  Training Accuracy:  0.317225\n",
      "Epoch:  48  Training Loss:  15.2595  Training Accuracy:  0.321575\n",
      "Epoch:  49  Training Loss:  14.4808  Training Accuracy:  0.327219\n",
      "Epoch:  50  Training Loss:  13.6793  Training Accuracy:  0.331099\n",
      "Epoch:  51  Training Loss:  12.9741  Training Accuracy:  0.335626\n",
      "Epoch:  52  Training Loss:  12.3676  Training Accuracy:  0.339506\n",
      "Epoch:  53  Training Loss:  11.7596  Training Accuracy:  0.343798\n",
      "Epoch:  54  Training Loss:  11.1722  Training Accuracy:  0.347972\n",
      "Epoch:  55  Training Loss:  10.6563  Training Accuracy:  0.352087\n",
      "Epoch:  56  Training Loss:  10.1307  Training Accuracy:  0.356085\n",
      "Epoch:  57  Training Loss:  9.61487  Training Accuracy:  0.360082\n",
      "Epoch:  58  Training Loss:  9.14715  Training Accuracy:  0.363845\n",
      "Epoch:  59  Training Loss:  8.67965  Training Accuracy:  0.36796\n",
      "Epoch:  60  Training Loss:  8.22166  Training Accuracy:  0.371546\n",
      "Epoch:  61  Training Loss:  7.80582  Training Accuracy:  0.376484\n",
      "Epoch:  62  Training Loss:  7.46344  Training Accuracy:  0.381305\n",
      "Epoch:  63  Training Loss:  7.14655  Training Accuracy:  0.38542\n",
      "Epoch:  64  Training Loss:  6.85417  Training Accuracy:  0.38883\n",
      "Epoch:  65  Training Loss:  6.61404  Training Accuracy:  0.392122\n",
      "Epoch:  66  Training Loss:  6.3417  Training Accuracy:  0.396473\n",
      "Epoch:  67  Training Loss:  6.07969  Training Accuracy:  0.398883\n",
      "Epoch:  68  Training Loss:  5.87291  Training Accuracy:  0.402881\n",
      "Epoch:  69  Training Loss:  5.66143  Training Accuracy:  0.40729\n",
      "Epoch:  70  Training Loss:  5.44124  Training Accuracy:  0.410641\n",
      "Epoch:  71  Training Loss:  5.26708  Training Accuracy:  0.413992\n",
      "Epoch:  72  Training Loss:  5.06774  Training Accuracy:  0.417107\n",
      "Epoch:  73  Training Loss:  4.91358  Training Accuracy:  0.420576\n",
      "Epoch:  74  Training Loss:  4.73617  Training Accuracy:  0.423633\n",
      "Epoch:  75  Training Loss:  4.55738  Training Accuracy:  0.426514\n",
      "Epoch:  76  Training Loss:  4.38856  Training Accuracy:  0.429218\n",
      "Epoch:  77  Training Loss:  4.22162  Training Accuracy:  0.431863\n",
      "Epoch:  78  Training Loss:  4.09105  Training Accuracy:  0.43445\n",
      "Epoch:  79  Training Loss:  3.95891  Training Accuracy:  0.436802\n",
      "Epoch:  80  Training Loss:  3.8231  Training Accuracy:  0.440623\n",
      "Epoch:  81  Training Loss:  3.70581  Training Accuracy:  0.442681\n",
      "Epoch:  82  Training Loss:  3.59928  Training Accuracy:  0.445796\n",
      "Epoch:  83  Training Loss:  3.43567  Training Accuracy:  0.448089\n",
      "Epoch:  84  Training Loss:  3.32572  Training Accuracy:  0.450852\n",
      "Epoch:  85  Training Loss:  3.25376  Training Accuracy:  0.454086\n",
      "Epoch:  86  Training Loss:  3.10878  Training Accuracy:  0.455379\n",
      "Epoch:  87  Training Loss:  3.01497  Training Accuracy:  0.457378\n",
      "Epoch:  88  Training Loss:  2.93522  Training Accuracy:  0.459377\n",
      "Epoch:  89  Training Loss:  2.83452  Training Accuracy:  0.461728\n",
      "Epoch:  90  Training Loss:  2.76661  Training Accuracy:  0.463962\n",
      "Epoch:  91  Training Loss:  2.69522  Training Accuracy:  0.466255\n",
      "Epoch:  92  Training Loss:  2.60717  Training Accuracy:  0.467842\n",
      "Epoch:  93  Training Loss:  2.53146  Training Accuracy:  0.469312\n",
      "Epoch:  94  Training Loss:  2.46728  Training Accuracy:  0.47137\n",
      "Epoch:  95  Training Loss:  2.38988  Training Accuracy:  0.473956\n",
      "Epoch:  96  Training Loss:  2.33484  Training Accuracy:  0.475132\n",
      "Epoch:  97  Training Loss:  2.26792  Training Accuracy:  0.477249\n",
      "Epoch:  98  Training Loss:  2.19856  Training Accuracy:  0.478424\n",
      "Epoch:  99  Training Loss:  2.1533  Training Accuracy:  0.479894\n",
      "Testing Accuracy: 0.441093\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 100\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  151.741  Training Accuracy:  0.0401529\n",
      "Epoch:  1  Training Loss:  152.047  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  145.162  Training Accuracy:  0.040682\n",
      "Epoch:  3  Training Loss:  133.363  Training Accuracy:  0.0447972\n",
      "Epoch:  4  Training Loss:  123.018  Training Accuracy:  0.0473251\n",
      "Epoch:  5  Training Loss:  114.525  Training Accuracy:  0.0508524\n",
      "Epoch:  6  Training Loss:  107.671  Training Accuracy:  0.0553792\n",
      "Epoch:  7  Training Loss:  101.787  Training Accuracy:  0.0607878\n",
      "Epoch:  8  Training Loss:  96.6536  Training Accuracy:  0.06602\n",
      "Epoch:  9  Training Loss:  92.0948  Training Accuracy:  0.0726043\n",
      "Epoch:  10  Training Loss:  87.8935  Training Accuracy:  0.0781893\n",
      "Epoch:  11  Training Loss:  83.8237  Training Accuracy:  0.0845973\n",
      "Epoch:  12  Training Loss:  79.6005  Training Accuracy:  0.0914756\n",
      "Epoch:  13  Training Loss:  75.2286  Training Accuracy:  0.0986478\n",
      "Epoch:  14  Training Loss:  70.7869  Training Accuracy:  0.106643\n",
      "Epoch:  15  Training Loss:  66.2823  Training Accuracy:  0.115403\n",
      "Epoch:  16  Training Loss:  61.8033  Training Accuracy:  0.122457\n",
      "Epoch:  17  Training Loss:  57.3271  Training Accuracy:  0.129688\n",
      "Epoch:  18  Training Loss:  53.0714  Training Accuracy:  0.135979\n",
      "Epoch:  19  Training Loss:  48.9388  Training Accuracy:  0.144738\n",
      "Epoch:  20  Training Loss:  45.0303  Training Accuracy:  0.151675\n",
      "Epoch:  21  Training Loss:  41.302  Training Accuracy:  0.158671\n",
      "Epoch:  22  Training Loss:  37.8326  Training Accuracy:  0.164785\n",
      "Epoch:  23  Training Loss:  34.6024  Training Accuracy:  0.170664\n",
      "Epoch:  24  Training Loss:  31.5994  Training Accuracy:  0.176896\n",
      "Epoch:  25  Training Loss:  28.8947  Training Accuracy:  0.183128\n",
      "Epoch:  26  Training Loss:  26.3982  Training Accuracy:  0.190594\n",
      "Epoch:  27  Training Loss:  24.1942  Training Accuracy:  0.19759\n",
      "Epoch:  28  Training Loss:  22.1712  Training Accuracy:  0.205115\n",
      "Epoch:  29  Training Loss:  20.3515  Training Accuracy:  0.211287\n",
      "Epoch:  30  Training Loss:  18.7344  Training Accuracy:  0.219577\n",
      "Epoch:  31  Training Loss:  17.2419  Training Accuracy:  0.228395\n",
      "Epoch:  32  Training Loss:  15.8929  Training Accuracy:  0.236096\n",
      "Epoch:  33  Training Loss:  14.6527  Training Accuracy:  0.243798\n",
      "Epoch:  34  Training Loss:  13.5524  Training Accuracy:  0.251029\n",
      "Epoch:  35  Training Loss:  12.5199  Training Accuracy:  0.258083\n",
      "Epoch:  36  Training Loss:  11.6053  Training Accuracy:  0.266138\n",
      "Epoch:  37  Training Loss:  10.7393  Training Accuracy:  0.274074\n",
      "Epoch:  38  Training Loss:  9.99703  Training Accuracy:  0.281011\n",
      "Epoch:  39  Training Loss:  9.33923  Training Accuracy:  0.287537\n",
      "Epoch:  40  Training Loss:  8.70876  Training Accuracy:  0.293298\n",
      "Epoch:  41  Training Loss:  8.15598  Training Accuracy:  0.298589\n",
      "Epoch:  42  Training Loss:  7.66305  Training Accuracy:  0.30435\n",
      "Epoch:  43  Training Loss:  7.17732  Training Accuracy:  0.310993\n",
      "Epoch:  44  Training Loss:  6.74117  Training Accuracy:  0.316226\n",
      "Epoch:  45  Training Loss:  6.37637  Training Accuracy:  0.322046\n",
      "Epoch:  46  Training Loss:  6.01331  Training Accuracy:  0.328395\n",
      "Epoch:  47  Training Loss:  5.70531  Training Accuracy:  0.332863\n",
      "Epoch:  48  Training Loss:  5.38806  Training Accuracy:  0.338036\n",
      "Epoch:  49  Training Loss:  5.11336  Training Accuracy:  0.342798\n",
      "Epoch:  50  Training Loss:  4.87109  Training Accuracy:  0.348207\n",
      "Epoch:  51  Training Loss:  4.62788  Training Accuracy:  0.352851\n",
      "Epoch:  52  Training Loss:  4.41195  Training Accuracy:  0.35679\n",
      "Epoch:  53  Training Loss:  4.20748  Training Accuracy:  0.360435\n",
      "Epoch:  54  Training Loss:  4.03274  Training Accuracy:  0.364197\n",
      "Epoch:  55  Training Loss:  3.85901  Training Accuracy:  0.368313\n",
      "Epoch:  56  Training Loss:  3.69404  Training Accuracy:  0.372016\n",
      "Epoch:  57  Training Loss:  3.5372  Training Accuracy:  0.376132\n",
      "Epoch:  58  Training Loss:  3.3766  Training Accuracy:  0.380012\n",
      "Epoch:  59  Training Loss:  3.24403  Training Accuracy:  0.382951\n",
      "Epoch:  60  Training Loss:  3.09773  Training Accuracy:  0.386655\n",
      "Epoch:  61  Training Loss:  2.99183  Training Accuracy:  0.389359\n",
      "Epoch:  62  Training Loss:  2.8726  Training Accuracy:  0.391828\n",
      "Epoch:  63  Training Loss:  2.77465  Training Accuracy:  0.394239\n",
      "Epoch:  64  Training Loss:  2.68013  Training Accuracy:  0.397002\n",
      "Epoch:  65  Training Loss:  2.5801  Training Accuracy:  0.400705\n",
      "Epoch:  66  Training Loss:  2.49583  Training Accuracy:  0.404585\n",
      "Epoch:  67  Training Loss:  2.40612  Training Accuracy:  0.407995\n",
      "Epoch:  68  Training Loss:  2.31264  Training Accuracy:  0.411523\n",
      "Epoch:  69  Training Loss:  2.24536  Training Accuracy:  0.414638\n",
      "Epoch:  70  Training Loss:  2.15782  Training Accuracy:  0.417166\n",
      "Epoch:  71  Training Loss:  2.09636  Training Accuracy:  0.420929\n",
      "Epoch:  72  Training Loss:  2.03853  Training Accuracy:  0.423574\n",
      "Epoch:  73  Training Loss:  1.97218  Training Accuracy:  0.426455\n",
      "Epoch:  74  Training Loss:  1.92724  Training Accuracy:  0.428983\n",
      "Epoch:  75  Training Loss:  1.85786  Training Accuracy:  0.431863\n",
      "Epoch:  76  Training Loss:  1.81338  Training Accuracy:  0.434568\n",
      "Epoch:  77  Training Loss:  1.7631  Training Accuracy:  0.436919\n",
      "Epoch:  78  Training Loss:  1.73593  Training Accuracy:  0.440388\n",
      "Epoch:  79  Training Loss:  1.68738  Training Accuracy:  0.443445\n",
      "Epoch:  80  Training Loss:  1.63498  Training Accuracy:  0.446678\n",
      "Epoch:  81  Training Loss:  1.59495  Training Accuracy:  0.449441\n",
      "Epoch:  82  Training Loss:  1.55683  Training Accuracy:  0.450735\n",
      "Epoch:  83  Training Loss:  1.52023  Training Accuracy:  0.453145\n",
      "Epoch:  84  Training Loss:  1.4978  Training Accuracy:  0.454615\n",
      "Epoch:  85  Training Loss:  1.47588  Training Accuracy:  0.456496\n",
      "Epoch:  86  Training Loss:  1.46845  Training Accuracy:  0.458495\n",
      "Epoch:  87  Training Loss:  1.4296  Training Accuracy:  0.460553\n",
      "Epoch:  88  Training Loss:  1.4401  Training Accuracy:  0.462375\n",
      "Epoch:  89  Training Loss:  1.39372  Training Accuracy:  0.46455\n",
      "Epoch:  90  Training Loss:  1.41299  Training Accuracy:  0.466667\n",
      "Epoch:  91  Training Loss:  1.34783  Training Accuracy:  0.469018\n",
      "Epoch:  92  Training Loss:  1.32331  Training Accuracy:  0.471076\n",
      "Epoch:  93  Training Loss:  1.31944  Training Accuracy:  0.473251\n",
      "Epoch:  94  Training Loss:  1.27336  Training Accuracy:  0.475661\n",
      "Epoch:  95  Training Loss:  1.24751  Training Accuracy:  0.478836\n",
      "Epoch:  96  Training Loss:  1.22904  Training Accuracy:  0.4806\n",
      "Epoch:  97  Training Loss:  1.20148  Training Accuracy:  0.482892\n",
      "Epoch:  98  Training Loss:  1.17498  Training Accuracy:  0.485185\n",
      "Epoch:  99  Training Loss:  1.15438  Training Accuracy:  0.486772\n",
      "Epoch:  100  Training Loss:  1.13212  Training Accuracy:  0.489124\n",
      "Epoch:  101  Training Loss:  1.11283  Training Accuracy:  0.492005\n",
      "Epoch:  102  Training Loss:  1.09684  Training Accuracy:  0.493886\n",
      "Epoch:  103  Training Loss:  1.07057  Training Accuracy:  0.49612\n",
      "Epoch:  104  Training Loss:  1.06287  Training Accuracy:  0.497707\n",
      "Epoch:  105  Training Loss:  1.03849  Training Accuracy:  0.500059\n",
      "Epoch:  106  Training Loss:  1.03061  Training Accuracy:  0.502293\n",
      "Epoch:  107  Training Loss:  1.01809  Training Accuracy:  0.504174\n",
      "Epoch:  108  Training Loss:  1.01014  Training Accuracy:  0.506349\n",
      "Epoch:  109  Training Loss:  0.985575  Training Accuracy:  0.508407\n",
      "Epoch:  110  Training Loss:  0.98599  Training Accuracy:  0.510229\n",
      "Epoch:  111  Training Loss:  0.957716  Training Accuracy:  0.512522\n",
      "Epoch:  112  Training Loss:  0.952563  Training Accuracy:  0.514344\n",
      "Epoch:  113  Training Loss:  0.934139  Training Accuracy:  0.515167\n",
      "Epoch:  114  Training Loss:  0.92484  Training Accuracy:  0.516872\n",
      "Epoch:  115  Training Loss:  0.91687  Training Accuracy:  0.518107\n",
      "Epoch:  116  Training Loss:  0.906117  Training Accuracy:  0.519283\n",
      "Epoch:  117  Training Loss:  0.90497  Training Accuracy:  0.520047\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  139.178  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  131.4  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  126.248  Training Accuracy:  0.0409759\n",
      "Epoch:  3  Training Loss:  112.831  Training Accuracy:  0.045679\n",
      "Epoch:  4  Training Loss:  102.25  Training Accuracy:  0.0492651\n",
      "Epoch:  5  Training Loss:  94.4986  Training Accuracy:  0.0559083\n",
      "Epoch:  6  Training Loss:  89.0949  Training Accuracy:  0.061552\n",
      "Epoch:  7  Training Loss:  85.0535  Training Accuracy:  0.0677249\n",
      "Epoch:  8  Training Loss:  81.7987  Training Accuracy:  0.0714286\n",
      "Epoch:  9  Training Loss:  79.0284  Training Accuracy:  0.0751911\n",
      "Epoch:  10  Training Loss:  76.5063  Training Accuracy:  0.080776\n",
      "Epoch:  11  Training Loss:  74.0989  Training Accuracy:  0.0864198\n",
      "Epoch:  12  Training Loss:  71.6682  Training Accuracy:  0.0944738\n",
      "Epoch:  13  Training Loss:  69.1208  Training Accuracy:  0.102058\n",
      "Epoch:  14  Training Loss:  66.5328  Training Accuracy:  0.109171\n",
      "Epoch:  15  Training Loss:  63.9081  Training Accuracy:  0.116108\n",
      "Epoch:  16  Training Loss:  61.2638  Training Accuracy:  0.122516\n",
      "Epoch:  17  Training Loss:  58.6373  Training Accuracy:  0.129688\n",
      "Epoch:  18  Training Loss:  56.1055  Training Accuracy:  0.136861\n",
      "Epoch:  19  Training Loss:  53.6911  Training Accuracy:  0.143034\n",
      "Epoch:  20  Training Loss:  51.3837  Training Accuracy:  0.149853\n",
      "Epoch:  21  Training Loss:  49.1577  Training Accuracy:  0.157496\n",
      "Epoch:  22  Training Loss:  46.9297  Training Accuracy:  0.165256\n",
      "Epoch:  23  Training Loss:  44.8208  Training Accuracy:  0.171899\n",
      "Epoch:  24  Training Loss:  42.7326  Training Accuracy:  0.179189\n",
      "Epoch:  25  Training Loss:  40.6962  Training Accuracy:  0.187889\n",
      "Epoch:  26  Training Loss:  38.7299  Training Accuracy:  0.194827\n",
      "Epoch:  27  Training Loss:  36.7883  Training Accuracy:  0.200941\n",
      "Epoch:  28  Training Loss:  34.908  Training Accuracy:  0.208172\n",
      "Epoch:  29  Training Loss:  33.0949  Training Accuracy:  0.215932\n",
      "Epoch:  30  Training Loss:  31.3082  Training Accuracy:  0.22234\n",
      "Epoch:  31  Training Loss:  29.6441  Training Accuracy:  0.229806\n",
      "Epoch:  32  Training Loss:  28.0058  Training Accuracy:  0.236684\n",
      "Epoch:  33  Training Loss:  26.3843  Training Accuracy:  0.243857\n",
      "Epoch:  34  Training Loss:  24.9015  Training Accuracy:  0.250323\n",
      "Epoch:  35  Training Loss:  23.4464  Training Accuracy:  0.256673\n",
      "Epoch:  36  Training Loss:  22.0344  Training Accuracy:  0.262669\n",
      "Epoch:  37  Training Loss:  20.7353  Training Accuracy:  0.268724\n",
      "Epoch:  38  Training Loss:  19.503  Training Accuracy:  0.274897\n",
      "Epoch:  39  Training Loss:  18.3685  Training Accuracy:  0.281893\n",
      "Epoch:  40  Training Loss:  17.2932  Training Accuracy:  0.287831\n",
      "Epoch:  41  Training Loss:  16.2471  Training Accuracy:  0.292416\n",
      "Epoch:  42  Training Loss:  15.32  Training Accuracy:  0.298177\n",
      "Epoch:  43  Training Loss:  14.4488  Training Accuracy:  0.304527\n",
      "Epoch:  44  Training Loss:  13.5953  Training Accuracy:  0.309112\n",
      "Epoch:  45  Training Loss:  12.747  Training Accuracy:  0.313521\n",
      "Epoch:  46  Training Loss:  11.9706  Training Accuracy:  0.319165\n",
      "Epoch:  47  Training Loss:  11.251  Training Accuracy:  0.323868\n",
      "Epoch:  48  Training Loss:  10.6118  Training Accuracy:  0.327925\n",
      "Epoch:  49  Training Loss:  9.9887  Training Accuracy:  0.332099\n",
      "Epoch:  50  Training Loss:  9.41552  Training Accuracy:  0.336684\n",
      "Epoch:  51  Training Loss:  8.91795  Training Accuracy:  0.34127\n",
      "Epoch:  52  Training Loss:  8.41267  Training Accuracy:  0.345679\n",
      "Epoch:  53  Training Loss:  7.98564  Training Accuracy:  0.349618\n",
      "Epoch:  54  Training Loss:  7.55667  Training Accuracy:  0.353204\n",
      "Epoch:  55  Training Loss:  7.21478  Training Accuracy:  0.35726\n",
      "Epoch:  56  Training Loss:  6.87872  Training Accuracy:  0.359553\n",
      "Epoch:  57  Training Loss:  6.56394  Training Accuracy:  0.363316\n",
      "Epoch:  58  Training Loss:  6.29707  Training Accuracy:  0.366725\n",
      "Epoch:  59  Training Loss:  6.03586  Training Accuracy:  0.369782\n",
      "Epoch:  60  Training Loss:  5.80913  Training Accuracy:  0.372957\n",
      "Epoch:  61  Training Loss:  5.57891  Training Accuracy:  0.376367\n",
      "Epoch:  62  Training Loss:  5.377  Training Accuracy:  0.380482\n",
      "Epoch:  63  Training Loss:  5.18093  Training Accuracy:  0.384186\n",
      "Epoch:  64  Training Loss:  5.01376  Training Accuracy:  0.388948\n",
      "Epoch:  65  Training Loss:  4.81779  Training Accuracy:  0.391299\n",
      "Epoch:  66  Training Loss:  4.71358  Training Accuracy:  0.395591\n",
      "Epoch:  67  Training Loss:  4.53976  Training Accuracy:  0.39706\n",
      "Epoch:  68  Training Loss:  4.415  Training Accuracy:  0.401352\n",
      "Epoch:  69  Training Loss:  4.31323  Training Accuracy:  0.40388\n",
      "Epoch:  70  Training Loss:  4.22273  Training Accuracy:  0.407525\n",
      "Epoch:  71  Training Loss:  4.09046  Training Accuracy:  0.410347\n",
      "Epoch:  72  Training Loss:  3.99125  Training Accuracy:  0.412816\n",
      "Epoch:  73  Training Loss:  3.92067  Training Accuracy:  0.415814\n",
      "Epoch:  74  Training Loss:  3.88007  Training Accuracy:  0.418342\n",
      "Epoch:  75  Training Loss:  3.80708  Training Accuracy:  0.420576\n",
      "Epoch:  76  Training Loss:  3.73778  Training Accuracy:  0.423457\n",
      "Epoch:  77  Training Loss:  3.62683  Training Accuracy:  0.425514\n",
      "Epoch:  78  Training Loss:  3.54991  Training Accuracy:  0.428219\n",
      "Epoch:  79  Training Loss:  3.51921  Training Accuracy:  0.430511\n",
      "Epoch:  80  Training Loss:  3.424  Training Accuracy:  0.433039\n",
      "Epoch:  81  Training Loss:  3.35562  Training Accuracy:  0.435802\n",
      "Epoch:  82  Training Loss:  3.2792  Training Accuracy:  0.438036\n",
      "Epoch:  83  Training Loss:  3.23936  Training Accuracy:  0.440564\n",
      "Epoch:  84  Training Loss:  3.19647  Training Accuracy:  0.443621\n",
      "Epoch:  85  Training Loss:  3.14032  Training Accuracy:  0.44609\n",
      "Epoch:  86  Training Loss:  3.10628  Training Accuracy:  0.448383\n",
      "Epoch:  87  Training Loss:  3.03865  Training Accuracy:  0.450852\n",
      "Epoch:  88  Training Loss:  3.01559  Training Accuracy:  0.453263\n",
      "Epoch:  89  Training Loss:  2.95222  Training Accuracy:  0.455849\n",
      "Epoch:  90  Training Loss:  2.90988  Training Accuracy:  0.45726\n",
      "Epoch:  91  Training Loss:  2.85822  Training Accuracy:  0.459259\n",
      "Epoch:  92  Training Loss:  2.82198  Training Accuracy:  0.461846\n",
      "Epoch:  93  Training Loss:  2.76677  Training Accuracy:  0.464491\n",
      "Epoch:  94  Training Loss:  2.73567  Training Accuracy:  0.466314\n",
      "Epoch:  95  Training Loss:  2.67147  Training Accuracy:  0.468195\n",
      "Epoch:  96  Training Loss:  2.65079  Training Accuracy:  0.470194\n",
      "Epoch:  97  Training Loss:  2.57549  Training Accuracy:  0.472134\n",
      "Epoch:  98  Training Loss:  2.54019  Training Accuracy:  0.474838\n",
      "Epoch:  99  Training Loss:  2.50402  Training Accuracy:  0.476367\n",
      "Epoch:  100  Training Loss:  2.46456  Training Accuracy:  0.478601\n",
      "Epoch:  101  Training Loss:  2.39987  Training Accuracy:  0.479659\n",
      "Epoch:  102  Training Loss:  2.3516  Training Accuracy:  0.481305\n",
      "Epoch:  103  Training Loss:  2.26927  Training Accuracy:  0.483245\n",
      "Epoch:  104  Training Loss:  2.23196  Training Accuracy:  0.484597\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 400\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  151.081  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  134.595  Training Accuracy:  0.0410935\n",
      "Epoch:  2  Training Loss:  125.717  Training Accuracy:  0.0455026\n",
      "Epoch:  3  Training Loss:  112.681  Training Accuracy:  0.0514991\n",
      "Epoch:  4  Training Loss:  101.946  Training Accuracy:  0.0566725\n",
      "Epoch:  5  Training Loss:  94.3619  Training Accuracy:  0.060435\n",
      "Epoch:  6  Training Loss:  88.8311  Training Accuracy:  0.0656672\n",
      "Epoch:  7  Training Loss:  84.7768  Training Accuracy:  0.0701352\n",
      "Epoch:  8  Training Loss:  81.6118  Training Accuracy:  0.0759553\n",
      "Epoch:  9  Training Loss:  78.9645  Training Accuracy:  0.0801293\n",
      "Epoch:  10  Training Loss:  76.4036  Training Accuracy:  0.0843033\n",
      "Epoch:  11  Training Loss:  73.7895  Training Accuracy:  0.0891828\n",
      "Epoch:  12  Training Loss:  70.9536  Training Accuracy:  0.0937684\n",
      "Epoch:  13  Training Loss:  67.83  Training Accuracy:  0.0985891\n",
      "Epoch:  14  Training Loss:  64.5351  Training Accuracy:  0.105703\n",
      "Epoch:  15  Training Loss:  61.1119  Training Accuracy:  0.111875\n",
      "Epoch:  16  Training Loss:  57.6731  Training Accuracy:  0.118989\n",
      "Epoch:  17  Training Loss:  54.2625  Training Accuracy:  0.126279\n",
      "Epoch:  18  Training Loss:  50.99  Training Accuracy:  0.134568\n",
      "Epoch:  19  Training Loss:  47.9116  Training Accuracy:  0.142504\n",
      "Epoch:  20  Training Loss:  45.0487  Training Accuracy:  0.152028\n",
      "Epoch:  21  Training Loss:  42.3378  Training Accuracy:  0.160259\n",
      "Epoch:  22  Training Loss:  39.8428  Training Accuracy:  0.169665\n",
      "Epoch:  23  Training Loss:  37.493  Training Accuracy:  0.178777\n",
      "Epoch:  24  Training Loss:  35.2799  Training Accuracy:  0.18689\n",
      "Epoch:  25  Training Loss:  33.1698  Training Accuracy:  0.196708\n",
      "Epoch:  26  Training Loss:  31.1781  Training Accuracy:  0.206878\n",
      "Epoch:  27  Training Loss:  29.2923  Training Accuracy:  0.215344\n",
      "Epoch:  28  Training Loss:  27.5584  Training Accuracy:  0.223222\n",
      "Epoch:  29  Training Loss:  25.9498  Training Accuracy:  0.231805\n",
      "Epoch:  30  Training Loss:  24.4213  Training Accuracy:  0.238272\n",
      "Epoch:  31  Training Loss:  22.9547  Training Accuracy:  0.245561\n",
      "Epoch:  32  Training Loss:  21.626  Training Accuracy:  0.251734\n",
      "Epoch:  33  Training Loss:  20.3905  Training Accuracy:  0.257966\n",
      "Epoch:  34  Training Loss:  19.2209  Training Accuracy:  0.263786\n",
      "Epoch:  35  Training Loss:  18.1147  Training Accuracy:  0.269312\n",
      "Epoch:  36  Training Loss:  17.0844  Training Accuracy:  0.276073\n",
      "Epoch:  37  Training Loss:  16.1024  Training Accuracy:  0.281893\n",
      "Epoch:  38  Training Loss:  15.1142  Training Accuracy:  0.287595\n",
      "Epoch:  39  Training Loss:  14.1993  Training Accuracy:  0.293357\n",
      "Epoch:  40  Training Loss:  13.3026  Training Accuracy:  0.298413\n",
      "Epoch:  41  Training Loss:  12.5273  Training Accuracy:  0.304821\n",
      "Epoch:  42  Training Loss:  11.7474  Training Accuracy:  0.310406\n",
      "Epoch:  43  Training Loss:  11.0257  Training Accuracy:  0.315873\n",
      "Epoch:  44  Training Loss:  10.2934  Training Accuracy:  0.321575\n",
      "Epoch:  45  Training Loss:  9.6376  Training Accuracy:  0.32669\n",
      "Epoch:  46  Training Loss:  9.03464  Training Accuracy:  0.331158\n",
      "Epoch:  47  Training Loss:  8.47468  Training Accuracy:  0.336508\n",
      "Epoch:  48  Training Loss:  7.92358  Training Accuracy:  0.341799\n",
      "Epoch:  49  Training Loss:  7.43678  Training Accuracy:  0.346678\n",
      "Epoch:  50  Training Loss:  6.9799  Training Accuracy:  0.351323\n",
      "Epoch:  51  Training Loss:  6.58725  Training Accuracy:  0.356555\n",
      "Epoch:  52  Training Loss:  6.19611  Training Accuracy:  0.361258\n",
      "Epoch:  53  Training Loss:  5.86329  Training Accuracy:  0.36696\n",
      "Epoch:  54  Training Loss:  5.54215  Training Accuracy:  0.371605\n",
      "Epoch:  55  Training Loss:  5.27542  Training Accuracy:  0.37619\n",
      "Epoch:  56  Training Loss:  5.03425  Training Accuracy:  0.381129\n",
      "Epoch:  57  Training Loss:  4.80204  Training Accuracy:  0.38495\n",
      "Epoch:  58  Training Loss:  4.61644  Training Accuracy:  0.389242\n",
      "Epoch:  59  Training Loss:  4.4643  Training Accuracy:  0.393533\n",
      "Epoch:  60  Training Loss:  4.25804  Training Accuracy:  0.397942\n",
      "Epoch:  61  Training Loss:  4.12332  Training Accuracy:  0.402645\n",
      "Epoch:  62  Training Loss:  3.9304  Training Accuracy:  0.406761\n",
      "Epoch:  63  Training Loss:  3.7902  Training Accuracy:  0.409582\n",
      "Epoch:  64  Training Loss:  3.64089  Training Accuracy:  0.412992\n",
      "Epoch:  65  Training Loss:  3.54376  Training Accuracy:  0.416755\n",
      "Epoch:  66  Training Loss:  3.39275  Training Accuracy:  0.419694\n",
      "Epoch:  67  Training Loss:  3.27071  Training Accuracy:  0.422986\n",
      "Epoch:  68  Training Loss:  3.14087  Training Accuracy:  0.42622\n",
      "Epoch:  69  Training Loss:  3.02703  Training Accuracy:  0.429923\n",
      "Epoch:  70  Training Loss:  2.90631  Training Accuracy:  0.433157\n",
      "Epoch:  71  Training Loss:  2.80955  Training Accuracy:  0.436508\n",
      "Epoch:  72  Training Loss:  2.70981  Training Accuracy:  0.438683\n",
      "Epoch:  73  Training Loss:  2.62504  Training Accuracy:  0.441916\n",
      "Epoch:  74  Training Loss:  2.52762  Training Accuracy:  0.444621\n",
      "Epoch:  75  Training Loss:  2.44757  Training Accuracy:  0.448383\n",
      "Epoch:  76  Training Loss:  2.36015  Training Accuracy:  0.451381\n",
      "Epoch:  77  Training Loss:  2.29294  Training Accuracy:  0.453968\n",
      "Epoch:  78  Training Loss:  2.2177  Training Accuracy:  0.457143\n",
      "Epoch:  79  Training Loss:  2.1741  Training Accuracy:  0.459494\n",
      "Epoch:  80  Training Loss:  2.12109  Training Accuracy:  0.461905\n",
      "Epoch:  81  Training Loss:  2.06829  Training Accuracy:  0.464139\n",
      "Epoch:  82  Training Loss:  2.00487  Training Accuracy:  0.467196\n",
      "Epoch:  83  Training Loss:  1.96041  Training Accuracy:  0.469665\n",
      "Epoch:  84  Training Loss:  1.90182  Training Accuracy:  0.472251\n",
      "Epoch:  85  Training Loss:  1.87558  Training Accuracy:  0.475191\n",
      "Epoch:  86  Training Loss:  1.82019  Training Accuracy:  0.477425\n",
      "Epoch:  87  Training Loss:  1.77528  Training Accuracy:  0.4796\n",
      "Epoch:  88  Training Loss:  1.72897  Training Accuracy:  0.48201\n",
      "Epoch:  89  Training Loss:  1.69126  Training Accuracy:  0.48348\n",
      "Epoch:  90  Training Loss:  1.64463  Training Accuracy:  0.486126\n",
      "Epoch:  91  Training Loss:  1.59626  Training Accuracy:  0.488301\n",
      "Epoch:  92  Training Loss:  1.55785  Training Accuracy:  0.489829\n",
      "Epoch:  93  Training Loss:  1.5116  Training Accuracy:  0.491711\n",
      "Epoch:  94  Training Loss:  1.47595  Training Accuracy:  0.493886\n",
      "Epoch:  95  Training Loss:  1.43931  Training Accuracy:  0.496002\n",
      "Epoch:  96  Training Loss:  1.41115  Training Accuracy:  0.497942\n",
      "Epoch:  97  Training Loss:  1.36923  Training Accuracy:  0.499941\n",
      "Epoch:  98  Training Loss:  1.34202  Training Accuracy:  0.501646\n",
      "Epoch:  99  Training Loss:  1.30442  Training Accuracy:  0.503762\n",
      "Epoch:  100  Training Loss:  1.27403  Training Accuracy:  0.506525\n",
      "Epoch:  101  Training Loss:  1.23427  Training Accuracy:  0.508936\n",
      "Epoch:  102  Training Loss:  1.20929  Training Accuracy:  0.510112\n",
      "Epoch:  103  Training Loss:  1.18207  Training Accuracy:  0.511758\n",
      "Epoch:  104  Training Loss:  1.15485  Training Accuracy:  0.513757\n",
      "Epoch:  105  Training Loss:  1.12398  Training Accuracy:  0.517049\n",
      "Epoch:  106  Training Loss:  1.11178  Training Accuracy:  0.518754\n",
      "Epoch:  107  Training Loss:  1.09352  Training Accuracy:  0.520635\n",
      "Epoch:  108  Training Loss:  1.08156  Training Accuracy:  0.521752\n",
      "Epoch:  109  Training Loss:  1.05786  Training Accuracy:  0.523692\n",
      "Epoch:  110  Training Loss:  1.03472  Training Accuracy:  0.525397\n",
      "Epoch:  111  Training Loss:  1.01648  Training Accuracy:  0.526925\n",
      "Epoch:  112  Training Loss:  0.991126  Training Accuracy:  0.527983\n",
      "Epoch:  113  Training Loss:  0.980508  Training Accuracy:  0.52863\n",
      "Epoch:  114  Training Loss:  0.954016  Training Accuracy:  0.529865\n",
      "Epoch:  115  Training Loss:  0.933015  Training Accuracy:  0.53157\n",
      "Epoch:  116  Training Loss:  0.937825  Training Accuracy:  0.532863\n",
      "Epoch:  117  Training Loss:  0.901756  Training Accuracy:  0.534685\n",
      "Epoch:  118  Training Loss:  0.893106  Training Accuracy:  0.536331\n",
      "Epoch:  119  Training Loss:  0.886001  Training Accuracy:  0.538154\n",
      "Epoch:  120  Training Loss:  0.860331  Training Accuracy:  0.539506\n",
      "Epoch:  121  Training Loss:  0.858049  Training Accuracy:  0.540329\n",
      "Epoch:  122  Training Loss:  0.841187  Training Accuracy:  0.54221\n",
      "Epoch:  123  Training Loss:  0.832114  Training Accuracy:  0.543739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  124  Training Loss:  0.829112  Training Accuracy:  0.544621\n",
      "Epoch:  125  Training Loss:  0.815811  Training Accuracy:  0.546443\n",
      "Epoch:  126  Training Loss:  0.802988  Training Accuracy:  0.547795\n",
      "Epoch:  127  Training Loss:  0.789127  Training Accuracy:  0.54856\n",
      "Epoch:  128  Training Loss:  0.78542  Training Accuracy:  0.549559\n",
      "Epoch:  129  Training Loss:  0.774797  Training Accuracy:  0.550147\n",
      "Epoch:  130  Training Loss:  0.778799  Training Accuracy:  0.550911\n",
      "Epoch:  131  Training Loss:  0.77433  Training Accuracy:  0.551675\n",
      "Epoch:  132  Training Loss:  0.749025  Training Accuracy:  0.552969\n",
      "Epoch:  133  Training Loss:  0.745318  Training Accuracy:  0.553909\n",
      "Epoch:  134  Training Loss:  0.728149  Training Accuracy:  0.554968\n",
      "Epoch:  135  Training Loss:  0.716109  Training Accuracy:  0.556555\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  124.676  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  112.47  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  105.829  Training Accuracy:  0.0446208\n",
      "Epoch:  3  Training Loss:  97.546  Training Accuracy:  0.0507349\n",
      "Epoch:  4  Training Loss:  90.6015  Training Accuracy:  0.0559083\n",
      "Epoch:  5  Training Loss:  85.3153  Training Accuracy:  0.060435\n",
      "Epoch:  6  Training Loss:  81.4132  Training Accuracy:  0.0636684\n",
      "Epoch:  7  Training Loss:  78.4299  Training Accuracy:  0.0662551\n",
      "Epoch:  8  Training Loss:  75.9824  Training Accuracy:  0.0698413\n",
      "Epoch:  9  Training Loss:  73.8974  Training Accuracy:  0.0738389\n",
      "Epoch:  10  Training Loss:  72.1534  Training Accuracy:  0.0800705\n",
      "Epoch:  11  Training Loss:  70.5575  Training Accuracy:  0.0848912\n",
      "Epoch:  12  Training Loss:  68.9934  Training Accuracy:  0.0898295\n",
      "Epoch:  13  Training Loss:  67.3526  Training Accuracy:  0.0962375\n",
      "Epoch:  14  Training Loss:  65.6532  Training Accuracy:  0.102528\n",
      "Epoch:  15  Training Loss:  63.8777  Training Accuracy:  0.109289\n",
      "Epoch:  16  Training Loss:  62.0126  Training Accuracy:  0.11699\n",
      "Epoch:  17  Training Loss:  60.0914  Training Accuracy:  0.123457\n",
      "Epoch:  18  Training Loss:  58.1933  Training Accuracy:  0.130218\n",
      "Epoch:  19  Training Loss:  56.3113  Training Accuracy:  0.136567\n",
      "Epoch:  20  Training Loss:  54.3957  Training Accuracy:  0.144386\n",
      "Epoch:  21  Training Loss:  52.5476  Training Accuracy:  0.151852\n",
      "Epoch:  22  Training Loss:  50.7377  Training Accuracy:  0.158495\n",
      "Epoch:  23  Training Loss:  49.0142  Training Accuracy:  0.167019\n",
      "Epoch:  24  Training Loss:  47.2918  Training Accuracy:  0.174015\n",
      "Epoch:  25  Training Loss:  45.656  Training Accuracy:  0.18107\n",
      "Epoch:  26  Training Loss:  44.0032  Training Accuracy:  0.188536\n",
      "Epoch:  27  Training Loss:  42.4318  Training Accuracy:  0.196884\n",
      "Epoch:  28  Training Loss:  40.8688  Training Accuracy:  0.204586\n",
      "Epoch:  29  Training Loss:  39.3388  Training Accuracy:  0.213463\n",
      "Epoch:  30  Training Loss:  37.9333  Training Accuracy:  0.220811\n",
      "Epoch:  31  Training Loss:  36.6072  Training Accuracy:  0.227102\n",
      "Epoch:  32  Training Loss:  35.243  Training Accuracy:  0.234392\n",
      "Epoch:  33  Training Loss:  34.0154  Training Accuracy:  0.24221\n",
      "Epoch:  34  Training Loss:  32.8057  Training Accuracy:  0.248912\n",
      "Epoch:  35  Training Loss:  31.6991  Training Accuracy:  0.256673\n",
      "Epoch:  36  Training Loss:  30.5745  Training Accuracy:  0.263374\n",
      "Epoch:  37  Training Loss:  29.6674  Training Accuracy:  0.268901\n",
      "Epoch:  38  Training Loss:  28.6864  Training Accuracy:  0.275955\n",
      "Epoch:  39  Training Loss:  27.7158  Training Accuracy:  0.282069\n",
      "Epoch:  40  Training Loss:  26.7833  Training Accuracy:  0.288183\n",
      "Epoch:  41  Training Loss:  25.8194  Training Accuracy:  0.295062\n",
      "Epoch:  42  Training Loss:  24.8266  Training Accuracy:  0.300588\n",
      "Epoch:  43  Training Loss:  23.8993  Training Accuracy:  0.305409\n",
      "Epoch:  44  Training Loss:  22.9846  Training Accuracy:  0.311523\n",
      "Epoch:  45  Training Loss:  22.0242  Training Accuracy:  0.317049\n",
      "Epoch:  46  Training Loss:  21.2035  Training Accuracy:  0.322281\n",
      "Epoch:  47  Training Loss:  20.4209  Training Accuracy:  0.327278\n",
      "Epoch:  48  Training Loss:  19.6464  Training Accuracy:  0.33104\n",
      "Epoch:  49  Training Loss:  18.8628  Training Accuracy:  0.336155\n",
      "Epoch:  50  Training Loss:  18.1965  Training Accuracy:  0.341446\n",
      "Epoch:  51  Training Loss:  17.5549  Training Accuracy:  0.347149\n",
      "Epoch:  52  Training Loss:  16.9419  Training Accuracy:  0.352263\n",
      "Epoch:  53  Training Loss:  16.3243  Training Accuracy:  0.355732\n",
      "Epoch:  54  Training Loss:  15.7462  Training Accuracy:  0.360259\n",
      "Epoch:  55  Training Loss:  15.1997  Training Accuracy:  0.364962\n",
      "Epoch:  56  Training Loss:  14.6507  Training Accuracy:  0.368901\n",
      "Epoch:  57  Training Loss:  14.119  Training Accuracy:  0.373133\n",
      "Epoch:  58  Training Loss:  13.6018  Training Accuracy:  0.37619\n",
      "Epoch:  59  Training Loss:  13.0561  Training Accuracy:  0.380482\n",
      "Epoch:  60  Training Loss:  12.5544  Training Accuracy:  0.383304\n",
      "Epoch:  61  Training Loss:  12.0577  Training Accuracy:  0.386126\n",
      "Epoch:  62  Training Loss:  11.6088  Training Accuracy:  0.389535\n",
      "Epoch:  63  Training Loss:  11.1264  Training Accuracy:  0.393474\n",
      "Epoch:  64  Training Loss:  10.6872  Training Accuracy:  0.397707\n",
      "Epoch:  65  Training Loss:  10.2596  Training Accuracy:  0.401352\n",
      "Epoch:  66  Training Loss:  9.92336  Training Accuracy:  0.405938\n",
      "Epoch:  67  Training Loss:  9.59406  Training Accuracy:  0.408877\n",
      "Epoch:  68  Training Loss:  9.18986  Training Accuracy:  0.413051\n",
      "Epoch:  69  Training Loss:  8.80954  Training Accuracy:  0.416578\n",
      "Epoch:  70  Training Loss:  8.4941  Training Accuracy:  0.420106\n",
      "Epoch:  71  Training Loss:  8.16385  Training Accuracy:  0.423221\n",
      "Epoch:  72  Training Loss:  7.82998  Training Accuracy:  0.426396\n",
      "Epoch:  73  Training Loss:  7.51322  Training Accuracy:  0.429336\n",
      "Epoch:  74  Training Loss:  7.18499  Training Accuracy:  0.432569\n",
      "Epoch:  75  Training Loss:  6.85862  Training Accuracy:  0.435156\n",
      "Epoch:  76  Training Loss:  6.59268  Training Accuracy:  0.437919\n",
      "Epoch:  77  Training Loss:  6.36793  Training Accuracy:  0.4398\n",
      "Epoch:  78  Training Loss:  6.11258  Training Accuracy:  0.442563\n",
      "Epoch:  79  Training Loss:  5.9109  Training Accuracy:  0.445032\n",
      "Epoch:  80  Training Loss:  5.693  Training Accuracy:  0.448677\n",
      "Epoch:  81  Training Loss:  5.46115  Training Accuracy:  0.45144\n",
      "Epoch:  82  Training Loss:  5.2413  Training Accuracy:  0.45485\n",
      "Epoch:  83  Training Loss:  5.06006  Training Accuracy:  0.457731\n",
      "Epoch:  84  Training Loss:  4.84951  Training Accuracy:  0.460082\n",
      "Epoch:  85  Training Loss:  4.6838  Training Accuracy:  0.462434\n",
      "Epoch:  86  Training Loss:  4.51939  Training Accuracy:  0.465314\n",
      "Epoch:  87  Training Loss:  4.38363  Training Accuracy:  0.467137\n",
      "Epoch:  88  Training Loss:  4.21091  Training Accuracy:  0.46943\n",
      "Epoch:  89  Training Loss:  4.11791  Training Accuracy:  0.47184\n",
      "Epoch:  90  Training Loss:  3.9841  Training Accuracy:  0.47425\n",
      "Epoch:  91  Training Loss:  3.8908  Training Accuracy:  0.476896\n",
      "Epoch:  92  Training Loss:  3.79366  Training Accuracy:  0.478836\n",
      "Epoch:  93  Training Loss:  3.72113  Training Accuracy:  0.480188\n",
      "Epoch:  94  Training Loss:  3.63265  Training Accuracy:  0.481599\n",
      "Epoch:  95  Training Loss:  3.56339  Training Accuracy:  0.483715\n",
      "Epoch:  96  Training Loss:  3.49207  Training Accuracy:  0.485773\n",
      "Epoch:  97  Training Loss:  3.44079  Training Accuracy:  0.487654\n",
      "Epoch:  98  Training Loss:  3.39562  Training Accuracy:  0.489418\n",
      "Epoch:  99  Training Loss:  3.3277  Training Accuracy:  0.490946\n",
      "Epoch:  100  Training Loss:  3.27873  Training Accuracy:  0.493239\n",
      "Epoch:  101  Training Loss:  3.24633  Training Accuracy:  0.495179\n",
      "Epoch:  102  Training Loss:  3.21056  Training Accuracy:  0.497531\n",
      "Epoch:  103  Training Loss:  3.16467  Training Accuracy:  0.499177\n",
      "Epoch:  104  Training Loss:  3.13905  Training Accuracy:  0.501999\n",
      "Epoch:  105  Training Loss:  3.08889  Training Accuracy:  0.503468\n",
      "Epoch:  106  Training Loss:  3.04928  Training Accuracy:  0.505585\n",
      "Epoch:  107  Training Loss:  3.01024  Training Accuracy:  0.507172\n",
      "Epoch:  108  Training Loss:  2.9765  Training Accuracy:  0.508936\n",
      "Epoch:  109  Training Loss:  2.93454  Training Accuracy:  0.510641\n",
      "Epoch:  110  Training Loss:  2.89399  Training Accuracy:  0.512228\n",
      "Epoch:  111  Training Loss:  2.85374  Training Accuracy:  0.513874\n",
      "Epoch:  112  Training Loss:  2.8169  Training Accuracy:  0.515285\n",
      "Epoch:  113  Training Loss:  2.7917  Training Accuracy:  0.517049\n",
      "Epoch:  114  Training Loss:  2.76165  Training Accuracy:  0.519224\n",
      "Epoch:  115  Training Loss:  2.73349  Training Accuracy:  0.520988\n",
      "Epoch:  116  Training Loss:  2.71008  Training Accuracy:  0.52281\n",
      "Epoch:  117  Training Loss:  2.6844  Training Accuracy:  0.524162\n",
      "Epoch:  118  Training Loss:  2.64226  Training Accuracy:  0.526279\n",
      "Epoch:  119  Training Loss:  2.63063  Training Accuracy:  0.527925\n",
      "Epoch:  120  Training Loss:  2.58563  Training Accuracy:  0.528924\n",
      "Epoch:  121  Training Loss:  2.56713  Training Accuracy:  0.530159\n",
      "Epoch:  122  Training Loss:  2.5297  Training Accuracy:  0.531276\n",
      "Epoch:  123  Training Loss:  2.50669  Training Accuracy:  0.533098\n",
      "Epoch:  124  Training Loss:  2.45553  Training Accuracy:  0.534979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  2.42248  Training Accuracy:  0.536978\n",
      "Epoch:  126  Training Loss:  2.3612  Training Accuracy:  0.538389\n",
      "Epoch:  127  Training Loss:  2.28188  Training Accuracy:  0.539095\n",
      "Epoch:  128  Training Loss:  2.23469  Training Accuracy:  0.540799\n",
      "Epoch:  129  Training Loss:  2.19192  Training Accuracy:  0.541916\n",
      "Epoch:  130  Training Loss:  2.14033  Training Accuracy:  0.543386\n",
      "Epoch:  131  Training Loss:  2.11167  Training Accuracy:  0.544327\n",
      "Epoch:  132  Training Loss:  2.07378  Training Accuracy:  0.545561\n",
      "Epoch:  133  Training Loss:  2.04454  Training Accuracy:  0.54709\n",
      "Epoch:  134  Training Loss:  1.99189  Training Accuracy:  0.548618\n",
      "Epoch:  135  Training Loss:  1.96143  Training Accuracy:  0.549853\n",
      "Epoch:  136  Training Loss:  1.89669  Training Accuracy:  0.550617\n",
      "Epoch:  137  Training Loss:  1.88048  Training Accuracy:  0.552322\n",
      "Epoch:  138  Training Loss:  1.83541  Training Accuracy:  0.553204\n",
      "Epoch:  139  Training Loss:  1.81003  Training Accuracy:  0.554674\n",
      "Epoch:  140  Training Loss:  1.75479  Training Accuracy:  0.555673\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 600\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  175.615  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  141.838  Training Accuracy:  0.0409171\n",
      "Epoch:  2  Training Loss:  127.823  Training Accuracy:  0.0462669\n",
      "Epoch:  3  Training Loss:  114.735  Training Accuracy:  0.0530864\n",
      "Epoch:  4  Training Loss:  104.378  Training Accuracy:  0.0582011\n",
      "Epoch:  5  Training Loss:  97.0706  Training Accuracy:  0.0634921\n",
      "Epoch:  6  Training Loss:  92.1192  Training Accuracy:  0.0680188\n",
      "Epoch:  7  Training Loss:  88.3668  Training Accuracy:  0.0730747\n",
      "Epoch:  8  Training Loss:  85.2273  Training Accuracy:  0.0769547\n",
      "Epoch:  9  Training Loss:  82.4338  Training Accuracy:  0.0814815\n",
      "Epoch:  10  Training Loss:  79.7954  Training Accuracy:  0.0877131\n",
      "Epoch:  11  Training Loss:  77.2156  Training Accuracy:  0.0943563\n",
      "Epoch:  12  Training Loss:  74.5958  Training Accuracy:  0.101529\n",
      "Epoch:  13  Training Loss:  71.8586  Training Accuracy:  0.109347\n",
      "Epoch:  14  Training Loss:  69.0806  Training Accuracy:  0.118812\n",
      "Epoch:  15  Training Loss:  66.2306  Training Accuracy:  0.126337\n",
      "Epoch:  16  Training Loss:  63.3231  Training Accuracy:  0.132981\n",
      "Epoch:  17  Training Loss:  60.4817  Training Accuracy:  0.139153\n",
      "Epoch:  18  Training Loss:  57.6301  Training Accuracy:  0.145797\n",
      "Epoch:  19  Training Loss:  54.7169  Training Accuracy:  0.152734\n",
      "Epoch:  20  Training Loss:  51.9101  Training Accuracy:  0.1592\n",
      "Epoch:  21  Training Loss:  49.2487  Training Accuracy:  0.167842\n",
      "Epoch:  22  Training Loss:  46.5861  Training Accuracy:  0.175426\n",
      "Epoch:  23  Training Loss:  44.0107  Training Accuracy:  0.182775\n",
      "Epoch:  24  Training Loss:  41.5013  Training Accuracy:  0.191005\n",
      "Epoch:  25  Training Loss:  39.0279  Training Accuracy:  0.199236\n",
      "Epoch:  26  Training Loss:  36.6764  Training Accuracy:  0.209112\n",
      "Epoch:  27  Training Loss:  34.4191  Training Accuracy:  0.218225\n",
      "Epoch:  28  Training Loss:  32.3597  Training Accuracy:  0.226279\n",
      "Epoch:  29  Training Loss:  30.4267  Training Accuracy:  0.233333\n",
      "Epoch:  30  Training Loss:  28.5162  Training Accuracy:  0.241387\n",
      "Epoch:  31  Training Loss:  26.8186  Training Accuracy:  0.24856\n",
      "Epoch:  32  Training Loss:  25.2061  Training Accuracy:  0.255438\n",
      "Epoch:  33  Training Loss:  23.7268  Training Accuracy:  0.264256\n",
      "Epoch:  34  Training Loss:  22.3962  Training Accuracy:  0.270488\n",
      "Epoch:  35  Training Loss:  21.0569  Training Accuracy:  0.277013\n",
      "Epoch:  36  Training Loss:  19.8395  Training Accuracy:  0.284362\n",
      "Epoch:  37  Training Loss:  18.7729  Training Accuracy:  0.290123\n",
      "Epoch:  38  Training Loss:  17.6406  Training Accuracy:  0.295708\n",
      "Epoch:  39  Training Loss:  16.6765  Training Accuracy:  0.300764\n",
      "Epoch:  40  Training Loss:  15.8434  Training Accuracy:  0.307525\n",
      "Epoch:  41  Training Loss:  15.0391  Training Accuracy:  0.312992\n",
      "Epoch:  42  Training Loss:  14.2711  Training Accuracy:  0.319283\n",
      "Epoch:  43  Training Loss:  13.538  Training Accuracy:  0.324397\n",
      "Epoch:  44  Training Loss:  12.8807  Training Accuracy:  0.328689\n",
      "Epoch:  45  Training Loss:  12.2041  Training Accuracy:  0.333392\n",
      "Epoch:  46  Training Loss:  11.5927  Training Accuracy:  0.338272\n",
      "Epoch:  47  Training Loss:  11.0398  Training Accuracy:  0.343092\n",
      "Epoch:  48  Training Loss:  10.455  Training Accuracy:  0.347501\n",
      "Epoch:  49  Training Loss:  9.93449  Training Accuracy:  0.353028\n",
      "Epoch:  50  Training Loss:  9.42705  Training Accuracy:  0.357789\n",
      "Epoch:  51  Training Loss:  9.05486  Training Accuracy:  0.362728\n",
      "Epoch:  52  Training Loss:  8.58062  Training Accuracy:  0.366784\n",
      "Epoch:  53  Training Loss:  8.20216  Training Accuracy:  0.372134\n",
      "Epoch:  54  Training Loss:  7.82149  Training Accuracy:  0.375955\n",
      "Epoch:  55  Training Loss:  7.5162  Training Accuracy:  0.38007\n",
      "Epoch:  56  Training Loss:  7.20222  Training Accuracy:  0.384538\n",
      "Epoch:  57  Training Loss:  6.97186  Training Accuracy:  0.388301\n",
      "Epoch:  58  Training Loss:  6.69191  Training Accuracy:  0.392181\n",
      "Epoch:  59  Training Loss:  6.41794  Training Accuracy:  0.39659\n",
      "Epoch:  60  Training Loss:  6.20903  Training Accuracy:  0.399941\n",
      "Epoch:  61  Training Loss:  5.95553  Training Accuracy:  0.402528\n",
      "Epoch:  62  Training Loss:  5.77545  Training Accuracy:  0.405291\n",
      "Epoch:  63  Training Loss:  5.57413  Training Accuracy:  0.408818\n",
      "Epoch:  64  Training Loss:  5.37681  Training Accuracy:  0.411229\n",
      "Epoch:  65  Training Loss:  5.18388  Training Accuracy:  0.414286\n",
      "Epoch:  66  Training Loss:  5.02898  Training Accuracy:  0.416696\n",
      "Epoch:  67  Training Loss:  4.88722  Training Accuracy:  0.419694\n",
      "Epoch:  68  Training Loss:  4.74004  Training Accuracy:  0.422046\n",
      "Epoch:  69  Training Loss:  4.64705  Training Accuracy:  0.424339\n",
      "Epoch:  70  Training Loss:  4.54412  Training Accuracy:  0.426866\n",
      "Epoch:  71  Training Loss:  4.47102  Training Accuracy:  0.429336\n",
      "Epoch:  72  Training Loss:  4.33668  Training Accuracy:  0.432569\n",
      "Epoch:  73  Training Loss:  4.17059  Training Accuracy:  0.435744\n",
      "Epoch:  74  Training Loss:  4.05004  Training Accuracy:  0.437919\n",
      "Epoch:  75  Training Loss:  3.9241  Training Accuracy:  0.440447\n",
      "Epoch:  76  Training Loss:  3.79711  Training Accuracy:  0.442445\n",
      "Epoch:  77  Training Loss:  3.65905  Training Accuracy:  0.446326\n",
      "Epoch:  78  Training Loss:  3.52553  Training Accuracy:  0.448795\n",
      "Epoch:  79  Training Loss:  3.45998  Training Accuracy:  0.450735\n",
      "Epoch:  80  Training Loss:  3.3673  Training Accuracy:  0.453263\n",
      "Epoch:  81  Training Loss:  3.3051  Training Accuracy:  0.456085\n",
      "Epoch:  82  Training Loss:  3.23287  Training Accuracy:  0.458377\n",
      "Epoch:  83  Training Loss:  3.16546  Training Accuracy:  0.461258\n",
      "Epoch:  84  Training Loss:  3.11229  Training Accuracy:  0.463492\n",
      "Epoch:  85  Training Loss:  3.05302  Training Accuracy:  0.466373\n",
      "Epoch:  86  Training Loss:  3.01315  Training Accuracy:  0.468548\n",
      "Epoch:  87  Training Loss:  2.96206  Training Accuracy:  0.470194\n",
      "Epoch:  88  Training Loss:  2.90141  Training Accuracy:  0.471664\n",
      "Epoch:  89  Training Loss:  2.8711  Training Accuracy:  0.47331\n",
      "Epoch:  90  Training Loss:  2.82875  Training Accuracy:  0.475779\n",
      "Epoch:  91  Training Loss:  2.78784  Training Accuracy:  0.477366\n",
      "Epoch:  92  Training Loss:  2.72802  Training Accuracy:  0.4796\n",
      "Epoch:  93  Training Loss:  2.71031  Training Accuracy:  0.481246\n",
      "Epoch:  94  Training Loss:  2.6703  Training Accuracy:  0.483598\n",
      "Epoch:  95  Training Loss:  2.65442  Training Accuracy:  0.485244\n",
      "Epoch:  96  Training Loss:  2.63242  Training Accuracy:  0.487243\n",
      "Epoch:  97  Training Loss:  2.6079  Training Accuracy:  0.48883\n",
      "Epoch:  98  Training Loss:  2.55533  Training Accuracy:  0.490829\n",
      "Epoch:  99  Training Loss:  2.51917  Training Accuracy:  0.493063\n",
      "Epoch:  100  Training Loss:  2.47583  Training Accuracy:  0.494826\n",
      "Epoch:  101  Training Loss:  2.4326  Training Accuracy:  0.496355\n",
      "Epoch:  102  Training Loss:  2.40664  Training Accuracy:  0.498177\n",
      "Epoch:  103  Training Loss:  2.3758  Training Accuracy:  0.499941\n",
      "Epoch:  104  Training Loss:  2.35225  Training Accuracy:  0.501999\n",
      "Epoch:  105  Training Loss:  2.31991  Training Accuracy:  0.503821\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  65.4361  Training Accuracy:  0.0450911\n",
      "Epoch:  1  Training Loss:  74.5786  Training Accuracy:  0.0451499\n",
      "Epoch:  2  Training Loss:  77.3696  Training Accuracy:  0.0477954\n",
      "Epoch:  3  Training Loss:  74.0615  Training Accuracy:  0.0507936\n",
      "Epoch:  4  Training Loss:  68.255  Training Accuracy:  0.0548501\n",
      "Epoch:  5  Training Loss:  62.7934  Training Accuracy:  0.0579659\n",
      "Epoch:  6  Training Loss:  58.4304  Training Accuracy:  0.0627278\n",
      "Epoch:  7  Training Loss:  54.8783  Training Accuracy:  0.0687831\n",
      "Epoch:  8  Training Loss:  51.2769  Training Accuracy:  0.074368\n",
      "Epoch:  9  Training Loss:  47.7494  Training Accuracy:  0.0802469\n",
      "Epoch:  10  Training Loss:  44.4792  Training Accuracy:  0.0900059\n",
      "Epoch:  11  Training Loss:  41.376  Training Accuracy:  0.0987066\n",
      "Epoch:  12  Training Loss:  38.5132  Training Accuracy:  0.107525\n",
      "Epoch:  13  Training Loss:  35.9985  Training Accuracy:  0.115344\n",
      "Epoch:  14  Training Loss:  33.8671  Training Accuracy:  0.124809\n",
      "Epoch:  15  Training Loss:  31.9693  Training Accuracy:  0.132922\n",
      "Epoch:  16  Training Loss:  30.5315  Training Accuracy:  0.141564\n",
      "Epoch:  17  Training Loss:  29.0848  Training Accuracy:  0.149383\n",
      "Epoch:  18  Training Loss:  27.7345  Training Accuracy:  0.157907\n",
      "Epoch:  19  Training Loss:  26.3122  Training Accuracy:  0.166314\n",
      "Epoch:  20  Training Loss:  25.0097  Training Accuracy:  0.175544\n",
      "Epoch:  21  Training Loss:  23.6486  Training Accuracy:  0.183774\n",
      "Epoch:  22  Training Loss:  22.1823  Training Accuracy:  0.193004\n",
      "Epoch:  23  Training Loss:  20.7498  Training Accuracy:  0.199765\n",
      "Epoch:  24  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  25  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  26  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  27  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  28  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  29  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  30  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  31  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  32  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  33  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  34  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  35  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  36  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  37  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  38  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  39  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  40  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  41  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  42  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  43  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  44  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  45  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  46  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  47  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  48  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  49  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  50  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  51  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  52  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  53  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  54  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  55  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  56  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  57  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  58  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  59  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  60  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  61  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  62  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  63  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  64  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  65  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  66  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  67  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  68  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  69  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  70  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  71  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  72  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  73  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  74  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  75  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  76  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  77  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  78  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  79  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  80  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  81  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  82  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  83  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  84  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  85  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  86  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  87  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  88  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  89  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  90  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  91  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  92  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  93  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  94  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  95  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  96  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  97  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  98  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  99  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  100  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  101  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  102  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  103  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  104  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  105  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  106  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  107  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  108  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  109  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  110  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  111  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  112  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  113  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  114  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  115  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  116  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  117  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  118  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  119  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  120  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  121  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  122  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  123  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  124  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  125  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  126  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  127  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  128  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  129  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  130  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  131  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  132  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  133  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  134  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  135  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  136  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  137  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  138  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  139  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  140  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  141  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  142  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  143  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  144  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  145  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  146  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  147  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  148  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  149  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  150  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  151  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  152  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  153  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  154  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  155  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  156  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  157  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  158  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  159  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  160  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  161  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  162  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  163  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  164  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  165  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  166  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  167  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  168  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  169  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  170  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 5\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  69.7262  Training Accuracy:  0.0409171\n",
      "Epoch:  1  Training Loss:  66.9482  Training Accuracy:  0.041505\n",
      "Epoch:  2  Training Loss:  61.5993  Training Accuracy:  0.0451499\n",
      "Epoch:  3  Training Loss:  56.999  Training Accuracy:  0.0503821\n",
      "Epoch:  4  Training Loss:  52.7865  Training Accuracy:  0.0557319\n",
      "Epoch:  5  Training Loss:  48.522  Training Accuracy:  0.0599647\n",
      "Epoch:  6  Training Loss:  44.2681  Training Accuracy:  0.0644327\n",
      "Epoch:  7  Training Loss:  39.8075  Training Accuracy:  0.0712522\n",
      "Epoch:  8  Training Loss:  35.6324  Training Accuracy:  0.0789536\n",
      "Epoch:  9  Training Loss:  31.7614  Training Accuracy:  0.0838918\n",
      "Epoch:  10  Training Loss:  28.2217  Training Accuracy:  0.0897707\n",
      "Epoch:  11  Training Loss:  25.0965  Training Accuracy:  0.0946502\n",
      "Epoch:  12  Training Loss:  22.2333  Training Accuracy:  0.0981775\n",
      "Epoch:  13  Training Loss:  19.8218  Training Accuracy:  0.100764\n",
      "Epoch:  14  Training Loss:  17.7094  Training Accuracy:  0.103233\n",
      "Epoch:  15  Training Loss:  15.8418  Training Accuracy:  0.106526\n",
      "Epoch:  16  Training Loss:  14.3256  Training Accuracy:  0.110641\n",
      "Epoch:  17  Training Loss:  12.9825  Training Accuracy:  0.115638\n",
      "Epoch:  18  Training Loss:  11.7207  Training Accuracy:  0.119342\n",
      "Epoch:  19  Training Loss:  10.6015  Training Accuracy:  0.123398\n",
      "Epoch:  20  Training Loss:  9.5426  Training Accuracy:  0.127807\n",
      "Epoch:  21  Training Loss:  8.62492  Training Accuracy:  0.133098\n",
      "Epoch:  22  Training Loss:  7.77816  Training Accuracy:  0.137684\n",
      "Epoch:  23  Training Loss:  7.00481  Training Accuracy:  0.142857\n",
      "Epoch:  24  Training Loss:  6.2912  Training Accuracy:  0.146972\n",
      "Epoch:  25  Training Loss:  5.5782  Training Accuracy:  0.151734\n",
      "Epoch:  26  Training Loss:  4.93537  Training Accuracy:  0.157672\n",
      "Epoch:  27  Training Loss:  4.36606  Training Accuracy:  0.162669\n",
      "Epoch:  28  Training Loss:  3.84657  Training Accuracy:  0.167901\n",
      "Epoch:  29  Training Loss:  3.40652  Training Accuracy:  0.17331\n",
      "Epoch:  30  Training Loss:  3.01193  Training Accuracy:  0.178483\n",
      "Epoch:  31  Training Loss:  2.65813  Training Accuracy:  0.182246\n",
      "Epoch:  32  Training Loss:  2.33902  Training Accuracy:  0.187713\n",
      "Epoch:  33  Training Loss:  2.08413  Training Accuracy:  0.192828\n",
      "Epoch:  34  Training Loss:  1.87275  Training Accuracy:  0.197061\n",
      "Epoch:  35  Training Loss:  1.66974  Training Accuracy:  0.202645\n",
      "Epoch:  36  Training Loss:  1.49162  Training Accuracy:  0.208995\n",
      "Epoch:  37  Training Loss:  1.34151  Training Accuracy:  0.213051\n",
      "Epoch:  38  Training Loss:  1.19347  Training Accuracy:  0.218283\n",
      "Epoch:  39  Training Loss:  1.08401  Training Accuracy:  0.223574\n",
      "Epoch:  40  Training Loss:  0.984065  Training Accuracy:  0.22863\n",
      "Epoch:  41  Training Loss:  0.891292  Training Accuracy:  0.233862\n",
      "Epoch:  42  Training Loss:  0.815183  Training Accuracy:  0.237978\n",
      "Epoch:  43  Training Loss:  0.754218  Training Accuracy:  0.242622\n",
      "Epoch:  44  Training Loss:  0.690551  Training Accuracy:  0.246384\n",
      "Epoch:  45  Training Loss:  0.640743  Training Accuracy:  0.250441\n",
      "Epoch:  46  Training Loss:  0.601151  Training Accuracy:  0.253263\n",
      "Epoch:  47  Training Loss:  0.563896  Training Accuracy:  0.257143\n",
      "Epoch:  48  Training Loss:  0.530632  Training Accuracy:  0.260846\n",
      "Epoch:  49  Training Loss:  0.501389  Training Accuracy:  0.263727\n",
      "Epoch:  50  Training Loss:  0.47503  Training Accuracy:  0.26749\n",
      "Epoch:  51  Training Loss:  0.446536  Training Accuracy:  0.271193\n",
      "Epoch:  52  Training Loss:  0.425161  Training Accuracy:  0.274662\n",
      "Epoch:  53  Training Loss:  0.402143  Training Accuracy:  0.277543\n",
      "Epoch:  54  Training Loss:  0.381916  Training Accuracy:  0.280541\n",
      "Epoch:  55  Training Loss:  0.363631  Training Accuracy:  0.283657\n",
      "Epoch:  56  Training Loss:  0.345084  Training Accuracy:  0.286478\n",
      "Epoch:  57  Training Loss:  0.328983  Training Accuracy:  0.290241\n",
      "Epoch:  58  Training Loss:  0.308703  Training Accuracy:  0.29318\n",
      "Epoch:  59  Training Loss:  0.292971  Training Accuracy:  0.295708\n",
      "Epoch:  60  Training Loss:  0.275013  Training Accuracy:  0.297942\n",
      "Epoch:  61  Training Loss:  0.262662  Training Accuracy:  0.300999\n",
      "Epoch:  62  Training Loss:  0.249828  Training Accuracy:  0.304115\n",
      "Epoch:  63  Training Loss:  0.239827  Training Accuracy:  0.306937\n",
      "Epoch:  64  Training Loss:  0.229177  Training Accuracy:  0.3107\n",
      "Epoch:  65  Training Loss:  0.218138  Training Accuracy:  0.313521\n",
      "Epoch:  66  Training Loss:  0.21099  Training Accuracy:  0.316872\n",
      "Epoch:  67  Training Loss:  0.205731  Training Accuracy:  0.319459\n",
      "Epoch:  68  Training Loss:  0.198921  Training Accuracy:  0.322575\n",
      "Epoch:  69  Training Loss:  0.191036  Training Accuracy:  0.325044\n",
      "Epoch:  70  Training Loss:  0.185471  Training Accuracy:  0.327983\n",
      "Epoch:  71  Training Loss:  0.180497  Training Accuracy:  0.331158\n",
      "Epoch:  72  Training Loss:  0.17155  Training Accuracy:  0.333804\n",
      "Epoch:  73  Training Loss:  0.163369  Training Accuracy:  0.336214\n",
      "Epoch:  74  Training Loss:  0.156175  Training Accuracy:  0.338448\n",
      "Epoch:  75  Training Loss:  0.150524  Training Accuracy:  0.342034\n",
      "Epoch:  76  Training Loss:  0.144566  Training Accuracy:  0.34562\n",
      "Epoch:  77  Training Loss:  0.14002  Training Accuracy:  0.347443\n",
      "Epoch:  78  Training Loss:  0.134756  Training Accuracy:  0.350147\n",
      "Epoch:  79  Training Loss:  0.129788  Training Accuracy:  0.352146\n",
      "Epoch:  80  Training Loss:  0.124766  Training Accuracy:  0.354791\n",
      "Epoch:  81  Training Loss:  0.120394  Training Accuracy:  0.357731\n",
      "Epoch:  82  Training Loss:  0.117502  Training Accuracy:  0.361552\n",
      "Epoch:  83  Training Loss:  0.112864  Training Accuracy:  0.364433\n",
      "Epoch:  84  Training Loss:  0.109497  Training Accuracy:  0.367254\n",
      "Epoch:  85  Training Loss:  0.106201  Training Accuracy:  0.369488\n",
      "Epoch:  86  Training Loss:  0.102531  Training Accuracy:  0.371958\n",
      "Epoch:  87  Training Loss:  0.0981943  Training Accuracy:  0.374133\n",
      "Epoch:  88  Training Loss:  0.0951869  Training Accuracy:  0.377072\n",
      "Epoch:  89  Training Loss:  0.0925819  Training Accuracy:  0.379306\n",
      "Epoch:  90  Training Loss:  0.0884952  Training Accuracy:  0.381834\n",
      "Epoch:  91  Training Loss:  0.0855832  Training Accuracy:  0.384362\n",
      "Epoch:  92  Training Loss:  0.0830908  Training Accuracy:  0.38689\n",
      "Epoch:  93  Training Loss:  0.0808336  Training Accuracy:  0.389712\n",
      "Epoch:  94  Training Loss:  0.0798286  Training Accuracy:  0.392651\n",
      "Epoch:  95  Training Loss:  0.0775888  Training Accuracy:  0.395649\n",
      "Epoch:  96  Training Loss:  0.0754824  Training Accuracy:  0.398354\n",
      "Epoch:  97  Training Loss:  0.0734549  Training Accuracy:  0.401293\n",
      "Epoch:  98  Training Loss:  0.0711003  Training Accuracy:  0.404821\n",
      "Epoch:  99  Training Loss:  0.069198  Training Accuracy:  0.407407\n",
      "Epoch:  100  Training Loss:  0.0674405  Training Accuracy:  0.410464\n",
      "Epoch:  101  Training Loss:  0.0650344  Training Accuracy:  0.41358\n",
      "Epoch:  102  Training Loss:  0.0630999  Training Accuracy:  0.416167\n",
      "Epoch:  103  Training Loss:  0.061106  Training Accuracy:  0.418518\n",
      "Epoch:  104  Training Loss:  0.0586662  Training Accuracy:  0.421869\n",
      "Epoch:  105  Training Loss:  0.056362  Training Accuracy:  0.424691\n",
      "Epoch:  106  Training Loss:  0.0541602  Training Accuracy:  0.427337\n",
      "Epoch:  107  Training Loss:  0.0522922  Training Accuracy:  0.431393\n",
      "Epoch:  108  Training Loss:  0.0505633  Training Accuracy:  0.434568\n",
      "Epoch:  109  Training Loss:  0.0492426  Training Accuracy:  0.439624\n",
      "Epoch:  110  Training Loss:  0.0478792  Training Accuracy:  0.443269\n",
      "Epoch:  111  Training Loss:  0.0464842  Training Accuracy:  0.445679\n",
      "Epoch:  112  Training Loss:  0.0452026  Training Accuracy:  0.4495\n",
      "Epoch:  113  Training Loss:  0.0438256  Training Accuracy:  0.451558\n",
      "Epoch:  114  Training Loss:  0.0427965  Training Accuracy:  0.455438\n",
      "Epoch:  115  Training Loss:  0.0416018  Training Accuracy:  0.457378\n",
      "Epoch:  116  Training Loss:  0.0400218  Training Accuracy:  0.4602\n",
      "Epoch:  117  Training Loss:  0.038856  Training Accuracy:  0.462963\n",
      "Epoch:  118  Training Loss:  0.0377117  Training Accuracy:  0.466314\n",
      "Epoch:  119  Training Loss:  0.0361967  Training Accuracy:  0.469253\n",
      "Epoch:  120  Training Loss:  0.0351206  Training Accuracy:  0.473016\n",
      "Epoch:  121  Training Loss:  0.0338979  Training Accuracy:  0.475896\n",
      "Epoch:  122  Training Loss:  0.0328176  Training Accuracy:  0.478836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  123  Training Loss:  0.0316841  Training Accuracy:  0.481364\n",
      "Epoch:  124  Training Loss:  0.0306716  Training Accuracy:  0.483539\n",
      "Epoch:  125  Training Loss:  0.0297519  Training Accuracy:  0.486537\n",
      "Epoch:  126  Training Loss:  0.0290915  Training Accuracy:  0.488301\n",
      "Epoch:  127  Training Loss:  0.0279839  Training Accuracy:  0.491417\n",
      "Epoch:  128  Training Loss:  0.0270411  Training Accuracy:  0.494415\n",
      "Epoch:  129  Training Loss:  0.0262986  Training Accuracy:  0.497296\n",
      "Epoch:  130  Training Loss:  0.0252793  Training Accuracy:  0.499294\n",
      "Epoch:  131  Training Loss:  0.0242238  Training Accuracy:  0.501528\n",
      "Epoch:  132  Training Loss:  0.0233571  Training Accuracy:  0.503998\n",
      "Epoch:  133  Training Loss:  0.0223578  Training Accuracy:  0.506055\n",
      "Epoch:  134  Training Loss:  0.0215175  Training Accuracy:  0.509289\n",
      "Epoch:  135  Training Loss:  0.0206517  Training Accuracy:  0.512757\n",
      "Epoch:  136  Training Loss:  0.019764  Training Accuracy:  0.515755\n",
      "Epoch:  137  Training Loss:  0.0191297  Training Accuracy:  0.518577\n",
      "Epoch:  138  Training Loss:  0.0186306  Training Accuracy:  0.521634\n",
      "Epoch:  139  Training Loss:  0.017994  Training Accuracy:  0.524162\n",
      "Epoch:  140  Training Loss:  0.0174789  Training Accuracy:  0.52669\n",
      "Epoch:  141  Training Loss:  0.0170825  Training Accuracy:  0.529982\n",
      "Epoch:  142  Training Loss:  0.016545  Training Accuracy:  0.532745\n",
      "Epoch:  143  Training Loss:  0.0161802  Training Accuracy:  0.535391\n",
      "Epoch:  144  Training Loss:  0.0157682  Training Accuracy:  0.537919\n",
      "Epoch:  145  Training Loss:  0.0153775  Training Accuracy:  0.540447\n",
      "Epoch:  146  Training Loss:  0.0149295  Training Accuracy:  0.543445\n",
      "Epoch:  147  Training Loss:  0.0146071  Training Accuracy:  0.546208\n",
      "Epoch:  148  Training Loss:  0.0141091  Training Accuracy:  0.549089\n",
      "Epoch:  149  Training Loss:  0.0138296  Training Accuracy:  0.55097\n",
      "Epoch:  150  Training Loss:  0.0135531  Training Accuracy:  0.553733\n",
      "Epoch:  151  Training Loss:  0.0131853  Training Accuracy:  0.556496\n",
      "Epoch:  152  Training Loss:  0.0129086  Training Accuracy:  0.558965\n",
      "Epoch:  153  Training Loss:  0.0126387  Training Accuracy:  0.560905\n",
      "Epoch:  154  Training Loss:  0.0124376  Training Accuracy:  0.563492\n",
      "Epoch:  155  Training Loss:  0.0121529  Training Accuracy:  0.566137\n",
      "Epoch:  156  Training Loss:  0.0118508  Training Accuracy:  0.568959\n",
      "Epoch:  157  Training Loss:  0.0115645  Training Accuracy:  0.571193\n",
      "Epoch:  158  Training Loss:  0.011421  Training Accuracy:  0.573839\n",
      "Epoch:  159  Training Loss:  0.0112735  Training Accuracy:  0.575955\n",
      "Epoch:  160  Training Loss:  0.0110581  Training Accuracy:  0.578424\n",
      "Epoch:  161  Training Loss:  0.0109499  Training Accuracy:  0.580364\n",
      "Epoch:  162  Training Loss:  0.0107773  Training Accuracy:  0.582951\n",
      "Epoch:  163  Training Loss:  0.0105966  Training Accuracy:  0.585773\n",
      "Epoch:  164  Training Loss:  0.0105649  Training Accuracy:  0.588713\n",
      "Epoch:  165  Training Loss:  0.0104457  Training Accuracy:  0.591182\n",
      "Epoch:  166  Training Loss:  0.0103333  Training Accuracy:  0.594003\n",
      "Epoch:  167  Training Loss:  0.0103441  Training Accuracy:  0.596767\n",
      "Epoch:  168  Training Loss:  0.0102433  Training Accuracy:  0.599059\n",
      "Epoch:  169  Training Loss:  0.0102263  Training Accuracy:  0.601705\n",
      "Epoch:  170  Training Loss:  0.0101564  Training Accuracy:  0.603645\n",
      "Epoch:  171  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  172  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  173  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  174  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  175  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  176  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  177  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  178  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  179  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  180  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  181  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  182  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  183  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  184  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  185  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  186  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  187  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  188  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  189  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  190  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  191  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  192  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  193  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  194  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  195  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  196  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  197  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  198  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  199  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  200  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  201  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  202  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  203  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  204  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  205  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  206  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  207  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  208  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  209  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  210  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  211  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  212  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  213  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  214  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  215  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  216  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  217  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  218  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  219  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  220  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  221  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  222  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  223  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  224  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  225  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  226  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  227  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  228  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  229  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  230  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  231  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  232  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  233  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  234  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  235  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  236  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  237  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  238  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  239  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  240  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  241  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  242  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  243  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  244  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  245  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  246  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  247  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  249  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  250  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  251  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  252  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  253  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  254  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  255  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  256  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  257  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  258  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  259  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  260  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  261  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  262  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  263  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  264  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  265  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  266  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  267  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  268  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  269  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  270  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  271  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  272  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  273  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  274  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  275  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  276  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  277  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  278  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  279  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  280  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  281  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  282  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  283  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  284  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  285  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  286  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  287  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  288  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  289  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  290  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  291  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  292  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  293  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  294  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  295  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  296  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  297  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  298  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  299  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  300  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  301  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  302  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  303  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  304  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  305  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  306  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  307  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  308  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  309  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  310  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  311  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  312  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  313  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  314  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  315  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  316  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  317  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  318  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  319  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  320  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  321  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  322  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  323  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  324  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  325  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  326  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  327  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  328  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  329  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  330  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  331  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  332  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  333  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  334  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  335  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  336  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  337  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  338  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  339  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  340  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  341  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  342  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  343  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  344  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  345  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  346  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  347  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  348  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  349  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  350  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  351  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  352  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  353  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  354  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  355  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  356  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  357  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  358  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  359  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  360  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  361  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  362  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  363  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  364  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  365  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  366  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  367  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  368  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  369  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  370  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  371  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  372  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  373  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  374  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  375  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  376  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  377  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  378  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  379  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  380  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  381  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  382  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  383  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  384  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  385  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  386  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  387  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  388  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  389  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  390  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  391  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  392  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  393  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  394  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  395  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  396  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  397  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  398  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  399  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  400  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  401  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  402  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  403  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  404  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  405  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  406  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  407  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  408  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  409  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  410  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  411  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  412  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  413  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  414  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  415  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  416  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  417  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  418  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  419  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  420  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  421  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  422  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  423  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  424  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  425  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  426  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  427  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  428  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  429  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  430  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  431  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  432  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  433  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  434  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  435  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  436  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  437  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  438  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  439  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  440  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  441  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  442  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  443  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  444  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  445  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  446  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  447  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  448  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  449  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  450  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  451  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  452  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  453  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  454  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  455  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  456  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  457  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  458  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  459  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  460  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  461  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  462  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  463  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  464  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  465  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  466  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  467  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  468  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  469  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  470  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  471  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  472  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  473  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  474  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  475  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  476  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  477  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  478  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  479  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  480  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  481  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  482  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  483  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  484  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  485  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  486  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  487  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  488  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  489  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  490  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  491  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  492  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  493  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  494  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  495  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  496  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  497  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  498  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  499  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  500  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  501  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  502  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  503  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  504  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  505  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  506  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  507  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  508  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  509  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  510  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  511  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  512  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  513  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  514  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  515  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  516  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  517  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  518  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  519  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  520  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  521  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  522  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  523  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  524  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  525  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  526  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  527  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  528  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  529  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  530  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  531  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  532  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  533  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  534  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  535  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  536  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  537  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  538  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  539  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  540  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  541  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  542  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  543  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  544  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  545  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  546  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  547  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  548  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 5\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  113.665  Training Accuracy:  0.0717813\n",
      "Epoch:  1  Training Loss:  66.5284  Training Accuracy:  0.0503233\n",
      "Epoch:  2  Training Loss:  70.4829  Training Accuracy:  0.0620223\n",
      "Epoch:  3  Training Loss:  72.4087  Training Accuracy:  0.0699001\n",
      "Epoch:  4  Training Loss:  73.4326  Training Accuracy:  0.0784245\n",
      "Epoch:  5  Training Loss:  73.7902  Training Accuracy:  0.0862434\n",
      "Epoch:  6  Training Loss:  73.6133  Training Accuracy:  0.0972957\n",
      "Epoch:  7  Training Loss:  73.0214  Training Accuracy:  0.104468\n",
      "Epoch:  8  Training Loss:  72.08  Training Accuracy:  0.109289\n",
      "Epoch:  9  Training Loss:  70.9796  Training Accuracy:  0.114815\n",
      "Epoch:  10  Training Loss:  69.6447  Training Accuracy:  0.120635\n",
      "Epoch:  11  Training Loss:  68.193  Training Accuracy:  0.127866\n",
      "Epoch:  12  Training Loss:  66.6143  Training Accuracy:  0.13545\n",
      "Epoch:  13  Training Loss:  64.9021  Training Accuracy:  0.14221\n",
      "Epoch:  14  Training Loss:  63.1572  Training Accuracy:  0.148854\n",
      "Epoch:  15  Training Loss:  61.386  Training Accuracy:  0.156085\n",
      "Epoch:  16  Training Loss:  59.6612  Training Accuracy:  0.162845\n",
      "Epoch:  17  Training Loss:  57.9848  Training Accuracy:  0.169195\n",
      "Epoch:  18  Training Loss:  56.3798  Training Accuracy:  0.177425\n",
      "Epoch:  19  Training Loss:  54.8957  Training Accuracy:  0.183951\n",
      "Epoch:  20  Training Loss:  53.4817  Training Accuracy:  0.19224\n",
      "Epoch:  21  Training Loss:  52.2403  Training Accuracy:  0.199824\n",
      "Epoch:  22  Training Loss:  51.0989  Training Accuracy:  0.208466\n",
      "Epoch:  23  Training Loss:  50.0557  Training Accuracy:  0.216461\n",
      "Epoch:  24  Training Loss:  49.1303  Training Accuracy:  0.22575\n",
      "Epoch:  25  Training Loss:  48.2887  Training Accuracy:  0.231452\n",
      "Epoch:  26  Training Loss:  47.5845  Training Accuracy:  0.236508\n",
      "Epoch:  27  Training Loss:  46.983  Training Accuracy:  0.242034\n",
      "Epoch:  28  Training Loss:  46.4643  Training Accuracy:  0.247795\n",
      "Epoch:  29  Training Loss:  45.9903  Training Accuracy:  0.254791\n",
      "Epoch:  30  Training Loss:  45.6403  Training Accuracy:  0.260376\n",
      "Epoch:  31  Training Loss:  45.3284  Training Accuracy:  0.266255\n",
      "Epoch:  32  Training Loss:  45.0759  Training Accuracy:  0.273369\n",
      "Epoch:  33  Training Loss:  44.8998  Training Accuracy:  0.279071\n",
      "Epoch:  34  Training Loss:  44.7772  Training Accuracy:  0.284715\n",
      "Epoch:  35  Training Loss:  44.6969  Training Accuracy:  0.289947\n",
      "Epoch:  36  Training Loss:  44.6471  Training Accuracy:  0.29512\n",
      "Epoch:  37  Training Loss:  44.6583  Training Accuracy:  0.301587\n",
      "Epoch:  38  Training Loss:  44.6892  Training Accuracy:  0.307995\n",
      "Epoch:  39  Training Loss:  44.7141  Training Accuracy:  0.314874\n",
      "Epoch:  40  Training Loss:  44.7822  Training Accuracy:  0.319929\n",
      "Epoch:  41  Training Loss:  44.8958  Training Accuracy:  0.324397\n",
      "Epoch:  42  Training Loss:  44.9879  Training Accuracy:  0.330394\n",
      "Epoch:  43  Training Loss:  45.1132  Training Accuracy:  0.335273\n",
      "Epoch:  44  Training Loss:  45.2268  Training Accuracy:  0.340741\n",
      "Epoch:  45  Training Loss:  45.3708  Training Accuracy:  0.344915\n",
      "Epoch:  46  Training Loss:  45.5135  Training Accuracy:  0.350911\n",
      "Epoch:  47  Training Loss:  45.6693  Training Accuracy:  0.356026\n",
      "Epoch:  48  Training Loss:  45.8285  Training Accuracy:  0.360905\n",
      "Epoch:  49  Training Loss:  45.9463  Training Accuracy:  0.365197\n",
      "Epoch:  50  Training Loss:  46.1452  Training Accuracy:  0.3699\n",
      "Epoch:  51  Training Loss:  46.3076  Training Accuracy:  0.373427\n",
      "Epoch:  52  Training Loss:  46.4425  Training Accuracy:  0.377366\n",
      "Epoch:  53  Training Loss:  46.5864  Training Accuracy:  0.380776\n",
      "Epoch:  54  Training Loss:  46.7222  Training Accuracy:  0.383715\n",
      "Epoch:  55  Training Loss:  46.8523  Training Accuracy:  0.387066\n",
      "Epoch:  56  Training Loss:  46.9524  Training Accuracy:  0.391064\n",
      "Epoch:  57  Training Loss:  47.0884  Training Accuracy:  0.394944\n",
      "Epoch:  58  Training Loss:  47.197  Training Accuracy:  0.398177\n",
      "Epoch:  59  Training Loss:  47.3602  Training Accuracy:  0.402058\n",
      "Epoch:  60  Training Loss:  47.4729  Training Accuracy:  0.404703\n",
      "Epoch:  61  Training Loss:  47.609  Training Accuracy:  0.407701\n",
      "Epoch:  62  Training Loss:  47.722  Training Accuracy:  0.411699\n",
      "Epoch:  63  Training Loss:  47.8605  Training Accuracy:  0.414991\n",
      "Epoch:  64  Training Loss:  47.9944  Training Accuracy:  0.418283\n",
      "Epoch:  65  Training Loss:  48.0905  Training Accuracy:  0.421693\n",
      "Epoch:  66  Training Loss:  48.2117  Training Accuracy:  0.424515\n",
      "Epoch:  67  Training Loss:  48.3272  Training Accuracy:  0.427454\n",
      "Epoch:  68  Training Loss:  48.4374  Training Accuracy:  0.430746\n",
      "Epoch:  69  Training Loss:  48.5496  Training Accuracy:  0.434097\n",
      "Epoch:  70  Training Loss:  48.6678  Training Accuracy:  0.437096\n",
      "Epoch:  71  Training Loss:  48.7561  Training Accuracy:  0.439682\n",
      "Epoch:  72  Training Loss:  48.877  Training Accuracy:  0.442269\n",
      "Epoch:  73  Training Loss:  48.9925  Training Accuracy:  0.444503\n",
      "Epoch:  74  Training Loss:  49.0543  Training Accuracy:  0.448089\n",
      "Epoch:  75  Training Loss:  49.166  Training Accuracy:  0.450676\n",
      "Epoch:  76  Training Loss:  49.2404  Training Accuracy:  0.452675\n",
      "Epoch:  77  Training Loss:  49.3434  Training Accuracy:  0.455673\n",
      "Epoch:  78  Training Loss:  49.418  Training Accuracy:  0.458906\n",
      "Epoch:  79  Training Loss:  49.5342  Training Accuracy:  0.461082\n",
      "Epoch:  80  Training Loss:  49.5802  Training Accuracy:  0.46361\n",
      "Epoch:  81  Training Loss:  49.6707  Training Accuracy:  0.466137\n",
      "Epoch:  82  Training Loss:  49.7303  Training Accuracy:  0.468842\n",
      "Epoch:  83  Training Loss:  49.8091  Training Accuracy:  0.471899\n",
      "Epoch:  84  Training Loss:  49.8438  Training Accuracy:  0.474427\n",
      "Epoch:  85  Training Loss:  49.9554  Training Accuracy:  0.476896\n",
      "Epoch:  86  Training Loss:  49.9772  Training Accuracy:  0.479659\n",
      "Epoch:  87  Training Loss:  50.0266  Training Accuracy:  0.483127\n",
      "Epoch:  88  Training Loss:  50.1078  Training Accuracy:  0.485126\n",
      "Epoch:  89  Training Loss:  50.1764  Training Accuracy:  0.487243\n",
      "Epoch:  90  Training Loss:  50.1994  Training Accuracy:  0.489359\n",
      "Epoch:  91  Training Loss:  50.268  Training Accuracy:  0.492534\n",
      "Epoch:  92  Training Loss:  50.3061  Training Accuracy:  0.494474\n",
      "Epoch:  93  Training Loss:  50.3591  Training Accuracy:  0.497354\n",
      "Epoch:  94  Training Loss:  50.4159  Training Accuracy:  0.500411\n",
      "Epoch:  95  Training Loss:  50.4764  Training Accuracy:  0.503292\n",
      "Epoch:  96  Training Loss:  50.5326  Training Accuracy:  0.50629\n",
      "Epoch:  97  Training Loss:  50.6278  Training Accuracy:  0.508407\n",
      "Epoch:  98  Training Loss:  50.6707  Training Accuracy:  0.511581\n",
      "Epoch:  99  Training Loss:  50.7412  Training Accuracy:  0.513992\n",
      "Epoch:  100  Training Loss:  50.8312  Training Accuracy:  0.516402\n",
      "Epoch:  101  Training Loss:  50.9146  Training Accuracy:  0.519165\n",
      "Epoch:  102  Training Loss:  50.9722  Training Accuracy:  0.521281\n",
      "Epoch:  103  Training Loss:  51.0405  Training Accuracy:  0.524103\n",
      "Epoch:  104  Training Loss:  51.144  Training Accuracy:  0.526514\n",
      "Epoch:  105  Training Loss:  51.185  Training Accuracy:  0.528042\n",
      "Epoch:  106  Training Loss:  51.3115  Training Accuracy:  0.530923\n",
      "Epoch:  107  Training Loss:  51.3748  Training Accuracy:  0.534391\n",
      "Epoch:  108  Training Loss:  51.4473  Training Accuracy:  0.537213\n",
      "Epoch:  109  Training Loss:  51.502  Training Accuracy:  0.540505\n",
      "Epoch:  110  Training Loss:  51.6631  Training Accuracy:  0.543621\n",
      "Epoch:  111  Training Loss:  51.7246  Training Accuracy:  0.547031\n",
      "Epoch:  112  Training Loss:  51.7597  Training Accuracy:  0.549794\n",
      "Epoch:  113  Training Loss:  51.915  Training Accuracy:  0.552851\n",
      "Epoch:  114  Training Loss:  51.9598  Training Accuracy:  0.554556\n",
      "Epoch:  115  Training Loss:  52.065  Training Accuracy:  0.555908\n",
      "Epoch:  116  Training Loss:  52.192  Training Accuracy:  0.558495\n",
      "Epoch:  117  Training Loss:  52.3506  Training Accuracy:  0.560141\n",
      "Epoch:  118  Training Loss:  52.4284  Training Accuracy:  0.562316\n",
      "Epoch:  119  Training Loss:  52.533  Training Accuracy:  0.564315\n",
      "Epoch:  120  Training Loss:  52.5846  Training Accuracy:  0.566608\n",
      "Epoch:  121  Training Loss:  52.7246  Training Accuracy:  0.568783\n",
      "Epoch:  122  Training Loss:  52.8193  Training Accuracy:  0.571017\n",
      "Epoch:  123  Training Loss:  52.9679  Training Accuracy:  0.572546\n",
      "Epoch:  124  Training Loss:  53.0131  Training Accuracy:  0.574721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  53.1867  Training Accuracy:  0.576426\n",
      "Epoch:  126  Training Loss:  53.2513  Training Accuracy:  0.578483\n",
      "Epoch:  127  Training Loss:  53.4364  Training Accuracy:  0.579483\n",
      "Epoch:  128  Training Loss:  53.5102  Training Accuracy:  0.580952\n",
      "Epoch:  129  Training Loss:  53.6514  Training Accuracy:  0.583186\n",
      "Epoch:  130  Training Loss:  53.7368  Training Accuracy:  0.585185\n",
      "Epoch:  131  Training Loss:  53.8742  Training Accuracy:  0.588007\n",
      "Epoch:  132  Training Loss:  53.9278  Training Accuracy:  0.58983\n",
      "Epoch:  133  Training Loss:  54.1115  Training Accuracy:  0.591534\n",
      "Epoch:  134  Training Loss:  54.1501  Training Accuracy:  0.593357\n",
      "Epoch:  135  Training Loss:  54.2919  Training Accuracy:  0.594768\n",
      "Epoch:  136  Training Loss:  54.3606  Training Accuracy:  0.596414\n",
      "Epoch:  137  Training Loss:  54.5238  Training Accuracy:  0.59853\n",
      "Epoch:  138  Training Loss:  54.6116  Training Accuracy:  0.599882\n",
      "Epoch:  139  Training Loss:  54.6699  Training Accuracy:  0.60147\n",
      "Epoch:  140  Training Loss:  54.7743  Training Accuracy:  0.604174\n",
      "Epoch:  141  Training Loss:  54.9269  Training Accuracy:  0.605879\n",
      "Epoch:  142  Training Loss:  54.9461  Training Accuracy:  0.607055\n",
      "Epoch:  143  Training Loss:  55.0862  Training Accuracy:  0.609053\n",
      "Epoch:  144  Training Loss:  55.1848  Training Accuracy:  0.610582\n",
      "Epoch:  145  Training Loss:  55.3426  Training Accuracy:  0.612287\n",
      "Epoch:  146  Training Loss:  55.3737  Training Accuracy:  0.613874\n",
      "Epoch:  147  Training Loss:  55.4973  Training Accuracy:  0.615755\n",
      "Epoch:  148  Training Loss:  55.6553  Training Accuracy:  0.616872\n",
      "Epoch:  149  Training Loss:  55.7062  Training Accuracy:  0.618048\n",
      "Epoch:  150  Training Loss:  55.8274  Training Accuracy:  0.619812\n",
      "Epoch:  151  Training Loss:  55.9793  Training Accuracy:  0.620752\n",
      "Epoch:  152  Training Loss:  56.0102  Training Accuracy:  0.62234\n",
      "Epoch:  153  Training Loss:  56.1283  Training Accuracy:  0.624162\n",
      "Epoch:  154  Training Loss:  56.2307  Training Accuracy:  0.625397\n",
      "Epoch:  155  Training Loss:  56.2863  Training Accuracy:  0.626984\n",
      "Epoch:  156  Training Loss:  56.4158  Training Accuracy:  0.628336\n",
      "Epoch:  157  Training Loss:  56.5474  Training Accuracy:  0.629806\n",
      "Epoch:  158  Training Loss:  56.629  Training Accuracy:  0.631628\n",
      "Epoch:  159  Training Loss:  56.6828  Training Accuracy:  0.632687\n",
      "Epoch:  160  Training Loss:  56.8229  Training Accuracy:  0.63351\n",
      "Epoch:  161  Training Loss:  56.8303  Training Accuracy:  0.634803\n",
      "Epoch:  162  Training Loss:  56.9711  Training Accuracy:  0.635744\n",
      "Epoch:  163  Training Loss:  56.9924  Training Accuracy:  0.636919\n",
      "Epoch:  164  Training Loss:  57.1832  Training Accuracy:  0.638095\n",
      "Epoch:  165  Training Loss:  57.2001  Training Accuracy:  0.639154\n",
      "Epoch:  166  Training Loss:  57.3277  Training Accuracy:  0.640035\n",
      "Epoch:  167  Training Loss:  57.323  Training Accuracy:  0.64127\n",
      "Epoch:  168  Training Loss:  57.4168  Training Accuracy:  0.642034\n",
      "Epoch:  169  Training Loss:  57.5145  Training Accuracy:  0.643504\n",
      "Epoch:  170  Training Loss:  57.5284  Training Accuracy:  0.644738\n",
      "Epoch:  171  Training Loss:  57.6013  Training Accuracy:  0.646502\n",
      "Epoch:  172  Training Loss:  57.6825  Training Accuracy:  0.64756\n",
      "Epoch:  173  Training Loss:  57.7186  Training Accuracy:  0.648736\n",
      "Epoch:  174  Training Loss:  57.7207  Training Accuracy:  0.650029\n",
      "Epoch:  175  Training Loss:  57.7844  Training Accuracy:  0.650794\n",
      "Epoch:  176  Training Loss:  57.8705  Training Accuracy:  0.651969\n",
      "Epoch:  177  Training Loss:  57.9154  Training Accuracy:  0.653263\n",
      "Epoch:  178  Training Loss:  57.9343  Training Accuracy:  0.65391\n",
      "Epoch:  179  Training Loss:  58.0478  Training Accuracy:  0.655144\n",
      "Epoch:  180  Training Loss:  58.0766  Training Accuracy:  0.656496\n",
      "Epoch:  181  Training Loss:  58.1052  Training Accuracy:  0.658142\n",
      "Epoch:  182  Training Loss:  58.1749  Training Accuracy:  0.659612\n",
      "Epoch:  183  Training Loss:  58.239  Training Accuracy:  0.660318\n",
      "Epoch:  184  Training Loss:  58.2809  Training Accuracy:  0.661317\n",
      "Epoch:  185  Training Loss:  58.3248  Training Accuracy:  0.662375\n",
      "Epoch:  186  Training Loss:  58.311  Training Accuracy:  0.663962\n",
      "Epoch:  187  Training Loss:  58.4225  Training Accuracy:  0.664786\n",
      "Epoch:  188  Training Loss:  58.4082  Training Accuracy:  0.665667\n",
      "Epoch:  189  Training Loss:  58.4637  Training Accuracy:  0.666902\n",
      "Epoch:  190  Training Loss:  58.4952  Training Accuracy:  0.667725\n",
      "Epoch:  191  Training Loss:  58.5315  Training Accuracy:  0.668724\n",
      "Epoch:  192  Training Loss:  58.5288  Training Accuracy:  0.669547\n",
      "Epoch:  193  Training Loss:  58.6169  Training Accuracy:  0.67037\n",
      "Epoch:  194  Training Loss:  58.6473  Training Accuracy:  0.671193\n",
      "Epoch:  195  Training Loss:  58.6227  Training Accuracy:  0.672604\n",
      "Epoch:  196  Training Loss:  58.6397  Training Accuracy:  0.673839\n",
      "Epoch:  197  Training Loss:  58.7211  Training Accuracy:  0.674486\n",
      "Epoch:  198  Training Loss:  58.7329  Training Accuracy:  0.675955\n",
      "Epoch:  199  Training Loss:  58.7876  Training Accuracy:  0.677131\n",
      "Epoch:  200  Training Loss:  58.8368  Training Accuracy:  0.678425\n",
      "Epoch:  201  Training Loss:  58.8137  Training Accuracy:  0.679953\n",
      "Epoch:  202  Training Loss:  58.8746  Training Accuracy:  0.680776\n",
      "Epoch:  203  Training Loss:  58.9452  Training Accuracy:  0.681776\n",
      "Epoch:  204  Training Loss:  58.9705  Training Accuracy:  0.682775\n",
      "Epoch:  205  Training Loss:  59.0071  Training Accuracy:  0.683716\n",
      "Epoch:  206  Training Loss:  59.01  Training Accuracy:  0.68495\n",
      "Epoch:  207  Training Loss:  59.0653  Training Accuracy:  0.686185\n",
      "Epoch:  208  Training Loss:  59.0943  Training Accuracy:  0.687184\n",
      "Epoch:  209  Training Loss:  59.0974  Training Accuracy:  0.688477\n",
      "Epoch:  210  Training Loss:  59.0906  Training Accuracy:  0.689653\n",
      "Epoch:  211  Training Loss:  59.159  Training Accuracy:  0.690653\n",
      "Epoch:  212  Training Loss:  59.1878  Training Accuracy:  0.691358\n",
      "Epoch:  213  Training Loss:  59.2085  Training Accuracy:  0.692534\n",
      "Epoch:  214  Training Loss:  59.2484  Training Accuracy:  0.693533\n",
      "Epoch:  215  Training Loss:  59.2551  Training Accuracy:  0.694415\n",
      "Epoch:  216  Training Loss:  59.2473  Training Accuracy:  0.69565\n",
      "Epoch:  217  Training Loss:  59.2972  Training Accuracy:  0.696414\n",
      "Epoch:  218  Training Loss:  59.2896  Training Accuracy:  0.697884\n",
      "Epoch:  219  Training Loss:  59.2936  Training Accuracy:  0.698472\n",
      "Epoch:  220  Training Loss:  59.295  Training Accuracy:  0.699706\n",
      "Epoch:  221  Training Loss:  59.3206  Training Accuracy:  0.700823\n",
      "Epoch:  222  Training Loss:  59.3309  Training Accuracy:  0.702117\n",
      "Epoch:  223  Training Loss:  59.354  Training Accuracy:  0.703175\n",
      "Epoch:  224  Training Loss:  59.3626  Training Accuracy:  0.703821\n",
      "Epoch:  225  Training Loss:  59.392  Training Accuracy:  0.70488\n",
      "Epoch:  226  Training Loss:  59.3956  Training Accuracy:  0.705879\n",
      "Epoch:  227  Training Loss:  59.4372  Training Accuracy:  0.707055\n",
      "Epoch:  228  Training Loss:  59.4817  Training Accuracy:  0.707819\n",
      "Epoch:  229  Training Loss:  59.5458  Training Accuracy:  0.70876\n",
      "Epoch:  230  Training Loss:  59.5781  Training Accuracy:  0.709406\n",
      "Epoch:  231  Training Loss:  59.6124  Training Accuracy:  0.710817\n",
      "Epoch:  232  Training Loss:  59.6849  Training Accuracy:  0.711699\n",
      "Epoch:  233  Training Loss:  59.6777  Training Accuracy:  0.712522\n",
      "Epoch:  234  Training Loss:  59.7278  Training Accuracy:  0.71358\n",
      "Epoch:  235  Training Loss:  59.794  Training Accuracy:  0.714227\n",
      "Epoch:  236  Training Loss:  59.8152  Training Accuracy:  0.715109\n",
      "Epoch:  237  Training Loss:  59.8268  Training Accuracy:  0.715697\n",
      "Epoch:  238  Training Loss:  59.9123  Training Accuracy:  0.71652\n",
      "Epoch:  239  Training Loss:  59.9163  Training Accuracy:  0.717049\n",
      "Epoch:  240  Training Loss:  59.8954  Training Accuracy:  0.718225\n",
      "Epoch:  241  Training Loss:  59.9633  Training Accuracy:  0.71893\n",
      "Epoch:  242  Training Loss:  59.9534  Training Accuracy:  0.7194\n",
      "Epoch:  243  Training Loss:  59.9713  Training Accuracy:  0.7204\n",
      "Epoch:  244  Training Loss:  59.9806  Training Accuracy:  0.721164\n",
      "Epoch:  245  Training Loss:  59.9989  Training Accuracy:  0.722164\n",
      "Epoch:  246  Training Loss:  60.0145  Training Accuracy:  0.722222\n",
      "Epoch:  247  Training Loss:  59.9818  Training Accuracy:  0.723222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  60.0481  Training Accuracy:  0.724045\n",
      "Epoch:  249  Training Loss:  60.038  Training Accuracy:  0.724633\n",
      "Epoch:  250  Training Loss:  60.0794  Training Accuracy:  0.725573\n",
      "Epoch:  251  Training Loss:  60.1391  Training Accuracy:  0.726749\n",
      "Epoch:  252  Training Loss:  60.1612  Training Accuracy:  0.728337\n",
      "Epoch:  253  Training Loss:  60.1166  Training Accuracy:  0.729453\n",
      "Epoch:  254  Training Loss:  60.1867  Training Accuracy:  0.730218\n",
      "Epoch:  255  Training Loss:  60.1858  Training Accuracy:  0.731041\n",
      "Epoch:  256  Training Loss:  60.2171  Training Accuracy:  0.731805\n",
      "Epoch:  257  Training Loss:  60.2451  Training Accuracy:  0.732687\n",
      "Epoch:  258  Training Loss:  60.2626  Training Accuracy:  0.733627\n",
      "Epoch:  259  Training Loss:  60.2416  Training Accuracy:  0.734215\n",
      "Epoch:  260  Training Loss:  60.3025  Training Accuracy:  0.734509\n",
      "Epoch:  261  Training Loss:  60.3008  Training Accuracy:  0.735685\n",
      "Epoch:  262  Training Loss:  60.3454  Training Accuracy:  0.736155\n",
      "Epoch:  263  Training Loss:  60.3409  Training Accuracy:  0.736978\n",
      "Epoch:  264  Training Loss:  60.3474  Training Accuracy:  0.73786\n",
      "Epoch:  265  Training Loss:  60.3805  Training Accuracy:  0.738801\n",
      "Epoch:  266  Training Loss:  60.4368  Training Accuracy:  0.739212\n",
      "Epoch:  267  Training Loss:  60.3638  Training Accuracy:  0.739859\n",
      "Epoch:  268  Training Loss:  60.3962  Training Accuracy:  0.740447\n",
      "Epoch:  269  Training Loss:  60.3901  Training Accuracy:  0.741152\n",
      "Epoch:  270  Training Loss:  60.4146  Training Accuracy:  0.742034\n",
      "Epoch:  271  Training Loss:  60.4452  Training Accuracy:  0.742269\n",
      "Epoch:  272  Training Loss:  60.4872  Training Accuracy:  0.743034\n",
      "Epoch:  273  Training Loss:  60.4896  Training Accuracy:  0.743798\n",
      "Epoch:  274  Training Loss:  60.5097  Training Accuracy:  0.74421\n",
      "Epoch:  275  Training Loss:  60.5181  Training Accuracy:  0.744797\n",
      "Epoch:  276  Training Loss:  60.6053  Training Accuracy:  0.745503\n",
      "Epoch:  277  Training Loss:  60.5648  Training Accuracy:  0.746149\n",
      "Epoch:  278  Training Loss:  60.6009  Training Accuracy:  0.74662\n",
      "Epoch:  279  Training Loss:  60.604  Training Accuracy:  0.747619\n",
      "Epoch:  280  Training Loss:  60.6416  Training Accuracy:  0.748383\n",
      "Epoch:  281  Training Loss:  60.6017  Training Accuracy:  0.749148\n",
      "Epoch:  282  Training Loss:  60.7035  Training Accuracy:  0.749736\n",
      "Epoch:  283  Training Loss:  60.6302  Training Accuracy:  0.750735\n",
      "Epoch:  284  Training Loss:  60.6904  Training Accuracy:  0.751264\n",
      "Epoch:  285  Training Loss:  60.6888  Training Accuracy:  0.752205\n",
      "Epoch:  286  Training Loss:  60.7248  Training Accuracy:  0.753028\n",
      "Epoch:  287  Training Loss:  60.6796  Training Accuracy:  0.753616\n",
      "Epoch:  288  Training Loss:  60.786  Training Accuracy:  0.754262\n",
      "Epoch:  289  Training Loss:  60.7581  Training Accuracy:  0.754674\n",
      "Epoch:  290  Training Loss:  60.8284  Training Accuracy:  0.755085\n",
      "Epoch:  291  Training Loss:  60.8557  Training Accuracy:  0.755497\n",
      "Epoch:  292  Training Loss:  60.8936  Training Accuracy:  0.756144\n",
      "Epoch:  293  Training Loss:  60.8976  Training Accuracy:  0.756732\n",
      "Epoch:  294  Training Loss:  61.0151  Training Accuracy:  0.757496\n",
      "Epoch:  295  Training Loss:  61.0119  Training Accuracy:  0.757966\n",
      "Epoch:  296  Training Loss:  61.1007  Training Accuracy:  0.758378\n",
      "Epoch:  297  Training Loss:  61.084  Training Accuracy:  0.759024\n",
      "Epoch:  298  Training Loss:  61.1093  Training Accuracy:  0.75973\n",
      "Epoch:  299  Training Loss:  61.1192  Training Accuracy:  0.760318\n",
      "Epoch:  300  Training Loss:  61.1498  Training Accuracy:  0.76067\n",
      "Epoch:  301  Training Loss:  61.1367  Training Accuracy:  0.761317\n",
      "Epoch:  302  Training Loss:  61.2341  Training Accuracy:  0.761729\n",
      "Epoch:  303  Training Loss:  61.2258  Training Accuracy:  0.762434\n",
      "Epoch:  304  Training Loss:  61.2605  Training Accuracy:  0.762669\n",
      "Epoch:  305  Training Loss:  61.2008  Training Accuracy:  0.763492\n",
      "Epoch:  306  Training Loss:  61.2606  Training Accuracy:  0.763963\n",
      "Epoch:  307  Training Loss:  61.2856  Training Accuracy:  0.764433\n",
      "Epoch:  308  Training Loss:  61.3321  Training Accuracy:  0.76508\n",
      "Epoch:  309  Training Loss:  61.2376  Training Accuracy:  0.766138\n",
      "Epoch:  310  Training Loss:  61.3414  Training Accuracy:  0.766608\n",
      "Epoch:  311  Training Loss:  61.3687  Training Accuracy:  0.767137\n",
      "Epoch:  312  Training Loss:  61.3093  Training Accuracy:  0.767725\n",
      "Epoch:  313  Training Loss:  61.3675  Training Accuracy:  0.768019\n",
      "Epoch:  314  Training Loss:  61.3668  Training Accuracy:  0.768842\n",
      "Epoch:  315  Training Loss:  61.3685  Training Accuracy:  0.769254\n",
      "Epoch:  316  Training Loss:  61.3456  Training Accuracy:  0.769783\n",
      "Epoch:  317  Training Loss:  61.3636  Training Accuracy:  0.770782\n",
      "Epoch:  318  Training Loss:  61.349  Training Accuracy:  0.771488\n",
      "Epoch:  319  Training Loss:  61.3877  Training Accuracy:  0.772311\n",
      "Epoch:  320  Training Loss:  61.3902  Training Accuracy:  0.772546\n",
      "Epoch:  321  Training Loss:  61.4086  Training Accuracy:  0.773134\n",
      "Epoch:  322  Training Loss:  61.3571  Training Accuracy:  0.773722\n",
      "Epoch:  323  Training Loss:  61.424  Training Accuracy:  0.773898\n",
      "Epoch:  324  Training Loss:  61.4308  Training Accuracy:  0.774251\n",
      "Epoch:  325  Training Loss:  61.4264  Training Accuracy:  0.774486\n",
      "Epoch:  326  Training Loss:  61.4244  Training Accuracy:  0.774956\n",
      "Epoch:  327  Training Loss:  61.482  Training Accuracy:  0.775368\n",
      "Epoch:  328  Training Loss:  61.4042  Training Accuracy:  0.775838\n",
      "Epoch:  329  Training Loss:  61.4688  Training Accuracy:  0.776602\n",
      "Epoch:  330  Training Loss:  61.4063  Training Accuracy:  0.777308\n",
      "Epoch:  331  Training Loss:  61.4767  Training Accuracy:  0.777719\n",
      "Epoch:  332  Training Loss:  61.4209  Training Accuracy:  0.778131\n",
      "Epoch:  333  Training Loss:  61.4553  Training Accuracy:  0.778601\n",
      "Epoch:  334  Training Loss:  61.4252  Training Accuracy:  0.779365\n",
      "Epoch:  335  Training Loss:  61.4875  Training Accuracy:  0.780247\n",
      "Epoch:  336  Training Loss:  61.4341  Training Accuracy:  0.780894\n",
      "Epoch:  337  Training Loss:  61.4905  Training Accuracy:  0.781247\n",
      "Epoch:  338  Training Loss:  61.4445  Training Accuracy:  0.781893\n",
      "Epoch:  339  Training Loss:  61.4483  Training Accuracy:  0.782422\n",
      "Epoch:  340  Training Loss:  61.4632  Training Accuracy:  0.783128\n",
      "Epoch:  341  Training Loss:  61.5009  Training Accuracy:  0.783657\n",
      "Epoch:  342  Training Loss:  61.475  Training Accuracy:  0.784127\n",
      "Epoch:  343  Training Loss:  61.4989  Training Accuracy:  0.784774\n",
      "Epoch:  344  Training Loss:  61.5086  Training Accuracy:  0.785362\n",
      "Epoch:  345  Training Loss:  61.5578  Training Accuracy:  0.786067\n",
      "Epoch:  346  Training Loss:  61.5576  Training Accuracy:  0.786596\n",
      "Epoch:  347  Training Loss:  61.5374  Training Accuracy:  0.786832\n",
      "Epoch:  348  Training Loss:  61.5196  Training Accuracy:  0.787772\n",
      "Epoch:  349  Training Loss:  61.4622  Training Accuracy:  0.788184\n",
      "Epoch:  350  Training Loss:  61.534  Training Accuracy:  0.788536\n",
      "Epoch:  351  Training Loss:  61.4561  Training Accuracy:  0.789007\n",
      "Epoch:  352  Training Loss:  61.5225  Training Accuracy:  0.789653\n",
      "Epoch:  353  Training Loss:  61.4817  Training Accuracy:  0.790359\n",
      "Epoch:  354  Training Loss:  61.5567  Training Accuracy:  0.790712\n",
      "Epoch:  355  Training Loss:  61.5326  Training Accuracy:  0.790888\n",
      "Epoch:  356  Training Loss:  61.5523  Training Accuracy:  0.791241\n",
      "Epoch:  357  Training Loss:  61.578  Training Accuracy:  0.791946\n",
      "Epoch:  358  Training Loss:  61.6113  Training Accuracy:  0.791887\n",
      "Epoch:  359  Training Loss:  61.5242  Training Accuracy:  0.792652\n",
      "Epoch:  360  Training Loss:  61.541  Training Accuracy:  0.793357\n",
      "Epoch:  361  Training Loss:  61.4951  Training Accuracy:  0.793886\n",
      "Epoch:  362  Training Loss:  61.538  Training Accuracy:  0.794827\n",
      "Epoch:  363  Training Loss:  61.5261  Training Accuracy:  0.795474\n",
      "Epoch:  364  Training Loss:  61.5535  Training Accuracy:  0.796297\n",
      "Epoch:  365  Training Loss:  61.5395  Training Accuracy:  0.797178\n",
      "Epoch:  366  Training Loss:  61.5787  Training Accuracy:  0.797649\n",
      "Epoch:  367  Training Loss:  61.5394  Training Accuracy:  0.798413\n",
      "Epoch:  368  Training Loss:  61.5687  Training Accuracy:  0.798883\n",
      "Epoch:  369  Training Loss:  61.5692  Training Accuracy:  0.799354\n",
      "Epoch:  370  Training Loss:  61.5758  Training Accuracy:  0.800177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  61.5304  Training Accuracy:  0.800647\n",
      "Epoch:  372  Training Loss:  61.5548  Training Accuracy:  0.801235\n",
      "Epoch:  373  Training Loss:  61.5657  Training Accuracy:  0.801999\n",
      "Epoch:  374  Training Loss:  61.5564  Training Accuracy:  0.802352\n",
      "Epoch:  375  Training Loss:  61.5373  Training Accuracy:  0.802999\n",
      "Epoch:  376  Training Loss:  61.5564  Training Accuracy:  0.803586\n",
      "Epoch:  377  Training Loss:  61.5779  Training Accuracy:  0.804645\n",
      "Epoch:  378  Training Loss:  61.6266  Training Accuracy:  0.805233\n",
      "Epoch:  379  Training Loss:  61.5388  Training Accuracy:  0.805879\n",
      "Epoch:  380  Training Loss:  61.5871  Training Accuracy:  0.806056\n",
      "Epoch:  381  Training Loss:  61.545  Training Accuracy:  0.806585\n",
      "Epoch:  382  Training Loss:  61.5559  Training Accuracy:  0.807055\n",
      "Epoch:  383  Training Loss:  61.595  Training Accuracy:  0.807231\n",
      "Epoch:  384  Training Loss:  61.543  Training Accuracy:  0.807819\n",
      "Epoch:  385  Training Loss:  61.5262  Training Accuracy:  0.808642\n",
      "Epoch:  386  Training Loss:  61.5556  Training Accuracy:  0.809113\n",
      "Epoch:  387  Training Loss:  61.5579  Training Accuracy:  0.8097\n",
      "Epoch:  388  Training Loss:  61.5409  Training Accuracy:  0.809994\n",
      "Epoch:  389  Training Loss:  61.4799  Training Accuracy:  0.810406\n",
      "Epoch:  390  Training Loss:  61.5467  Training Accuracy:  0.8107\n",
      "Epoch:  391  Training Loss:  61.5027  Training Accuracy:  0.810935\n",
      "Epoch:  392  Training Loss:  61.5715  Training Accuracy:  0.811464\n",
      "Epoch:  393  Training Loss:  61.5189  Training Accuracy:  0.811934\n",
      "Epoch:  394  Training Loss:  61.5956  Training Accuracy:  0.812522\n",
      "Epoch:  395  Training Loss:  61.4732  Training Accuracy:  0.813169\n",
      "Epoch:  396  Training Loss:  61.5403  Training Accuracy:  0.814168\n",
      "Epoch:  397  Training Loss:  61.4828  Training Accuracy:  0.814756\n",
      "Epoch:  398  Training Loss:  61.5016  Training Accuracy:  0.815109\n",
      "Epoch:  399  Training Loss:  61.4856  Training Accuracy:  0.815638\n",
      "Epoch:  400  Training Loss:  61.5023  Training Accuracy:  0.816108\n",
      "Epoch:  401  Training Loss:  61.43  Training Accuracy:  0.81652\n",
      "Epoch:  402  Training Loss:  61.5034  Training Accuracy:  0.817049\n",
      "Epoch:  403  Training Loss:  61.4316  Training Accuracy:  0.817519\n",
      "Epoch:  404  Training Loss:  61.4711  Training Accuracy:  0.817872\n",
      "Epoch:  405  Training Loss:  61.3603  Training Accuracy:  0.818401\n",
      "Epoch:  406  Training Loss:  61.4227  Training Accuracy:  0.819048\n",
      "Epoch:  407  Training Loss:  61.3893  Training Accuracy:  0.819459\n",
      "Epoch:  408  Training Loss:  61.4206  Training Accuracy:  0.819989\n",
      "Epoch:  409  Training Loss:  61.3672  Training Accuracy:  0.820576\n",
      "Epoch:  410  Training Loss:  61.3501  Training Accuracy:  0.820929\n",
      "Epoch:  411  Training Loss:  61.332  Training Accuracy:  0.821576\n",
      "Epoch:  412  Training Loss:  61.3639  Training Accuracy:  0.821752\n",
      "Epoch:  413  Training Loss:  61.2278  Training Accuracy:  0.822575\n",
      "Epoch:  414  Training Loss:  61.3581  Training Accuracy:  0.823104\n",
      "Epoch:  415  Training Loss:  61.2137  Training Accuracy:  0.823575\n",
      "Epoch:  416  Training Loss:  61.2806  Training Accuracy:  0.823986\n",
      "Epoch:  417  Training Loss:  61.1999  Training Accuracy:  0.824456\n",
      "Epoch:  418  Training Loss:  61.223  Training Accuracy:  0.824633\n",
      "Epoch:  419  Training Loss:  61.1276  Training Accuracy:  0.825574\n",
      "Epoch:  420  Training Loss:  61.2521  Training Accuracy:  0.825867\n",
      "Epoch:  421  Training Loss:  61.1537  Training Accuracy:  0.826632\n",
      "Epoch:  422  Training Loss:  61.149  Training Accuracy:  0.827043\n",
      "Epoch:  423  Training Loss:  61.151  Training Accuracy:  0.827337\n",
      "Epoch:  424  Training Loss:  61.1185  Training Accuracy:  0.827807\n",
      "Epoch:  425  Training Loss:  61.1644  Training Accuracy:  0.828219\n",
      "Epoch:  426  Training Loss:  61.1461  Training Accuracy:  0.828689\n",
      "Epoch:  427  Training Loss:  61.1689  Training Accuracy:  0.829454\n",
      "Epoch:  428  Training Loss:  61.1152  Training Accuracy:  0.829983\n",
      "Epoch:  429  Training Loss:  61.1688  Training Accuracy:  0.830571\n",
      "Epoch:  430  Training Loss:  61.1434  Training Accuracy:  0.831158\n",
      "Epoch:  431  Training Loss:  61.1526  Training Accuracy:  0.831805\n",
      "Epoch:  432  Training Loss:  61.1446  Training Accuracy:  0.832746\n",
      "Epoch:  433  Training Loss:  61.1545  Training Accuracy:  0.83351\n",
      "Epoch:  434  Training Loss:  61.1364  Training Accuracy:  0.833804\n",
      "Epoch:  435  Training Loss:  61.0854  Training Accuracy:  0.833922\n",
      "Epoch:  436  Training Loss:  61.0804  Training Accuracy:  0.834451\n",
      "Epoch:  437  Training Loss:  61.0933  Training Accuracy:  0.834862\n",
      "Epoch:  438  Training Loss:  61.0713  Training Accuracy:  0.835509\n",
      "Epoch:  439  Training Loss:  61.0936  Training Accuracy:  0.835979\n",
      "Epoch:  440  Training Loss:  61.036  Training Accuracy:  0.836332\n",
      "Epoch:  441  Training Loss:  61.0797  Training Accuracy:  0.837037\n",
      "Epoch:  442  Training Loss:  60.9842  Training Accuracy:  0.837214\n",
      "Epoch:  443  Training Loss:  61.0201  Training Accuracy:  0.83739\n",
      "Epoch:  444  Training Loss:  60.9698  Training Accuracy:  0.838213\n",
      "Epoch:  445  Training Loss:  60.968  Training Accuracy:  0.838625\n",
      "Epoch:  446  Training Loss:  60.9271  Training Accuracy:  0.838919\n",
      "Epoch:  447  Training Loss:  60.9061  Training Accuracy:  0.839271\n",
      "Epoch:  448  Training Loss:  60.8737  Training Accuracy:  0.8398\n",
      "Epoch:  449  Training Loss:  60.8577  Training Accuracy:  0.840388\n",
      "Epoch:  450  Training Loss:  60.8049  Training Accuracy:  0.840565\n",
      "Epoch:  451  Training Loss:  60.7725  Training Accuracy:  0.841211\n",
      "Epoch:  452  Training Loss:  60.786  Training Accuracy:  0.841623\n",
      "Epoch:  453  Training Loss:  60.7387  Training Accuracy:  0.842093\n",
      "Epoch:  454  Training Loss:  60.7042  Training Accuracy:  0.842505\n",
      "Epoch:  455  Training Loss:  60.7291  Training Accuracy:  0.843151\n",
      "Epoch:  456  Training Loss:  60.6701  Training Accuracy:  0.843328\n",
      "Epoch:  457  Training Loss:  60.5852  Training Accuracy:  0.843857\n",
      "Epoch:  458  Training Loss:  60.6413  Training Accuracy:  0.84421\n",
      "Epoch:  459  Training Loss:  60.5552  Training Accuracy:  0.844915\n",
      "Epoch:  460  Training Loss:  60.5891  Training Accuracy:  0.845385\n",
      "Epoch:  461  Training Loss:  60.5577  Training Accuracy:  0.845856\n",
      "Epoch:  462  Training Loss:  60.632  Training Accuracy:  0.846385\n",
      "Epoch:  463  Training Loss:  60.4972  Training Accuracy:  0.846973\n",
      "Epoch:  464  Training Loss:  60.5981  Training Accuracy:  0.847443\n",
      "Epoch:  465  Training Loss:  60.5396  Training Accuracy:  0.847972\n",
      "Epoch:  466  Training Loss:  60.5154  Training Accuracy:  0.848384\n",
      "Epoch:  467  Training Loss:  60.4986  Training Accuracy:  0.849148\n",
      "Epoch:  468  Training Loss:  60.5043  Training Accuracy:  0.849324\n",
      "Epoch:  469  Training Loss:  60.4274  Training Accuracy:  0.849853\n",
      "Epoch:  470  Training Loss:  60.5431  Training Accuracy:  0.850206\n",
      "Epoch:  471  Training Loss:  60.4222  Training Accuracy:  0.850618\n",
      "Epoch:  472  Training Loss:  60.3728  Training Accuracy:  0.851206\n",
      "Epoch:  473  Training Loss:  60.3817  Training Accuracy:  0.851676\n",
      "Epoch:  474  Training Loss:  60.4338  Training Accuracy:  0.852264\n",
      "Epoch:  475  Training Loss:  60.3305  Training Accuracy:  0.852381\n",
      "Epoch:  476  Training Loss:  60.3655  Training Accuracy:  0.852852\n",
      "Epoch:  477  Training Loss:  60.2669  Training Accuracy:  0.853087\n",
      "Epoch:  478  Training Loss:  60.3441  Training Accuracy:  0.853616\n",
      "Epoch:  479  Training Loss:  60.4074  Training Accuracy:  0.85391\n",
      "Epoch:  480  Training Loss:  60.341  Training Accuracy:  0.854145\n",
      "Epoch:  481  Training Loss:  60.3621  Training Accuracy:  0.854498\n",
      "Epoch:  482  Training Loss:  60.357  Training Accuracy:  0.854792\n",
      "Epoch:  483  Training Loss:  60.349  Training Accuracy:  0.855086\n",
      "Epoch:  484  Training Loss:  60.3533  Training Accuracy:  0.855556\n",
      "Epoch:  485  Training Loss:  60.3544  Training Accuracy:  0.856144\n",
      "Epoch:  486  Training Loss:  60.3215  Training Accuracy:  0.856791\n",
      "Epoch:  487  Training Loss:  60.3306  Training Accuracy:  0.85679\n",
      "Epoch:  488  Training Loss:  60.3414  Training Accuracy:  0.857143\n",
      "Epoch:  489  Training Loss:  60.354  Training Accuracy:  0.857731\n",
      "Epoch:  490  Training Loss:  60.3903  Training Accuracy:  0.858201\n",
      "Epoch:  491  Training Loss:  60.3759  Training Accuracy:  0.858554\n",
      "Epoch:  492  Training Loss:  60.384  Training Accuracy:  0.859318\n",
      "Epoch:  493  Training Loss:  60.3693  Training Accuracy:  0.859554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  60.3745  Training Accuracy:  0.860142\n",
      "Epoch:  495  Training Loss:  60.3641  Training Accuracy:  0.860671\n",
      "Epoch:  496  Training Loss:  60.3937  Training Accuracy:  0.8612\n",
      "Epoch:  497  Training Loss:  60.3861  Training Accuracy:  0.861611\n",
      "Epoch:  498  Training Loss:  60.3791  Training Accuracy:  0.861788\n",
      "Epoch:  499  Training Loss:  60.3267  Training Accuracy:  0.862199\n",
      "Epoch:  500  Training Loss:  60.3876  Training Accuracy:  0.862846\n",
      "Epoch:  501  Training Loss:  60.3723  Training Accuracy:  0.863434\n",
      "Epoch:  502  Training Loss:  60.3899  Training Accuracy:  0.863963\n",
      "Epoch:  503  Training Loss:  60.3477  Training Accuracy:  0.864257\n",
      "Epoch:  504  Training Loss:  60.3225  Training Accuracy:  0.865197\n",
      "Epoch:  505  Training Loss:  60.3666  Training Accuracy:  0.865433\n",
      "Epoch:  506  Training Loss:  60.4021  Training Accuracy:  0.86602\n",
      "Epoch:  507  Training Loss:  60.3366  Training Accuracy:  0.866138\n",
      "Epoch:  508  Training Loss:  60.3545  Training Accuracy:  0.86655\n",
      "Epoch:  509  Training Loss:  60.399  Training Accuracy:  0.866843\n",
      "Epoch:  510  Training Loss:  60.3278  Training Accuracy:  0.86702\n",
      "Epoch:  511  Training Loss:  60.2613  Training Accuracy:  0.86749\n",
      "Epoch:  512  Training Loss:  60.3263  Training Accuracy:  0.868019\n",
      "Epoch:  513  Training Loss:  60.3278  Training Accuracy:  0.868372\n",
      "Epoch:  514  Training Loss:  60.3693  Training Accuracy:  0.868842\n",
      "Epoch:  515  Training Loss:  60.2503  Training Accuracy:  0.869371\n",
      "Epoch:  516  Training Loss:  60.3299  Training Accuracy:  0.869548\n",
      "Epoch:  517  Training Loss:  60.3002  Training Accuracy:  0.869783\n",
      "Epoch:  518  Training Loss:  60.3614  Training Accuracy:  0.870018\n",
      "Epoch:  519  Training Loss:  60.2166  Training Accuracy:  0.870488\n",
      "Epoch:  520  Training Loss:  60.3009  Training Accuracy:  0.871135\n",
      "Epoch:  521  Training Loss:  60.2421  Training Accuracy:  0.871547\n",
      "Epoch:  522  Training Loss:  60.3305  Training Accuracy:  0.871782\n",
      "Epoch:  523  Training Loss:  60.2506  Training Accuracy:  0.872252\n",
      "Epoch:  524  Training Loss:  60.3295  Training Accuracy:  0.872428\n",
      "Epoch:  525  Training Loss:  60.1856  Training Accuracy:  0.873134\n",
      "Epoch:  526  Training Loss:  60.393  Training Accuracy:  0.873487\n",
      "Epoch:  527  Training Loss:  60.1607  Training Accuracy:  0.873545\n",
      "Epoch:  528  Training Loss:  60.3631  Training Accuracy:  0.873957\n",
      "Epoch:  529  Training Loss:  60.1719  Training Accuracy:  0.874251\n",
      "Epoch:  530  Training Loss:  60.3532  Training Accuracy:  0.874662\n",
      "Epoch:  531  Training Loss:  60.2178  Training Accuracy:  0.875191\n",
      "Epoch:  532  Training Loss:  60.4257  Training Accuracy:  0.875544\n",
      "Epoch:  533  Training Loss:  60.2187  Training Accuracy:  0.87625\n",
      "Epoch:  534  Training Loss:  60.411  Training Accuracy:  0.876602\n",
      "Epoch:  535  Training Loss:  60.2433  Training Accuracy:  0.876955\n",
      "Epoch:  536  Training Loss:  60.4275  Training Accuracy:  0.877249\n",
      "Epoch:  537  Training Loss:  60.2667  Training Accuracy:  0.877425\n",
      "Epoch:  538  Training Loss:  60.3478  Training Accuracy:  0.877661\n",
      "Epoch:  539  Training Loss:  60.3044  Training Accuracy:  0.877837\n",
      "Epoch:  540  Training Loss:  60.3754  Training Accuracy:  0.878307\n",
      "Epoch:  541  Training Loss:  60.2985  Training Accuracy:  0.878778\n",
      "Epoch:  542  Training Loss:  60.3705  Training Accuracy:  0.879366\n",
      "Epoch:  543  Training Loss:  60.3613  Training Accuracy:  0.879659\n",
      "Epoch:  544  Training Loss:  60.3561  Training Accuracy:  0.880189\n",
      "Epoch:  545  Training Loss:  60.2623  Training Accuracy:  0.880718\n",
      "Epoch:  546  Training Loss:  60.4212  Training Accuracy:  0.881012\n",
      "Epoch:  547  Training Loss:  60.2636  Training Accuracy:  0.881423\n",
      "Epoch:  548  Training Loss:  60.3974  Training Accuracy:  0.882011\n",
      "Epoch:  549  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  550  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  551  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  552  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  553  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  554  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  555  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  556  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  557  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  558  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  559  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  560  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  561  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  562  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  563  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  564  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  565  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  566  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  567  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  568  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  569  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  570  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  571  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  572  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  573  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  574  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  575  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  576  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  577  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  578  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  579  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  580  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  581  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  582  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  583  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  584  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  585  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  586  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  587  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  588  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  589  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  590  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  591  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  592  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  593  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  594  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  595  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  596  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  597  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  598  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  599  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  600  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  601  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  602  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  603  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  604  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  605  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  606  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  607  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  608  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  609  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  610  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  611  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  612  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  613  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  614  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  615  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  616  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  617  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  618  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  619  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  620  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  621  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  622  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  623  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  624  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  625  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  626  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  627  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  628  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  629  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  630  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  631  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  632  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  633  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  634  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  635  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  636  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  637  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  638  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  639  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  640  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  641  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  642  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  643  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  644  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  645  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  646  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  647  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  648  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  649  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  650  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  651  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  652  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  653  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  654  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  655  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  656  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  657  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  658  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  659  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  660  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  661  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  662  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  663  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  664  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  665  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  666  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  667  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  668  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  669  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  670  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  671  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  672  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  673  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  674  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  675  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  676  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  677  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  678  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  679  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  680  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  681  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  682  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  683  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  684  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  685  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  686  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  687  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  688  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  689  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  690  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  691  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  692  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  693  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  694  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  695  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  696  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  697  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  698  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  699  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  700  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  701  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  702  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  703  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  704  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  705  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  706  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  707  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  708  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  709  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  710  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  711  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  712  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  713  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  714  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  715  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  716  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  717  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  718  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  719  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  720  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  721  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  722  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  723  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  724  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  725  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  726  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  727  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  728  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  729  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  730  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  731  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  732  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  733  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  734  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  735  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  736  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  737  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  738  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  739  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  740  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  741  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  742  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  743  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  744  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  745  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  746  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  747  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  748  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  749  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  750  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  751  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  752  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  753  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  754  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  755  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  756  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  757  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  758  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  759  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  760  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  761  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  762  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  763  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  764  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  765  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  766  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  767  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  768  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  769  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  770  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  771  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  772  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  773  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  774  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  775  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  776  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  777  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  778  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  779  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  780  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  781  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  782  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  783  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  784  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  785  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  786  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  787  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  788  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  789  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  790  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  791  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  792  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  793  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  794  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  795  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  796  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  797  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  798  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  799  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  800  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  801  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  802  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  803  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  804  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  805  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  806  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  807  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  808  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  809  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  810  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  811  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  812  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  813  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  814  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  815  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  816  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  817  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  818  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  819  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  820  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  821  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  822  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  823  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  824  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  825  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  826  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  827  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  828  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  829  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  830  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  831  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  832  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  833  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  834  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  835  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  836  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  837  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  838  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  839  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  840  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  841  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  842  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  843  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  844  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  845  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  846  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  847  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  848  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  849  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  850  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  851  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  852  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  853  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  854  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  855  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  856  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  857  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  858  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  859  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  860  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  861  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  862  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  863  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  864  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  865  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  866  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  867  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  868  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  869  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  870  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  871  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  872  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  873  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  874  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  875  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  876  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  877  Training Loss:  nan  Training Accuracy:  0.0266314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  878  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  879  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  880  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  881  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  882  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  883  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  884  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  885  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  886  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  887  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  888  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  889  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  890  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  891  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  892  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  893  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  894  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  895  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  896  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  897  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  898  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  899  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  900  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  901  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  902  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  903  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  904  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  905  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  906  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  907  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  908  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  909  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  910  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  911  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  912  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  913  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  914  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  915  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  916  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  917  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  918  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  919  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  920  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  921  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  922  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  923  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  924  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  925  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  926  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  927  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  928  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  929  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  930  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  931  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  932  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  933  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  934  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  935  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  936  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  937  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  938  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  939  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  940  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  941  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  942  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  943  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  944  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  945  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  946  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  947  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  948  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  949  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  950  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  951  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  952  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  953  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  954  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  955  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  956  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  957  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  958  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  959  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  960  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  961  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  962  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  963  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  964  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  965  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  966  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  967  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  968  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  969  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  970  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  971  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  972  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  973  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  974  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  975  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  976  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  977  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  978  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  979  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  980  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  981  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  982  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  983  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  984  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  985  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  986  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  987  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  988  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  989  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  990  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  991  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  992  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  993  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  994  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  995  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  996  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  997  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  998  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Epoch:  999  Training Loss:  nan  Training Accuracy:  0.0266314\n",
      "Testing Accuracy: 0.028811\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  104.313  Training Accuracy:  0.0436802\n",
      "Epoch:  1  Training Loss:  88.487  Training Accuracy:  0.047913\n",
      "Epoch:  2  Training Loss:  76.1234  Training Accuracy:  0.0475015\n",
      "Epoch:  3  Training Loss:  66.9187  Training Accuracy:  0.053204\n",
      "Epoch:  4  Training Loss:  60.2365  Training Accuracy:  0.0557907\n",
      "Epoch:  5  Training Loss:  54.7811  Training Accuracy:  0.0574956\n",
      "Epoch:  6  Training Loss:  49.6584  Training Accuracy:  0.0595532\n",
      "Epoch:  7  Training Loss:  45.3041  Training Accuracy:  0.0623751\n",
      "Epoch:  8  Training Loss:  41.7439  Training Accuracy:  0.065726\n",
      "Epoch:  9  Training Loss:  38.7723  Training Accuracy:  0.0701352\n",
      "Epoch:  10  Training Loss:  36.2648  Training Accuracy:  0.0756614\n",
      "Epoch:  11  Training Loss:  33.9904  Training Accuracy:  0.0824221\n",
      "Epoch:  12  Training Loss:  31.9361  Training Accuracy:  0.0905938\n",
      "Epoch:  13  Training Loss:  30.2027  Training Accuracy:  0.0984715\n",
      "Epoch:  14  Training Loss:  28.7777  Training Accuracy:  0.106937\n",
      "Epoch:  15  Training Loss:  27.6334  Training Accuracy:  0.11505\n",
      "Epoch:  16  Training Loss:  26.5631  Training Accuracy:  0.122222\n",
      "Epoch:  17  Training Loss:  25.6744  Training Accuracy:  0.129747\n",
      "Epoch:  18  Training Loss:  24.965  Training Accuracy:  0.137743\n",
      "Epoch:  19  Training Loss:  24.4718  Training Accuracy:  0.144268\n",
      "Epoch:  20  Training Loss:  24.0486  Training Accuracy:  0.151499\n",
      "Epoch:  21  Training Loss:  23.6266  Training Accuracy:  0.159318\n",
      "Epoch:  22  Training Loss:  23.2411  Training Accuracy:  0.16649\n",
      "Epoch:  23  Training Loss:  22.9222  Training Accuracy:  0.172546\n",
      "Epoch:  24  Training Loss:  22.5795  Training Accuracy:  0.179071\n",
      "Epoch:  25  Training Loss:  22.268  Training Accuracy:  0.185597\n",
      "Epoch:  26  Training Loss:  22.0129  Training Accuracy:  0.19124\n",
      "Epoch:  27  Training Loss:  21.7986  Training Accuracy:  0.196296\n",
      "Epoch:  28  Training Loss:  21.6266  Training Accuracy:  0.201999\n",
      "Epoch:  29  Training Loss:  21.4352  Training Accuracy:  0.207936\n",
      "Epoch:  30  Training Loss:  21.2675  Training Accuracy:  0.212522\n",
      "Epoch:  31  Training Loss:  21.0964  Training Accuracy:  0.217813\n",
      "Epoch:  32  Training Loss:  20.9464  Training Accuracy:  0.222634\n",
      "Epoch:  33  Training Loss:  20.8669  Training Accuracy:  0.228219\n",
      "Epoch:  34  Training Loss:  20.7563  Training Accuracy:  0.233216\n",
      "Epoch:  35  Training Loss:  20.7239  Training Accuracy:  0.238213\n",
      "Epoch:  36  Training Loss:  20.523  Training Accuracy:  0.243739\n",
      "Epoch:  37  Training Loss:  20.3994  Training Accuracy:  0.247913\n",
      "Epoch:  38  Training Loss:  20.2096  Training Accuracy:  0.252205\n",
      "Epoch:  39  Training Loss:  19.9966  Training Accuracy:  0.255849\n",
      "Epoch:  40  Training Loss:  19.8686  Training Accuracy:  0.259847\n",
      "Epoch:  41  Training Loss:  19.6956  Training Accuracy:  0.264139\n",
      "Epoch:  42  Training Loss:  19.5439  Training Accuracy:  0.268489\n",
      "Epoch:  43  Training Loss:  19.4192  Training Accuracy:  0.272428\n",
      "Epoch:  44  Training Loss:  19.3337  Training Accuracy:  0.276308\n",
      "Epoch:  45  Training Loss:  19.2259  Training Accuracy:  0.279718\n",
      "Epoch:  46  Training Loss:  19.1415  Training Accuracy:  0.282775\n",
      "Epoch:  47  Training Loss:  19.0463  Training Accuracy:  0.286714\n",
      "Epoch:  48  Training Loss:  19.0014  Training Accuracy:  0.289947\n",
      "Epoch:  49  Training Loss:  18.9391  Training Accuracy:  0.29371\n",
      "Epoch:  50  Training Loss:  18.8623  Training Accuracy:  0.297825\n",
      "Epoch:  51  Training Loss:  18.7532  Training Accuracy:  0.301822\n",
      "Epoch:  52  Training Loss:  18.662  Training Accuracy:  0.305056\n",
      "Epoch:  53  Training Loss:  18.5537  Training Accuracy:  0.30823\n",
      "Epoch:  54  Training Loss:  18.4406  Training Accuracy:  0.312052\n",
      "Epoch:  55  Training Loss:  18.3364  Training Accuracy:  0.315697\n",
      "Epoch:  56  Training Loss:  18.1652  Training Accuracy:  0.318166\n",
      "Epoch:  57  Training Loss:  18.053  Training Accuracy:  0.321634\n",
      "Epoch:  58  Training Loss:  17.9227  Training Accuracy:  0.324221\n",
      "Epoch:  59  Training Loss:  17.7842  Training Accuracy:  0.327278\n",
      "Epoch:  60  Training Loss:  17.6437  Training Accuracy:  0.330629\n",
      "Epoch:  61  Training Loss:  17.5189  Training Accuracy:  0.334333\n",
      "Epoch:  62  Training Loss:  17.3741  Training Accuracy:  0.336861\n",
      "Epoch:  63  Training Loss:  17.2397  Training Accuracy:  0.340035\n",
      "Epoch:  64  Training Loss:  17.112  Training Accuracy:  0.343445\n",
      "Epoch:  65  Training Loss:  16.9758  Training Accuracy:  0.345855\n",
      "Epoch:  66  Training Loss:  16.8391  Training Accuracy:  0.348971\n",
      "Epoch:  67  Training Loss:  16.725  Training Accuracy:  0.35144\n",
      "Epoch:  68  Training Loss:  16.6151  Training Accuracy:  0.354615\n",
      "Epoch:  69  Training Loss:  16.3704  Training Accuracy:  0.357143\n",
      "Epoch:  70  Training Loss:  16.3325  Training Accuracy:  0.359788\n",
      "Epoch:  71  Training Loss:  16.2559  Training Accuracy:  0.362199\n",
      "Epoch:  72  Training Loss:  16.1693  Training Accuracy:  0.36502\n",
      "Epoch:  73  Training Loss:  16.0913  Training Accuracy:  0.368313\n",
      "Epoch:  74  Training Loss:  15.9671  Training Accuracy:  0.371605\n",
      "Epoch:  75  Training Loss:  15.8564  Training Accuracy:  0.374015\n",
      "Epoch:  76  Training Loss:  15.7391  Training Accuracy:  0.376543\n",
      "Epoch:  77  Training Loss:  15.6358  Training Accuracy:  0.379189\n",
      "Epoch:  78  Training Loss:  15.5384  Training Accuracy:  0.382069\n",
      "Epoch:  79  Training Loss:  15.4089  Training Accuracy:  0.384303\n",
      "Epoch:  80  Training Loss:  15.3246  Training Accuracy:  0.387595\n",
      "Epoch:  81  Training Loss:  15.2837  Training Accuracy:  0.390182\n",
      "Epoch:  82  Training Loss:  15.2086  Training Accuracy:  0.392475\n",
      "Epoch:  83  Training Loss:  15.1278  Training Accuracy:  0.39512\n",
      "Epoch:  84  Training Loss:  15.0647  Training Accuracy:  0.396766\n",
      "Epoch:  85  Training Loss:  14.9939  Training Accuracy:  0.399471\n",
      "Epoch:  86  Training Loss:  14.9357  Training Accuracy:  0.402116\n",
      "Epoch:  87  Training Loss:  14.8666  Training Accuracy:  0.405056\n",
      "Epoch:  88  Training Loss:  14.7771  Training Accuracy:  0.40729\n",
      "Epoch:  89  Training Loss:  14.6891  Training Accuracy:  0.4097\n",
      "Epoch:  90  Training Loss:  14.6018  Training Accuracy:  0.411816\n",
      "Epoch:  91  Training Loss:  14.5288  Training Accuracy:  0.414638\n",
      "Epoch:  92  Training Loss:  14.4716  Training Accuracy:  0.417401\n",
      "Epoch:  93  Training Loss:  14.4124  Training Accuracy:  0.4204\n",
      "Epoch:  94  Training Loss:  14.3512  Training Accuracy:  0.422634\n",
      "Epoch:  95  Training Loss:  14.3021  Training Accuracy:  0.425691\n",
      "Epoch:  96  Training Loss:  14.2491  Training Accuracy:  0.428924\n",
      "Epoch:  97  Training Loss:  14.202  Training Accuracy:  0.431805\n",
      "Epoch:  98  Training Loss:  14.152  Training Accuracy:  0.434627\n",
      "Epoch:  99  Training Loss:  14.0796  Training Accuracy:  0.436449\n",
      "Epoch:  100  Training Loss:  14.0188  Training Accuracy:  0.438624\n",
      "Epoch:  101  Training Loss:  13.9273  Training Accuracy:  0.440623\n",
      "Epoch:  102  Training Loss:  13.8487  Training Accuracy:  0.442445\n",
      "Epoch:  103  Training Loss:  13.771  Training Accuracy:  0.445326\n",
      "Epoch:  104  Training Loss:  13.7022  Training Accuracy:  0.447325\n",
      "Epoch:  105  Training Loss:  13.6305  Training Accuracy:  0.4495\n",
      "Epoch:  106  Training Loss:  13.5867  Training Accuracy:  0.45191\n",
      "Epoch:  107  Training Loss:  13.5231  Training Accuracy:  0.453674\n",
      "Epoch:  108  Training Loss:  13.469  Training Accuracy:  0.455732\n",
      "Epoch:  109  Training Loss:  13.4063  Training Accuracy:  0.458319\n",
      "Epoch:  110  Training Loss:  13.3666  Training Accuracy:  0.460905\n",
      "Epoch:  111  Training Loss:  13.3113  Training Accuracy:  0.463668\n",
      "Epoch:  112  Training Loss:  13.2392  Training Accuracy:  0.465373\n",
      "Epoch:  113  Training Loss:  13.1908  Training Accuracy:  0.467901\n",
      "Epoch:  114  Training Loss:  13.1582  Training Accuracy:  0.470253\n",
      "Epoch:  115  Training Loss:  13.1259  Training Accuracy:  0.472722\n",
      "Epoch:  116  Training Loss:  13.0746  Training Accuracy:  0.474721\n",
      "Epoch:  117  Training Loss:  13.0475  Training Accuracy:  0.476896\n",
      "Epoch:  118  Training Loss:  13.0238  Training Accuracy:  0.479012\n",
      "Epoch:  119  Training Loss:  12.9841  Training Accuracy:  0.480776\n",
      "Epoch:  120  Training Loss:  12.9592  Training Accuracy:  0.482481\n",
      "Epoch:  121  Training Loss:  12.9308  Training Accuracy:  0.48495\n",
      "Epoch:  122  Training Loss:  12.909  Training Accuracy:  0.486949\n",
      "Epoch:  123  Training Loss:  12.8942  Training Accuracy:  0.488948\n",
      "Epoch:  124  Training Loss:  12.8661  Training Accuracy:  0.490888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  12.8617  Training Accuracy:  0.492592\n",
      "Epoch:  126  Training Loss:  12.8336  Training Accuracy:  0.494062\n",
      "Epoch:  127  Training Loss:  12.8134  Training Accuracy:  0.495179\n",
      "Epoch:  128  Training Loss:  12.794  Training Accuracy:  0.497707\n",
      "Epoch:  129  Training Loss:  12.7971  Training Accuracy:  0.499294\n",
      "Epoch:  130  Training Loss:  12.7889  Training Accuracy:  0.500823\n",
      "Epoch:  131  Training Loss:  12.7941  Training Accuracy:  0.502645\n",
      "Epoch:  132  Training Loss:  12.7875  Training Accuracy:  0.503939\n",
      "Epoch:  133  Training Loss:  12.788  Training Accuracy:  0.505526\n",
      "Epoch:  134  Training Loss:  12.7811  Training Accuracy:  0.506996\n",
      "Epoch:  135  Training Loss:  12.7623  Training Accuracy:  0.508642\n",
      "Epoch:  136  Training Loss:  12.7541  Training Accuracy:  0.509582\n",
      "Epoch:  137  Training Loss:  12.7206  Training Accuracy:  0.51117\n",
      "Epoch:  138  Training Loss:  12.722  Training Accuracy:  0.513169\n",
      "Epoch:  139  Training Loss:  12.7032  Training Accuracy:  0.514403\n",
      "Epoch:  140  Training Loss:  12.7086  Training Accuracy:  0.51599\n",
      "Epoch:  141  Training Loss:  12.6892  Training Accuracy:  0.516755\n",
      "Epoch:  142  Training Loss:  12.6941  Training Accuracy:  0.518754\n",
      "Epoch:  143  Training Loss:  12.6873  Training Accuracy:  0.520106\n",
      "Epoch:  144  Training Loss:  12.6893  Training Accuracy:  0.521399\n",
      "Epoch:  145  Training Loss:  12.6566  Training Accuracy:  0.522751\n",
      "Epoch:  146  Training Loss:  12.6486  Training Accuracy:  0.524574\n",
      "Epoch:  147  Training Loss:  12.6354  Training Accuracy:  0.526102\n",
      "Epoch:  148  Training Loss:  12.6446  Training Accuracy:  0.527337\n",
      "Epoch:  149  Training Loss:  12.6113  Training Accuracy:  0.528571\n",
      "Epoch:  150  Training Loss:  12.6279  Training Accuracy:  0.53057\n",
      "Epoch:  151  Training Loss:  12.6124  Training Accuracy:  0.532393\n",
      "Epoch:  152  Training Loss:  12.6231  Training Accuracy:  0.533745\n",
      "Epoch:  153  Training Loss:  12.6044  Training Accuracy:  0.53592\n",
      "Epoch:  154  Training Loss:  12.6039  Training Accuracy:  0.537213\n",
      "Epoch:  155  Training Loss:  12.5811  Training Accuracy:  0.538683\n",
      "Epoch:  156  Training Loss:  12.6019  Training Accuracy:  0.540917\n",
      "Epoch:  157  Training Loss:  12.6032  Training Accuracy:  0.542622\n",
      "Epoch:  158  Training Loss:  12.6205  Training Accuracy:  0.543798\n",
      "Epoch:  159  Training Loss:  12.6451  Training Accuracy:  0.544797\n",
      "Epoch:  160  Training Loss:  12.6769  Training Accuracy:  0.546208\n",
      "Epoch:  161  Training Loss:  12.6814  Training Accuracy:  0.547795\n",
      "Epoch:  162  Training Loss:  12.673  Training Accuracy:  0.548795\n",
      "Epoch:  163  Training Loss:  12.7039  Training Accuracy:  0.550264\n",
      "Epoch:  164  Training Loss:  12.6961  Training Accuracy:  0.551146\n",
      "Epoch:  165  Training Loss:  12.7014  Training Accuracy:  0.553086\n",
      "Epoch:  166  Training Loss:  12.6985  Training Accuracy:  0.554674\n",
      "Epoch:  167  Training Loss:  12.6865  Training Accuracy:  0.556085\n",
      "Epoch:  168  Training Loss:  12.6855  Training Accuracy:  0.557554\n",
      "Epoch:  169  Training Loss:  12.6453  Training Accuracy:  0.558671\n",
      "Epoch:  170  Training Loss:  12.6796  Training Accuracy:  0.560317\n",
      "Epoch:  171  Training Loss:  12.6383  Training Accuracy:  0.561552\n",
      "Epoch:  172  Training Loss:  12.6495  Training Accuracy:  0.563786\n",
      "Epoch:  173  Training Loss:  12.6312  Training Accuracy:  0.565961\n",
      "Epoch:  174  Training Loss:  12.6492  Training Accuracy:  0.568078\n",
      "Epoch:  175  Training Loss:  12.639  Training Accuracy:  0.569724\n",
      "Epoch:  176  Training Loss:  12.6483  Training Accuracy:  0.570958\n",
      "Epoch:  177  Training Loss:  12.6536  Training Accuracy:  0.573133\n",
      "Epoch:  178  Training Loss:  12.6753  Training Accuracy:  0.574603\n",
      "Epoch:  179  Training Loss:  12.6788  Training Accuracy:  0.575897\n",
      "Epoch:  180  Training Loss:  12.6914  Training Accuracy:  0.578013\n",
      "Epoch:  181  Training Loss:  12.6978  Training Accuracy:  0.579424\n",
      "Epoch:  182  Training Loss:  12.6971  Training Accuracy:  0.581246\n",
      "Epoch:  183  Training Loss:  12.6879  Training Accuracy:  0.583657\n",
      "Epoch:  184  Training Loss:  12.6962  Training Accuracy:  0.585655\n",
      "Epoch:  185  Training Loss:  12.7127  Training Accuracy:  0.587243\n",
      "Epoch:  186  Training Loss:  12.7134  Training Accuracy:  0.588948\n",
      "Epoch:  187  Training Loss:  12.7171  Training Accuracy:  0.590535\n",
      "Epoch:  188  Training Loss:  12.7142  Training Accuracy:  0.592416\n",
      "Epoch:  189  Training Loss:  12.7214  Training Accuracy:  0.592887\n",
      "Epoch:  190  Training Loss:  12.7397  Training Accuracy:  0.594885\n",
      "Epoch:  191  Training Loss:  12.7446  Training Accuracy:  0.596061\n",
      "Epoch:  192  Training Loss:  12.7595  Training Accuracy:  0.597472\n",
      "Epoch:  193  Training Loss:  12.7675  Training Accuracy:  0.598413\n",
      "Epoch:  194  Training Loss:  12.7734  Training Accuracy:  0.599765\n",
      "Epoch:  195  Training Loss:  12.7731  Training Accuracy:  0.601176\n",
      "Epoch:  196  Training Loss:  12.8096  Training Accuracy:  0.602939\n",
      "Epoch:  197  Training Loss:  12.7821  Training Accuracy:  0.604644\n",
      "Epoch:  198  Training Loss:  12.8166  Training Accuracy:  0.606173\n",
      "Epoch:  199  Training Loss:  12.7763  Training Accuracy:  0.607701\n",
      "Epoch:  200  Training Loss:  12.8228  Training Accuracy:  0.608877\n",
      "Epoch:  201  Training Loss:  12.7635  Training Accuracy:  0.609994\n",
      "Epoch:  202  Training Loss:  12.8422  Training Accuracy:  0.611346\n",
      "Epoch:  203  Training Loss:  12.7701  Training Accuracy:  0.612757\n",
      "Epoch:  204  Training Loss:  12.8213  Training Accuracy:  0.614051\n",
      "Epoch:  205  Training Loss:  12.7894  Training Accuracy:  0.615403\n",
      "Epoch:  206  Training Loss:  12.8157  Training Accuracy:  0.616755\n",
      "Epoch:  207  Training Loss:  12.7851  Training Accuracy:  0.618048\n",
      "Epoch:  208  Training Loss:  12.8478  Training Accuracy:  0.619518\n",
      "Epoch:  209  Training Loss:  12.7841  Training Accuracy:  0.621634\n",
      "Epoch:  210  Training Loss:  12.8359  Training Accuracy:  0.62328\n",
      "Epoch:  211  Training Loss:  12.8192  Training Accuracy:  0.624456\n",
      "Epoch:  212  Training Loss:  12.8465  Training Accuracy:  0.625867\n",
      "Epoch:  213  Training Loss:  12.8257  Training Accuracy:  0.627337\n",
      "Epoch:  214  Training Loss:  12.8487  Training Accuracy:  0.628513\n",
      "Epoch:  215  Training Loss:  12.8106  Training Accuracy:  0.629571\n",
      "Epoch:  216  Training Loss:  12.8442  Training Accuracy:  0.631452\n",
      "Epoch:  217  Training Loss:  12.8015  Training Accuracy:  0.632569\n",
      "Epoch:  218  Training Loss:  12.8145  Training Accuracy:  0.634333\n",
      "Epoch:  219  Training Loss:  12.7993  Training Accuracy:  0.635567\n",
      "Epoch:  220  Training Loss:  12.7908  Training Accuracy:  0.637213\n",
      "Epoch:  221  Training Loss:  12.8191  Training Accuracy:  0.638624\n",
      "Epoch:  222  Training Loss:  12.8086  Training Accuracy:  0.639153\n",
      "Epoch:  223  Training Loss:  12.7471  Training Accuracy:  0.640506\n",
      "Epoch:  224  Training Loss:  12.7639  Training Accuracy:  0.641917\n",
      "Epoch:  225  Training Loss:  12.7455  Training Accuracy:  0.643798\n",
      "Epoch:  226  Training Loss:  12.7768  Training Accuracy:  0.64515\n",
      "Epoch:  227  Training Loss:  12.7556  Training Accuracy:  0.646443\n",
      "Epoch:  228  Training Loss:  12.7564  Training Accuracy:  0.648031\n",
      "Epoch:  229  Training Loss:  12.7453  Training Accuracy:  0.649794\n",
      "Epoch:  230  Training Loss:  12.7426  Training Accuracy:  0.65144\n",
      "Epoch:  231  Training Loss:  12.7346  Training Accuracy:  0.652793\n",
      "Epoch:  232  Training Loss:  12.7247  Training Accuracy:  0.654262\n",
      "Epoch:  233  Training Loss:  12.7292  Training Accuracy:  0.656026\n",
      "Epoch:  234  Training Loss:  12.7363  Training Accuracy:  0.657202\n",
      "Epoch:  235  Training Loss:  12.7327  Training Accuracy:  0.65779\n",
      "Epoch:  236  Training Loss:  12.7491  Training Accuracy:  0.659612\n",
      "Epoch:  237  Training Loss:  12.7141  Training Accuracy:  0.661199\n",
      "Epoch:  238  Training Loss:  12.7284  Training Accuracy:  0.66261\n",
      "Epoch:  239  Training Loss:  12.7102  Training Accuracy:  0.66408\n",
      "Epoch:  240  Training Loss:  12.7201  Training Accuracy:  0.665256\n",
      "Epoch:  241  Training Loss:  12.6939  Training Accuracy:  0.666432\n",
      "Epoch:  242  Training Loss:  12.679  Training Accuracy:  0.667901\n",
      "Epoch:  243  Training Loss:  12.6709  Training Accuracy:  0.669312\n",
      "Epoch:  244  Training Loss:  12.6547  Training Accuracy:  0.670077\n",
      "Epoch:  245  Training Loss:  12.6491  Training Accuracy:  0.671135\n",
      "Epoch:  246  Training Loss:  12.6648  Training Accuracy:  0.672428\n",
      "Epoch:  247  Training Loss:  12.6399  Training Accuracy:  0.673957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  12.6369  Training Accuracy:  0.675485\n",
      "Epoch:  249  Training Loss:  12.6201  Training Accuracy:  0.676955\n",
      "Epoch:  250  Training Loss:  12.5989  Training Accuracy:  0.677778\n",
      "Epoch:  251  Training Loss:  12.5912  Training Accuracy:  0.679483\n",
      "Epoch:  252  Training Loss:  12.5819  Training Accuracy:  0.680717\n",
      "Epoch:  253  Training Loss:  12.5687  Training Accuracy:  0.682011\n",
      "Epoch:  254  Training Loss:  12.5634  Training Accuracy:  0.683186\n",
      "Epoch:  255  Training Loss:  12.5406  Training Accuracy:  0.684597\n",
      "Epoch:  256  Training Loss:  12.519  Training Accuracy:  0.68595\n",
      "Epoch:  257  Training Loss:  12.5133  Training Accuracy:  0.687066\n",
      "Epoch:  258  Training Loss:  12.4728  Training Accuracy:  0.688125\n",
      "Epoch:  259  Training Loss:  12.4643  Training Accuracy:  0.689477\n",
      "Epoch:  260  Training Loss:  12.4505  Training Accuracy:  0.690653\n",
      "Epoch:  261  Training Loss:  12.4387  Training Accuracy:  0.692358\n",
      "Epoch:  262  Training Loss:  12.4069  Training Accuracy:  0.693416\n",
      "Epoch:  263  Training Loss:  12.3851  Training Accuracy:  0.694356\n",
      "Epoch:  264  Training Loss:  12.3686  Training Accuracy:  0.695415\n",
      "Epoch:  265  Training Loss:  12.3343  Training Accuracy:  0.696649\n",
      "Epoch:  266  Training Loss:  12.3442  Training Accuracy:  0.697413\n",
      "Epoch:  267  Training Loss:  12.2896  Training Accuracy:  0.698942\n",
      "Epoch:  268  Training Loss:  12.2904  Training Accuracy:  0.700118\n",
      "Epoch:  269  Training Loss:  12.3082  Training Accuracy:  0.70147\n",
      "Epoch:  270  Training Loss:  12.2664  Training Accuracy:  0.701999\n",
      "Epoch:  271  Training Loss:  12.2494  Training Accuracy:  0.702763\n",
      "Epoch:  272  Training Loss:  12.2336  Training Accuracy:  0.704233\n",
      "Epoch:  273  Training Loss:  12.1254  Training Accuracy:  0.705585\n",
      "Epoch:  274  Training Loss:  12.149  Training Accuracy:  0.706878\n",
      "Epoch:  275  Training Loss:  12.1249  Training Accuracy:  0.707995\n",
      "Epoch:  276  Training Loss:  12.1036  Training Accuracy:  0.708936\n",
      "Epoch:  277  Training Loss:  12.1237  Training Accuracy:  0.709994\n",
      "Epoch:  278  Training Loss:  12.0931  Training Accuracy:  0.71117\n",
      "Epoch:  279  Training Loss:  12.1274  Training Accuracy:  0.711875\n",
      "Epoch:  280  Training Loss:  12.0767  Training Accuracy:  0.713639\n",
      "Epoch:  281  Training Loss:  12.0719  Training Accuracy:  0.714991\n",
      "Epoch:  282  Training Loss:  12.0326  Training Accuracy:  0.715932\n",
      "Epoch:  283  Training Loss:  12.0384  Training Accuracy:  0.717108\n",
      "Epoch:  284  Training Loss:  12.0197  Training Accuracy:  0.718519\n",
      "Epoch:  285  Training Loss:  11.9583  Training Accuracy:  0.719694\n",
      "Epoch:  286  Training Loss:  11.9495  Training Accuracy:  0.720635\n",
      "Epoch:  287  Training Loss:  11.8952  Training Accuracy:  0.721811\n",
      "Epoch:  288  Training Loss:  11.8947  Training Accuracy:  0.722458\n",
      "Epoch:  289  Training Loss:  11.8824  Training Accuracy:  0.723575\n",
      "Epoch:  290  Training Loss:  11.8967  Training Accuracy:  0.724339\n",
      "Epoch:  291  Training Loss:  11.9182  Training Accuracy:  0.72575\n",
      "Epoch:  292  Training Loss:  11.7987  Training Accuracy:  0.726926\n",
      "Epoch:  293  Training Loss:  11.8304  Training Accuracy:  0.727866\n",
      "Epoch:  294  Training Loss:  11.7788  Training Accuracy:  0.729277\n",
      "Epoch:  295  Training Loss:  11.7898  Training Accuracy:  0.730512\n",
      "Epoch:  296  Training Loss:  11.7507  Training Accuracy:  0.731393\n",
      "Epoch:  297  Training Loss:  11.8546  Training Accuracy:  0.732334\n",
      "Epoch:  298  Training Loss:  11.6522  Training Accuracy:  0.733569\n",
      "Epoch:  299  Training Loss:  11.7551  Training Accuracy:  0.734215\n",
      "Epoch:  300  Training Loss:  11.6361  Training Accuracy:  0.735509\n",
      "Epoch:  301  Training Loss:  11.6541  Training Accuracy:  0.736391\n",
      "Epoch:  302  Training Loss:  11.6027  Training Accuracy:  0.737684\n",
      "Epoch:  303  Training Loss:  11.7036  Training Accuracy:  0.738801\n",
      "Epoch:  304  Training Loss:  11.548  Training Accuracy:  0.740036\n",
      "Epoch:  305  Training Loss:  11.5682  Training Accuracy:  0.741152\n",
      "Epoch:  306  Training Loss:  11.5614  Training Accuracy:  0.742152\n",
      "Epoch:  307  Training Loss:  11.4794  Training Accuracy:  0.743504\n",
      "Epoch:  308  Training Loss:  11.59  Training Accuracy:  0.743974\n",
      "Epoch:  309  Training Loss:  11.4494  Training Accuracy:  0.745738\n",
      "Epoch:  310  Training Loss:  11.4332  Training Accuracy:  0.746855\n",
      "Epoch:  311  Training Loss:  11.4322  Training Accuracy:  0.747854\n",
      "Epoch:  312  Training Loss:  11.4391  Training Accuracy:  0.748501\n",
      "Epoch:  313  Training Loss:  11.4976  Training Accuracy:  0.749794\n",
      "Epoch:  314  Training Loss:  11.2948  Training Accuracy:  0.750794\n",
      "Epoch:  315  Training Loss:  11.3705  Training Accuracy:  0.751558\n",
      "Epoch:  316  Training Loss:  11.4409  Training Accuracy:  0.752734\n",
      "Epoch:  317  Training Loss:  11.2522  Training Accuracy:  0.753381\n",
      "Epoch:  318  Training Loss:  11.2653  Training Accuracy:  0.75438\n",
      "Epoch:  319  Training Loss:  11.3087  Training Accuracy:  0.755144\n",
      "Epoch:  320  Training Loss:  11.2023  Training Accuracy:  0.756614\n",
      "Epoch:  321  Training Loss:  11.3748  Training Accuracy:  0.757084\n",
      "Epoch:  322  Training Loss:  11.1676  Training Accuracy:  0.758554\n",
      "Epoch:  323  Training Loss:  11.1525  Training Accuracy:  0.759671\n",
      "Epoch:  324  Training Loss:  11.3707  Training Accuracy:  0.760906\n",
      "Epoch:  325  Training Loss:  11.0957  Training Accuracy:  0.76261\n",
      "Epoch:  326  Training Loss:  11.1517  Training Accuracy:  0.763198\n",
      "Epoch:  327  Training Loss:  11.0628  Training Accuracy:  0.764315\n",
      "Epoch:  328  Training Loss:  11.2806  Training Accuracy:  0.764844\n",
      "Epoch:  329  Training Loss:  11.0824  Training Accuracy:  0.765726\n",
      "Epoch:  330  Training Loss:  11.1356  Training Accuracy:  0.766608\n",
      "Epoch:  331  Training Loss:  10.9423  Training Accuracy:  0.767666\n",
      "Epoch:  332  Training Loss:  11.1817  Training Accuracy:  0.768431\n",
      "Epoch:  333  Training Loss:  11.0374  Training Accuracy:  0.768842\n",
      "Epoch:  334  Training Loss:  10.8558  Training Accuracy:  0.770371\n",
      "Epoch:  335  Training Loss:  11.1982  Training Accuracy:  0.770665\n",
      "Epoch:  336  Training Loss:  10.879  Training Accuracy:  0.771488\n",
      "Epoch:  337  Training Loss:  11.1175  Training Accuracy:  0.771958\n",
      "Epoch:  338  Training Loss:  10.7916  Training Accuracy:  0.773428\n",
      "Epoch:  339  Training Loss:  10.9779  Training Accuracy:  0.773545\n",
      "Epoch:  340  Training Loss:  10.7827  Training Accuracy:  0.774486\n",
      "Epoch:  341  Training Loss:  10.9733  Training Accuracy:  0.774897\n",
      "Epoch:  342  Training Loss:  10.8781  Training Accuracy:  0.775309\n",
      "Epoch:  343  Training Loss:  10.7464  Training Accuracy:  0.776367\n",
      "Epoch:  344  Training Loss:  10.7968  Training Accuracy:  0.776837\n",
      "Epoch:  345  Training Loss:  10.7929  Training Accuracy:  0.77766\n",
      "Epoch:  346  Training Loss:  10.6909  Training Accuracy:  0.778366\n",
      "Epoch:  347  Training Loss:  10.7565  Training Accuracy:  0.7796\n",
      "Epoch:  348  Training Loss:  10.6534  Training Accuracy:  0.78013\n",
      "Epoch:  349  Training Loss:  10.7034  Training Accuracy:  0.781893\n",
      "Epoch:  350  Training Loss:  10.6148  Training Accuracy:  0.781952\n",
      "Epoch:  351  Training Loss:  10.6011  Training Accuracy:  0.783128\n",
      "Epoch:  352  Training Loss:  10.5651  Training Accuracy:  0.783716\n",
      "Epoch:  353  Training Loss:  10.6777  Training Accuracy:  0.78448\n",
      "Epoch:  354  Training Loss:  10.4804  Training Accuracy:  0.785891\n",
      "Epoch:  355  Training Loss:  10.6601  Training Accuracy:  0.785773\n",
      "Epoch:  356  Training Loss:  10.3986  Training Accuracy:  0.787361\n",
      "Epoch:  357  Training Loss:  10.6886  Training Accuracy:  0.787243\n",
      "Epoch:  358  Training Loss:  10.411  Training Accuracy:  0.789183\n",
      "Epoch:  359  Training Loss:  10.5048  Training Accuracy:  0.789301\n",
      "Epoch:  360  Training Loss:  10.439  Training Accuracy:  0.790418\n",
      "Epoch:  361  Training Loss:  10.5635  Training Accuracy:  0.791123\n",
      "Epoch:  362  Training Loss:  10.3638  Training Accuracy:  0.791946\n",
      "Epoch:  363  Training Loss:  10.5517  Training Accuracy:  0.792593\n",
      "Epoch:  364  Training Loss:  10.3458  Training Accuracy:  0.793592\n",
      "Epoch:  365  Training Loss:  10.4961  Training Accuracy:  0.793416\n",
      "Epoch:  366  Training Loss:  10.2635  Training Accuracy:  0.794709\n",
      "Epoch:  367  Training Loss:  10.4557  Training Accuracy:  0.795356\n",
      "Epoch:  368  Training Loss:  10.2394  Training Accuracy:  0.796238\n",
      "Epoch:  369  Training Loss:  10.5522  Training Accuracy:  0.796179\n",
      "Epoch:  370  Training Loss:  10.2437  Training Accuracy:  0.797649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  10.3915  Training Accuracy:  0.798178\n",
      "Epoch:  372  Training Loss:  10.2973  Training Accuracy:  0.798237\n",
      "Epoch:  373  Training Loss:  10.229  Training Accuracy:  0.799236\n",
      "Epoch:  374  Training Loss:  10.2913  Training Accuracy:  0.799883\n",
      "Epoch:  375  Training Loss:  10.1325  Training Accuracy:  0.800823\n",
      "Epoch:  376  Training Loss:  10.401  Training Accuracy:  0.801117\n",
      "Epoch:  377  Training Loss:  10.0446  Training Accuracy:  0.802352\n",
      "Epoch:  378  Training Loss:  10.241  Training Accuracy:  0.802646\n",
      "Epoch:  379  Training Loss:  10.2537  Training Accuracy:  0.803645\n",
      "Epoch:  380  Training Loss:  10.1803  Training Accuracy:  0.803939\n",
      "Epoch:  381  Training Loss:  10.0958  Training Accuracy:  0.805644\n",
      "Epoch:  382  Training Loss:  10.0419  Training Accuracy:  0.805997\n",
      "Epoch:  383  Training Loss:  10.2988  Training Accuracy:  0.805644\n",
      "Epoch:  384  Training Loss:  9.97553  Training Accuracy:  0.807525\n",
      "Epoch:  385  Training Loss:  10.1376  Training Accuracy:  0.808466\n",
      "Epoch:  386  Training Loss:  10.1058  Training Accuracy:  0.808407\n",
      "Epoch:  387  Training Loss:  9.96048  Training Accuracy:  0.809818\n",
      "Epoch:  388  Training Loss:  10.0967  Training Accuracy:  0.810288\n",
      "Epoch:  389  Training Loss:  9.94354  Training Accuracy:  0.811111\n",
      "Epoch:  390  Training Loss:  10.0721  Training Accuracy:  0.811347\n",
      "Epoch:  391  Training Loss:  9.89449  Training Accuracy:  0.812581\n",
      "Epoch:  392  Training Loss:  10.1695  Training Accuracy:  0.812699\n",
      "Epoch:  393  Training Loss:  9.84223  Training Accuracy:  0.814462\n",
      "Epoch:  394  Training Loss:  9.94049  Training Accuracy:  0.815168\n",
      "Epoch:  395  Training Loss:  9.81697  Training Accuracy:  0.815815\n",
      "Epoch:  396  Training Loss:  9.92396  Training Accuracy:  0.817108\n",
      "Epoch:  397  Training Loss:  10.0907  Training Accuracy:  0.816285\n",
      "Epoch:  398  Training Loss:  9.7344  Training Accuracy:  0.819401\n",
      "Epoch:  399  Training Loss:  9.98108  Training Accuracy:  0.818049\n",
      "Epoch:  400  Training Loss:  9.74558  Training Accuracy:  0.820224\n",
      "Epoch:  401  Training Loss:  9.80066  Training Accuracy:  0.821106\n",
      "Epoch:  402  Training Loss:  9.80304  Training Accuracy:  0.820224\n",
      "Epoch:  403  Training Loss:  9.79242  Training Accuracy:  0.82234\n",
      "Epoch:  404  Training Loss:  9.68799  Training Accuracy:  0.822869\n",
      "Epoch:  405  Training Loss:  9.79557  Training Accuracy:  0.823869\n",
      "Epoch:  406  Training Loss:  9.75306  Training Accuracy:  0.823869\n",
      "Epoch:  407  Training Loss:  9.73785  Training Accuracy:  0.825809\n",
      "Epoch:  408  Training Loss:  9.55989  Training Accuracy:  0.826808\n",
      "Epoch:  409  Training Loss:  9.68016  Training Accuracy:  0.82722\n",
      "Epoch:  410  Training Loss:  9.66606  Training Accuracy:  0.827455\n",
      "Epoch:  411  Training Loss:  9.60289  Training Accuracy:  0.828807\n",
      "Epoch:  412  Training Loss:  9.57306  Training Accuracy:  0.829336\n",
      "Epoch:  413  Training Loss:  9.51739  Training Accuracy:  0.830335\n",
      "Epoch:  414  Training Loss:  9.63758  Training Accuracy:  0.829983\n",
      "Epoch:  415  Training Loss:  9.4631  Training Accuracy:  0.83204\n",
      "Epoch:  416  Training Loss:  9.47486  Training Accuracy:  0.832805\n",
      "Epoch:  417  Training Loss:  9.47042  Training Accuracy:  0.833569\n",
      "Epoch:  418  Training Loss:  9.6124  Training Accuracy:  0.833686\n",
      "Epoch:  419  Training Loss:  9.44569  Training Accuracy:  0.835039\n",
      "Epoch:  420  Training Loss:  9.44769  Training Accuracy:  0.835509\n",
      "Epoch:  421  Training Loss:  9.45178  Training Accuracy:  0.836273\n",
      "Epoch:  422  Training Loss:  9.5713  Training Accuracy:  0.836743\n",
      "Epoch:  423  Training Loss:  9.40851  Training Accuracy:  0.83786\n",
      "Epoch:  424  Training Loss:  9.4163  Training Accuracy:  0.838037\n",
      "Epoch:  425  Training Loss:  9.40603  Training Accuracy:  0.839036\n",
      "Epoch:  426  Training Loss:  9.53113  Training Accuracy:  0.838801\n",
      "Epoch:  427  Training Loss:  9.3588  Training Accuracy:  0.840153\n",
      "Epoch:  428  Training Loss:  9.34719  Training Accuracy:  0.840859\n",
      "Epoch:  429  Training Loss:  9.47759  Training Accuracy:  0.8408\n",
      "Epoch:  430  Training Loss:  9.31561  Training Accuracy:  0.841976\n",
      "Epoch:  431  Training Loss:  9.31086  Training Accuracy:  0.842622\n",
      "Epoch:  432  Training Loss:  9.32842  Training Accuracy:  0.842916\n",
      "Epoch:  433  Training Loss:  9.29883  Training Accuracy:  0.843563\n",
      "Epoch:  434  Training Loss:  9.32291  Training Accuracy:  0.843857\n",
      "Epoch:  435  Training Loss:  9.25174  Training Accuracy:  0.845091\n",
      "Epoch:  436  Training Loss:  9.31516  Training Accuracy:  0.845033\n",
      "Epoch:  437  Training Loss:  9.26401  Training Accuracy:  0.845562\n",
      "Epoch:  438  Training Loss:  9.23347  Training Accuracy:  0.846267\n",
      "Epoch:  439  Training Loss:  9.24162  Training Accuracy:  0.846679\n",
      "Epoch:  440  Training Loss:  9.21814  Training Accuracy:  0.847267\n",
      "Epoch:  441  Training Loss:  9.23559  Training Accuracy:  0.847443\n",
      "Epoch:  442  Training Loss:  9.18206  Training Accuracy:  0.848207\n",
      "Epoch:  443  Training Loss:  9.22182  Training Accuracy:  0.848207\n",
      "Epoch:  444  Training Loss:  9.14621  Training Accuracy:  0.849501\n",
      "Epoch:  445  Training Loss:  9.19951  Training Accuracy:  0.849324\n",
      "Epoch:  446  Training Loss:  9.13868  Training Accuracy:  0.850618\n",
      "Epoch:  447  Training Loss:  9.11668  Training Accuracy:  0.850912\n",
      "Epoch:  448  Training Loss:  9.15477  Training Accuracy:  0.850618\n",
      "Epoch:  449  Training Loss:  9.08356  Training Accuracy:  0.85197\n",
      "Epoch:  450  Training Loss:  9.08684  Training Accuracy:  0.851793\n",
      "Epoch:  451  Training Loss:  9.08338  Training Accuracy:  0.85244\n",
      "Epoch:  452  Training Loss:  9.07857  Training Accuracy:  0.852499\n",
      "Epoch:  453  Training Loss:  9.03563  Training Accuracy:  0.853028\n",
      "Epoch:  454  Training Loss:  9.06927  Training Accuracy:  0.853204\n",
      "Epoch:  455  Training Loss:  8.98271  Training Accuracy:  0.854439\n",
      "Epoch:  456  Training Loss:  8.97931  Training Accuracy:  0.854909\n",
      "Epoch:  457  Training Loss:  9.02728  Training Accuracy:  0.854733\n",
      "Epoch:  458  Training Loss:  8.95902  Training Accuracy:  0.855497\n",
      "Epoch:  459  Training Loss:  8.96355  Training Accuracy:  0.856203\n",
      "Epoch:  460  Training Loss:  8.97167  Training Accuracy:  0.856261\n",
      "Epoch:  461  Training Loss:  8.94186  Training Accuracy:  0.856967\n",
      "Epoch:  462  Training Loss:  8.94661  Training Accuracy:  0.857378\n",
      "Epoch:  463  Training Loss:  8.91065  Training Accuracy:  0.858025\n",
      "Epoch:  464  Training Loss:  8.97311  Training Accuracy:  0.858143\n",
      "Epoch:  465  Training Loss:  8.89276  Training Accuracy:  0.858848\n",
      "Epoch:  466  Training Loss:  8.87086  Training Accuracy:  0.859495\n",
      "Epoch:  467  Training Loss:  8.91438  Training Accuracy:  0.85973\n",
      "Epoch:  468  Training Loss:  8.86527  Training Accuracy:  0.860377\n",
      "Epoch:  469  Training Loss:  8.92623  Training Accuracy:  0.860729\n",
      "Epoch:  470  Training Loss:  8.83876  Training Accuracy:  0.86167\n",
      "Epoch:  471  Training Loss:  8.84735  Training Accuracy:  0.862375\n",
      "Epoch:  472  Training Loss:  8.86393  Training Accuracy:  0.862787\n",
      "Epoch:  473  Training Loss:  8.84208  Training Accuracy:  0.863375\n",
      "Epoch:  474  Training Loss:  8.84758  Training Accuracy:  0.863845\n",
      "Epoch:  475  Training Loss:  8.81763  Training Accuracy:  0.864257\n",
      "Epoch:  476  Training Loss:  8.85667  Training Accuracy:  0.864551\n",
      "Epoch:  477  Training Loss:  8.78653  Training Accuracy:  0.86508\n",
      "Epoch:  478  Training Loss:  8.77886  Training Accuracy:  0.86555\n",
      "Epoch:  479  Training Loss:  8.80343  Training Accuracy:  0.865785\n",
      "Epoch:  480  Training Loss:  8.76271  Training Accuracy:  0.866256\n",
      "Epoch:  481  Training Loss:  8.81981  Training Accuracy:  0.866667\n",
      "Epoch:  482  Training Loss:  8.73339  Training Accuracy:  0.867079\n",
      "Epoch:  483  Training Loss:  8.72365  Training Accuracy:  0.867314\n",
      "Epoch:  484  Training Loss:  8.74926  Training Accuracy:  0.867431\n",
      "Epoch:  485  Training Loss:  8.72251  Training Accuracy:  0.86796\n",
      "Epoch:  486  Training Loss:  8.76193  Training Accuracy:  0.868078\n",
      "Epoch:  487  Training Loss:  8.6968  Training Accuracy:  0.86849\n",
      "Epoch:  488  Training Loss:  8.67607  Training Accuracy:  0.868842\n",
      "Epoch:  489  Training Loss:  8.72536  Training Accuracy:  0.869136\n",
      "Epoch:  490  Training Loss:  8.66548  Training Accuracy:  0.869489\n",
      "Epoch:  491  Training Loss:  8.66165  Training Accuracy:  0.869842\n",
      "Epoch:  492  Training Loss:  8.72413  Training Accuracy:  0.869901\n",
      "Epoch:  493  Training Loss:  8.64823  Training Accuracy:  0.870371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  8.67809  Training Accuracy:  0.870665\n",
      "Epoch:  495  Training Loss:  8.62016  Training Accuracy:  0.8709\n",
      "Epoch:  496  Training Loss:  8.59817  Training Accuracy:  0.871253\n",
      "Epoch:  497  Training Loss:  8.66621  Training Accuracy:  0.871723\n",
      "Epoch:  498  Training Loss:  8.58877  Training Accuracy:  0.871841\n",
      "Epoch:  499  Training Loss:  8.58116  Training Accuracy:  0.87237\n",
      "Epoch:  500  Training Loss:  8.61217  Training Accuracy:  0.872781\n",
      "Epoch:  501  Training Loss:  8.56334  Training Accuracy:  0.873134\n",
      "Epoch:  502  Training Loss:  8.54465  Training Accuracy:  0.873251\n",
      "Epoch:  503  Training Loss:  8.66144  Training Accuracy:  0.873487\n",
      "Epoch:  504  Training Loss:  8.53645  Training Accuracy:  0.874016\n",
      "Epoch:  505  Training Loss:  8.52263  Training Accuracy:  0.874368\n",
      "Epoch:  506  Training Loss:  8.56783  Training Accuracy:  0.874604\n",
      "Epoch:  507  Training Loss:  8.50972  Training Accuracy:  0.874956\n",
      "Epoch:  508  Training Loss:  8.48791  Training Accuracy:  0.875191\n",
      "Epoch:  509  Training Loss:  8.54621  Training Accuracy:  0.875368\n",
      "Epoch:  510  Training Loss:  8.46327  Training Accuracy:  0.875779\n",
      "Epoch:  511  Training Loss:  8.46016  Training Accuracy:  0.876308\n",
      "Epoch:  512  Training Loss:  8.51082  Training Accuracy:  0.876544\n",
      "Epoch:  513  Training Loss:  8.45364  Training Accuracy:  0.876838\n",
      "Epoch:  514  Training Loss:  8.42623  Training Accuracy:  0.877132\n",
      "Epoch:  515  Training Loss:  8.48364  Training Accuracy:  0.877719\n",
      "Epoch:  516  Training Loss:  8.41492  Training Accuracy:  0.878013\n",
      "Epoch:  517  Training Loss:  8.40516  Training Accuracy:  0.878249\n",
      "Epoch:  518  Training Loss:  8.44104  Training Accuracy:  0.878895\n",
      "Epoch:  519  Training Loss:  8.3982  Training Accuracy:  0.87913\n",
      "Epoch:  520  Training Loss:  8.36139  Training Accuracy:  0.879366\n",
      "Epoch:  521  Training Loss:  8.41283  Training Accuracy:  0.879895\n",
      "Epoch:  522  Training Loss:  8.35477  Training Accuracy:  0.880247\n",
      "Epoch:  523  Training Loss:  8.34669  Training Accuracy:  0.8806\n",
      "Epoch:  524  Training Loss:  8.38797  Training Accuracy:  0.881012\n",
      "Epoch:  525  Training Loss:  8.33586  Training Accuracy:  0.881129\n",
      "Epoch:  526  Training Loss:  8.30265  Training Accuracy:  0.881423\n",
      "Epoch:  527  Training Loss:  8.35762  Training Accuracy:  0.882364\n",
      "Epoch:  528  Training Loss:  8.29592  Training Accuracy:  0.882658\n",
      "Epoch:  529  Training Loss:  8.28287  Training Accuracy:  0.882716\n",
      "Epoch:  530  Training Loss:  8.31365  Training Accuracy:  0.88301\n",
      "Epoch:  531  Training Loss:  8.27551  Training Accuracy:  0.883246\n",
      "Epoch:  532  Training Loss:  8.29948  Training Accuracy:  0.883598\n",
      "Epoch:  533  Training Loss:  8.25359  Training Accuracy:  0.88401\n",
      "Epoch:  534  Training Loss:  8.28176  Training Accuracy:  0.884127\n",
      "Epoch:  535  Training Loss:  8.23233  Training Accuracy:  0.88448\n",
      "Epoch:  536  Training Loss:  8.20632  Training Accuracy:  0.884833\n",
      "Epoch:  537  Training Loss:  8.26433  Training Accuracy:  0.885244\n",
      "Epoch:  538  Training Loss:  8.20808  Training Accuracy:  0.885362\n",
      "Epoch:  539  Training Loss:  8.19207  Training Accuracy:  0.885597\n",
      "Epoch:  540  Training Loss:  8.2276  Training Accuracy:  0.886126\n",
      "Epoch:  541  Training Loss:  8.18063  Training Accuracy:  0.886067\n",
      "Epoch:  542  Training Loss:  8.20897  Training Accuracy:  0.886597\n",
      "Epoch:  543  Training Loss:  8.16501  Training Accuracy:  0.886773\n",
      "Epoch:  544  Training Loss:  8.14439  Training Accuracy:  0.887126\n",
      "Epoch:  545  Training Loss:  8.18055  Training Accuracy:  0.887537\n",
      "Epoch:  546  Training Loss:  8.12728  Training Accuracy:  0.887537\n",
      "Epoch:  547  Training Loss:  8.13778  Training Accuracy:  0.887714\n",
      "Epoch:  548  Training Loss:  8.20919  Training Accuracy:  0.888301\n",
      "Epoch:  549  Training Loss:  8.11864  Training Accuracy:  0.888419\n",
      "Epoch:  550  Training Loss:  8.08896  Training Accuracy:  0.888654\n",
      "Epoch:  551  Training Loss:  8.10297  Training Accuracy:  0.888772\n",
      "Epoch:  552  Training Loss:  8.11119  Training Accuracy:  0.888948\n",
      "Epoch:  553  Training Loss:  8.10942  Training Accuracy:  0.889418\n",
      "Epoch:  554  Training Loss:  8.08836  Training Accuracy:  0.889595\n",
      "Epoch:  555  Training Loss:  8.08225  Training Accuracy:  0.890065\n",
      "Epoch:  556  Training Loss:  8.1368  Training Accuracy:  0.8903\n",
      "Epoch:  557  Training Loss:  8.04606  Training Accuracy:  0.890535\n",
      "Epoch:  558  Training Loss:  8.01186  Training Accuracy:  0.890829\n",
      "Epoch:  559  Training Loss:  8.04455  Training Accuracy:  0.891358\n",
      "Epoch:  560  Training Loss:  8.02454  Training Accuracy:  0.891652\n",
      "Epoch:  561  Training Loss:  8.03388  Training Accuracy:  0.892064\n",
      "Epoch:  562  Training Loss:  8.01361  Training Accuracy:  0.892123\n",
      "Epoch:  563  Training Loss:  8.06787  Training Accuracy:  0.892475\n",
      "Epoch:  564  Training Loss:  7.97009  Training Accuracy:  0.892769\n",
      "Epoch:  565  Training Loss:  7.98542  Training Accuracy:  0.893005\n",
      "Epoch:  566  Training Loss:  7.98479  Training Accuracy:  0.89324\n",
      "Epoch:  567  Training Loss:  7.98339  Training Accuracy:  0.893475\n",
      "Epoch:  568  Training Loss:  7.97073  Training Accuracy:  0.893828\n",
      "Epoch:  569  Training Loss:  8.01488  Training Accuracy:  0.894239\n",
      "Epoch:  570  Training Loss:  7.9267  Training Accuracy:  0.894592\n",
      "Epoch:  571  Training Loss:  7.94482  Training Accuracy:  0.894709\n",
      "Epoch:  572  Training Loss:  7.93353  Training Accuracy:  0.895003\n",
      "Epoch:  573  Training Loss:  7.92378  Training Accuracy:  0.895297\n",
      "Epoch:  574  Training Loss:  7.92116  Training Accuracy:  0.895532\n",
      "Epoch:  575  Training Loss:  7.91362  Training Accuracy:  0.895885\n",
      "Epoch:  576  Training Loss:  7.89613  Training Accuracy:  0.896297\n",
      "Epoch:  577  Training Loss:  7.89406  Training Accuracy:  0.896297\n",
      "Epoch:  578  Training Loss:  7.93626  Training Accuracy:  0.896708\n",
      "Epoch:  579  Training Loss:  7.854  Training Accuracy:  0.896767\n",
      "Epoch:  580  Training Loss:  7.85867  Training Accuracy:  0.897002\n",
      "Epoch:  581  Training Loss:  7.85015  Training Accuracy:  0.89712\n",
      "Epoch:  582  Training Loss:  7.84493  Training Accuracy:  0.897355\n",
      "Epoch:  583  Training Loss:  7.84632  Training Accuracy:  0.897766\n",
      "Epoch:  584  Training Loss:  7.83496  Training Accuracy:  0.897943\n",
      "Epoch:  585  Training Loss:  7.86433  Training Accuracy:  0.898237\n",
      "Epoch:  586  Training Loss:  7.82433  Training Accuracy:  0.898354\n",
      "Epoch:  587  Training Loss:  7.78352  Training Accuracy:  0.898766\n",
      "Epoch:  588  Training Loss:  7.76426  Training Accuracy:  0.898942\n",
      "Epoch:  589  Training Loss:  7.83306  Training Accuracy:  0.899177\n",
      "Epoch:  590  Training Loss:  7.80715  Training Accuracy:  0.899589\n",
      "Epoch:  591  Training Loss:  7.76138  Training Accuracy:  0.899883\n",
      "Epoch:  592  Training Loss:  7.7765  Training Accuracy:  0.900236\n",
      "Epoch:  593  Training Loss:  7.78496  Training Accuracy:  0.900236\n",
      "Epoch:  594  Training Loss:  7.77416  Training Accuracy:  0.900471\n",
      "Epoch:  595  Training Loss:  7.77413  Training Accuracy:  0.900765\n",
      "Epoch:  596  Training Loss:  7.76357  Training Accuracy:  0.901294\n",
      "Epoch:  597  Training Loss:  7.78949  Training Accuracy:  0.90147\n",
      "Epoch:  598  Training Loss:  7.7491  Training Accuracy:  0.901764\n",
      "Epoch:  599  Training Loss:  7.7101  Training Accuracy:  0.901999\n",
      "Epoch:  600  Training Loss:  7.72544  Training Accuracy:  0.902293\n",
      "Epoch:  601  Training Loss:  7.72938  Training Accuracy:  0.90247\n",
      "Epoch:  602  Training Loss:  7.72055  Training Accuracy:  0.902881\n",
      "Epoch:  603  Training Loss:  7.71813  Training Accuracy:  0.902822\n",
      "Epoch:  604  Training Loss:  7.70313  Training Accuracy:  0.903057\n",
      "Epoch:  605  Training Loss:  7.7357  Training Accuracy:  0.903116\n",
      "Epoch:  606  Training Loss:  7.69536  Training Accuracy:  0.903528\n",
      "Epoch:  607  Training Loss:  7.65249  Training Accuracy:  0.904057\n",
      "Epoch:  608  Training Loss:  7.65508  Training Accuracy:  0.904175\n",
      "Epoch:  609  Training Loss:  7.6782  Training Accuracy:  0.904233\n",
      "Epoch:  610  Training Loss:  7.63654  Training Accuracy:  0.90488\n",
      "Epoch:  611  Training Loss:  7.63404  Training Accuracy:  0.905056\n",
      "Epoch:  612  Training Loss:  7.6572  Training Accuracy:  0.905291\n",
      "Epoch:  613  Training Loss:  7.659  Training Accuracy:  0.90535\n",
      "Epoch:  614  Training Loss:  7.65437  Training Accuracy:  0.905703\n",
      "Epoch:  615  Training Loss:  7.64978  Training Accuracy:  0.905703\n",
      "Epoch:  616  Training Loss:  7.63536  Training Accuracy:  0.905821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  7.63187  Training Accuracy:  0.906232\n",
      "Epoch:  618  Training Loss:  7.62945  Training Accuracy:  0.90682\n",
      "Epoch:  619  Training Loss:  7.61442  Training Accuracy:  0.90729\n",
      "Epoch:  620  Training Loss:  7.6405  Training Accuracy:  0.907408\n",
      "Epoch:  621  Training Loss:  7.59476  Training Accuracy:  0.907996\n",
      "Epoch:  622  Training Loss:  7.59539  Training Accuracy:  0.908113\n",
      "Epoch:  623  Training Loss:  7.60094  Training Accuracy:  0.908349\n",
      "Epoch:  624  Training Loss:  7.5891  Training Accuracy:  0.908466\n",
      "Epoch:  625  Training Loss:  7.58318  Training Accuracy:  0.908878\n",
      "Epoch:  626  Training Loss:  7.57714  Training Accuracy:  0.90923\n",
      "Epoch:  627  Training Loss:  7.57636  Training Accuracy:  0.909172\n",
      "Epoch:  628  Training Loss:  7.56204  Training Accuracy:  0.909407\n",
      "Epoch:  629  Training Loss:  7.56379  Training Accuracy:  0.909524\n",
      "Epoch:  630  Training Loss:  7.5436  Training Accuracy:  0.909936\n",
      "Epoch:  631  Training Loss:  7.57347  Training Accuracy:  0.909936\n",
      "Epoch:  632  Training Loss:  7.52896  Training Accuracy:  0.910347\n",
      "Epoch:  633  Training Loss:  7.52635  Training Accuracy:  0.910582\n",
      "Epoch:  634  Training Loss:  7.52361  Training Accuracy:  0.910935\n",
      "Epoch:  635  Training Loss:  7.51936  Training Accuracy:  0.910994\n",
      "Epoch:  636  Training Loss:  7.51463  Training Accuracy:  0.911406\n",
      "Epoch:  637  Training Loss:  7.51594  Training Accuracy:  0.911582\n",
      "Epoch:  638  Training Loss:  7.50814  Training Accuracy:  0.911935\n",
      "Epoch:  639  Training Loss:  7.50795  Training Accuracy:  0.911935\n",
      "Epoch:  640  Training Loss:  7.49119  Training Accuracy:  0.912464\n",
      "Epoch:  641  Training Loss:  7.51679  Training Accuracy:  0.912522\n",
      "Epoch:  642  Training Loss:  7.47234  Training Accuracy:  0.912993\n",
      "Epoch:  643  Training Loss:  7.47162  Training Accuracy:  0.913169\n",
      "Epoch:  644  Training Loss:  7.46636  Training Accuracy:  0.913346\n",
      "Epoch:  645  Training Loss:  7.46682  Training Accuracy:  0.913639\n",
      "Epoch:  646  Training Loss:  7.45984  Training Accuracy:  0.913875\n",
      "Epoch:  647  Training Loss:  7.45931  Training Accuracy:  0.914051\n",
      "Epoch:  648  Training Loss:  7.45108  Training Accuracy:  0.914169\n",
      "Epoch:  649  Training Loss:  7.44893  Training Accuracy:  0.914463\n",
      "Epoch:  650  Training Loss:  7.44285  Training Accuracy:  0.91458\n",
      "Epoch:  651  Training Loss:  7.44315  Training Accuracy:  0.914757\n",
      "Epoch:  652  Training Loss:  7.43644  Training Accuracy:  0.914992\n",
      "Epoch:  653  Training Loss:  7.43471  Training Accuracy:  0.915109\n",
      "Epoch:  654  Training Loss:  7.42901  Training Accuracy:  0.915403\n",
      "Epoch:  655  Training Loss:  7.42221  Training Accuracy:  0.915462\n",
      "Epoch:  656  Training Loss:  7.41133  Training Accuracy:  0.915756\n",
      "Epoch:  657  Training Loss:  7.42556  Training Accuracy:  0.915815\n",
      "Epoch:  658  Training Loss:  7.3939  Training Accuracy:  0.916167\n",
      "Epoch:  659  Training Loss:  7.38527  Training Accuracy:  0.916109\n",
      "Epoch:  660  Training Loss:  7.37975  Training Accuracy:  0.916461\n",
      "Epoch:  661  Training Loss:  7.37656  Training Accuracy:  0.916638\n",
      "Epoch:  662  Training Loss:  7.37289  Training Accuracy:  0.916991\n",
      "Epoch:  663  Training Loss:  7.36734  Training Accuracy:  0.917402\n",
      "Epoch:  664  Training Loss:  7.36091  Training Accuracy:  0.917813\n",
      "Epoch:  665  Training Loss:  7.35333  Training Accuracy:  0.918049\n",
      "Epoch:  666  Training Loss:  7.35024  Training Accuracy:  0.91846\n",
      "Epoch:  667  Training Loss:  7.34164  Training Accuracy:  0.918637\n",
      "Epoch:  668  Training Loss:  7.33493  Training Accuracy:  0.919107\n",
      "Epoch:  669  Training Loss:  7.33052  Training Accuracy:  0.919577\n",
      "Epoch:  670  Training Loss:  7.32747  Training Accuracy:  0.91993\n",
      "Epoch:  671  Training Loss:  7.32058  Training Accuracy:  0.920224\n",
      "Epoch:  672  Training Loss:  7.32354  Training Accuracy:  0.9204\n",
      "Epoch:  673  Training Loss:  7.31573  Training Accuracy:  0.920812\n",
      "Epoch:  674  Training Loss:  7.31371  Training Accuracy:  0.920988\n",
      "Epoch:  675  Training Loss:  7.31029  Training Accuracy:  0.921223\n",
      "Epoch:  676  Training Loss:  7.30515  Training Accuracy:  0.9214\n",
      "Epoch:  677  Training Loss:  7.30275  Training Accuracy:  0.921458\n",
      "Epoch:  678  Training Loss:  7.29472  Training Accuracy:  0.921752\n",
      "Epoch:  679  Training Loss:  7.30128  Training Accuracy:  0.921988\n",
      "Epoch:  680  Training Loss:  7.29336  Training Accuracy:  0.922458\n",
      "Epoch:  681  Training Loss:  7.28428  Training Accuracy:  0.922693\n",
      "Epoch:  682  Training Loss:  7.27797  Training Accuracy:  0.923104\n",
      "Epoch:  683  Training Loss:  7.28219  Training Accuracy:  0.923398\n",
      "Epoch:  684  Training Loss:  7.27151  Training Accuracy:  0.923516\n",
      "Epoch:  685  Training Loss:  7.26762  Training Accuracy:  0.923751\n",
      "Epoch:  686  Training Loss:  7.26754  Training Accuracy:  0.923751\n",
      "Epoch:  687  Training Loss:  7.25824  Training Accuracy:  0.924222\n",
      "Epoch:  688  Training Loss:  7.24779  Training Accuracy:  0.92428\n",
      "Epoch:  689  Training Loss:  7.25176  Training Accuracy:  0.924457\n",
      "Epoch:  690  Training Loss:  7.24243  Training Accuracy:  0.924692\n",
      "Epoch:  691  Training Loss:  7.23687  Training Accuracy:  0.924751\n",
      "Epoch:  692  Training Loss:  7.2383  Training Accuracy:  0.924809\n",
      "Epoch:  693  Training Loss:  7.23138  Training Accuracy:  0.925103\n",
      "Epoch:  694  Training Loss:  7.22557  Training Accuracy:  0.92528\n",
      "Epoch:  695  Training Loss:  7.21717  Training Accuracy:  0.925515\n",
      "Epoch:  696  Training Loss:  7.22166  Training Accuracy:  0.925632\n",
      "Epoch:  697  Training Loss:  7.20907  Training Accuracy:  0.925868\n",
      "Epoch:  698  Training Loss:  7.21036  Training Accuracy:  0.926397\n",
      "Epoch:  699  Training Loss:  7.20141  Training Accuracy:  0.926691\n",
      "Epoch:  700  Training Loss:  7.20576  Training Accuracy:  0.926985\n",
      "Epoch:  701  Training Loss:  7.19633  Training Accuracy:  0.927102\n",
      "Epoch:  702  Training Loss:  7.20111  Training Accuracy:  0.927279\n",
      "Epoch:  703  Training Loss:  7.19044  Training Accuracy:  0.927455\n",
      "Epoch:  704  Training Loss:  7.19474  Training Accuracy:  0.927396\n",
      "Epoch:  705  Training Loss:  7.18567  Training Accuracy:  0.927808\n",
      "Epoch:  706  Training Loss:  7.18075  Training Accuracy:  0.927925\n",
      "Epoch:  707  Training Loss:  7.17606  Training Accuracy:  0.928043\n",
      "Epoch:  708  Training Loss:  7.17624  Training Accuracy:  0.92816\n",
      "Epoch:  709  Training Loss:  7.17261  Training Accuracy:  0.928631\n",
      "Epoch:  710  Training Loss:  7.16834  Training Accuracy:  0.928807\n",
      "Epoch:  711  Training Loss:  7.16531  Training Accuracy:  0.929101\n",
      "Epoch:  712  Training Loss:  7.16282  Training Accuracy:  0.929336\n",
      "Epoch:  713  Training Loss:  7.15823  Training Accuracy:  0.92963\n",
      "Epoch:  714  Training Loss:  7.15527  Training Accuracy:  0.929748\n",
      "Epoch:  715  Training Loss:  7.153  Training Accuracy:  0.9301\n",
      "Epoch:  716  Training Loss:  7.15356  Training Accuracy:  0.930336\n",
      "Epoch:  717  Training Loss:  7.14325  Training Accuracy:  0.930688\n",
      "Epoch:  718  Training Loss:  7.13916  Training Accuracy:  0.930923\n",
      "Epoch:  719  Training Loss:  7.13472  Training Accuracy:  0.931276\n",
      "Epoch:  720  Training Loss:  7.13313  Training Accuracy:  0.93157\n",
      "Epoch:  721  Training Loss:  7.12882  Training Accuracy:  0.931805\n",
      "Epoch:  722  Training Loss:  7.12398  Training Accuracy:  0.931923\n",
      "Epoch:  723  Training Loss:  7.12063  Training Accuracy:  0.93204\n",
      "Epoch:  724  Training Loss:  7.11794  Training Accuracy:  0.932276\n",
      "Epoch:  725  Training Loss:  7.11101  Training Accuracy:  0.93257\n",
      "Epoch:  726  Training Loss:  7.10889  Training Accuracy:  0.932687\n",
      "Epoch:  727  Training Loss:  7.10493  Training Accuracy:  0.93304\n",
      "Epoch:  728  Training Loss:  7.10293  Training Accuracy:  0.933216\n",
      "Epoch:  729  Training Loss:  7.09845  Training Accuracy:  0.933216\n",
      "Epoch:  730  Training Loss:  7.09589  Training Accuracy:  0.933451\n",
      "Epoch:  731  Training Loss:  7.09442  Training Accuracy:  0.933687\n",
      "Epoch:  732  Training Loss:  7.09274  Training Accuracy:  0.933863\n",
      "Epoch:  733  Training Loss:  7.08179  Training Accuracy:  0.933922\n",
      "Epoch:  734  Training Loss:  7.0766  Training Accuracy:  0.934333\n",
      "Epoch:  735  Training Loss:  7.0761  Training Accuracy:  0.934686\n",
      "Epoch:  736  Training Loss:  7.07001  Training Accuracy:  0.934862\n",
      "Epoch:  737  Training Loss:  7.06567  Training Accuracy:  0.935097\n",
      "Epoch:  738  Training Loss:  7.06237  Training Accuracy:  0.935274\n",
      "Epoch:  739  Training Loss:  7.0585  Training Accuracy:  0.935333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  740  Training Loss:  7.05498  Training Accuracy:  0.93545\n",
      "Epoch:  741  Training Loss:  7.05322  Training Accuracy:  0.935685\n",
      "Epoch:  742  Training Loss:  7.04774  Training Accuracy:  0.935862\n",
      "Epoch:  743  Training Loss:  7.04505  Training Accuracy:  0.936038\n",
      "Epoch:  744  Training Loss:  7.03927  Training Accuracy:  0.936097\n",
      "Epoch:  745  Training Loss:  7.03758  Training Accuracy:  0.936097\n",
      "Epoch:  746  Training Loss:  7.03586  Training Accuracy:  0.936273\n",
      "Epoch:  747  Training Loss:  7.03174  Training Accuracy:  0.936567\n",
      "Epoch:  748  Training Loss:  7.02883  Training Accuracy:  0.936626\n",
      "Epoch:  749  Training Loss:  7.02379  Training Accuracy:  0.936802\n",
      "Epoch:  750  Training Loss:  7.02081  Training Accuracy:  0.936861\n",
      "Epoch:  751  Training Loss:  7.01574  Training Accuracy:  0.937038\n",
      "Epoch:  752  Training Loss:  7.01181  Training Accuracy:  0.937273\n",
      "Epoch:  753  Training Loss:  7.0124  Training Accuracy:  0.937508\n",
      "Epoch:  754  Training Loss:  7.00867  Training Accuracy:  0.937684\n",
      "Epoch:  755  Training Loss:  7.00693  Training Accuracy:  0.937861\n",
      "Epoch:  756  Training Loss:  7.0025  Training Accuracy:  0.938037\n",
      "Epoch:  757  Training Loss:  6.98559  Training Accuracy:  0.938037\n",
      "Epoch:  758  Training Loss:  6.95876  Training Accuracy:  0.938448\n",
      "Epoch:  759  Training Loss:  6.99505  Training Accuracy:  0.938801\n",
      "Epoch:  760  Training Loss:  6.99371  Training Accuracy:  0.939036\n",
      "Epoch:  761  Training Loss:  6.98832  Training Accuracy:  0.93933\n",
      "Epoch:  762  Training Loss:  6.9894  Training Accuracy:  0.939389\n",
      "Epoch:  763  Training Loss:  6.98201  Training Accuracy:  0.939389\n",
      "Epoch:  764  Training Loss:  6.9822  Training Accuracy:  0.939624\n",
      "Epoch:  765  Training Loss:  6.97648  Training Accuracy:  0.939624\n",
      "Epoch:  766  Training Loss:  6.97592  Training Accuracy:  0.939801\n",
      "Epoch:  767  Training Loss:  6.96667  Training Accuracy:  0.939918\n",
      "Epoch:  768  Training Loss:  6.97029  Training Accuracy:  0.940212\n",
      "Epoch:  769  Training Loss:  6.96485  Training Accuracy:  0.94033\n",
      "Epoch:  770  Training Loss:  6.96412  Training Accuracy:  0.940506\n",
      "Epoch:  771  Training Loss:  6.95492  Training Accuracy:  0.940682\n",
      "Epoch:  772  Training Loss:  6.94094  Training Accuracy:  0.9408\n",
      "Epoch:  773  Training Loss:  6.91639  Training Accuracy:  0.94127\n",
      "Epoch:  774  Training Loss:  6.93949  Training Accuracy:  0.941447\n",
      "Epoch:  775  Training Loss:  6.94246  Training Accuracy:  0.941623\n",
      "Epoch:  776  Training Loss:  6.94195  Training Accuracy:  0.941799\n",
      "Epoch:  777  Training Loss:  6.93834  Training Accuracy:  0.941917\n",
      "Epoch:  778  Training Loss:  6.93716  Training Accuracy:  0.942035\n",
      "Epoch:  779  Training Loss:  6.93039  Training Accuracy:  0.94227\n",
      "Epoch:  780  Training Loss:  6.92917  Training Accuracy:  0.942505\n",
      "Epoch:  781  Training Loss:  6.92433  Training Accuracy:  0.942681\n",
      "Epoch:  782  Training Loss:  6.92207  Training Accuracy:  0.942799\n",
      "Epoch:  783  Training Loss:  6.91832  Training Accuracy:  0.943034\n",
      "Epoch:  784  Training Loss:  6.91913  Training Accuracy:  0.942975\n",
      "Epoch:  785  Training Loss:  6.91149  Training Accuracy:  0.943328\n",
      "Epoch:  786  Training Loss:  6.91265  Training Accuracy:  0.943446\n",
      "Epoch:  787  Training Loss:  6.9079  Training Accuracy:  0.943681\n",
      "Epoch:  788  Training Loss:  6.90628  Training Accuracy:  0.943681\n",
      "Epoch:  789  Training Loss:  6.89951  Training Accuracy:  0.943916\n",
      "Epoch:  790  Training Loss:  6.90092  Training Accuracy:  0.943975\n",
      "Epoch:  791  Training Loss:  6.89705  Training Accuracy:  0.94421\n",
      "Epoch:  792  Training Loss:  6.89308  Training Accuracy:  0.944445\n",
      "Epoch:  793  Training Loss:  6.8875  Training Accuracy:  0.944563\n",
      "Epoch:  794  Training Loss:  6.87924  Training Accuracy:  0.94468\n",
      "Epoch:  795  Training Loss:  6.85117  Training Accuracy:  0.94468\n",
      "Epoch:  796  Training Loss:  6.8819  Training Accuracy:  0.944915\n",
      "Epoch:  797  Training Loss:  6.88163  Training Accuracy:  0.945209\n",
      "Epoch:  798  Training Loss:  6.8872  Training Accuracy:  0.945444\n",
      "Epoch:  799  Training Loss:  6.87634  Training Accuracy:  0.945503\n",
      "Epoch:  800  Training Loss:  6.8692  Training Accuracy:  0.945562\n",
      "Epoch:  801  Training Loss:  6.84607  Training Accuracy:  0.945621\n",
      "Epoch:  802  Training Loss:  6.86886  Training Accuracy:  0.94568\n",
      "Epoch:  803  Training Loss:  6.86925  Training Accuracy:  0.945738\n",
      "Epoch:  804  Training Loss:  6.87135  Training Accuracy:  0.945797\n",
      "Epoch:  805  Training Loss:  6.86814  Training Accuracy:  0.946032\n",
      "Epoch:  806  Training Loss:  6.86226  Training Accuracy:  0.94615\n",
      "Epoch:  807  Training Loss:  6.86135  Training Accuracy:  0.946326\n",
      "Epoch:  808  Training Loss:  6.85904  Training Accuracy:  0.946503\n",
      "Epoch:  809  Training Loss:  6.85749  Training Accuracy:  0.946738\n",
      "Epoch:  810  Training Loss:  6.85501  Training Accuracy:  0.947032\n",
      "Epoch:  811  Training Loss:  6.85284  Training Accuracy:  0.947502\n",
      "Epoch:  812  Training Loss:  6.8472  Training Accuracy:  0.94762\n",
      "Epoch:  813  Training Loss:  6.84778  Training Accuracy:  0.947796\n",
      "Epoch:  814  Training Loss:  6.83283  Training Accuracy:  0.947796\n",
      "Epoch:  815  Training Loss:  6.81082  Training Accuracy:  0.947914\n",
      "Epoch:  816  Training Loss:  6.83836  Training Accuracy:  0.94809\n",
      "Epoch:  817  Training Loss:  6.84047  Training Accuracy:  0.948384\n",
      "Epoch:  818  Training Loss:  6.8331  Training Accuracy:  0.948325\n",
      "Epoch:  819  Training Loss:  6.80855  Training Accuracy:  0.948443\n",
      "Epoch:  820  Training Loss:  6.83554  Training Accuracy:  0.948619\n",
      "Epoch:  821  Training Loss:  6.83582  Training Accuracy:  0.948913\n",
      "Epoch:  822  Training Loss:  6.83257  Training Accuracy:  0.94903\n",
      "Epoch:  823  Training Loss:  6.83003  Training Accuracy:  0.949207\n",
      "Epoch:  824  Training Loss:  6.82911  Training Accuracy:  0.949324\n",
      "Epoch:  825  Training Loss:  6.8272  Training Accuracy:  0.949383\n",
      "Epoch:  826  Training Loss:  6.82713  Training Accuracy:  0.949442\n",
      "Epoch:  827  Training Loss:  6.82363  Training Accuracy:  0.949736\n",
      "Epoch:  828  Training Loss:  6.82413  Training Accuracy:  0.949971\n",
      "Epoch:  829  Training Loss:  6.81989  Training Accuracy:  0.950089\n",
      "Epoch:  830  Training Loss:  6.82126  Training Accuracy:  0.950265\n",
      "Epoch:  831  Training Loss:  6.81993  Training Accuracy:  0.950265\n",
      "Epoch:  832  Training Loss:  6.81983  Training Accuracy:  0.950441\n",
      "Epoch:  833  Training Loss:  6.81569  Training Accuracy:  0.950559\n",
      "Epoch:  834  Training Loss:  6.81214  Training Accuracy:  0.950618\n",
      "Epoch:  835  Training Loss:  6.80357  Training Accuracy:  0.950677\n",
      "Epoch:  836  Training Loss:  6.78379  Training Accuracy:  0.951029\n",
      "Epoch:  837  Training Loss:  6.81321  Training Accuracy:  0.951147\n",
      "Epoch:  838  Training Loss:  6.81147  Training Accuracy:  0.951265\n",
      "Epoch:  839  Training Loss:  6.81108  Training Accuracy:  0.951558\n",
      "Epoch:  840  Training Loss:  6.81252  Training Accuracy:  0.951676\n",
      "Epoch:  841  Training Loss:  6.8084  Training Accuracy:  0.951911\n",
      "Epoch:  842  Training Loss:  6.80879  Training Accuracy:  0.952088\n",
      "Epoch:  843  Training Loss:  6.80744  Training Accuracy:  0.952323\n",
      "Epoch:  844  Training Loss:  6.80822  Training Accuracy:  0.95244\n",
      "Epoch:  845  Training Loss:  6.8028  Training Accuracy:  0.952558\n",
      "Epoch:  846  Training Loss:  6.79653  Training Accuracy:  0.952617\n",
      "Epoch:  847  Training Loss:  6.76899  Training Accuracy:  0.952675\n",
      "Epoch:  848  Training Loss:  6.80162  Training Accuracy:  0.952852\n",
      "Epoch:  849  Training Loss:  6.80008  Training Accuracy:  0.952852\n",
      "Epoch:  850  Training Loss:  6.7994  Training Accuracy:  0.952911\n",
      "Epoch:  851  Training Loss:  6.80036  Training Accuracy:  0.952969\n",
      "Epoch:  852  Training Loss:  6.80137  Training Accuracy:  0.952969\n",
      "Epoch:  853  Training Loss:  6.79667  Training Accuracy:  0.953087\n",
      "Epoch:  854  Training Loss:  6.78927  Training Accuracy:  0.953205\n",
      "Epoch:  855  Training Loss:  6.76318  Training Accuracy:  0.953381\n",
      "Epoch:  856  Training Loss:  6.79054  Training Accuracy:  0.953381\n",
      "Epoch:  857  Training Loss:  6.79032  Training Accuracy:  0.95344\n",
      "Epoch:  858  Training Loss:  6.79048  Training Accuracy:  0.953557\n",
      "Epoch:  859  Training Loss:  6.78663  Training Accuracy:  0.953675\n",
      "Epoch:  860  Training Loss:  6.7848  Training Accuracy:  0.953675\n",
      "Epoch:  861  Training Loss:  6.78348  Training Accuracy:  0.953851\n",
      "Epoch:  862  Training Loss:  6.77445  Training Accuracy:  0.953851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  863  Training Loss:  6.75424  Training Accuracy:  0.954086\n",
      "Epoch:  864  Training Loss:  6.77484  Training Accuracy:  0.954145\n",
      "Epoch:  865  Training Loss:  6.78054  Training Accuracy:  0.954204\n",
      "Epoch:  866  Training Loss:  6.77555  Training Accuracy:  0.954439\n",
      "Epoch:  867  Training Loss:  6.75269  Training Accuracy:  0.954733\n",
      "Epoch:  868  Training Loss:  6.78055  Training Accuracy:  0.954674\n",
      "Epoch:  869  Training Loss:  6.78491  Training Accuracy:  0.954792\n",
      "Epoch:  870  Training Loss:  6.78574  Training Accuracy:  0.954733\n",
      "Epoch:  871  Training Loss:  6.77935  Training Accuracy:  0.954968\n",
      "Epoch:  872  Training Loss:  6.78037  Training Accuracy:  0.955086\n",
      "Epoch:  873  Training Loss:  6.77539  Training Accuracy:  0.955321\n",
      "Epoch:  874  Training Loss:  6.77743  Training Accuracy:  0.955321\n",
      "Epoch:  875  Training Loss:  6.77647  Training Accuracy:  0.955615\n",
      "Epoch:  876  Training Loss:  6.76826  Training Accuracy:  0.955615\n",
      "Epoch:  877  Training Loss:  6.74744  Training Accuracy:  0.955909\n",
      "Epoch:  878  Training Loss:  6.77771  Training Accuracy:  0.955968\n",
      "Epoch:  879  Training Loss:  6.77995  Training Accuracy:  0.956203\n",
      "Epoch:  880  Training Loss:  6.77854  Training Accuracy:  0.956203\n",
      "Epoch:  881  Training Loss:  6.77446  Training Accuracy:  0.956438\n",
      "Epoch:  882  Training Loss:  6.77307  Training Accuracy:  0.956497\n",
      "Epoch:  883  Training Loss:  6.77284  Training Accuracy:  0.956732\n",
      "Epoch:  884  Training Loss:  6.77304  Training Accuracy:  0.956791\n",
      "Epoch:  885  Training Loss:  6.77169  Training Accuracy:  0.956908\n",
      "Epoch:  886  Training Loss:  6.774  Training Accuracy:  0.957085\n",
      "Epoch:  887  Training Loss:  6.76979  Training Accuracy:  0.957379\n",
      "Epoch:  888  Training Loss:  6.77066  Training Accuracy:  0.957437\n",
      "Epoch:  889  Training Loss:  6.76721  Training Accuracy:  0.957614\n",
      "Epoch:  890  Training Loss:  6.76865  Training Accuracy:  0.957614\n",
      "Epoch:  891  Training Loss:  6.76646  Training Accuracy:  0.957731\n",
      "Epoch:  892  Training Loss:  6.76678  Training Accuracy:  0.957672\n",
      "Epoch:  893  Training Loss:  6.76575  Training Accuracy:  0.95779\n",
      "Epoch:  894  Training Loss:  6.76512  Training Accuracy:  0.957849\n",
      "Epoch:  895  Training Loss:  6.76363  Training Accuracy:  0.957849\n",
      "Epoch:  896  Training Loss:  6.76475  Training Accuracy:  0.957966\n",
      "Epoch:  897  Training Loss:  6.76328  Training Accuracy:  0.958084\n",
      "Epoch:  898  Training Loss:  6.76507  Training Accuracy:  0.958319\n",
      "Epoch:  899  Training Loss:  6.75969  Training Accuracy:  0.958496\n",
      "Epoch:  900  Training Loss:  6.76332  Training Accuracy:  0.958437\n",
      "Epoch:  901  Training Loss:  6.76098  Training Accuracy:  0.958613\n",
      "Epoch:  902  Training Loss:  6.76272  Training Accuracy:  0.958613\n",
      "Epoch:  903  Training Loss:  6.76096  Training Accuracy:  0.958789\n",
      "Epoch:  904  Training Loss:  6.75934  Training Accuracy:  0.958848\n",
      "Epoch:  905  Training Loss:  6.75923  Training Accuracy:  0.958966\n",
      "Epoch:  906  Training Loss:  6.76101  Training Accuracy:  0.959025\n",
      "Epoch:  907  Training Loss:  6.75811  Training Accuracy:  0.959201\n",
      "Epoch:  908  Training Loss:  6.75698  Training Accuracy:  0.959142\n",
      "Epoch:  909  Training Loss:  6.74741  Training Accuracy:  0.959201\n",
      "Epoch:  910  Training Loss:  6.73334  Training Accuracy:  0.95926\n",
      "Epoch:  911  Training Loss:  6.75371  Training Accuracy:  0.95926\n",
      "Epoch:  912  Training Loss:  6.75532  Training Accuracy:  0.95926\n",
      "Epoch:  913  Training Loss:  6.74775  Training Accuracy:  0.959436\n",
      "Epoch:  914  Training Loss:  6.73672  Training Accuracy:  0.95973\n",
      "Epoch:  915  Training Loss:  6.72896  Training Accuracy:  0.959965\n",
      "Epoch:  916  Training Loss:  6.76131  Training Accuracy:  0.960024\n",
      "Epoch:  917  Training Loss:  6.76165  Training Accuracy:  0.9602\n",
      "Epoch:  918  Training Loss:  6.76288  Training Accuracy:  0.960377\n",
      "Epoch:  919  Training Loss:  6.75053  Training Accuracy:  0.960494\n",
      "Epoch:  920  Training Loss:  6.73533  Training Accuracy:  0.960436\n",
      "Epoch:  921  Training Loss:  6.74951  Training Accuracy:  0.960553\n",
      "Epoch:  922  Training Loss:  6.7505  Training Accuracy:  0.960553\n",
      "Epoch:  923  Training Loss:  6.73462  Training Accuracy:  0.960671\n",
      "Epoch:  924  Training Loss:  6.72976  Training Accuracy:  0.960788\n",
      "Epoch:  925  Training Loss:  6.75646  Training Accuracy:  0.960965\n",
      "Epoch:  926  Training Loss:  6.76013  Training Accuracy:  0.960965\n",
      "Epoch:  927  Training Loss:  6.75578  Training Accuracy:  0.9612\n",
      "Epoch:  928  Training Loss:  6.7356  Training Accuracy:  0.961141\n",
      "Epoch:  929  Training Loss:  6.75787  Training Accuracy:  0.9612\n",
      "Epoch:  930  Training Loss:  6.75988  Training Accuracy:  0.9612\n",
      "Epoch:  931  Training Loss:  6.75338  Training Accuracy:  0.961435\n",
      "Epoch:  932  Training Loss:  6.73652  Training Accuracy:  0.961435\n",
      "Epoch:  933  Training Loss:  6.74644  Training Accuracy:  0.96167\n",
      "Epoch:  934  Training Loss:  6.73352  Training Accuracy:  0.961964\n",
      "Epoch:  935  Training Loss:  6.73884  Training Accuracy:  0.962023\n",
      "Epoch:  936  Training Loss:  6.74806  Training Accuracy:  0.962023\n",
      "Epoch:  937  Training Loss:  6.7389  Training Accuracy:  0.962258\n",
      "Epoch:  938  Training Loss:  6.73531  Training Accuracy:  0.962199\n",
      "Epoch:  939  Training Loss:  6.75602  Training Accuracy:  0.962317\n",
      "Epoch:  940  Training Loss:  6.74683  Training Accuracy:  0.962258\n",
      "Epoch:  941  Training Loss:  6.7642  Training Accuracy:  0.962434\n",
      "Epoch:  942  Training Loss:  6.76779  Training Accuracy:  0.96267\n",
      "Epoch:  943  Training Loss:  6.75075  Training Accuracy:  0.962905\n",
      "Epoch:  944  Training Loss:  6.74232  Training Accuracy:  0.962963\n",
      "Epoch:  945  Training Loss:  6.75976  Training Accuracy:  0.963081\n",
      "Epoch:  946  Training Loss:  6.76656  Training Accuracy:  0.96314\n",
      "Epoch:  947  Training Loss:  6.75911  Training Accuracy:  0.963316\n",
      "Epoch:  948  Training Loss:  6.74229  Training Accuracy:  0.963316\n",
      "Epoch:  949  Training Loss:  6.74106  Training Accuracy:  0.963493\n",
      "Epoch:  950  Training Loss:  6.76217  Training Accuracy:  0.963375\n",
      "Epoch:  951  Training Loss:  6.76863  Training Accuracy:  0.963786\n",
      "Epoch:  952  Training Loss:  6.7567  Training Accuracy:  0.963728\n",
      "Epoch:  953  Training Loss:  6.74367  Training Accuracy:  0.963845\n",
      "Epoch:  954  Training Loss:  6.76509  Training Accuracy:  0.963787\n",
      "Epoch:  955  Training Loss:  6.77183  Training Accuracy:  0.963904\n",
      "Epoch:  956  Training Loss:  6.77048  Training Accuracy:  0.963904\n",
      "Epoch:  957  Training Loss:  6.75053  Training Accuracy:  0.964022\n",
      "Epoch:  958  Training Loss:  6.77132  Training Accuracy:  0.96408\n",
      "Epoch:  959  Training Loss:  6.77068  Training Accuracy:  0.964316\n",
      "Epoch:  960  Training Loss:  6.76414  Training Accuracy:  0.964374\n",
      "Epoch:  961  Training Loss:  6.74884  Training Accuracy:  0.964433\n",
      "Epoch:  962  Training Loss:  6.746  Training Accuracy:  0.964433\n",
      "Epoch:  963  Training Loss:  6.76602  Training Accuracy:  0.964551\n",
      "Epoch:  964  Training Loss:  6.75019  Training Accuracy:  0.964845\n",
      "Epoch:  965  Training Loss:  6.77549  Training Accuracy:  0.964727\n",
      "Epoch:  966  Training Loss:  6.779  Training Accuracy:  0.964904\n",
      "Epoch:  967  Training Loss:  6.77689  Training Accuracy:  0.964962\n",
      "Epoch:  968  Training Loss:  6.75576  Training Accuracy:  0.96508\n",
      "Epoch:  969  Training Loss:  6.77558  Training Accuracy:  0.96508\n",
      "Epoch:  970  Training Loss:  6.77671  Training Accuracy:  0.965256\n",
      "Epoch:  971  Training Loss:  6.77795  Training Accuracy:  0.965433\n",
      "Epoch:  972  Training Loss:  6.76086  Training Accuracy:  0.965433\n",
      "Epoch:  973  Training Loss:  6.77692  Training Accuracy:  0.965491\n",
      "Epoch:  974  Training Loss:  6.78128  Training Accuracy:  0.965491\n",
      "Epoch:  975  Training Loss:  6.76052  Training Accuracy:  0.965668\n",
      "Epoch:  976  Training Loss:  6.78  Training Accuracy:  0.965668\n",
      "Epoch:  977  Training Loss:  6.78317  Training Accuracy:  0.965785\n",
      "Epoch:  978  Training Loss:  6.78497  Training Accuracy:  0.965844\n",
      "Epoch:  979  Training Loss:  6.76504  Training Accuracy:  0.966138\n",
      "Epoch:  980  Training Loss:  6.78564  Training Accuracy:  0.966021\n",
      "Epoch:  981  Training Loss:  6.7833  Training Accuracy:  0.966197\n",
      "Epoch:  982  Training Loss:  6.78489  Training Accuracy:  0.966197\n",
      "Epoch:  983  Training Loss:  6.76299  Training Accuracy:  0.966491\n",
      "Epoch:  984  Training Loss:  6.78789  Training Accuracy:  0.966256\n",
      "Epoch:  985  Training Loss:  6.78552  Training Accuracy:  0.966491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  986  Training Loss:  6.78843  Training Accuracy:  0.96655\n",
      "Epoch:  987  Training Loss:  6.76816  Training Accuracy:  0.966785\n",
      "Epoch:  988  Training Loss:  6.79121  Training Accuracy:  0.966785\n",
      "Epoch:  989  Training Loss:  6.78658  Training Accuracy:  0.966844\n",
      "Epoch:  990  Training Loss:  6.77548  Training Accuracy:  0.966902\n",
      "Epoch:  991  Training Loss:  6.78957  Training Accuracy:  0.966961\n",
      "Epoch:  992  Training Loss:  6.79432  Training Accuracy:  0.966902\n",
      "Epoch:  993  Training Loss:  6.79037  Training Accuracy:  0.966961\n",
      "Epoch:  994  Training Loss:  6.7775  Training Accuracy:  0.967079\n",
      "Epoch:  995  Training Loss:  6.79459  Training Accuracy:  0.967138\n",
      "Epoch:  996  Training Loss:  6.80014  Training Accuracy:  0.967196\n",
      "Epoch:  997  Training Loss:  6.79529  Training Accuracy:  0.967255\n",
      "Epoch:  998  Training Loss:  6.77956  Training Accuracy:  0.967549\n",
      "Epoch:  999  Training Loss:  6.79999  Training Accuracy:  0.967608\n",
      "Testing Accuracy: 0.863114\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  550.361  Training Accuracy:  0.0377425\n",
      "Epoch:  1  Training Loss:  537.614  Training Accuracy:  0.0385655\n",
      "Epoch:  2  Training Loss:  526.434  Training Accuracy:  0.0451499\n",
      "Epoch:  3  Training Loss:  507.046  Training Accuracy:  0.0527925\n",
      "Epoch:  4  Training Loss:  492.235  Training Accuracy:  0.0603175\n",
      "Epoch:  5  Training Loss:  481.211  Training Accuracy:  0.0684303\n",
      "Epoch:  6  Training Loss:  472.337  Training Accuracy:  0.0774838\n",
      "Epoch:  7  Training Loss:  464.859  Training Accuracy:  0.0835979\n",
      "Epoch:  8  Training Loss:  458.446  Training Accuracy:  0.0901235\n",
      "Epoch:  9  Training Loss:  452.958  Training Accuracy:  0.0987654\n",
      "Epoch:  10  Training Loss:  448.272  Training Accuracy:  0.105644\n",
      "Epoch:  11  Training Loss:  444.255  Training Accuracy:  0.11311\n",
      "Epoch:  12  Training Loss:  440.8  Training Accuracy:  0.122928\n",
      "Epoch:  13  Training Loss:  437.823  Training Accuracy:  0.130864\n",
      "Epoch:  14  Training Loss:  435.213  Training Accuracy:  0.139095\n",
      "Epoch:  15  Training Loss:  432.924  Training Accuracy:  0.148207\n",
      "Epoch:  16  Training Loss:  430.879  Training Accuracy:  0.159083\n",
      "Epoch:  17  Training Loss:  429.027  Training Accuracy:  0.168724\n",
      "Epoch:  18  Training Loss:  427.322  Training Accuracy:  0.178483\n",
      "Epoch:  19  Training Loss:  425.739  Training Accuracy:  0.185655\n",
      "Epoch:  20  Training Loss:  424.249  Training Accuracy:  0.19418\n",
      "Epoch:  21  Training Loss:  422.818  Training Accuracy:  0.204409\n",
      "Epoch:  22  Training Loss:  421.454  Training Accuracy:  0.213757\n",
      "Epoch:  23  Training Loss:  420.142  Training Accuracy:  0.221164\n",
      "Epoch:  24  Training Loss:  418.863  Training Accuracy:  0.227866\n",
      "Epoch:  25  Training Loss:  417.622  Training Accuracy:  0.237331\n",
      "Epoch:  26  Training Loss:  416.373  Training Accuracy:  0.247678\n",
      "Epoch:  27  Training Loss:  415.169  Training Accuracy:  0.257966\n",
      "Epoch:  28  Training Loss:  413.957  Training Accuracy:  0.265021\n",
      "Epoch:  29  Training Loss:  412.765  Training Accuracy:  0.273133\n",
      "Epoch:  30  Training Loss:  411.557  Training Accuracy:  0.279071\n",
      "Epoch:  31  Training Loss:  410.38  Training Accuracy:  0.285597\n",
      "Epoch:  32  Training Loss:  409.203  Training Accuracy:  0.291299\n",
      "Epoch:  33  Training Loss:  408.03  Training Accuracy:  0.296649\n",
      "Epoch:  34  Training Loss:  406.854  Training Accuracy:  0.30241\n",
      "Epoch:  35  Training Loss:  405.704  Training Accuracy:  0.307466\n",
      "Epoch:  36  Training Loss:  404.531  Training Accuracy:  0.312052\n",
      "Epoch:  37  Training Loss:  403.376  Training Accuracy:  0.31652\n",
      "Epoch:  38  Training Loss:  402.215  Training Accuracy:  0.320341\n",
      "Epoch:  39  Training Loss:  401.047  Training Accuracy:  0.324221\n",
      "Epoch:  40  Training Loss:  399.879  Training Accuracy:  0.328454\n",
      "Epoch:  41  Training Loss:  398.73  Training Accuracy:  0.332393\n",
      "Epoch:  42  Training Loss:  397.558  Training Accuracy:  0.33545\n",
      "Epoch:  43  Training Loss:  396.39  Training Accuracy:  0.33933\n",
      "Epoch:  44  Training Loss:  395.23  Training Accuracy:  0.342269\n",
      "Epoch:  45  Training Loss:  394.028  Training Accuracy:  0.346326\n",
      "Epoch:  46  Training Loss:  392.849  Training Accuracy:  0.349618\n",
      "Epoch:  47  Training Loss:  391.663  Training Accuracy:  0.353204\n",
      "Epoch:  48  Training Loss:  390.478  Training Accuracy:  0.356672\n",
      "Epoch:  49  Training Loss:  389.275  Training Accuracy:  0.361023\n",
      "Epoch:  50  Training Loss:  388.089  Training Accuracy:  0.363962\n",
      "Epoch:  51  Training Loss:  386.866  Training Accuracy:  0.367019\n",
      "Epoch:  52  Training Loss:  385.658  Training Accuracy:  0.370253\n",
      "Epoch:  53  Training Loss:  384.419  Training Accuracy:  0.373369\n",
      "Epoch:  54  Training Loss:  383.225  Training Accuracy:  0.376896\n",
      "Epoch:  55  Training Loss:  382.0  Training Accuracy:  0.379483\n",
      "Epoch:  56  Training Loss:  380.783  Training Accuracy:  0.38254\n",
      "Epoch:  57  Training Loss:  379.57  Training Accuracy:  0.386067\n",
      "Epoch:  58  Training Loss:  378.349  Training Accuracy:  0.389065\n",
      "Epoch:  59  Training Loss:  377.14  Training Accuracy:  0.392181\n",
      "Epoch:  60  Training Loss:  375.943  Training Accuracy:  0.394709\n",
      "Epoch:  61  Training Loss:  374.748  Training Accuracy:  0.397648\n",
      "Epoch:  62  Training Loss:  373.534  Training Accuracy:  0.4\n",
      "Epoch:  63  Training Loss:  372.289  Training Accuracy:  0.403057\n",
      "Epoch:  64  Training Loss:  371.057  Training Accuracy:  0.405232\n",
      "Epoch:  65  Training Loss:  369.85  Training Accuracy:  0.40729\n",
      "Epoch:  66  Training Loss:  368.602  Training Accuracy:  0.4097\n",
      "Epoch:  67  Training Loss:  367.379  Training Accuracy:  0.411464\n",
      "Epoch:  68  Training Loss:  366.166  Training Accuracy:  0.413815\n",
      "Epoch:  69  Training Loss:  364.932  Training Accuracy:  0.416226\n",
      "Epoch:  70  Training Loss:  363.701  Training Accuracy:  0.418754\n",
      "Epoch:  71  Training Loss:  362.457  Training Accuracy:  0.421046\n",
      "Epoch:  72  Training Loss:  361.217  Training Accuracy:  0.423045\n",
      "Epoch:  73  Training Loss:  360.0  Training Accuracy:  0.425514\n",
      "Epoch:  74  Training Loss:  358.737  Training Accuracy:  0.427454\n",
      "Epoch:  75  Training Loss:  357.527  Training Accuracy:  0.429923\n",
      "Epoch:  76  Training Loss:  356.303  Training Accuracy:  0.431922\n",
      "Epoch:  77  Training Loss:  355.076  Training Accuracy:  0.433627\n",
      "Epoch:  78  Training Loss:  353.85  Training Accuracy:  0.436214\n",
      "Epoch:  79  Training Loss:  352.666  Training Accuracy:  0.438918\n",
      "Epoch:  80  Training Loss:  351.444  Training Accuracy:  0.44174\n",
      "Epoch:  81  Training Loss:  350.27  Training Accuracy:  0.44368\n",
      "Epoch:  82  Training Loss:  349.046  Training Accuracy:  0.445855\n",
      "Epoch:  83  Training Loss:  347.882  Training Accuracy:  0.448795\n",
      "Epoch:  84  Training Loss:  346.652  Training Accuracy:  0.45097\n",
      "Epoch:  85  Training Loss:  345.481  Training Accuracy:  0.452851\n",
      "Epoch:  86  Training Loss:  344.271  Training Accuracy:  0.454968\n",
      "Epoch:  87  Training Loss:  343.109  Training Accuracy:  0.457495\n",
      "Epoch:  88  Training Loss:  341.91  Training Accuracy:  0.459553\n",
      "Epoch:  89  Training Loss:  340.777  Training Accuracy:  0.46114\n",
      "Epoch:  90  Training Loss:  339.599  Training Accuracy:  0.463668\n",
      "Epoch:  91  Training Loss:  338.473  Training Accuracy:  0.46502\n",
      "Epoch:  92  Training Loss:  337.304  Training Accuracy:  0.466608\n",
      "Epoch:  93  Training Loss:  336.178  Training Accuracy:  0.46843\n",
      "Epoch:  94  Training Loss:  335.045  Training Accuracy:  0.470429\n",
      "Epoch:  95  Training Loss:  333.937  Training Accuracy:  0.472369\n",
      "Epoch:  96  Training Loss:  332.805  Training Accuracy:  0.474015\n",
      "Epoch:  97  Training Loss:  331.713  Training Accuracy:  0.475661\n",
      "Epoch:  98  Training Loss:  330.607  Training Accuracy:  0.477895\n",
      "Epoch:  99  Training Loss:  329.547  Training Accuracy:  0.479659\n",
      "Epoch:  100  Training Loss:  328.415  Training Accuracy:  0.481658\n",
      "Epoch:  101  Training Loss:  327.347  Training Accuracy:  0.483539\n",
      "Epoch:  102  Training Loss:  326.277  Training Accuracy:  0.485361\n",
      "Epoch:  103  Training Loss:  325.212  Training Accuracy:  0.486949\n",
      "Epoch:  104  Training Loss:  324.132  Training Accuracy:  0.488771\n",
      "Epoch:  105  Training Loss:  323.108  Training Accuracy:  0.491064\n",
      "Epoch:  106  Training Loss:  322.03  Training Accuracy:  0.49318\n",
      "Epoch:  107  Training Loss:  321.009  Training Accuracy:  0.494709\n",
      "Epoch:  108  Training Loss:  319.957  Training Accuracy:  0.496943\n",
      "Epoch:  109  Training Loss:  318.959  Training Accuracy:  0.499177\n",
      "Epoch:  110  Training Loss:  317.895  Training Accuracy:  0.501352\n",
      "Epoch:  111  Training Loss:  316.913  Training Accuracy:  0.502939\n",
      "Epoch:  112  Training Loss:  315.871  Training Accuracy:  0.504644\n",
      "Epoch:  113  Training Loss:  314.914  Training Accuracy:  0.506231\n",
      "Epoch:  114  Training Loss:  313.915  Training Accuracy:  0.508701\n",
      "Epoch:  115  Training Loss:  312.926  Training Accuracy:  0.509876\n",
      "Epoch:  116  Training Loss:  311.948  Training Accuracy:  0.51117\n",
      "Epoch:  117  Training Loss:  310.993  Training Accuracy:  0.512404\n",
      "Epoch:  118  Training Loss:  310.027  Training Accuracy:  0.51458\n",
      "Epoch:  119  Training Loss:  309.083  Training Accuracy:  0.51699\n",
      "Epoch:  120  Training Loss:  308.152  Training Accuracy:  0.518166\n",
      "Epoch:  121  Training Loss:  307.223  Training Accuracy:  0.520047\n",
      "Epoch:  122  Training Loss:  306.292  Training Accuracy:  0.521987\n",
      "Epoch:  123  Training Loss:  305.409  Training Accuracy:  0.523574\n",
      "Epoch:  124  Training Loss:  304.489  Training Accuracy:  0.525103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  303.612  Training Accuracy:  0.526925\n",
      "Epoch:  126  Training Loss:  302.695  Training Accuracy:  0.528983\n",
      "Epoch:  127  Training Loss:  301.858  Training Accuracy:  0.531217\n",
      "Epoch:  128  Training Loss:  300.956  Training Accuracy:  0.53351\n",
      "Epoch:  129  Training Loss:  300.126  Training Accuracy:  0.535685\n",
      "Epoch:  130  Training Loss:  299.234  Training Accuracy:  0.537331\n",
      "Epoch:  131  Training Loss:  298.397  Training Accuracy:  0.538977\n",
      "Epoch:  132  Training Loss:  297.522  Training Accuracy:  0.541152\n",
      "Epoch:  133  Training Loss:  296.722  Training Accuracy:  0.543151\n",
      "Epoch:  134  Training Loss:  295.871  Training Accuracy:  0.54468\n",
      "Epoch:  135  Training Loss:  295.069  Training Accuracy:  0.54562\n",
      "Epoch:  136  Training Loss:  294.217  Training Accuracy:  0.547443\n",
      "Epoch:  137  Training Loss:  293.445  Training Accuracy:  0.548912\n",
      "Epoch:  138  Training Loss:  292.615  Training Accuracy:  0.5505\n",
      "Epoch:  139  Training Loss:  291.853  Training Accuracy:  0.551852\n",
      "Epoch:  140  Training Loss:  291.022  Training Accuracy:  0.553498\n",
      "Epoch:  141  Training Loss:  290.297  Training Accuracy:  0.554674\n",
      "Epoch:  142  Training Loss:  289.528  Training Accuracy:  0.556379\n",
      "Epoch:  143  Training Loss:  288.779  Training Accuracy:  0.557731\n",
      "Epoch:  144  Training Loss:  288.005  Training Accuracy:  0.558554\n",
      "Epoch:  145  Training Loss:  287.285  Training Accuracy:  0.559965\n",
      "Epoch:  146  Training Loss:  286.538  Training Accuracy:  0.561023\n",
      "Epoch:  147  Training Loss:  285.851  Training Accuracy:  0.562551\n",
      "Epoch:  148  Training Loss:  285.127  Training Accuracy:  0.563845\n",
      "Epoch:  149  Training Loss:  284.46  Training Accuracy:  0.565432\n",
      "Epoch:  150  Training Loss:  283.75  Training Accuracy:  0.566725\n",
      "Epoch:  151  Training Loss:  283.072  Training Accuracy:  0.56843\n",
      "Epoch:  152  Training Loss:  282.416  Training Accuracy:  0.570429\n",
      "Epoch:  153  Training Loss:  281.758  Training Accuracy:  0.571605\n",
      "Epoch:  154  Training Loss:  281.073  Training Accuracy:  0.573075\n",
      "Epoch:  155  Training Loss:  280.443  Training Accuracy:  0.574779\n",
      "Epoch:  156  Training Loss:  279.785  Training Accuracy:  0.576367\n",
      "Epoch:  157  Training Loss:  279.16  Training Accuracy:  0.577719\n",
      "Epoch:  158  Training Loss:  278.548  Training Accuracy:  0.579012\n",
      "Epoch:  159  Training Loss:  277.939  Training Accuracy:  0.5806\n",
      "Epoch:  160  Training Loss:  277.289  Training Accuracy:  0.582128\n",
      "Epoch:  161  Training Loss:  276.71  Training Accuracy:  0.58301\n",
      "Epoch:  162  Training Loss:  276.113  Training Accuracy:  0.584362\n",
      "Epoch:  163  Training Loss:  275.506  Training Accuracy:  0.585479\n",
      "Epoch:  164  Training Loss:  274.885  Training Accuracy:  0.586479\n",
      "Epoch:  165  Training Loss:  274.327  Training Accuracy:  0.587654\n",
      "Epoch:  166  Training Loss:  273.756  Training Accuracy:  0.589183\n",
      "Epoch:  167  Training Loss:  273.179  Training Accuracy:  0.590359\n",
      "Epoch:  168  Training Loss:  272.588  Training Accuracy:  0.591064\n",
      "Epoch:  169  Training Loss:  272.059  Training Accuracy:  0.592475\n",
      "Epoch:  170  Training Loss:  271.48  Training Accuracy:  0.59371\n",
      "Epoch:  171  Training Loss:  270.935  Training Accuracy:  0.594827\n",
      "Epoch:  172  Training Loss:  270.386  Training Accuracy:  0.595356\n",
      "Epoch:  173  Training Loss:  269.853  Training Accuracy:  0.596473\n",
      "Epoch:  174  Training Loss:  269.319  Training Accuracy:  0.598119\n",
      "Epoch:  175  Training Loss:  268.809  Training Accuracy:  0.599588\n",
      "Epoch:  176  Training Loss:  268.28  Training Accuracy:  0.600647\n",
      "Epoch:  177  Training Loss:  267.788  Training Accuracy:  0.602175\n",
      "Epoch:  178  Training Loss:  267.305  Training Accuracy:  0.603292\n",
      "Epoch:  179  Training Loss:  266.771  Training Accuracy:  0.604292\n",
      "Epoch:  180  Training Loss:  266.316  Training Accuracy:  0.605409\n",
      "Epoch:  181  Training Loss:  265.788  Training Accuracy:  0.606408\n",
      "Epoch:  182  Training Loss:  265.317  Training Accuracy:  0.60729\n",
      "Epoch:  183  Training Loss:  264.846  Training Accuracy:  0.60823\n",
      "Epoch:  184  Training Loss:  264.396  Training Accuracy:  0.609465\n",
      "Epoch:  185  Training Loss:  263.901  Training Accuracy:  0.610758\n",
      "Epoch:  186  Training Loss:  263.444  Training Accuracy:  0.611523\n",
      "Epoch:  187  Training Loss:  262.965  Training Accuracy:  0.612934\n",
      "Epoch:  188  Training Loss:  262.516  Training Accuracy:  0.614286\n",
      "Epoch:  189  Training Loss:  262.071  Training Accuracy:  0.615168\n",
      "Epoch:  190  Training Loss:  261.608  Training Accuracy:  0.616343\n",
      "Epoch:  191  Training Loss:  261.148  Training Accuracy:  0.617578\n",
      "Epoch:  192  Training Loss:  260.712  Training Accuracy:  0.618636\n",
      "Epoch:  193  Training Loss:  260.268  Training Accuracy:  0.619459\n",
      "Epoch:  194  Training Loss:  259.827  Training Accuracy:  0.620223\n",
      "Epoch:  195  Training Loss:  259.416  Training Accuracy:  0.621105\n",
      "Epoch:  196  Training Loss:  258.973  Training Accuracy:  0.622046\n",
      "Epoch:  197  Training Loss:  258.537  Training Accuracy:  0.623104\n",
      "Epoch:  198  Training Loss:  258.149  Training Accuracy:  0.623927\n",
      "Epoch:  199  Training Loss:  257.704  Training Accuracy:  0.625162\n",
      "Epoch:  200  Training Loss:  257.303  Training Accuracy:  0.626102\n",
      "Epoch:  201  Training Loss:  256.871  Training Accuracy:  0.627219\n",
      "Epoch:  202  Training Loss:  256.485  Training Accuracy:  0.628042\n",
      "Epoch:  203  Training Loss:  256.079  Training Accuracy:  0.628983\n",
      "Epoch:  204  Training Loss:  255.733  Training Accuracy:  0.629982\n",
      "Epoch:  205  Training Loss:  255.293  Training Accuracy:  0.631158\n",
      "Epoch:  206  Training Loss:  254.937  Training Accuracy:  0.631864\n",
      "Epoch:  207  Training Loss:  254.535  Training Accuracy:  0.632922\n",
      "Epoch:  208  Training Loss:  254.146  Training Accuracy:  0.633862\n",
      "Epoch:  209  Training Loss:  253.761  Training Accuracy:  0.634274\n",
      "Epoch:  210  Training Loss:  253.419  Training Accuracy:  0.635391\n",
      "Epoch:  211  Training Loss:  252.992  Training Accuracy:  0.636214\n",
      "Epoch:  212  Training Loss:  252.655  Training Accuracy:  0.637155\n",
      "Epoch:  213  Training Loss:  252.279  Training Accuracy:  0.638037\n",
      "Epoch:  214  Training Loss:  251.92  Training Accuracy:  0.639271\n",
      "Epoch:  215  Training Loss:  251.568  Training Accuracy:  0.639624\n",
      "Epoch:  216  Training Loss:  251.232  Training Accuracy:  0.640447\n",
      "Epoch:  217  Training Loss:  250.81  Training Accuracy:  0.641035\n",
      "Epoch:  218  Training Loss:  250.5  Training Accuracy:  0.642328\n",
      "Epoch:  219  Training Loss:  250.137  Training Accuracy:  0.643151\n",
      "Epoch:  220  Training Loss:  249.784  Training Accuracy:  0.644503\n",
      "Epoch:  221  Training Loss:  249.418  Training Accuracy:  0.645326\n",
      "Epoch:  222  Training Loss:  249.11  Training Accuracy:  0.646326\n",
      "Epoch:  223  Training Loss:  248.699  Training Accuracy:  0.64756\n",
      "Epoch:  224  Training Loss:  248.391  Training Accuracy:  0.64856\n",
      "Epoch:  225  Training Loss:  248.042  Training Accuracy:  0.649677\n",
      "Epoch:  226  Training Loss:  247.685  Training Accuracy:  0.650441\n",
      "Epoch:  227  Training Loss:  247.353  Training Accuracy:  0.651676\n",
      "Epoch:  228  Training Loss:  246.997  Training Accuracy:  0.652499\n",
      "Epoch:  229  Training Loss:  246.651  Training Accuracy:  0.652851\n",
      "Epoch:  230  Training Loss:  246.338  Training Accuracy:  0.653792\n",
      "Epoch:  231  Training Loss:  246.025  Training Accuracy:  0.654674\n",
      "Epoch:  232  Training Loss:  245.685  Training Accuracy:  0.655203\n",
      "Epoch:  233  Training Loss:  245.369  Training Accuracy:  0.656379\n",
      "Epoch:  234  Training Loss:  245.02  Training Accuracy:  0.657319\n",
      "Epoch:  235  Training Loss:  244.713  Training Accuracy:  0.657907\n",
      "Epoch:  236  Training Loss:  244.399  Training Accuracy:  0.658671\n",
      "Epoch:  237  Training Loss:  244.111  Training Accuracy:  0.659788\n",
      "Epoch:  238  Training Loss:  243.796  Training Accuracy:  0.660553\n",
      "Epoch:  239  Training Loss:  243.491  Training Accuracy:  0.661435\n",
      "Epoch:  240  Training Loss:  243.205  Training Accuracy:  0.662963\n",
      "Epoch:  241  Training Loss:  242.875  Training Accuracy:  0.663727\n",
      "Epoch:  242  Training Loss:  242.589  Training Accuracy:  0.664492\n",
      "Epoch:  243  Training Loss:  242.29  Training Accuracy:  0.665021\n",
      "Epoch:  244  Training Loss:  241.989  Training Accuracy:  0.665961\n",
      "Epoch:  245  Training Loss:  241.68  Training Accuracy:  0.666196\n",
      "Epoch:  246  Training Loss:  241.386  Training Accuracy:  0.666961\n",
      "Epoch:  247  Training Loss:  241.065  Training Accuracy:  0.667843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  240.789  Training Accuracy:  0.668724\n",
      "Epoch:  249  Training Loss:  240.491  Training Accuracy:  0.669018\n",
      "Epoch:  250  Training Loss:  240.188  Training Accuracy:  0.669724\n",
      "Epoch:  251  Training Loss:  239.894  Training Accuracy:  0.670723\n",
      "Epoch:  252  Training Loss:  239.599  Training Accuracy:  0.671193\n",
      "Epoch:  253  Training Loss:  239.277  Training Accuracy:  0.671958\n",
      "Epoch:  254  Training Loss:  239.014  Training Accuracy:  0.672663\n",
      "Epoch:  255  Training Loss:  238.683  Training Accuracy:  0.673134\n",
      "Epoch:  256  Training Loss:  238.422  Training Accuracy:  0.673545\n",
      "Epoch:  257  Training Loss:  238.126  Training Accuracy:  0.674486\n",
      "Epoch:  258  Training Loss:  237.826  Training Accuracy:  0.675485\n",
      "Epoch:  259  Training Loss:  237.556  Training Accuracy:  0.675779\n",
      "Epoch:  260  Training Loss:  237.282  Training Accuracy:  0.676132\n",
      "Epoch:  261  Training Loss:  236.948  Training Accuracy:  0.676837\n",
      "Epoch:  262  Training Loss:  236.72  Training Accuracy:  0.677543\n",
      "Epoch:  263  Training Loss:  236.401  Training Accuracy:  0.677895\n",
      "Epoch:  264  Training Loss:  236.137  Training Accuracy:  0.678542\n",
      "Epoch:  265  Training Loss:  235.867  Training Accuracy:  0.67913\n",
      "Epoch:  266  Training Loss:  235.622  Training Accuracy:  0.679542\n",
      "Epoch:  267  Training Loss:  235.286  Training Accuracy:  0.680423\n",
      "Epoch:  268  Training Loss:  235.059  Training Accuracy:  0.680659\n",
      "Epoch:  269  Training Loss:  234.768  Training Accuracy:  0.681364\n",
      "Epoch:  270  Training Loss:  234.5  Training Accuracy:  0.682069\n",
      "Epoch:  271  Training Loss:  234.232  Training Accuracy:  0.683128\n",
      "Epoch:  272  Training Loss:  233.944  Training Accuracy:  0.683598\n",
      "Epoch:  273  Training Loss:  233.681  Training Accuracy:  0.684186\n",
      "Epoch:  274  Training Loss:  233.439  Training Accuracy:  0.68495\n",
      "Epoch:  275  Training Loss:  233.146  Training Accuracy:  0.685479\n",
      "Epoch:  276  Training Loss:  232.914  Training Accuracy:  0.686185\n",
      "Epoch:  277  Training Loss:  232.618  Training Accuracy:  0.686596\n",
      "Epoch:  278  Training Loss:  232.387  Training Accuracy:  0.687125\n",
      "Epoch:  279  Training Loss:  232.118  Training Accuracy:  0.687831\n",
      "Epoch:  280  Training Loss:  231.865  Training Accuracy:  0.688771\n",
      "Epoch:  281  Training Loss:  231.613  Training Accuracy:  0.689712\n",
      "Epoch:  282  Training Loss:  231.312  Training Accuracy:  0.689888\n",
      "Epoch:  283  Training Loss:  231.081  Training Accuracy:  0.690535\n",
      "Epoch:  284  Training Loss:  230.829  Training Accuracy:  0.691064\n",
      "Epoch:  285  Training Loss:  230.563  Training Accuracy:  0.69177\n",
      "Epoch:  286  Training Loss:  230.306  Training Accuracy:  0.692475\n",
      "Epoch:  287  Training Loss:  230.028  Training Accuracy:  0.692769\n",
      "Epoch:  288  Training Loss:  229.758  Training Accuracy:  0.693416\n",
      "Epoch:  289  Training Loss:  229.531  Training Accuracy:  0.693827\n",
      "Epoch:  290  Training Loss:  229.251  Training Accuracy:  0.694356\n",
      "Epoch:  291  Training Loss:  229.001  Training Accuracy:  0.694827\n",
      "Epoch:  292  Training Loss:  228.774  Training Accuracy:  0.695356\n",
      "Epoch:  293  Training Loss:  228.531  Training Accuracy:  0.695885\n",
      "Epoch:  294  Training Loss:  228.237  Training Accuracy:  0.696355\n",
      "Epoch:  295  Training Loss:  228.01  Training Accuracy:  0.696708\n",
      "Epoch:  296  Training Loss:  227.775  Training Accuracy:  0.697413\n",
      "Epoch:  297  Training Loss:  227.496  Training Accuracy:  0.697472\n",
      "Epoch:  298  Training Loss:  227.221  Training Accuracy:  0.698119\n",
      "Epoch:  299  Training Loss:  226.987  Training Accuracy:  0.698648\n",
      "Epoch:  300  Training Loss:  226.745  Training Accuracy:  0.699177\n",
      "Epoch:  301  Training Loss:  226.491  Training Accuracy:  0.699941\n",
      "Epoch:  302  Training Loss:  226.232  Training Accuracy:  0.700529\n",
      "Epoch:  303  Training Loss:  225.975  Training Accuracy:  0.701352\n",
      "Epoch:  304  Training Loss:  225.759  Training Accuracy:  0.701881\n",
      "Epoch:  305  Training Loss:  225.469  Training Accuracy:  0.70241\n",
      "Epoch:  306  Training Loss:  225.24  Training Accuracy:  0.703116\n",
      "Epoch:  307  Training Loss:  224.987  Training Accuracy:  0.703763\n",
      "Epoch:  308  Training Loss:  224.76  Training Accuracy:  0.704351\n",
      "Epoch:  309  Training Loss:  224.468  Training Accuracy:  0.70488\n",
      "Epoch:  310  Training Loss:  224.282  Training Accuracy:  0.705526\n",
      "Epoch:  311  Training Loss:  223.971  Training Accuracy:  0.705761\n",
      "Epoch:  312  Training Loss:  223.745  Training Accuracy:  0.706526\n",
      "Epoch:  313  Training Loss:  223.522  Training Accuracy:  0.707055\n",
      "Epoch:  314  Training Loss:  223.26  Training Accuracy:  0.707408\n",
      "Epoch:  315  Training Loss:  223.026  Training Accuracy:  0.707995\n",
      "Epoch:  316  Training Loss:  222.768  Training Accuracy:  0.708642\n",
      "Epoch:  317  Training Loss:  222.518  Training Accuracy:  0.70923\n",
      "Epoch:  318  Training Loss:  222.278  Training Accuracy:  0.709583\n",
      "Epoch:  319  Training Loss:  222.042  Training Accuracy:  0.709642\n",
      "Epoch:  320  Training Loss:  221.78  Training Accuracy:  0.709935\n",
      "Epoch:  321  Training Loss:  221.502  Training Accuracy:  0.710288\n",
      "Epoch:  322  Training Loss:  221.324  Training Accuracy:  0.7107\n",
      "Epoch:  323  Training Loss:  221.026  Training Accuracy:  0.711052\n",
      "Epoch:  324  Training Loss:  220.809  Training Accuracy:  0.711699\n",
      "Epoch:  325  Training Loss:  220.528  Training Accuracy:  0.711934\n",
      "Epoch:  326  Training Loss:  220.325  Training Accuracy:  0.712405\n",
      "Epoch:  327  Training Loss:  220.028  Training Accuracy:  0.712875\n",
      "Epoch:  328  Training Loss:  219.819  Training Accuracy:  0.713228\n",
      "Epoch:  329  Training Loss:  219.566  Training Accuracy:  0.713639\n",
      "Epoch:  330  Training Loss:  219.328  Training Accuracy:  0.714462\n",
      "Epoch:  331  Training Loss:  219.081  Training Accuracy:  0.714933\n",
      "Epoch:  332  Training Loss:  218.818  Training Accuracy:  0.715403\n",
      "Epoch:  333  Training Loss:  218.58  Training Accuracy:  0.715873\n",
      "Epoch:  334  Training Loss:  218.363  Training Accuracy:  0.716344\n",
      "Epoch:  335  Training Loss:  218.083  Training Accuracy:  0.716931\n",
      "Epoch:  336  Training Loss:  217.852  Training Accuracy:  0.717284\n",
      "Epoch:  337  Training Loss:  217.588  Training Accuracy:  0.717931\n",
      "Epoch:  338  Training Loss:  217.407  Training Accuracy:  0.718284\n",
      "Epoch:  339  Training Loss:  217.087  Training Accuracy:  0.718871\n",
      "Epoch:  340  Training Loss:  216.875  Training Accuracy:  0.7194\n",
      "Epoch:  341  Training Loss:  216.613  Training Accuracy:  0.719871\n",
      "Epoch:  342  Training Loss:  216.41  Training Accuracy:  0.720341\n",
      "Epoch:  343  Training Loss:  216.136  Training Accuracy:  0.720811\n",
      "Epoch:  344  Training Loss:  215.878  Training Accuracy:  0.721105\n",
      "Epoch:  345  Training Loss:  215.666  Training Accuracy:  0.721576\n",
      "Epoch:  346  Training Loss:  215.442  Training Accuracy:  0.722105\n",
      "Epoch:  347  Training Loss:  215.141  Training Accuracy:  0.722516\n",
      "Epoch:  348  Training Loss:  214.94  Training Accuracy:  0.723045\n",
      "Epoch:  349  Training Loss:  214.662  Training Accuracy:  0.723575\n",
      "Epoch:  350  Training Loss:  214.442  Training Accuracy:  0.72428\n",
      "Epoch:  351  Training Loss:  214.185  Training Accuracy:  0.72475\n",
      "Epoch:  352  Training Loss:  213.957  Training Accuracy:  0.725103\n",
      "Epoch:  353  Training Loss:  213.699  Training Accuracy:  0.725397\n",
      "Epoch:  354  Training Loss:  213.469  Training Accuracy:  0.725632\n",
      "Epoch:  355  Training Loss:  213.221  Training Accuracy:  0.726044\n",
      "Epoch:  356  Training Loss:  213.023  Training Accuracy:  0.726573\n",
      "Epoch:  357  Training Loss:  212.726  Training Accuracy:  0.727043\n",
      "Epoch:  358  Training Loss:  212.477  Training Accuracy:  0.727337\n",
      "Epoch:  359  Training Loss:  212.254  Training Accuracy:  0.727866\n",
      "Epoch:  360  Training Loss:  212.033  Training Accuracy:  0.728278\n",
      "Epoch:  361  Training Loss:  211.782  Training Accuracy:  0.728983\n",
      "Epoch:  362  Training Loss:  211.514  Training Accuracy:  0.729512\n",
      "Epoch:  363  Training Loss:  211.3  Training Accuracy:  0.729806\n",
      "Epoch:  364  Training Loss:  211.052  Training Accuracy:  0.730218\n",
      "Epoch:  365  Training Loss:  210.815  Training Accuracy:  0.730688\n",
      "Epoch:  366  Training Loss:  210.586  Training Accuracy:  0.731276\n",
      "Epoch:  367  Training Loss:  210.317  Training Accuracy:  0.73204\n",
      "Epoch:  368  Training Loss:  210.114  Training Accuracy:  0.732334\n",
      "Epoch:  369  Training Loss:  209.829  Training Accuracy:  0.732569\n",
      "Epoch:  370  Training Loss:  209.657  Training Accuracy:  0.732922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  209.434  Training Accuracy:  0.733392\n",
      "Epoch:  372  Training Loss:  209.145  Training Accuracy:  0.733863\n",
      "Epoch:  373  Training Loss:  208.944  Training Accuracy:  0.734744\n",
      "Epoch:  374  Training Loss:  208.721  Training Accuracy:  0.735274\n",
      "Epoch:  375  Training Loss:  208.474  Training Accuracy:  0.73545\n",
      "Epoch:  376  Training Loss:  208.244  Training Accuracy:  0.735685\n",
      "Epoch:  377  Training Loss:  208.032  Training Accuracy:  0.735979\n",
      "Epoch:  378  Training Loss:  207.767  Training Accuracy:  0.736273\n",
      "Epoch:  379  Training Loss:  207.524  Training Accuracy:  0.736508\n",
      "Epoch:  380  Training Loss:  207.298  Training Accuracy:  0.73692\n",
      "Epoch:  381  Training Loss:  207.03  Training Accuracy:  0.737449\n",
      "Epoch:  382  Training Loss:  206.852  Training Accuracy:  0.737801\n",
      "Epoch:  383  Training Loss:  206.603  Training Accuracy:  0.738095\n",
      "Epoch:  384  Training Loss:  206.345  Training Accuracy:  0.738566\n",
      "Epoch:  385  Training Loss:  206.113  Training Accuracy:  0.739212\n",
      "Epoch:  386  Training Loss:  205.88  Training Accuracy:  0.739565\n",
      "Epoch:  387  Training Loss:  205.657  Training Accuracy:  0.7398\n",
      "Epoch:  388  Training Loss:  205.423  Training Accuracy:  0.740623\n",
      "Epoch:  389  Training Loss:  205.182  Training Accuracy:  0.7408\n",
      "Epoch:  390  Training Loss:  204.968  Training Accuracy:  0.741094\n",
      "Epoch:  391  Training Loss:  204.699  Training Accuracy:  0.741329\n",
      "Epoch:  392  Training Loss:  204.463  Training Accuracy:  0.741799\n",
      "Epoch:  393  Training Loss:  204.234  Training Accuracy:  0.742093\n",
      "Epoch:  394  Training Loss:  203.997  Training Accuracy:  0.742387\n",
      "Epoch:  395  Training Loss:  203.74  Training Accuracy:  0.743092\n",
      "Epoch:  396  Training Loss:  203.524  Training Accuracy:  0.743504\n",
      "Epoch:  397  Training Loss:  203.284  Training Accuracy:  0.743974\n",
      "Epoch:  398  Training Loss:  203.048  Training Accuracy:  0.744268\n",
      "Epoch:  399  Training Loss:  202.802  Training Accuracy:  0.744739\n",
      "Epoch:  400  Training Loss:  202.58  Training Accuracy:  0.745091\n",
      "Epoch:  401  Training Loss:  202.334  Training Accuracy:  0.745268\n",
      "Epoch:  402  Training Loss:  202.088  Training Accuracy:  0.74562\n",
      "Epoch:  403  Training Loss:  201.882  Training Accuracy:  0.745738\n",
      "Epoch:  404  Training Loss:  201.624  Training Accuracy:  0.74615\n",
      "Epoch:  405  Training Loss:  201.387  Training Accuracy:  0.746502\n",
      "Epoch:  406  Training Loss:  201.162  Training Accuracy:  0.746737\n",
      "Epoch:  407  Training Loss:  200.948  Training Accuracy:  0.74709\n",
      "Epoch:  408  Training Loss:  200.682  Training Accuracy:  0.747502\n",
      "Epoch:  409  Training Loss:  200.477  Training Accuracy:  0.747678\n",
      "Epoch:  410  Training Loss:  200.21  Training Accuracy:  0.747972\n",
      "Epoch:  411  Training Loss:  200.009  Training Accuracy:  0.748442\n",
      "Epoch:  412  Training Loss:  199.772  Training Accuracy:  0.748971\n",
      "Epoch:  413  Training Loss:  199.53  Training Accuracy:  0.749089\n",
      "Epoch:  414  Training Loss:  199.333  Training Accuracy:  0.749559\n",
      "Epoch:  415  Training Loss:  199.116  Training Accuracy:  0.750088\n",
      "Epoch:  416  Training Loss:  198.869  Training Accuracy:  0.750441\n",
      "Epoch:  417  Training Loss:  198.654  Training Accuracy:  0.75097\n",
      "Epoch:  418  Training Loss:  198.405  Training Accuracy:  0.751441\n",
      "Epoch:  419  Training Loss:  198.19  Training Accuracy:  0.751852\n",
      "Epoch:  420  Training Loss:  197.955  Training Accuracy:  0.752499\n",
      "Epoch:  421  Training Loss:  197.735  Training Accuracy:  0.753145\n",
      "Epoch:  422  Training Loss:  197.482  Training Accuracy:  0.753557\n",
      "Epoch:  423  Training Loss:  197.277  Training Accuracy:  0.753968\n",
      "Epoch:  424  Training Loss:  197.021  Training Accuracy:  0.75438\n",
      "Epoch:  425  Training Loss:  196.832  Training Accuracy:  0.754791\n",
      "Epoch:  426  Training Loss:  196.591  Training Accuracy:  0.755203\n",
      "Epoch:  427  Training Loss:  196.354  Training Accuracy:  0.755321\n",
      "Epoch:  428  Training Loss:  196.127  Training Accuracy:  0.756026\n",
      "Epoch:  429  Training Loss:  195.884  Training Accuracy:  0.756261\n",
      "Epoch:  430  Training Loss:  195.686  Training Accuracy:  0.75679\n",
      "Epoch:  431  Training Loss:  195.467  Training Accuracy:  0.757143\n",
      "Epoch:  432  Training Loss:  195.24  Training Accuracy:  0.757496\n",
      "Epoch:  433  Training Loss:  194.996  Training Accuracy:  0.758201\n",
      "Epoch:  434  Training Loss:  194.793  Training Accuracy:  0.75873\n",
      "Epoch:  435  Training Loss:  194.571  Training Accuracy:  0.759142\n",
      "Epoch:  436  Training Loss:  194.313  Training Accuracy:  0.759553\n",
      "Epoch:  437  Training Loss:  194.121  Training Accuracy:  0.759906\n",
      "Epoch:  438  Training Loss:  193.893  Training Accuracy:  0.760024\n",
      "Epoch:  439  Training Loss:  193.641  Training Accuracy:  0.760612\n",
      "Epoch:  440  Training Loss:  193.474  Training Accuracy:  0.761141\n",
      "Epoch:  441  Training Loss:  193.194  Training Accuracy:  0.761435\n",
      "Epoch:  442  Training Loss:  192.962  Training Accuracy:  0.761846\n",
      "Epoch:  443  Training Loss:  192.793  Training Accuracy:  0.762258\n",
      "Epoch:  444  Training Loss:  192.53  Training Accuracy:  0.762375\n",
      "Epoch:  445  Training Loss:  192.264  Training Accuracy:  0.762669\n",
      "Epoch:  446  Training Loss:  192.099  Training Accuracy:  0.763198\n",
      "Epoch:  447  Training Loss:  191.855  Training Accuracy:  0.763727\n",
      "Epoch:  448  Training Loss:  191.627  Training Accuracy:  0.764198\n",
      "Epoch:  449  Training Loss:  191.395  Training Accuracy:  0.764492\n",
      "Epoch:  450  Training Loss:  191.197  Training Accuracy:  0.764786\n",
      "Epoch:  451  Training Loss:  190.954  Training Accuracy:  0.765197\n",
      "Epoch:  452  Training Loss:  190.763  Training Accuracy:  0.765432\n",
      "Epoch:  453  Training Loss:  190.494  Training Accuracy:  0.765961\n",
      "Epoch:  454  Training Loss:  190.322  Training Accuracy:  0.766255\n",
      "Epoch:  455  Training Loss:  190.1  Training Accuracy:  0.766726\n",
      "Epoch:  456  Training Loss:  189.878  Training Accuracy:  0.766961\n",
      "Epoch:  457  Training Loss:  189.612  Training Accuracy:  0.767372\n",
      "Epoch:  458  Training Loss:  189.454  Training Accuracy:  0.767431\n",
      "Epoch:  459  Training Loss:  189.196  Training Accuracy:  0.767901\n",
      "Epoch:  460  Training Loss:  188.997  Training Accuracy:  0.76796\n",
      "Epoch:  461  Training Loss:  188.747  Training Accuracy:  0.768313\n",
      "Epoch:  462  Training Loss:  188.548  Training Accuracy:  0.768666\n",
      "Epoch:  463  Training Loss:  188.313  Training Accuracy:  0.76896\n",
      "Epoch:  464  Training Loss:  188.091  Training Accuracy:  0.769489\n",
      "Epoch:  465  Training Loss:  187.871  Training Accuracy:  0.769783\n",
      "Epoch:  466  Training Loss:  187.654  Training Accuracy:  0.770077\n",
      "Epoch:  467  Training Loss:  187.413  Training Accuracy:  0.770312\n",
      "Epoch:  468  Training Loss:  187.215  Training Accuracy:  0.770782\n",
      "Epoch:  469  Training Loss:  186.984  Training Accuracy:  0.771076\n",
      "Epoch:  470  Training Loss:  186.754  Training Accuracy:  0.771017\n",
      "Epoch:  471  Training Loss:  186.539  Training Accuracy:  0.771252\n",
      "Epoch:  472  Training Loss:  186.344  Training Accuracy:  0.771488\n",
      "Epoch:  473  Training Loss:  186.112  Training Accuracy:  0.771546\n",
      "Epoch:  474  Training Loss:  185.89  Training Accuracy:  0.77184\n",
      "Epoch:  475  Training Loss:  185.668  Training Accuracy:  0.772017\n",
      "Epoch:  476  Training Loss:  185.442  Training Accuracy:  0.772605\n",
      "Epoch:  477  Training Loss:  185.237  Training Accuracy:  0.772781\n",
      "Epoch:  478  Training Loss:  185.028  Training Accuracy:  0.77331\n",
      "Epoch:  479  Training Loss:  184.785  Training Accuracy:  0.773428\n",
      "Epoch:  480  Training Loss:  184.581  Training Accuracy:  0.773839\n",
      "Epoch:  481  Training Loss:  184.385  Training Accuracy:  0.774192\n",
      "Epoch:  482  Training Loss:  184.189  Training Accuracy:  0.774545\n",
      "Epoch:  483  Training Loss:  183.992  Training Accuracy:  0.774956\n",
      "Epoch:  484  Training Loss:  183.695  Training Accuracy:  0.775191\n",
      "Epoch:  485  Training Loss:  183.584  Training Accuracy:  0.775485\n",
      "Epoch:  486  Training Loss:  183.333  Training Accuracy:  0.775838\n",
      "Epoch:  487  Training Loss:  183.118  Training Accuracy:  0.776249\n",
      "Epoch:  488  Training Loss:  182.866  Training Accuracy:  0.776779\n",
      "Epoch:  489  Training Loss:  182.702  Training Accuracy:  0.777308\n",
      "Epoch:  490  Training Loss:  182.462  Training Accuracy:  0.777543\n",
      "Epoch:  491  Training Loss:  182.228  Training Accuracy:  0.777719\n",
      "Epoch:  492  Training Loss:  182.025  Training Accuracy:  0.778131\n",
      "Epoch:  493  Training Loss:  181.866  Training Accuracy:  0.778542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  181.576  Training Accuracy:  0.779071\n",
      "Epoch:  495  Training Loss:  181.401  Training Accuracy:  0.779307\n",
      "Epoch:  496  Training Loss:  181.131  Training Accuracy:  0.779601\n",
      "Epoch:  497  Training Loss:  180.94  Training Accuracy:  0.780012\n",
      "Epoch:  498  Training Loss:  180.745  Training Accuracy:  0.780776\n",
      "Epoch:  499  Training Loss:  180.522  Training Accuracy:  0.781129\n",
      "Epoch:  500  Training Loss:  180.288  Training Accuracy:  0.781482\n",
      "Epoch:  501  Training Loss:  180.084  Training Accuracy:  0.781834\n",
      "Epoch:  502  Training Loss:  179.89  Training Accuracy:  0.78207\n",
      "Epoch:  503  Training Loss:  179.626  Training Accuracy:  0.78254\n",
      "Epoch:  504  Training Loss:  179.458  Training Accuracy:  0.782834\n",
      "Epoch:  505  Training Loss:  179.212  Training Accuracy:  0.78301\n",
      "Epoch:  506  Training Loss:  178.989  Training Accuracy:  0.783128\n",
      "Epoch:  507  Training Loss:  178.822  Training Accuracy:  0.783304\n",
      "Epoch:  508  Training Loss:  178.547  Training Accuracy:  0.783657\n",
      "Epoch:  509  Training Loss:  178.385  Training Accuracy:  0.783833\n",
      "Epoch:  510  Training Loss:  178.149  Training Accuracy:  0.783951\n",
      "Epoch:  511  Training Loss:  177.918  Training Accuracy:  0.784186\n",
      "Epoch:  512  Training Loss:  177.752  Training Accuracy:  0.784539\n",
      "Epoch:  513  Training Loss:  177.506  Training Accuracy:  0.785127\n",
      "Epoch:  514  Training Loss:  177.281  Training Accuracy:  0.785362\n",
      "Epoch:  515  Training Loss:  177.099  Training Accuracy:  0.785656\n",
      "Epoch:  516  Training Loss:  176.845  Training Accuracy:  0.786008\n",
      "Epoch:  517  Training Loss:  176.673  Training Accuracy:  0.786361\n",
      "Epoch:  518  Training Loss:  176.451  Training Accuracy:  0.78642\n",
      "Epoch:  519  Training Loss:  176.259  Training Accuracy:  0.786832\n",
      "Epoch:  520  Training Loss:  176.0  Training Accuracy:  0.78689\n",
      "Epoch:  521  Training Loss:  175.819  Training Accuracy:  0.787008\n",
      "Epoch:  522  Training Loss:  175.624  Training Accuracy:  0.787596\n",
      "Epoch:  523  Training Loss:  175.367  Training Accuracy:  0.787831\n",
      "Epoch:  524  Training Loss:  175.128  Training Accuracy:  0.787831\n",
      "Epoch:  525  Training Loss:  174.939  Training Accuracy:  0.788184\n",
      "Epoch:  526  Training Loss:  174.722  Training Accuracy:  0.788184\n",
      "Epoch:  527  Training Loss:  174.533  Training Accuracy:  0.788595\n",
      "Epoch:  528  Training Loss:  174.236  Training Accuracy:  0.788948\n",
      "Epoch:  529  Training Loss:  174.097  Training Accuracy:  0.789477\n",
      "Epoch:  530  Training Loss:  173.837  Training Accuracy:  0.789712\n",
      "Epoch:  531  Training Loss:  173.68  Training Accuracy:  0.78983\n",
      "Epoch:  532  Training Loss:  173.42  Training Accuracy:  0.790065\n",
      "Epoch:  533  Training Loss:  173.246  Training Accuracy:  0.790418\n",
      "Epoch:  534  Training Loss:  172.981  Training Accuracy:  0.790829\n",
      "Epoch:  535  Training Loss:  172.857  Training Accuracy:  0.791064\n",
      "Epoch:  536  Training Loss:  172.536  Training Accuracy:  0.791241\n",
      "Epoch:  537  Training Loss:  172.41  Training Accuracy:  0.791535\n",
      "Epoch:  538  Training Loss:  172.136  Training Accuracy:  0.791829\n",
      "Epoch:  539  Training Loss:  171.905  Training Accuracy:  0.79224\n",
      "Epoch:  540  Training Loss:  171.761  Training Accuracy:  0.792593\n",
      "Epoch:  541  Training Loss:  171.52  Training Accuracy:  0.793004\n",
      "Epoch:  542  Training Loss:  171.288  Training Accuracy:  0.793063\n",
      "Epoch:  543  Training Loss:  171.105  Training Accuracy:  0.793357\n",
      "Epoch:  544  Training Loss:  170.866  Training Accuracy:  0.793534\n",
      "Epoch:  545  Training Loss:  170.688  Training Accuracy:  0.79371\n",
      "Epoch:  546  Training Loss:  170.496  Training Accuracy:  0.794004\n",
      "Epoch:  547  Training Loss:  170.308  Training Accuracy:  0.794298\n",
      "Epoch:  548  Training Loss:  170.0  Training Accuracy:  0.794592\n",
      "Epoch:  549  Training Loss:  169.917  Training Accuracy:  0.79465\n",
      "Epoch:  550  Training Loss:  169.628  Training Accuracy:  0.795121\n",
      "Epoch:  551  Training Loss:  169.46  Training Accuracy:  0.795415\n",
      "Epoch:  552  Training Loss:  169.224  Training Accuracy:  0.795709\n",
      "Epoch:  553  Training Loss:  169.052  Training Accuracy:  0.796179\n",
      "Epoch:  554  Training Loss:  168.823  Training Accuracy:  0.796708\n",
      "Epoch:  555  Training Loss:  168.625  Training Accuracy:  0.796826\n",
      "Epoch:  556  Training Loss:  168.46  Training Accuracy:  0.797061\n",
      "Epoch:  557  Training Loss:  168.218  Training Accuracy:  0.797296\n",
      "Epoch:  558  Training Loss:  168.052  Training Accuracy:  0.797531\n",
      "Epoch:  559  Training Loss:  167.838  Training Accuracy:  0.797943\n",
      "Epoch:  560  Training Loss:  167.614  Training Accuracy:  0.79806\n",
      "Epoch:  561  Training Loss:  167.471  Training Accuracy:  0.798295\n",
      "Epoch:  562  Training Loss:  167.149  Training Accuracy:  0.798413\n",
      "Epoch:  563  Training Loss:  167.035  Training Accuracy:  0.798707\n",
      "Epoch:  564  Training Loss:  166.781  Training Accuracy:  0.799001\n",
      "Epoch:  565  Training Loss:  166.607  Training Accuracy:  0.799177\n",
      "Epoch:  566  Training Loss:  166.358  Training Accuracy:  0.799471\n",
      "Epoch:  567  Training Loss:  166.242  Training Accuracy:  0.799883\n",
      "Epoch:  568  Training Loss:  165.939  Training Accuracy:  0.800118\n",
      "Epoch:  569  Training Loss:  165.771  Training Accuracy:  0.800412\n",
      "Epoch:  570  Training Loss:  165.572  Training Accuracy:  0.800765\n",
      "Epoch:  571  Training Loss:  165.38  Training Accuracy:  0.801117\n",
      "Epoch:  572  Training Loss:  165.137  Training Accuracy:  0.801529\n",
      "Epoch:  573  Training Loss:  165.0  Training Accuracy:  0.801999\n",
      "Epoch:  574  Training Loss:  164.733  Training Accuracy:  0.802117\n",
      "Epoch:  575  Training Loss:  164.564  Training Accuracy:  0.802293\n",
      "Epoch:  576  Training Loss:  164.341  Training Accuracy:  0.802822\n",
      "Epoch:  577  Training Loss:  164.161  Training Accuracy:  0.80294\n",
      "Epoch:  578  Training Loss:  163.924  Training Accuracy:  0.80341\n",
      "Epoch:  579  Training Loss:  163.734  Training Accuracy:  0.803586\n",
      "Epoch:  580  Training Loss:  163.519  Training Accuracy:  0.803645\n",
      "Epoch:  581  Training Loss:  163.386  Training Accuracy:  0.804115\n",
      "Epoch:  582  Training Loss:  163.106  Training Accuracy:  0.80441\n",
      "Epoch:  583  Training Loss:  162.942  Training Accuracy:  0.804645\n",
      "Epoch:  584  Training Loss:  162.672  Training Accuracy:  0.80488\n",
      "Epoch:  585  Training Loss:  162.599  Training Accuracy:  0.805233\n",
      "Epoch:  586  Training Loss:  162.32  Training Accuracy:  0.805526\n",
      "Epoch:  587  Training Loss:  162.133  Training Accuracy:  0.805585\n",
      "Epoch:  588  Training Loss:  161.886  Training Accuracy:  0.805997\n",
      "Epoch:  589  Training Loss:  161.736  Training Accuracy:  0.806232\n",
      "Epoch:  590  Training Loss:  161.46  Training Accuracy:  0.806467\n",
      "Epoch:  591  Training Loss:  161.381  Training Accuracy:  0.806879\n",
      "Epoch:  592  Training Loss:  161.137  Training Accuracy:  0.807231\n",
      "Epoch:  593  Training Loss:  160.887  Training Accuracy:  0.80729\n",
      "Epoch:  594  Training Loss:  160.679  Training Accuracy:  0.807525\n",
      "Epoch:  595  Training Loss:  160.502  Training Accuracy:  0.807996\n",
      "Epoch:  596  Training Loss:  160.31  Training Accuracy:  0.808231\n",
      "Epoch:  597  Training Loss:  160.111  Training Accuracy:  0.808583\n",
      "Epoch:  598  Training Loss:  159.889  Training Accuracy:  0.808642\n",
      "Epoch:  599  Training Loss:  159.717  Training Accuracy:  0.808936\n",
      "Epoch:  600  Training Loss:  159.499  Training Accuracy:  0.809407\n",
      "Epoch:  601  Training Loss:  159.328  Training Accuracy:  0.809583\n",
      "Epoch:  602  Training Loss:  159.082  Training Accuracy:  0.809818\n",
      "Epoch:  603  Training Loss:  158.908  Training Accuracy:  0.81023\n",
      "Epoch:  604  Training Loss:  158.707  Training Accuracy:  0.810524\n",
      "Epoch:  605  Training Loss:  158.505  Training Accuracy:  0.810641\n",
      "Epoch:  606  Training Loss:  158.315  Training Accuracy:  0.810817\n",
      "Epoch:  607  Training Loss:  158.152  Training Accuracy:  0.811288\n",
      "Epoch:  608  Training Loss:  157.934  Training Accuracy:  0.811582\n",
      "Epoch:  609  Training Loss:  157.726  Training Accuracy:  0.811934\n",
      "Epoch:  610  Training Loss:  157.483  Training Accuracy:  0.812228\n",
      "Epoch:  611  Training Loss:  157.384  Training Accuracy:  0.812758\n",
      "Epoch:  612  Training Loss:  157.084  Training Accuracy:  0.812993\n",
      "Epoch:  613  Training Loss:  156.954  Training Accuracy:  0.813345\n",
      "Epoch:  614  Training Loss:  156.676  Training Accuracy:  0.813933\n",
      "Epoch:  615  Training Loss:  156.514  Training Accuracy:  0.814168\n",
      "Epoch:  616  Training Loss:  156.297  Training Accuracy:  0.814521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  156.139  Training Accuracy:  0.814698\n",
      "Epoch:  618  Training Loss:  155.845  Training Accuracy:  0.815109\n",
      "Epoch:  619  Training Loss:  155.727  Training Accuracy:  0.815227\n",
      "Epoch:  620  Training Loss:  155.495  Training Accuracy:  0.815403\n",
      "Epoch:  621  Training Loss:  155.31  Training Accuracy:  0.815873\n",
      "Epoch:  622  Training Loss:  155.066  Training Accuracy:  0.815991\n",
      "Epoch:  623  Training Loss:  154.934  Training Accuracy:  0.816108\n",
      "Epoch:  624  Training Loss:  154.695  Training Accuracy:  0.816696\n",
      "Epoch:  625  Training Loss:  154.529  Training Accuracy:  0.817284\n",
      "Epoch:  626  Training Loss:  154.293  Training Accuracy:  0.817343\n",
      "Epoch:  627  Training Loss:  154.072  Training Accuracy:  0.817755\n",
      "Epoch:  628  Training Loss:  153.896  Training Accuracy:  0.81799\n",
      "Epoch:  629  Training Loss:  153.722  Training Accuracy:  0.818401\n",
      "Epoch:  630  Training Loss:  153.462  Training Accuracy:  0.818813\n",
      "Epoch:  631  Training Loss:  153.32  Training Accuracy:  0.81893\n",
      "Epoch:  632  Training Loss:  153.032  Training Accuracy:  0.819283\n",
      "Epoch:  633  Training Loss:  152.885  Training Accuracy:  0.819518\n",
      "Epoch:  634  Training Loss:  152.619  Training Accuracy:  0.819812\n",
      "Epoch:  635  Training Loss:  152.501  Training Accuracy:  0.819989\n",
      "Epoch:  636  Training Loss:  152.241  Training Accuracy:  0.820106\n",
      "Epoch:  637  Training Loss:  152.097  Training Accuracy:  0.820518\n",
      "Epoch:  638  Training Loss:  151.856  Training Accuracy:  0.82087\n",
      "Epoch:  639  Training Loss:  151.665  Training Accuracy:  0.821106\n",
      "Epoch:  640  Training Loss:  151.39  Training Accuracy:  0.821282\n",
      "Epoch:  641  Training Loss:  151.25  Training Accuracy:  0.821517\n",
      "Epoch:  642  Training Loss:  150.994  Training Accuracy:  0.821752\n",
      "Epoch:  643  Training Loss:  150.855  Training Accuracy:  0.821929\n",
      "Epoch:  644  Training Loss:  150.548  Training Accuracy:  0.822046\n",
      "Epoch:  645  Training Loss:  150.487  Training Accuracy:  0.822164\n",
      "Epoch:  646  Training Loss:  150.194  Training Accuracy:  0.822458\n",
      "Epoch:  647  Training Loss:  150.061  Training Accuracy:  0.822634\n",
      "Epoch:  648  Training Loss:  149.784  Training Accuracy:  0.82281\n",
      "Epoch:  649  Training Loss:  149.642  Training Accuracy:  0.823104\n",
      "Epoch:  650  Training Loss:  149.412  Training Accuracy:  0.82334\n",
      "Epoch:  651  Training Loss:  149.217  Training Accuracy:  0.823575\n",
      "Epoch:  652  Training Loss:  149.03  Training Accuracy:  0.823869\n",
      "Epoch:  653  Training Loss:  148.854  Training Accuracy:  0.824045\n",
      "Epoch:  654  Training Loss:  148.615  Training Accuracy:  0.824221\n",
      "Epoch:  655  Training Loss:  148.415  Training Accuracy:  0.824398\n",
      "Epoch:  656  Training Loss:  148.211  Training Accuracy:  0.824633\n",
      "Epoch:  657  Training Loss:  148.043  Training Accuracy:  0.824986\n",
      "Epoch:  658  Training Loss:  147.837  Training Accuracy:  0.82528\n",
      "Epoch:  659  Training Loss:  147.577  Training Accuracy:  0.825515\n",
      "Epoch:  660  Training Loss:  147.428  Training Accuracy:  0.825573\n",
      "Epoch:  661  Training Loss:  147.187  Training Accuracy:  0.825574\n",
      "Epoch:  662  Training Loss:  147.051  Training Accuracy:  0.82575\n",
      "Epoch:  663  Training Loss:  146.772  Training Accuracy:  0.82622\n",
      "Epoch:  664  Training Loss:  146.648  Training Accuracy:  0.826397\n",
      "Epoch:  665  Training Loss:  146.39  Training Accuracy:  0.826749\n",
      "Epoch:  666  Training Loss:  146.234  Training Accuracy:  0.826984\n",
      "Epoch:  667  Training Loss:  146.032  Training Accuracy:  0.827161\n",
      "Epoch:  668  Training Loss:  145.865  Training Accuracy:  0.827337\n",
      "Epoch:  669  Training Loss:  145.636  Training Accuracy:  0.82769\n",
      "Epoch:  670  Training Loss:  145.485  Training Accuracy:  0.828043\n",
      "Epoch:  671  Training Loss:  145.248  Training Accuracy:  0.828395\n",
      "Epoch:  672  Training Loss:  145.047  Training Accuracy:  0.828454\n",
      "Epoch:  673  Training Loss:  144.852  Training Accuracy:  0.828572\n",
      "Epoch:  674  Training Loss:  144.718  Training Accuracy:  0.828866\n",
      "Epoch:  675  Training Loss:  144.48  Training Accuracy:  0.828983\n",
      "Epoch:  676  Training Loss:  144.325  Training Accuracy:  0.829571\n",
      "Epoch:  677  Training Loss:  144.129  Training Accuracy:  0.829512\n",
      "Epoch:  678  Training Loss:  143.917  Training Accuracy:  0.829747\n",
      "Epoch:  679  Training Loss:  143.721  Training Accuracy:  0.829865\n",
      "Epoch:  680  Training Loss:  143.545  Training Accuracy:  0.830394\n",
      "Epoch:  681  Training Loss:  143.315  Training Accuracy:  0.830747\n",
      "Epoch:  682  Training Loss:  143.149  Training Accuracy:  0.830864\n",
      "Epoch:  683  Training Loss:  142.952  Training Accuracy:  0.831158\n",
      "Epoch:  684  Training Loss:  142.732  Training Accuracy:  0.831452\n",
      "Epoch:  685  Training Loss:  142.543  Training Accuracy:  0.831688\n",
      "Epoch:  686  Training Loss:  142.359  Training Accuracy:  0.831982\n",
      "Epoch:  687  Training Loss:  142.189  Training Accuracy:  0.832099\n",
      "Epoch:  688  Training Loss:  141.965  Training Accuracy:  0.832334\n",
      "Epoch:  689  Training Loss:  141.759  Training Accuracy:  0.832511\n",
      "Epoch:  690  Training Loss:  141.596  Training Accuracy:  0.832628\n",
      "Epoch:  691  Training Loss:  141.383  Training Accuracy:  0.833099\n",
      "Epoch:  692  Training Loss:  141.182  Training Accuracy:  0.833393\n",
      "Epoch:  693  Training Loss:  141.025  Training Accuracy:  0.833745\n",
      "Epoch:  694  Training Loss:  140.821  Training Accuracy:  0.833922\n",
      "Epoch:  695  Training Loss:  140.63  Training Accuracy:  0.834216\n",
      "Epoch:  696  Training Loss:  140.426  Training Accuracy:  0.834627\n",
      "Epoch:  697  Training Loss:  140.252  Training Accuracy:  0.83498\n",
      "Epoch:  698  Training Loss:  140.004  Training Accuracy:  0.835274\n",
      "Epoch:  699  Training Loss:  139.862  Training Accuracy:  0.835274\n",
      "Epoch:  700  Training Loss:  139.634  Training Accuracy:  0.835685\n",
      "Epoch:  701  Training Loss:  139.49  Training Accuracy:  0.83592\n",
      "Epoch:  702  Training Loss:  139.28  Training Accuracy:  0.836332\n",
      "Epoch:  703  Training Loss:  139.035  Training Accuracy:  0.836214\n",
      "Epoch:  704  Training Loss:  138.911  Training Accuracy:  0.836626\n",
      "Epoch:  705  Training Loss:  138.69  Training Accuracy:  0.836685\n",
      "Epoch:  706  Training Loss:  138.52  Training Accuracy:  0.836743\n",
      "Epoch:  707  Training Loss:  138.333  Training Accuracy:  0.837155\n",
      "Epoch:  708  Training Loss:  138.138  Training Accuracy:  0.837625\n",
      "Epoch:  709  Training Loss:  137.969  Training Accuracy:  0.837684\n",
      "Epoch:  710  Training Loss:  137.753  Training Accuracy:  0.837919\n",
      "Epoch:  711  Training Loss:  137.562  Training Accuracy:  0.837978\n",
      "Epoch:  712  Training Loss:  137.402  Training Accuracy:  0.838331\n",
      "Epoch:  713  Training Loss:  137.16  Training Accuracy:  0.838331\n",
      "Epoch:  714  Training Loss:  137.028  Training Accuracy:  0.838448\n",
      "Epoch:  715  Training Loss:  136.808  Training Accuracy:  0.838919\n",
      "Epoch:  716  Training Loss:  136.639  Training Accuracy:  0.839154\n",
      "Epoch:  717  Training Loss:  136.463  Training Accuracy:  0.839507\n",
      "Epoch:  718  Training Loss:  136.244  Training Accuracy:  0.839683\n",
      "Epoch:  719  Training Loss:  136.085  Training Accuracy:  0.840036\n",
      "Epoch:  720  Training Loss:  135.879  Training Accuracy:  0.840153\n",
      "Epoch:  721  Training Loss:  135.714  Training Accuracy:  0.840565\n",
      "Epoch:  722  Training Loss:  135.537  Training Accuracy:  0.840565\n",
      "Epoch:  723  Training Loss:  135.326  Training Accuracy:  0.840917\n",
      "Epoch:  724  Training Loss:  135.116  Training Accuracy:  0.841094\n",
      "Epoch:  725  Training Loss:  134.994  Training Accuracy:  0.841388\n",
      "Epoch:  726  Training Loss:  134.798  Training Accuracy:  0.841623\n",
      "Epoch:  727  Training Loss:  134.611  Training Accuracy:  0.841976\n",
      "Epoch:  728  Training Loss:  134.447  Training Accuracy:  0.842093\n",
      "Epoch:  729  Training Loss:  134.239  Training Accuracy:  0.842446\n",
      "Epoch:  730  Training Loss:  134.069  Training Accuracy:  0.84274\n",
      "Epoch:  731  Training Loss:  133.896  Training Accuracy:  0.843093\n",
      "Epoch:  732  Training Loss:  133.717  Training Accuracy:  0.843269\n",
      "Epoch:  733  Training Loss:  133.489  Training Accuracy:  0.843622\n",
      "Epoch:  734  Training Loss:  133.359  Training Accuracy:  0.843916\n",
      "Epoch:  735  Training Loss:  133.171  Training Accuracy:  0.844092\n",
      "Epoch:  736  Training Loss:  133.011  Training Accuracy:  0.844327\n",
      "Epoch:  737  Training Loss:  132.809  Training Accuracy:  0.844445\n",
      "Epoch:  738  Training Loss:  132.659  Training Accuracy:  0.844797\n",
      "Epoch:  739  Training Loss:  132.437  Training Accuracy:  0.844974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  740  Training Loss:  132.31  Training Accuracy:  0.845268\n",
      "Epoch:  741  Training Loss:  132.067  Training Accuracy:  0.845385\n",
      "Epoch:  742  Training Loss:  131.912  Training Accuracy:  0.845503\n",
      "Epoch:  743  Training Loss:  131.72  Training Accuracy:  0.845797\n",
      "Epoch:  744  Training Loss:  131.574  Training Accuracy:  0.846091\n",
      "Epoch:  745  Training Loss:  131.375  Training Accuracy:  0.846444\n",
      "Epoch:  746  Training Loss:  131.177  Training Accuracy:  0.846738\n",
      "Epoch:  747  Training Loss:  131.032  Training Accuracy:  0.84709\n",
      "Epoch:  748  Training Loss:  130.844  Training Accuracy:  0.847149\n",
      "Epoch:  749  Training Loss:  130.647  Training Accuracy:  0.847325\n",
      "Epoch:  750  Training Loss:  130.494  Training Accuracy:  0.847443\n",
      "Epoch:  751  Training Loss:  130.28  Training Accuracy:  0.847737\n",
      "Epoch:  752  Training Loss:  130.128  Training Accuracy:  0.848031\n",
      "Epoch:  753  Training Loss:  129.922  Training Accuracy:  0.848325\n",
      "Epoch:  754  Training Loss:  129.781  Training Accuracy:  0.848501\n",
      "Epoch:  755  Training Loss:  129.581  Training Accuracy:  0.848913\n",
      "Epoch:  756  Training Loss:  129.404  Training Accuracy:  0.849089\n",
      "Epoch:  757  Training Loss:  129.226  Training Accuracy:  0.849148\n",
      "Epoch:  758  Training Loss:  129.047  Training Accuracy:  0.849501\n",
      "Epoch:  759  Training Loss:  128.865  Training Accuracy:  0.849677\n",
      "Epoch:  760  Training Loss:  128.65  Training Accuracy:  0.849795\n",
      "Epoch:  761  Training Loss:  128.505  Training Accuracy:  0.85003\n",
      "Epoch:  762  Training Loss:  128.324  Training Accuracy:  0.850265\n",
      "Epoch:  763  Training Loss:  128.121  Training Accuracy:  0.850383\n",
      "Epoch:  764  Training Loss:  127.976  Training Accuracy:  0.850559\n",
      "Epoch:  765  Training Loss:  127.764  Training Accuracy:  0.850618\n",
      "Epoch:  766  Training Loss:  127.604  Training Accuracy:  0.851206\n",
      "Epoch:  767  Training Loss:  127.405  Training Accuracy:  0.851264\n",
      "Epoch:  768  Training Loss:  127.266  Training Accuracy:  0.851382\n",
      "Epoch:  769  Training Loss:  127.06  Training Accuracy:  0.851676\n",
      "Epoch:  770  Training Loss:  126.89  Training Accuracy:  0.851852\n",
      "Epoch:  771  Training Loss:  126.713  Training Accuracy:  0.851911\n",
      "Epoch:  772  Training Loss:  126.569  Training Accuracy:  0.852146\n",
      "Epoch:  773  Training Loss:  126.379  Training Accuracy:  0.85244\n",
      "Epoch:  774  Training Loss:  126.212  Training Accuracy:  0.852852\n",
      "Epoch:  775  Training Loss:  126.02  Training Accuracy:  0.853204\n",
      "Epoch:  776  Training Loss:  125.869  Training Accuracy:  0.85344\n",
      "Epoch:  777  Training Loss:  125.664  Training Accuracy:  0.853616\n",
      "Epoch:  778  Training Loss:  125.511  Training Accuracy:  0.853969\n",
      "Epoch:  779  Training Loss:  125.291  Training Accuracy:  0.854086\n",
      "Epoch:  780  Training Loss:  125.145  Training Accuracy:  0.854321\n",
      "Epoch:  781  Training Loss:  124.946  Training Accuracy:  0.854498\n",
      "Epoch:  782  Training Loss:  124.773  Training Accuracy:  0.854615\n",
      "Epoch:  783  Training Loss:  124.584  Training Accuracy:  0.854851\n",
      "Epoch:  784  Training Loss:  124.469  Training Accuracy:  0.855203\n",
      "Epoch:  785  Training Loss:  124.23  Training Accuracy:  0.85538\n",
      "Epoch:  786  Training Loss:  124.112  Training Accuracy:  0.855674\n",
      "Epoch:  787  Training Loss:  123.897  Training Accuracy:  0.855674\n",
      "Epoch:  788  Training Loss:  123.725  Training Accuracy:  0.855967\n",
      "Epoch:  789  Training Loss:  123.55  Training Accuracy:  0.856261\n",
      "Epoch:  790  Training Loss:  123.417  Training Accuracy:  0.856261\n",
      "Epoch:  791  Training Loss:  123.185  Training Accuracy:  0.856497\n",
      "Epoch:  792  Training Loss:  123.04  Training Accuracy:  0.856555\n",
      "Epoch:  793  Training Loss:  122.841  Training Accuracy:  0.856732\n",
      "Epoch:  794  Training Loss:  122.705  Training Accuracy:  0.856967\n",
      "Epoch:  795  Training Loss:  122.469  Training Accuracy:  0.857084\n",
      "Epoch:  796  Training Loss:  122.377  Training Accuracy:  0.857496\n",
      "Epoch:  797  Training Loss:  122.148  Training Accuracy:  0.857555\n",
      "Epoch:  798  Training Loss:  122.002  Training Accuracy:  0.857672\n",
      "Epoch:  799  Training Loss:  121.822  Training Accuracy:  0.857907\n",
      "Epoch:  800  Training Loss:  121.669  Training Accuracy:  0.857849\n",
      "Epoch:  801  Training Loss:  121.456  Training Accuracy:  0.857966\n",
      "Epoch:  802  Training Loss:  121.309  Training Accuracy:  0.858025\n",
      "Epoch:  803  Training Loss:  121.163  Training Accuracy:  0.858201\n",
      "Epoch:  804  Training Loss:  120.987  Training Accuracy:  0.858495\n",
      "Epoch:  805  Training Loss:  120.799  Training Accuracy:  0.858731\n",
      "Epoch:  806  Training Loss:  120.65  Training Accuracy:  0.858848\n",
      "Epoch:  807  Training Loss:  120.477  Training Accuracy:  0.858966\n",
      "Epoch:  808  Training Loss:  120.313  Training Accuracy:  0.85926\n",
      "Epoch:  809  Training Loss:  120.103  Training Accuracy:  0.85926\n",
      "Epoch:  810  Training Loss:  120.001  Training Accuracy:  0.85926\n",
      "Epoch:  811  Training Loss:  119.766  Training Accuracy:  0.859436\n",
      "Epoch:  812  Training Loss:  119.599  Training Accuracy:  0.859554\n",
      "Epoch:  813  Training Loss:  119.43  Training Accuracy:  0.859789\n",
      "Epoch:  814  Training Loss:  119.287  Training Accuracy:  0.859906\n",
      "Epoch:  815  Training Loss:  119.078  Training Accuracy:  0.860259\n",
      "Epoch:  816  Training Loss:  118.94  Training Accuracy:  0.860494\n",
      "Epoch:  817  Training Loss:  118.76  Training Accuracy:  0.860906\n",
      "Epoch:  818  Training Loss:  118.605  Training Accuracy:  0.861023\n",
      "Epoch:  819  Training Loss:  118.407  Training Accuracy:  0.861259\n",
      "Epoch:  820  Training Loss:  118.243  Training Accuracy:  0.861435\n",
      "Epoch:  821  Training Loss:  118.068  Training Accuracy:  0.861552\n",
      "Epoch:  822  Training Loss:  117.892  Training Accuracy:  0.861552\n",
      "Epoch:  823  Training Loss:  117.706  Training Accuracy:  0.86167\n",
      "Epoch:  824  Training Loss:  117.603  Training Accuracy:  0.861905\n",
      "Epoch:  825  Training Loss:  117.385  Training Accuracy:  0.862082\n",
      "Epoch:  826  Training Loss:  117.266  Training Accuracy:  0.862023\n",
      "Epoch:  827  Training Loss:  117.025  Training Accuracy:  0.862199\n",
      "Epoch:  828  Training Loss:  116.898  Training Accuracy:  0.862375\n",
      "Epoch:  829  Training Loss:  116.713  Training Accuracy:  0.862434\n",
      "Epoch:  830  Training Loss:  116.515  Training Accuracy:  0.862669\n",
      "Epoch:  831  Training Loss:  116.374  Training Accuracy:  0.862846\n",
      "Epoch:  832  Training Loss:  116.224  Training Accuracy:  0.862963\n",
      "Epoch:  833  Training Loss:  116.014  Training Accuracy:  0.863257\n",
      "Epoch:  834  Training Loss:  115.875  Training Accuracy:  0.863434\n",
      "Epoch:  835  Training Loss:  115.685  Training Accuracy:  0.863375\n",
      "Epoch:  836  Training Loss:  115.547  Training Accuracy:  0.86361\n",
      "Epoch:  837  Training Loss:  115.331  Training Accuracy:  0.86361\n",
      "Epoch:  838  Training Loss:  115.197  Training Accuracy:  0.863845\n",
      "Epoch:  839  Training Loss:  115.015  Training Accuracy:  0.863963\n",
      "Epoch:  840  Training Loss:  114.873  Training Accuracy:  0.863845\n",
      "Epoch:  841  Training Loss:  114.657  Training Accuracy:  0.864021\n",
      "Epoch:  842  Training Loss:  114.522  Training Accuracy:  0.864139\n",
      "Epoch:  843  Training Loss:  114.339  Training Accuracy:  0.864316\n",
      "Epoch:  844  Training Loss:  114.185  Training Accuracy:  0.864374\n",
      "Epoch:  845  Training Loss:  114.005  Training Accuracy:  0.864492\n",
      "Epoch:  846  Training Loss:  113.855  Training Accuracy:  0.864492\n",
      "Epoch:  847  Training Loss:  113.705  Training Accuracy:  0.864609\n",
      "Epoch:  848  Training Loss:  113.509  Training Accuracy:  0.864786\n",
      "Epoch:  849  Training Loss:  113.362  Training Accuracy:  0.864845\n",
      "Epoch:  850  Training Loss:  113.223  Training Accuracy:  0.864903\n",
      "Epoch:  851  Training Loss:  113.02  Training Accuracy:  0.86508\n",
      "Epoch:  852  Training Loss:  112.845  Training Accuracy:  0.865197\n",
      "Epoch:  853  Training Loss:  112.682  Training Accuracy:  0.865256\n",
      "Epoch:  854  Training Loss:  112.523  Training Accuracy:  0.865433\n",
      "Epoch:  855  Training Loss:  112.37  Training Accuracy:  0.865491\n",
      "Epoch:  856  Training Loss:  112.195  Training Accuracy:  0.865609\n",
      "Epoch:  857  Training Loss:  112.027  Training Accuracy:  0.865785\n",
      "Epoch:  858  Training Loss:  111.852  Training Accuracy:  0.865903\n",
      "Epoch:  859  Training Loss:  111.721  Training Accuracy:  0.86602\n",
      "Epoch:  860  Training Loss:  111.53  Training Accuracy:  0.866138\n",
      "Epoch:  861  Training Loss:  111.363  Training Accuracy:  0.866197\n",
      "Epoch:  862  Training Loss:  111.194  Training Accuracy:  0.866491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  863  Training Loss:  111.03  Training Accuracy:  0.866608\n",
      "Epoch:  864  Training Loss:  110.91  Training Accuracy:  0.866961\n",
      "Epoch:  865  Training Loss:  110.707  Training Accuracy:  0.867137\n",
      "Epoch:  866  Training Loss:  110.554  Training Accuracy:  0.867196\n",
      "Epoch:  867  Training Loss:  110.421  Training Accuracy:  0.867255\n",
      "Epoch:  868  Training Loss:  110.222  Training Accuracy:  0.867373\n",
      "Epoch:  869  Training Loss:  110.068  Training Accuracy:  0.867608\n",
      "Epoch:  870  Training Loss:  109.868  Training Accuracy:  0.867666\n",
      "Epoch:  871  Training Loss:  109.726  Training Accuracy:  0.867725\n",
      "Epoch:  872  Training Loss:  109.562  Training Accuracy:  0.867725\n",
      "Epoch:  873  Training Loss:  109.404  Training Accuracy:  0.867902\n",
      "Epoch:  874  Training Loss:  109.199  Training Accuracy:  0.86796\n",
      "Epoch:  875  Training Loss:  109.098  Training Accuracy:  0.868196\n",
      "Epoch:  876  Training Loss:  108.871  Training Accuracy:  0.868313\n",
      "Epoch:  877  Training Loss:  108.759  Training Accuracy:  0.868431\n",
      "Epoch:  878  Training Loss:  108.55  Training Accuracy:  0.868548\n",
      "Epoch:  879  Training Loss:  108.363  Training Accuracy:  0.868607\n",
      "Epoch:  880  Training Loss:  108.253  Training Accuracy:  0.868783\n",
      "Epoch:  881  Training Loss:  108.037  Training Accuracy:  0.869077\n",
      "Epoch:  882  Training Loss:  107.942  Training Accuracy:  0.869254\n",
      "Epoch:  883  Training Loss:  107.728  Training Accuracy:  0.86943\n",
      "Epoch:  884  Training Loss:  107.596  Training Accuracy:  0.86943\n",
      "Epoch:  885  Training Loss:  107.405  Training Accuracy:  0.869606\n",
      "Epoch:  886  Training Loss:  107.246  Training Accuracy:  0.869783\n",
      "Epoch:  887  Training Loss:  107.141  Training Accuracy:  0.870077\n",
      "Epoch:  888  Training Loss:  106.964  Training Accuracy:  0.870312\n",
      "Epoch:  889  Training Loss:  106.72  Training Accuracy:  0.870547\n",
      "Epoch:  890  Training Loss:  106.611  Training Accuracy:  0.870782\n",
      "Epoch:  891  Training Loss:  106.419  Training Accuracy:  0.871017\n",
      "Epoch:  892  Training Loss:  106.259  Training Accuracy:  0.871076\n",
      "Epoch:  893  Training Loss:  106.079  Training Accuracy:  0.871547\n",
      "Epoch:  894  Training Loss:  105.906  Training Accuracy:  0.871664\n",
      "Epoch:  895  Training Loss:  105.74  Training Accuracy:  0.871605\n",
      "Epoch:  896  Training Loss:  105.703  Training Accuracy:  0.871664\n",
      "Epoch:  897  Training Loss:  105.437  Training Accuracy:  0.871782\n",
      "Epoch:  898  Training Loss:  105.29  Training Accuracy:  0.871958\n",
      "Epoch:  899  Training Loss:  105.125  Training Accuracy:  0.872017\n",
      "Epoch:  900  Training Loss:  104.941  Training Accuracy:  0.872134\n",
      "Epoch:  901  Training Loss:  104.788  Training Accuracy:  0.872076\n",
      "Epoch:  902  Training Loss:  104.646  Training Accuracy:  0.872311\n",
      "Epoch:  903  Training Loss:  104.484  Training Accuracy:  0.872487\n",
      "Epoch:  904  Training Loss:  104.284  Training Accuracy:  0.872664\n",
      "Epoch:  905  Training Loss:  104.218  Training Accuracy:  0.87284\n",
      "Epoch:  906  Training Loss:  104.017  Training Accuracy:  0.872781\n",
      "Epoch:  907  Training Loss:  103.807  Training Accuracy:  0.873016\n",
      "Epoch:  908  Training Loss:  103.666  Training Accuracy:  0.873251\n",
      "Epoch:  909  Training Loss:  103.585  Training Accuracy:  0.873428\n",
      "Epoch:  910  Training Loss:  103.382  Training Accuracy:  0.873545\n",
      "Epoch:  911  Training Loss:  103.177  Training Accuracy:  0.873545\n",
      "Epoch:  912  Training Loss:  103.007  Training Accuracy:  0.873722\n",
      "Epoch:  913  Training Loss:  102.848  Training Accuracy:  0.873663\n",
      "Epoch:  914  Training Loss:  102.728  Training Accuracy:  0.873722\n",
      "Epoch:  915  Training Loss:  102.608  Training Accuracy:  0.873957\n",
      "Epoch:  916  Training Loss:  102.434  Training Accuracy:  0.874133\n",
      "Epoch:  917  Training Loss:  102.228  Training Accuracy:  0.874427\n",
      "Epoch:  918  Training Loss:  102.067  Training Accuracy:  0.874545\n",
      "Epoch:  919  Training Loss:  101.938  Training Accuracy:  0.874662\n",
      "Epoch:  920  Training Loss:  101.752  Training Accuracy:  0.874721\n",
      "Epoch:  921  Training Loss:  101.616  Training Accuracy:  0.874839\n",
      "Epoch:  922  Training Loss:  101.511  Training Accuracy:  0.875015\n",
      "Epoch:  923  Training Loss:  101.271  Training Accuracy:  0.875192\n",
      "Epoch:  924  Training Loss:  101.13  Training Accuracy:  0.87525\n",
      "Epoch:  925  Training Loss:  101.081  Training Accuracy:  0.875544\n",
      "Epoch:  926  Training Loss:  100.849  Training Accuracy:  0.875603\n",
      "Epoch:  927  Training Loss:  100.645  Training Accuracy:  0.875721\n",
      "Epoch:  928  Training Loss:  100.521  Training Accuracy:  0.875897\n",
      "Epoch:  929  Training Loss:  100.45  Training Accuracy:  0.876073\n",
      "Epoch:  930  Training Loss:  100.233  Training Accuracy:  0.87625\n",
      "Epoch:  931  Training Loss:  100.02  Training Accuracy:  0.876544\n",
      "Epoch:  932  Training Loss:  99.9167  Training Accuracy:  0.877073\n",
      "Epoch:  933  Training Loss:  99.7234  Training Accuracy:  0.87719\n",
      "Epoch:  934  Training Loss:  99.5655  Training Accuracy:  0.877308\n",
      "Epoch:  935  Training Loss:  99.5145  Training Accuracy:  0.877602\n",
      "Epoch:  936  Training Loss:  99.2482  Training Accuracy:  0.877896\n",
      "Epoch:  937  Training Loss:  99.1372  Training Accuracy:  0.878131\n",
      "Epoch:  938  Training Loss:  98.94  Training Accuracy:  0.878307\n",
      "Epoch:  939  Training Loss:  98.8663  Training Accuracy:  0.878542\n",
      "Epoch:  940  Training Loss:  98.6569  Training Accuracy:  0.878542\n",
      "Epoch:  941  Training Loss:  98.4856  Training Accuracy:  0.878954\n",
      "Epoch:  942  Training Loss:  98.3393  Training Accuracy:  0.879072\n",
      "Epoch:  943  Training Loss:  98.1786  Training Accuracy:  0.879189\n",
      "Epoch:  944  Training Loss:  98.0952  Training Accuracy:  0.879072\n",
      "Epoch:  945  Training Loss:  97.8823  Training Accuracy:  0.879659\n",
      "Epoch:  946  Training Loss:  97.7278  Training Accuracy:  0.879601\n",
      "Epoch:  947  Training Loss:  97.5751  Training Accuracy:  0.879895\n",
      "Epoch:  948  Training Loss:  97.4265  Training Accuracy:  0.879836\n",
      "Epoch:  949  Training Loss:  97.2865  Training Accuracy:  0.880071\n",
      "Epoch:  950  Training Loss:  97.1121  Training Accuracy:  0.8806\n",
      "Epoch:  951  Training Loss:  96.9634  Training Accuracy:  0.8806\n",
      "Epoch:  952  Training Loss:  96.8406  Training Accuracy:  0.880835\n",
      "Epoch:  953  Training Loss:  96.6709  Training Accuracy:  0.880953\n",
      "Epoch:  954  Training Loss:  96.6178  Training Accuracy:  0.881129\n",
      "Epoch:  955  Training Loss:  96.4221  Training Accuracy:  0.881247\n",
      "Epoch:  956  Training Loss:  96.2028  Training Accuracy:  0.881423\n",
      "Epoch:  957  Training Loss:  96.0831  Training Accuracy:  0.881482\n",
      "Epoch:  958  Training Loss:  95.9356  Training Accuracy:  0.881717\n",
      "Epoch:  959  Training Loss:  95.7788  Training Accuracy:  0.881952\n",
      "Epoch:  960  Training Loss:  95.5844  Training Accuracy:  0.881952\n",
      "Epoch:  961  Training Loss:  95.4762  Training Accuracy:  0.882129\n",
      "Epoch:  962  Training Loss:  95.2899  Training Accuracy:  0.882423\n",
      "Epoch:  963  Training Loss:  95.1318  Training Accuracy:  0.882716\n",
      "Epoch:  964  Training Loss:  95.0882  Training Accuracy:  0.882717\n",
      "Epoch:  965  Training Loss:  94.9014  Training Accuracy:  0.882952\n",
      "Epoch:  966  Training Loss:  94.6664  Training Accuracy:  0.883246\n",
      "Epoch:  967  Training Loss:  94.5589  Training Accuracy:  0.883657\n",
      "Epoch:  968  Training Loss:  94.3639  Training Accuracy:  0.883833\n",
      "Epoch:  969  Training Loss:  94.3184  Training Accuracy:  0.883892\n",
      "Epoch:  970  Training Loss:  94.1082  Training Accuracy:  0.884127\n",
      "Epoch:  971  Training Loss:  93.9466  Training Accuracy:  0.884304\n",
      "Epoch:  972  Training Loss:  93.7231  Training Accuracy:  0.884421\n",
      "Epoch:  973  Training Loss:  93.7  Training Accuracy:  0.884715\n",
      "Epoch:  974  Training Loss:  93.4529  Training Accuracy:  0.884774\n",
      "Epoch:  975  Training Loss:  93.3591  Training Accuracy:  0.885127\n",
      "Epoch:  976  Training Loss:  93.1833  Training Accuracy:  0.885244\n",
      "Epoch:  977  Training Loss:  93.0654  Training Accuracy:  0.88548\n",
      "Epoch:  978  Training Loss:  92.9463  Training Accuracy:  0.885715\n",
      "Epoch:  979  Training Loss:  92.769  Training Accuracy:  0.88595\n",
      "Epoch:  980  Training Loss:  92.6002  Training Accuracy:  0.886067\n",
      "Epoch:  981  Training Loss:  92.5228  Training Accuracy:  0.886303\n",
      "Epoch:  982  Training Loss:  92.3342  Training Accuracy:  0.886361\n",
      "Epoch:  983  Training Loss:  92.174  Training Accuracy:  0.886538\n",
      "Epoch:  984  Training Loss:  92.003  Training Accuracy:  0.886832\n",
      "Epoch:  985  Training Loss:  91.9411  Training Accuracy:  0.887008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  986  Training Loss:  91.7361  Training Accuracy:  0.887185\n",
      "Epoch:  987  Training Loss:  91.5954  Training Accuracy:  0.887185\n",
      "Epoch:  988  Training Loss:  91.411  Training Accuracy:  0.887361\n",
      "Epoch:  989  Training Loss:  91.3472  Training Accuracy:  0.887596\n",
      "Epoch:  990  Training Loss:  91.1772  Training Accuracy:  0.88789\n",
      "Epoch:  991  Training Loss:  90.9548  Training Accuracy:  0.887949\n",
      "Epoch:  992  Training Loss:  90.8527  Training Accuracy:  0.888125\n",
      "Epoch:  993  Training Loss:  90.6733  Training Accuracy:  0.888301\n",
      "Epoch:  994  Training Loss:  90.6268  Training Accuracy:  0.888301\n",
      "Epoch:  995  Training Loss:  90.4072  Training Accuracy:  0.888595\n",
      "Epoch:  996  Training Loss:  90.2517  Training Accuracy:  0.888713\n",
      "Epoch:  997  Training Loss:  90.0956  Training Accuracy:  0.888772\n",
      "Epoch:  998  Training Loss:  89.8934  Training Accuracy:  0.888831\n",
      "Epoch:  999  Training Loss:  89.8086  Training Accuracy:  0.889007\n",
      "Testing Accuracy: 0.773976\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 9\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
