{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "#create\n",
    "def plot_subject(subject,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(subject)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"user-id\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = read_data('actitracker_raw2.txt')\n",
    "dataset = dataset.replace(\";\",\"\",regex=True)#.replace(';',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-223c9458af1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-57cc2f865b90>\u001b[0m in \u001b[0;36mfeature_normalize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6340\u001b[0m                                       skipna=skipna)\n\u001b[1;32m   6341\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m-> 6342\u001b[0;31m                             numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   6343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   2379\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x-axis</th>\n",
       "      <th>y-axis</th>\n",
       "      <th>z-axis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.198203</td>\n",
       "      <td>0.804142</td>\n",
       "      <td>0.50395286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>0.635039</td>\n",
       "      <td>0.594170</td>\n",
       "      <td>0.95342433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.537639</td>\n",
       "      <td>-0.08172209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.186271</td>\n",
       "      <td>1.666240</td>\n",
       "      <td>3.0237172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-0.269795</td>\n",
       "      <td>0.719346</td>\n",
       "      <td>7.205164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106442306000</td>\n",
       "      <td>0.104071</td>\n",
       "      <td>-1.444986</td>\n",
       "      <td>-6.510526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106542312000</td>\n",
       "      <td>-0.186271</td>\n",
       "      <td>0.491202</td>\n",
       "      <td>5.706926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106652389000</td>\n",
       "      <td>-0.170362</td>\n",
       "      <td>0.991906</td>\n",
       "      <td>7.0553403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106762313000</td>\n",
       "      <td>-1.327754</td>\n",
       "      <td>0.616378</td>\n",
       "      <td>5.134871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106872299000</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>-0.871599</td>\n",
       "      <td>1.6480621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106982315000</td>\n",
       "      <td>-1.293948</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>2.7240696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107092330000</td>\n",
       "      <td>0.110037</td>\n",
       "      <td>-0.217454</td>\n",
       "      <td>2.982856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107202316000</td>\n",
       "      <td>-0.371216</td>\n",
       "      <td>-1.517668</td>\n",
       "      <td>-0.29964766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107312332000</td>\n",
       "      <td>-0.991673</td>\n",
       "      <td>-0.059975</td>\n",
       "      <td>-8.158588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107422348000</td>\n",
       "      <td>0.754358</td>\n",
       "      <td>1.593558</td>\n",
       "      <td>8.539958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107522293000</td>\n",
       "      <td>0.819984</td>\n",
       "      <td>-0.633361</td>\n",
       "      <td>2.9147544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107632339000</td>\n",
       "      <td>-0.325477</td>\n",
       "      <td>0.156054</td>\n",
       "      <td>-1.4573772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107742355000</td>\n",
       "      <td>0.418277</td>\n",
       "      <td>0.939413</td>\n",
       "      <td>9.425281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107852340000</td>\n",
       "      <td>-0.393091</td>\n",
       "      <td>-1.921461</td>\n",
       "      <td>-10.18802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49107962326000</td>\n",
       "      <td>0.306913</td>\n",
       "      <td>0.456880</td>\n",
       "      <td>-9.724928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108062271000</td>\n",
       "      <td>0.424243</td>\n",
       "      <td>0.951527</td>\n",
       "      <td>1.5390993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108172348000</td>\n",
       "      <td>-0.170362</td>\n",
       "      <td>-0.502129</td>\n",
       "      <td>3.718355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108272262000</td>\n",
       "      <td>-0.432864</td>\n",
       "      <td>-0.825163</td>\n",
       "      <td>0.08172209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108382370000</td>\n",
       "      <td>-0.617808</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>6.510526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108492294000</td>\n",
       "      <td>-0.214113</td>\n",
       "      <td>-1.564104</td>\n",
       "      <td>-4.630918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108602371000</td>\n",
       "      <td>-0.023202</td>\n",
       "      <td>0.531582</td>\n",
       "      <td>13.525005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108702285000</td>\n",
       "      <td>0.736460</td>\n",
       "      <td>1.236201</td>\n",
       "      <td>6.1700177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108812332000</td>\n",
       "      <td>-1.361561</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>4.0180025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49108922378000</td>\n",
       "      <td>-0.291670</td>\n",
       "      <td>-0.893808</td>\n",
       "      <td>2.3699405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49109022293000</td>\n",
       "      <td>-0.766956</td>\n",
       "      <td>1.825739</td>\n",
       "      <td>4.7126403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098174</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622091524000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.263769</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098175</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622131471000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098176</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622171541000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098177</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622211580000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.286004</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098178</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622291475000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098179</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622331483000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098180</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622371522000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.244499</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098181</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622451479000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098182</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622491487000</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098183</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622531465000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.241534</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098184</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622571443000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.244499</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098185</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622611635000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098186</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622691469000</td>\n",
       "      <td>1.205587</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098187</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622731477000</td>\n",
       "      <td>1.183686</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098188</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622771486000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098189</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622851472000</td>\n",
       "      <td>1.144265</td>\n",
       "      <td>-1.297862</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098190</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622891511000</td>\n",
       "      <td>1.110683</td>\n",
       "      <td>-1.320097</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098191</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622931490000</td>\n",
       "      <td>1.211427</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098192</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131622971498000</td>\n",
       "      <td>1.250849</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098193</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623051485000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.257840</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098194</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623091524000</td>\n",
       "      <td>1.150105</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098195</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623131471000</td>\n",
       "      <td>1.167626</td>\n",
       "      <td>-1.269698</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098196</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623172578000</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>-1.263769</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098197</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623251466000</td>\n",
       "      <td>1.233328</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098198</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623291475000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.297862</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098199</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623331483000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.308238</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098200</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623371431000</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098201</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623411592000</td>\n",
       "      <td>1.228948</td>\n",
       "      <td>-1.280074</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098202</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623491487000</td>\n",
       "      <td>1.217268</td>\n",
       "      <td>-1.291933</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098203</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623531465000</td>\n",
       "      <td>1.199747</td>\n",
       "      <td>-1.272663</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1098204 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user-id activity        timestamp    x-axis    y-axis       z-axis\n",
       "0             33  Jogging   49105962326000 -0.198203  0.804142   0.50395286\n",
       "1             33  Jogging   49106062271000  0.635039  0.594170   0.95342433\n",
       "2             33  Jogging   49106112167000  0.619130  0.537639  -0.08172209\n",
       "3             33  Jogging   49106222305000 -0.186271  1.666240    3.0237172\n",
       "4             33  Jogging   49106332290000 -0.269795  0.719346     7.205164\n",
       "5             33  Jogging   49106442306000  0.104071 -1.444986    -6.510526\n",
       "6             33  Jogging   49106542312000 -0.186271  0.491202     5.706926\n",
       "7             33  Jogging   49106652389000 -0.170362  0.991906    7.0553403\n",
       "8             33  Jogging   49106762313000 -1.327754  0.616378     5.134871\n",
       "9             33  Jogging   49106872299000  0.042423 -0.871599    1.6480621\n",
       "10            33  Jogging   49106982315000 -1.293948  1.825739    2.7240696\n",
       "11            33  Jogging   49107092330000  0.110037 -0.217454     2.982856\n",
       "12            33  Jogging   49107202316000 -0.371216 -1.517668  -0.29964766\n",
       "13            33  Jogging   49107312332000 -0.991673 -0.059975    -8.158588\n",
       "14            33  Jogging   49107422348000  0.754358  1.593558     8.539958\n",
       "15            33  Jogging   49107522293000  0.819984 -0.633361    2.9147544\n",
       "16            33  Jogging   49107632339000 -0.325477  0.156054   -1.4573772\n",
       "17            33  Jogging   49107742355000  0.418277  0.939413     9.425281\n",
       "18            33  Jogging   49107852340000 -0.393091 -1.921461    -10.18802\n",
       "19            33  Jogging   49107962326000  0.306913  0.456880    -9.724928\n",
       "20            33  Jogging   49108062271000  0.424243  0.951527    1.5390993\n",
       "21            33  Jogging   49108172348000 -0.170362 -0.502129     3.718355\n",
       "22            33  Jogging   49108272262000 -0.432864 -0.825163   0.08172209\n",
       "23            33  Jogging   49108382370000 -0.617808  1.825739     6.510526\n",
       "24            33  Jogging   49108492294000 -0.214113 -1.564104    -4.630918\n",
       "25            33  Jogging   49108602371000 -0.023202  0.531582    13.525005\n",
       "26            33  Jogging   49108702285000  0.736460  1.236201    6.1700177\n",
       "27            33  Jogging   49108812332000 -1.361561  0.002613    4.0180025\n",
       "28            33  Jogging   49108922378000 -0.291670 -0.893808    2.3699405\n",
       "29            33  Jogging   49109022293000 -0.766956  1.825739    4.7126403\n",
       "...          ...      ...              ...       ...       ...          ...\n",
       "1098174       19  Sitting  131622091524000  1.205587 -1.263769         2.22\n",
       "1098175       19  Sitting  131622131471000  1.211427 -1.269698         2.26\n",
       "1098176       19  Sitting  131622171541000  1.205587 -1.272663         2.18\n",
       "1098177       19  Sitting  131622211580000  1.195367 -1.286004         2.26\n",
       "1098178       19  Sitting  131622291475000  1.199747 -1.280074         2.41\n",
       "1098179       19  Sitting  131622331483000  1.205587 -1.269698          2.3\n",
       "1098180       19  Sitting  131622371522000  1.205587 -1.244499         2.26\n",
       "1098181       19  Sitting  131622451479000  1.211427 -1.241534         2.34\n",
       "1098182       19  Sitting  131622491487000  1.223108 -1.241534         2.41\n",
       "1098183       19  Sitting  131622531465000  1.199747 -1.241534         2.37\n",
       "1098184       19  Sitting  131622571443000  1.199747 -1.244499         2.37\n",
       "1098185       19  Sitting  131622611635000  1.205587 -1.257840         2.45\n",
       "1098186       19  Sitting  131622691469000  1.205587 -1.257840         2.45\n",
       "1098187       19  Sitting  131622731477000  1.183686 -1.272663         2.53\n",
       "1098188       19  Sitting  131622771486000  1.211427 -1.280074          2.6\n",
       "1098189       19  Sitting  131622851472000  1.144265 -1.297862         2.56\n",
       "1098190       19  Sitting  131622891511000  1.110683 -1.320097         2.11\n",
       "1098191       19  Sitting  131622931490000  1.211427 -1.291933          2.3\n",
       "1098192       19  Sitting  131622971498000  1.250849 -1.291933         2.26\n",
       "1098193       19  Sitting  131623051485000  1.195367 -1.257840         2.26\n",
       "1098194       19  Sitting  131623091524000  1.150105 -1.269698         2.49\n",
       "1098195       19  Sitting  131623131471000  1.167626 -1.269698         2.37\n",
       "1098196       19  Sitting  131623172578000  1.195367 -1.263769         2.18\n",
       "1098197       19  Sitting  131623251466000  1.233328 -1.280074         1.95\n",
       "1098198       19  Sitting  131623291475000  1.217268 -1.297862          1.8\n",
       "1098199       19  Sitting  131623331483000  1.217268 -1.308238         1.69\n",
       "1098200       19  Sitting  131623371431000  1.223108 -1.291933         1.73\n",
       "1098201       19  Sitting  131623411592000  1.228948 -1.280074         1.69\n",
       "1098202       19  Sitting  131623491487000  1.217268 -1.291933         1.73\n",
       "1098203       19  Sitting  131623531465000  1.199747 -1.272663         1.61\n",
       "\n",
       "[1098204 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-169114f33f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user-id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user-id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplot_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fb8ab1dffe65>\u001b[0m in \u001b[0;36mplot_subject\u001b[0;34m(subject, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mplot_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'z-axis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fb8ab1dffe65>\u001b[0m in \u001b[0;36mplot_axis\u001b[0;34m(ax, x, y, title)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m     return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6360\u001b[0m                                       skipna=skipna, ddof=ddof)\n\u001b[1;32m   6361\u001b[0m         return self._reduce(f, name, axis=axis, numeric_only=numeric_only,\n\u001b[0;32m-> 6362\u001b[0;31m                             skipna=skipna, ddof=ddof)\n\u001b[0m\u001b[1;32m   6363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   2379\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   2380\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mbottleneck_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0msqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAJCCAYAAABj8z68AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a4177ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#modify taerget(user-id)\n",
    "for subject in np.unique(dataset[\"user-id\"]):\n",
    "    subset = dataset[dataset[\"user-id\"] == subject][:40]\n",
    "    plot_subject(subject,subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 200\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17152, 1, 90, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  362.583  Training Accuracy:  0.0402285\n",
      "Epoch:  1  Training Loss:  350.113  Training Accuracy:  0.0404618\n",
      "Epoch:  2  Training Loss:  327.752  Training Accuracy:  0.0413363\n",
      "Epoch:  3  Training Loss:  310.012  Training Accuracy:  0.0444263\n",
      "Epoch:  4  Training Loss:  295.755  Training Accuracy:  0.0503731\n",
      "Epoch:  5  Training Loss:  285.349  Training Accuracy:  0.056903\n",
      "Epoch:  6  Training Loss:  276.388  Training Accuracy:  0.0614506\n",
      "Epoch:  7  Training Loss:  268.185  Training Accuracy:  0.0656483\n",
      "Epoch:  8  Training Loss:  261.591  Training Accuracy:  0.0687966\n",
      "Epoch:  9  Training Loss:  256.177  Training Accuracy:  0.0730527\n",
      "Epoch:  10  Training Loss:  250.486  Training Accuracy:  0.0796409\n",
      "Epoch:  11  Training Loss:  245.803  Training Accuracy:  0.0866954\n",
      "Epoch:  12  Training Loss:  242.079  Training Accuracy:  0.0932836\n",
      "Epoch:  13  Training Loss:  237.89  Training Accuracy:  0.0998717\n",
      "Epoch:  14  Training Loss:  234.16  Training Accuracy:  0.108151\n",
      "Epoch:  15  Training Loss:  230.062  Training Accuracy:  0.116838\n",
      "Epoch:  16  Training Loss:  226.611  Training Accuracy:  0.124009\n",
      "Epoch:  17  Training Loss:  223.936  Training Accuracy:  0.131297\n",
      "Epoch:  18  Training Loss:  221.333  Training Accuracy:  0.139342\n",
      "Epoch:  19  Training Loss:  218.569  Training Accuracy:  0.146397\n",
      "Epoch:  20  Training Loss:  216.214  Training Accuracy:  0.153918\n",
      "Epoch:  21  Training Loss:  214.093  Training Accuracy:  0.160331\n",
      "Epoch:  22  Training Loss:  211.517  Training Accuracy:  0.168319\n",
      "Epoch:  23  Training Loss:  209.071  Training Accuracy:  0.175023\n",
      "Epoch:  24  Training Loss:  206.785  Training Accuracy:  0.182428\n",
      "Epoch:  25  Training Loss:  204.299  Training Accuracy:  0.189307\n",
      "Epoch:  26  Training Loss:  202.234  Training Accuracy:  0.197528\n",
      "Epoch:  27  Training Loss:  200.079  Training Accuracy:  0.205457\n",
      "Epoch:  28  Training Loss:  197.643  Training Accuracy:  0.213211\n",
      "Epoch:  29  Training Loss:  195.383  Training Accuracy:  0.219916\n",
      "Epoch:  30  Training Loss:  193.074  Training Accuracy:  0.226737\n",
      "Epoch:  31  Training Loss:  190.895  Training Accuracy:  0.234492\n",
      "Epoch:  32  Training Loss:  188.797  Training Accuracy:  0.24108\n",
      "Epoch:  33  Training Loss:  186.651  Training Accuracy:  0.248601\n",
      "Epoch:  34  Training Loss:  184.624  Training Accuracy:  0.254606\n",
      "Epoch:  35  Training Loss:  182.543  Training Accuracy:  0.260611\n",
      "Epoch:  36  Training Loss:  180.49  Training Accuracy:  0.266908\n",
      "Epoch:  37  Training Loss:  178.488  Training Accuracy:  0.272621\n",
      "Epoch:  38  Training Loss:  176.471  Training Accuracy:  0.278277\n",
      "Epoch:  39  Training Loss:  174.472  Training Accuracy:  0.283815\n",
      "Epoch:  40  Training Loss:  172.418  Training Accuracy:  0.288829\n",
      "Epoch:  41  Training Loss:  170.568  Training Accuracy:  0.29431\n",
      "Epoch:  42  Training Loss:  168.613  Training Accuracy:  0.298974\n",
      "Epoch:  43  Training Loss:  166.796  Training Accuracy:  0.30358\n",
      "Epoch:  44  Training Loss:  164.935  Training Accuracy:  0.307661\n",
      "Epoch:  45  Training Loss:  163.405  Training Accuracy:  0.312908\n",
      "Epoch:  46  Training Loss:  161.518  Training Accuracy:  0.318272\n",
      "Epoch:  47  Training Loss:  159.928  Training Accuracy:  0.322936\n",
      "Epoch:  48  Training Loss:  158.009  Training Accuracy:  0.327367\n",
      "Epoch:  49  Training Loss:  156.486  Training Accuracy:  0.331681\n",
      "Epoch:  50  Training Loss:  154.54  Training Accuracy:  0.335879\n",
      "Epoch:  51  Training Loss:  152.992  Training Accuracy:  0.338794\n",
      "Epoch:  52  Training Loss:  151.117  Training Accuracy:  0.343342\n",
      "Epoch:  53  Training Loss:  149.334  Training Accuracy:  0.347365\n",
      "Epoch:  54  Training Loss:  147.575  Training Accuracy:  0.350979\n",
      "Epoch:  55  Training Loss:  145.909  Training Accuracy:  0.354244\n",
      "Epoch:  56  Training Loss:  144.054  Training Accuracy:  0.358267\n",
      "Epoch:  57  Training Loss:  142.344  Training Accuracy:  0.362873\n",
      "Epoch:  58  Training Loss:  140.489  Training Accuracy:  0.367187\n",
      "Epoch:  59  Training Loss:  138.703  Training Accuracy:  0.370686\n",
      "Epoch:  60  Training Loss:  136.868  Training Accuracy:  0.375\n",
      "Epoch:  61  Training Loss:  135.122  Training Accuracy:  0.378731\n",
      "Epoch:  62  Training Loss:  133.284  Training Accuracy:  0.383162\n",
      "Epoch:  63  Training Loss:  131.584  Training Accuracy:  0.386719\n",
      "Epoch:  64  Training Loss:  129.749  Training Accuracy:  0.389867\n",
      "Epoch:  65  Training Loss:  127.929  Training Accuracy:  0.393365\n",
      "Epoch:  66  Training Loss:  126.048  Training Accuracy:  0.39698\n",
      "Epoch:  67  Training Loss:  124.479  Training Accuracy:  0.40007\n",
      "Epoch:  68  Training Loss:  122.458  Training Accuracy:  0.402752\n",
      "Epoch:  69  Training Loss:  120.707  Training Accuracy:  0.406017\n",
      "Epoch:  70  Training Loss:  118.907  Training Accuracy:  0.409223\n",
      "Epoch:  71  Training Loss:  117.234  Training Accuracy:  0.41208\n",
      "Epoch:  72  Training Loss:  115.301  Training Accuracy:  0.415112\n",
      "Epoch:  73  Training Loss:  113.753  Training Accuracy:  0.417677\n",
      "Epoch:  74  Training Loss:  111.714  Training Accuracy:  0.420942\n",
      "Epoch:  75  Training Loss:  110.164  Training Accuracy:  0.424265\n",
      "Epoch:  76  Training Loss:  108.285  Training Accuracy:  0.427763\n",
      "Epoch:  77  Training Loss:  106.819  Training Accuracy:  0.431553\n",
      "Epoch:  78  Training Loss:  104.833  Training Accuracy:  0.434993\n",
      "Epoch:  79  Training Loss:  103.354  Training Accuracy:  0.4382\n",
      "Epoch:  80  Training Loss:  101.407  Training Accuracy:  0.441231\n",
      "Epoch:  81  Training Loss:  100.013  Training Accuracy:  0.443855\n",
      "Epoch:  82  Training Loss:  98.1098  Training Accuracy:  0.447586\n",
      "Epoch:  83  Training Loss:  96.7401  Training Accuracy:  0.450793\n",
      "Epoch:  84  Training Loss:  94.94  Training Accuracy:  0.453358\n",
      "Epoch:  85  Training Loss:  93.6142  Training Accuracy:  0.455632\n",
      "Epoch:  86  Training Loss:  92.0079  Training Accuracy:  0.458547\n",
      "Epoch:  87  Training Loss:  90.4837  Training Accuracy:  0.460646\n",
      "Epoch:  88  Training Loss:  89.0844  Training Accuracy:  0.463444\n",
      "Epoch:  89  Training Loss:  87.5244  Training Accuracy:  0.466418\n",
      "Epoch:  90  Training Loss:  86.1359  Training Accuracy:  0.468925\n",
      "Epoch:  91  Training Loss:  84.7156  Training Accuracy:  0.472131\n",
      "Epoch:  92  Training Loss:  83.3845  Training Accuracy:  0.474988\n",
      "Epoch:  93  Training Loss:  82.0565  Training Accuracy:  0.477553\n",
      "Epoch:  94  Training Loss:  80.7206  Training Accuracy:  0.479886\n",
      "Epoch:  95  Training Loss:  79.3606  Training Accuracy:  0.482334\n",
      "Epoch:  96  Training Loss:  78.1035  Training Accuracy:  0.485133\n",
      "Epoch:  97  Training Loss:  76.8564  Training Accuracy:  0.488864\n",
      "Epoch:  98  Training Loss:  75.5887  Training Accuracy:  0.491255\n",
      "Epoch:  99  Training Loss:  74.3916  Training Accuracy:  0.493587\n",
      "Epoch:  100  Training Loss:  73.2552  Training Accuracy:  0.496443\n",
      "Epoch:  101  Training Loss:  72.0493  Training Accuracy:  0.498834\n",
      "Epoch:  102  Training Loss:  70.9347  Training Accuracy:  0.500933\n",
      "Epoch:  103  Training Loss:  69.8237  Training Accuracy:  0.502915\n",
      "Epoch:  104  Training Loss:  68.845  Training Accuracy:  0.504547\n",
      "Epoch:  105  Training Loss:  67.7585  Training Accuracy:  0.507288\n",
      "Epoch:  106  Training Loss:  66.7455  Training Accuracy:  0.509736\n",
      "Epoch:  107  Training Loss:  65.7315  Training Accuracy:  0.511719\n",
      "Epoch:  108  Training Loss:  64.825  Training Accuracy:  0.514517\n",
      "Epoch:  109  Training Loss:  63.8895  Training Accuracy:  0.516674\n",
      "Epoch:  110  Training Loss:  62.9886  Training Accuracy:  0.519065\n",
      "Epoch:  111  Training Loss:  62.0635  Training Accuracy:  0.521397\n",
      "Epoch:  112  Training Loss:  61.2309  Training Accuracy:  0.523845\n",
      "Epoch:  113  Training Loss:  60.3199  Training Accuracy:  0.526003\n",
      "Epoch:  114  Training Loss:  59.5664  Training Accuracy:  0.527752\n",
      "Epoch:  115  Training Loss:  58.6423  Training Accuracy:  0.530084\n",
      "Epoch:  116  Training Loss:  57.8428  Training Accuracy:  0.5316\n",
      "Epoch:  117  Training Loss:  57.0123  Training Accuracy:  0.53329\n",
      "Epoch:  118  Training Loss:  56.3435  Training Accuracy:  0.534748\n",
      "Epoch:  119  Training Loss:  55.4927  Training Accuracy:  0.535914\n",
      "Epoch:  120  Training Loss:  54.7565  Training Accuracy:  0.53778\n",
      "Epoch:  121  Training Loss:  53.9858  Training Accuracy:  0.539529\n",
      "Epoch:  122  Training Loss:  53.3112  Training Accuracy:  0.540986\n",
      "Epoch:  123  Training Loss:  52.6145  Training Accuracy:  0.541861\n",
      "Epoch:  124  Training Loss:  51.9798  Training Accuracy:  0.54326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  51.2802  Training Accuracy:  0.545184\n",
      "Epoch:  126  Training Loss:  50.6173  Training Accuracy:  0.546875\n",
      "Epoch:  127  Training Loss:  49.9668  Training Accuracy:  0.548332\n",
      "Epoch:  128  Training Loss:  49.3606  Training Accuracy:  0.549673\n",
      "Epoch:  129  Training Loss:  48.7454  Training Accuracy:  0.551772\n",
      "Epoch:  130  Training Loss:  48.1504  Training Accuracy:  0.554338\n",
      "Epoch:  131  Training Loss:  47.5793  Training Accuracy:  0.555853\n",
      "Epoch:  132  Training Loss:  47.0548  Training Accuracy:  0.558768\n",
      "Epoch:  133  Training Loss:  46.5198  Training Accuracy:  0.560634\n",
      "Epoch:  134  Training Loss:  45.9708  Training Accuracy:  0.562208\n",
      "Epoch:  135  Training Loss:  45.4361  Training Accuracy:  0.564482\n",
      "Epoch:  136  Training Loss:  44.888  Training Accuracy:  0.566348\n",
      "Epoch:  137  Training Loss:  44.3834  Training Accuracy:  0.568213\n",
      "Epoch:  138  Training Loss:  43.888  Training Accuracy:  0.569379\n",
      "Epoch:  139  Training Loss:  43.4  Training Accuracy:  0.570487\n",
      "Epoch:  140  Training Loss:  42.9322  Training Accuracy:  0.57177\n",
      "Epoch:  141  Training Loss:  42.4208  Training Accuracy:  0.573286\n",
      "Epoch:  142  Training Loss:  41.9859  Training Accuracy:  0.574802\n",
      "Epoch:  143  Training Loss:  41.5212  Training Accuracy:  0.576084\n",
      "Epoch:  144  Training Loss:  41.0647  Training Accuracy:  0.57725\n",
      "Epoch:  145  Training Loss:  40.6313  Training Accuracy:  0.579174\n",
      "Epoch:  146  Training Loss:  40.2007  Training Accuracy:  0.581273\n",
      "Epoch:  147  Training Loss:  39.7779  Training Accuracy:  0.58413\n",
      "Epoch:  148  Training Loss:  39.3783  Training Accuracy:  0.585762\n",
      "Epoch:  149  Training Loss:  38.922  Training Accuracy:  0.587628\n",
      "Epoch:  150  Training Loss:  38.5699  Training Accuracy:  0.589086\n",
      "Epoch:  151  Training Loss:  38.1331  Training Accuracy:  0.590893\n",
      "Epoch:  152  Training Loss:  37.7546  Training Accuracy:  0.592292\n",
      "Epoch:  153  Training Loss:  37.3846  Training Accuracy:  0.594216\n",
      "Epoch:  154  Training Loss:  37.0506  Training Accuracy:  0.595849\n",
      "Epoch:  155  Training Loss:  36.6175  Training Accuracy:  0.597306\n",
      "Epoch:  156  Training Loss:  36.2892  Training Accuracy:  0.598764\n",
      "Epoch:  157  Training Loss:  35.9229  Training Accuracy:  0.600105\n",
      "Epoch:  158  Training Loss:  35.6414  Training Accuracy:  0.602437\n",
      "Epoch:  159  Training Loss:  35.2666  Training Accuracy:  0.603661\n",
      "Epoch:  160  Training Loss:  34.9341  Training Accuracy:  0.605235\n",
      "Epoch:  161  Training Loss:  34.6105  Training Accuracy:  0.606809\n",
      "Epoch:  162  Training Loss:  34.2962  Training Accuracy:  0.608034\n",
      "Epoch:  163  Training Loss:  34.0022  Training Accuracy:  0.609491\n",
      "Epoch:  164  Training Loss:  33.6759  Training Accuracy:  0.610774\n",
      "Epoch:  165  Training Loss:  33.3706  Training Accuracy:  0.612581\n",
      "Epoch:  166  Training Loss:  33.0461  Training Accuracy:  0.614039\n",
      "Epoch:  167  Training Loss:  32.7337  Training Accuracy:  0.615263\n",
      "Epoch:  168  Training Loss:  32.4413  Training Accuracy:  0.616662\n",
      "Epoch:  169  Training Loss:  32.1431  Training Accuracy:  0.61812\n",
      "Epoch:  170  Training Loss:  31.863  Training Accuracy:  0.619286\n",
      "Epoch:  171  Training Loss:  31.5575  Training Accuracy:  0.620452\n",
      "Epoch:  172  Training Loss:  31.2387  Training Accuracy:  0.621793\n",
      "Epoch:  173  Training Loss:  30.9894  Training Accuracy:  0.623309\n",
      "Epoch:  174  Training Loss:  30.6783  Training Accuracy:  0.624825\n",
      "Epoch:  175  Training Loss:  30.4032  Training Accuracy:  0.626457\n",
      "Epoch:  176  Training Loss:  30.1136  Training Accuracy:  0.628323\n",
      "Epoch:  177  Training Loss:  29.8976  Training Accuracy:  0.629547\n",
      "Epoch:  178  Training Loss:  29.5934  Training Accuracy:  0.631122\n",
      "Epoch:  179  Training Loss:  29.3736  Training Accuracy:  0.63287\n",
      "Epoch:  180  Training Loss:  29.1045  Training Accuracy:  0.634328\n",
      "Epoch:  181  Training Loss:  28.8621  Training Accuracy:  0.635377\n",
      "Epoch:  182  Training Loss:  28.5501  Training Accuracy:  0.63666\n",
      "Epoch:  183  Training Loss:  28.3614  Training Accuracy:  0.638526\n",
      "Epoch:  184  Training Loss:  28.0647  Training Accuracy:  0.639692\n",
      "Epoch:  185  Training Loss:  27.8869  Training Accuracy:  0.641149\n",
      "Epoch:  186  Training Loss:  27.5835  Training Accuracy:  0.642199\n",
      "Epoch:  187  Training Loss:  27.397  Training Accuracy:  0.643656\n",
      "Epoch:  188  Training Loss:  27.1277  Training Accuracy:  0.644822\n",
      "Epoch:  189  Training Loss:  26.923  Training Accuracy:  0.645989\n",
      "Epoch:  190  Training Loss:  26.6263  Training Accuracy:  0.647563\n",
      "Epoch:  191  Training Loss:  26.4469  Training Accuracy:  0.649137\n",
      "Epoch:  192  Training Loss:  26.1898  Training Accuracy:  0.650769\n",
      "Epoch:  193  Training Loss:  25.9769  Training Accuracy:  0.652052\n",
      "Epoch:  194  Training Loss:  25.7144  Training Accuracy:  0.653684\n",
      "Epoch:  195  Training Loss:  25.5586  Training Accuracy:  0.655025\n",
      "Epoch:  196  Training Loss:  25.2773  Training Accuracy:  0.656191\n",
      "Epoch:  197  Training Loss:  25.0931  Training Accuracy:  0.657474\n",
      "Epoch:  198  Training Loss:  24.8413  Training Accuracy:  0.658465\n",
      "Epoch:  199  Training Loss:  24.6673  Training Accuracy:  0.659456\n",
      "Testing Accuracy: 0.622811\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  414.358  Training Accuracy:  0.0404034\n",
      "Epoch:  1  Training Loss:  339.82  Training Accuracy:  0.0419193\n",
      "Epoch:  2  Training Loss:  318.763  Training Accuracy:  0.0439016\n",
      "Epoch:  3  Training Loss:  304.583  Training Accuracy:  0.0529967\n",
      "Epoch:  4  Training Loss:  295.077  Training Accuracy:  0.0570196\n",
      "Epoch:  5  Training Loss:  288.822  Training Accuracy:  0.0613923\n",
      "Epoch:  6  Training Loss:  283.434  Training Accuracy:  0.0645989\n",
      "Epoch:  7  Training Loss:  278.24  Training Accuracy:  0.0677472\n",
      "Epoch:  8  Training Loss:  272.86  Training Accuracy:  0.0721782\n",
      "Epoch:  9  Training Loss:  268.1  Training Accuracy:  0.0774254\n",
      "Epoch:  10  Training Loss:  264.53  Training Accuracy:  0.0819146\n",
      "Epoch:  11  Training Loss:  261.466  Training Accuracy:  0.0866954\n",
      "Epoch:  12  Training Loss:  259.32  Training Accuracy:  0.0920592\n",
      "Epoch:  13  Training Loss:  256.504  Training Accuracy:  0.0966068\n",
      "Epoch:  14  Training Loss:  254.046  Training Accuracy:  0.101504\n",
      "Epoch:  15  Training Loss:  251.649  Training Accuracy:  0.106693\n",
      "Epoch:  16  Training Loss:  249.169  Training Accuracy:  0.112698\n",
      "Epoch:  17  Training Loss:  246.623  Training Accuracy:  0.11847\n",
      "Epoch:  18  Training Loss:  244.137  Training Accuracy:  0.125816\n",
      "Epoch:  19  Training Loss:  241.792  Training Accuracy:  0.133454\n",
      "Epoch:  20  Training Loss:  239.394  Training Accuracy:  0.141383\n",
      "Epoch:  21  Training Loss:  237.226  Training Accuracy:  0.14937\n",
      "Epoch:  22  Training Loss:  235.058  Training Accuracy:  0.154909\n",
      "Epoch:  23  Training Loss:  232.844  Training Accuracy:  0.162255\n",
      "Epoch:  24  Training Loss:  230.806  Training Accuracy:  0.169135\n",
      "Epoch:  25  Training Loss:  228.789  Training Accuracy:  0.17718\n",
      "Epoch:  26  Training Loss:  226.71  Training Accuracy:  0.185051\n",
      "Epoch:  27  Training Loss:  224.772  Training Accuracy:  0.191523\n",
      "Epoch:  28  Training Loss:  222.7  Training Accuracy:  0.20056\n",
      "Epoch:  29  Training Loss:  220.732  Training Accuracy:  0.208372\n",
      "Epoch:  30  Training Loss:  218.944  Training Accuracy:  0.214902\n",
      "Epoch:  31  Training Loss:  216.947  Training Accuracy:  0.221665\n",
      "Epoch:  32  Training Loss:  214.808  Training Accuracy:  0.228836\n",
      "Epoch:  33  Training Loss:  212.449  Training Accuracy:  0.236241\n",
      "Epoch:  34  Training Loss:  210.299  Training Accuracy:  0.242188\n",
      "Epoch:  35  Training Loss:  208.019  Training Accuracy:  0.248193\n",
      "Epoch:  36  Training Loss:  205.851  Training Accuracy:  0.25344\n",
      "Epoch:  37  Training Loss:  203.635  Training Accuracy:  0.258745\n",
      "Epoch:  38  Training Loss:  201.499  Training Accuracy:  0.264284\n",
      "Epoch:  39  Training Loss:  199.455  Training Accuracy:  0.269648\n",
      "Epoch:  40  Training Loss:  197.286  Training Accuracy:  0.275653\n",
      "Epoch:  41  Training Loss:  195.169  Training Accuracy:  0.281308\n",
      "Epoch:  42  Training Loss:  192.99  Training Accuracy:  0.286439\n",
      "Epoch:  43  Training Loss:  190.814  Training Accuracy:  0.29052\n",
      "Epoch:  44  Training Loss:  188.798  Training Accuracy:  0.297341\n",
      "Epoch:  45  Training Loss:  186.669  Training Accuracy:  0.302705\n",
      "Epoch:  46  Training Loss:  184.61  Training Accuracy:  0.308302\n",
      "Epoch:  47  Training Loss:  182.459  Training Accuracy:  0.31285\n",
      "Epoch:  48  Training Loss:  180.426  Training Accuracy:  0.316581\n",
      "Epoch:  49  Training Loss:  178.313  Training Accuracy:  0.321129\n",
      "Epoch:  50  Training Loss:  176.311  Training Accuracy:  0.325618\n",
      "Epoch:  51  Training Loss:  174.365  Training Accuracy:  0.33034\n",
      "Epoch:  52  Training Loss:  172.389  Training Accuracy:  0.334014\n",
      "Epoch:  53  Training Loss:  170.39  Training Accuracy:  0.337278\n",
      "Epoch:  54  Training Loss:  168.347  Training Accuracy:  0.341884\n",
      "Epoch:  55  Training Loss:  166.383  Training Accuracy:  0.34614\n",
      "Epoch:  56  Training Loss:  164.507  Training Accuracy:  0.35063\n",
      "Epoch:  57  Training Loss:  162.542  Training Accuracy:  0.354244\n",
      "Epoch:  58  Training Loss:  160.789  Training Accuracy:  0.358559\n",
      "Epoch:  59  Training Loss:  158.72  Training Accuracy:  0.362465\n",
      "Epoch:  60  Training Loss:  156.717  Training Accuracy:  0.36608\n",
      "Epoch:  61  Training Loss:  154.834  Training Accuracy:  0.368353\n",
      "Epoch:  62  Training Loss:  152.861  Training Accuracy:  0.37191\n",
      "Epoch:  63  Training Loss:  150.837  Training Accuracy:  0.375291\n",
      "Epoch:  64  Training Loss:  148.905  Training Accuracy:  0.379081\n",
      "Epoch:  65  Training Loss:  146.944  Training Accuracy:  0.382521\n",
      "Epoch:  66  Training Loss:  144.891  Training Accuracy:  0.386777\n",
      "Epoch:  67  Training Loss:  143.083  Training Accuracy:  0.3908\n",
      "Epoch:  68  Training Loss:  141.191  Training Accuracy:  0.394298\n",
      "Epoch:  69  Training Loss:  139.386  Training Accuracy:  0.398029\n",
      "Epoch:  70  Training Loss:  137.508  Training Accuracy:  0.401236\n",
      "Epoch:  71  Training Loss:  135.775  Training Accuracy:  0.404326\n",
      "Epoch:  72  Training Loss:  133.821  Training Accuracy:  0.407591\n",
      "Epoch:  73  Training Loss:  132.283  Training Accuracy:  0.411206\n",
      "Epoch:  74  Training Loss:  130.462  Training Accuracy:  0.414529\n",
      "Epoch:  75  Training Loss:  128.758  Training Accuracy:  0.417735\n",
      "Epoch:  76  Training Loss:  127.053  Training Accuracy:  0.419659\n",
      "Epoch:  77  Training Loss:  125.375  Training Accuracy:  0.423274\n",
      "Epoch:  78  Training Loss:  123.726  Training Accuracy:  0.426073\n",
      "Epoch:  79  Training Loss:  122.092  Training Accuracy:  0.428755\n",
      "Epoch:  80  Training Loss:  120.458  Training Accuracy:  0.431611\n",
      "Epoch:  81  Training Loss:  118.831  Training Accuracy:  0.434818\n",
      "Epoch:  82  Training Loss:  117.015  Training Accuracy:  0.437908\n",
      "Epoch:  83  Training Loss:  115.749  Training Accuracy:  0.440765\n",
      "Epoch:  84  Training Loss:  114.187  Training Accuracy:  0.443388\n",
      "Epoch:  85  Training Loss:  112.818  Training Accuracy:  0.446012\n",
      "Epoch:  86  Training Loss:  111.449  Training Accuracy:  0.448402\n",
      "Epoch:  87  Training Loss:  109.945  Training Accuracy:  0.451318\n",
      "Epoch:  88  Training Loss:  108.491  Training Accuracy:  0.453358\n",
      "Epoch:  89  Training Loss:  107.07  Training Accuracy:  0.45569\n",
      "Epoch:  90  Training Loss:  105.582  Training Accuracy:  0.457847\n",
      "Epoch:  91  Training Loss:  104.24  Training Accuracy:  0.460529\n",
      "Epoch:  92  Training Loss:  102.797  Training Accuracy:  0.462337\n",
      "Epoch:  93  Training Loss:  101.462  Training Accuracy:  0.464435\n",
      "Epoch:  94  Training Loss:  100.131  Training Accuracy:  0.466476\n",
      "Epoch:  95  Training Loss:  98.8236  Training Accuracy:  0.468866\n",
      "Epoch:  96  Training Loss:  97.5586  Training Accuracy:  0.471432\n",
      "Epoch:  97  Training Loss:  96.3077  Training Accuracy:  0.473297\n",
      "Epoch:  98  Training Loss:  94.9941  Training Accuracy:  0.474813\n",
      "Epoch:  99  Training Loss:  93.752  Training Accuracy:  0.476621\n",
      "Epoch:  100  Training Loss:  92.4233  Training Accuracy:  0.478894\n",
      "Epoch:  101  Training Loss:  91.2476  Training Accuracy:  0.481518\n",
      "Epoch:  102  Training Loss:  89.933  Training Accuracy:  0.484083\n",
      "Epoch:  103  Training Loss:  88.7385  Training Accuracy:  0.485657\n",
      "Epoch:  104  Training Loss:  87.4932  Training Accuracy:  0.487756\n",
      "Epoch:  105  Training Loss:  86.3184  Training Accuracy:  0.49038\n",
      "Epoch:  106  Training Loss:  85.13  Training Accuracy:  0.492537\n",
      "Epoch:  107  Training Loss:  83.9837  Training Accuracy:  0.49586\n",
      "Epoch:  108  Training Loss:  82.8734  Training Accuracy:  0.499242\n",
      "Epoch:  109  Training Loss:  81.7539  Training Accuracy:  0.500816\n",
      "Epoch:  110  Training Loss:  80.6148  Training Accuracy:  0.50309\n",
      "Epoch:  111  Training Loss:  79.5524  Training Accuracy:  0.505072\n",
      "Epoch:  112  Training Loss:  78.496  Training Accuracy:  0.506005\n",
      "Epoch:  113  Training Loss:  77.4064  Training Accuracy:  0.508454\n",
      "Epoch:  114  Training Loss:  76.3852  Training Accuracy:  0.511369\n",
      "Epoch:  115  Training Loss:  75.3623  Training Accuracy:  0.514051\n",
      "Epoch:  116  Training Loss:  74.3991  Training Accuracy:  0.516499\n",
      "Epoch:  117  Training Loss:  73.4271  Training Accuracy:  0.518248\n",
      "Epoch:  118  Training Loss:  72.5439  Training Accuracy:  0.519881\n",
      "Epoch:  119  Training Loss:  71.5495  Training Accuracy:  0.52128\n",
      "Epoch:  120  Training Loss:  70.6761  Training Accuracy:  0.523204\n",
      "Epoch:  121  Training Loss:  69.7604  Training Accuracy:  0.525186\n",
      "Epoch:  122  Training Loss:  68.8773  Training Accuracy:  0.527402\n",
      "Epoch:  123  Training Loss:  68.0193  Training Accuracy:  0.529209\n",
      "Epoch:  124  Training Loss:  67.1215  Training Accuracy:  0.531017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  66.3068  Training Accuracy:  0.532941\n",
      "Epoch:  126  Training Loss:  65.4645  Training Accuracy:  0.534515\n",
      "Epoch:  127  Training Loss:  64.6741  Training Accuracy:  0.536672\n",
      "Epoch:  128  Training Loss:  63.9058  Training Accuracy:  0.538188\n",
      "Epoch:  129  Training Loss:  63.0818  Training Accuracy:  0.540228\n",
      "Epoch:  130  Training Loss:  62.3642  Training Accuracy:  0.54256\n",
      "Epoch:  131  Training Loss:  61.5894  Training Accuracy:  0.544426\n",
      "Epoch:  132  Training Loss:  60.8073  Training Accuracy:  0.546233\n",
      "Epoch:  133  Training Loss:  60.118  Training Accuracy:  0.547808\n",
      "Epoch:  134  Training Loss:  59.4309  Training Accuracy:  0.549615\n",
      "Epoch:  135  Training Loss:  58.7796  Training Accuracy:  0.550723\n",
      "Epoch:  136  Training Loss:  58.0839  Training Accuracy:  0.552297\n",
      "Epoch:  137  Training Loss:  57.4277  Training Accuracy:  0.553696\n",
      "Epoch:  138  Training Loss:  56.8009  Training Accuracy:  0.554979\n",
      "Epoch:  139  Training Loss:  56.1514  Training Accuracy:  0.557194\n",
      "Epoch:  140  Training Loss:  55.5394  Training Accuracy:  0.558477\n",
      "Epoch:  141  Training Loss:  54.9155  Training Accuracy:  0.560168\n",
      "Epoch:  142  Training Loss:  54.3154  Training Accuracy:  0.561392\n",
      "Epoch:  143  Training Loss:  53.6912  Training Accuracy:  0.562966\n",
      "Epoch:  144  Training Loss:  53.1264  Training Accuracy:  0.564948\n",
      "Epoch:  145  Training Loss:  52.5498  Training Accuracy:  0.566173\n",
      "Epoch:  146  Training Loss:  51.9668  Training Accuracy:  0.56763\n",
      "Epoch:  147  Training Loss:  51.4089  Training Accuracy:  0.569438\n",
      "Epoch:  148  Training Loss:  50.8661  Training Accuracy:  0.570837\n",
      "Epoch:  149  Training Loss:  50.276  Training Accuracy:  0.571828\n",
      "Epoch:  150  Training Loss:  49.7786  Training Accuracy:  0.573461\n",
      "Epoch:  151  Training Loss:  49.2245  Training Accuracy:  0.57451\n",
      "Epoch:  152  Training Loss:  48.6832  Training Accuracy:  0.575676\n",
      "Epoch:  153  Training Loss:  48.1496  Training Accuracy:  0.577134\n",
      "Epoch:  154  Training Loss:  47.658  Training Accuracy:  0.578475\n",
      "Epoch:  155  Training Loss:  47.1178  Training Accuracy:  0.579582\n",
      "Epoch:  156  Training Loss:  46.6528  Training Accuracy:  0.580982\n",
      "Epoch:  157  Training Loss:  46.1469  Training Accuracy:  0.582439\n",
      "Epoch:  158  Training Loss:  45.6687  Training Accuracy:  0.583838\n",
      "Epoch:  159  Training Loss:  45.1939  Training Accuracy:  0.585238\n",
      "Epoch:  160  Training Loss:  44.7357  Training Accuracy:  0.586987\n",
      "Epoch:  161  Training Loss:  44.2692  Training Accuracy:  0.589202\n",
      "Epoch:  162  Training Loss:  43.8635  Training Accuracy:  0.590951\n",
      "Epoch:  163  Training Loss:  43.3988  Training Accuracy:  0.592467\n",
      "Epoch:  164  Training Loss:  42.9987  Training Accuracy:  0.593866\n",
      "Epoch:  165  Training Loss:  42.6085  Training Accuracy:  0.595849\n",
      "Epoch:  166  Training Loss:  42.2413  Training Accuracy:  0.598064\n",
      "Epoch:  167  Training Loss:  41.8082  Training Accuracy:  0.60028\n",
      "Epoch:  168  Training Loss:  41.4481  Training Accuracy:  0.602495\n",
      "Epoch:  169  Training Loss:  41.0452  Training Accuracy:  0.604011\n",
      "Epoch:  170  Training Loss:  40.65  Training Accuracy:  0.605877\n",
      "Epoch:  171  Training Loss:  40.2406  Training Accuracy:  0.608209\n",
      "Epoch:  172  Training Loss:  39.8704  Training Accuracy:  0.609375\n",
      "Epoch:  173  Training Loss:  39.4358  Training Accuracy:  0.611124\n",
      "Epoch:  174  Training Loss:  39.0813  Training Accuracy:  0.612173\n",
      "Epoch:  175  Training Loss:  38.7139  Training Accuracy:  0.614039\n",
      "Epoch:  176  Training Loss:  38.3263  Training Accuracy:  0.616371\n",
      "Epoch:  177  Training Loss:  37.9796  Training Accuracy:  0.61812\n",
      "Epoch:  178  Training Loss:  37.5939  Training Accuracy:  0.619636\n",
      "Epoch:  179  Training Loss:  37.2434  Training Accuracy:  0.621502\n",
      "Epoch:  180  Training Loss:  36.8792  Training Accuracy:  0.623309\n",
      "Epoch:  181  Training Loss:  36.5669  Training Accuracy:  0.624592\n",
      "Epoch:  182  Training Loss:  36.2048  Training Accuracy:  0.626516\n",
      "Epoch:  183  Training Loss:  35.8908  Training Accuracy:  0.627798\n",
      "Epoch:  184  Training Loss:  35.5385  Training Accuracy:  0.628964\n",
      "Epoch:  185  Training Loss:  35.2327  Training Accuracy:  0.630247\n",
      "Epoch:  186  Training Loss:  34.9041  Training Accuracy:  0.631705\n",
      "Epoch:  187  Training Loss:  34.5911  Training Accuracy:  0.633279\n",
      "Epoch:  188  Training Loss:  34.2692  Training Accuracy:  0.634386\n",
      "Epoch:  189  Training Loss:  33.9892  Training Accuracy:  0.635844\n",
      "Epoch:  190  Training Loss:  33.6606  Training Accuracy:  0.636893\n",
      "Epoch:  191  Training Loss:  33.3926  Training Accuracy:  0.638176\n",
      "Epoch:  192  Training Loss:  33.088  Training Accuracy:  0.638701\n",
      "Epoch:  193  Training Loss:  32.7916  Training Accuracy:  0.6401\n",
      "Epoch:  194  Training Loss:  32.5162  Training Accuracy:  0.641149\n",
      "Epoch:  195  Training Loss:  32.2377  Training Accuracy:  0.642724\n",
      "Epoch:  196  Training Loss:  31.9478  Training Accuracy:  0.64354\n",
      "Epoch:  197  Training Loss:  31.6735  Training Accuracy:  0.644822\n",
      "Epoch:  198  Training Loss:  31.4002  Training Accuracy:  0.647038\n",
      "Epoch:  199  Training Loss:  31.1382  Training Accuracy:  0.648904\n",
      "Epoch:  200  Training Loss:  30.8529  Training Accuracy:  0.649661\n",
      "Epoch:  201  Training Loss:  30.6009  Training Accuracy:  0.651003\n",
      "Epoch:  202  Training Loss:  30.3052  Training Accuracy:  0.652169\n",
      "Epoch:  203  Training Loss:  30.0498  Training Accuracy:  0.653684\n",
      "Epoch:  204  Training Loss:  29.7809  Training Accuracy:  0.655492\n",
      "Epoch:  205  Training Loss:  29.5151  Training Accuracy:  0.656541\n",
      "Epoch:  206  Training Loss:  29.2505  Training Accuracy:  0.658115\n",
      "Epoch:  207  Training Loss:  28.9995  Training Accuracy:  0.659048\n",
      "Epoch:  208  Training Loss:  28.7643  Training Accuracy:  0.660389\n",
      "Epoch:  209  Training Loss:  28.5088  Training Accuracy:  0.661672\n",
      "Epoch:  210  Training Loss:  28.2655  Training Accuracy:  0.662663\n",
      "Epoch:  211  Training Loss:  28.0318  Training Accuracy:  0.663712\n",
      "Epoch:  212  Training Loss:  27.8008  Training Accuracy:  0.665287\n",
      "Epoch:  213  Training Loss:  27.5666  Training Accuracy:  0.667152\n",
      "Epoch:  214  Training Loss:  27.3471  Training Accuracy:  0.668843\n",
      "Epoch:  215  Training Loss:  27.114  Training Accuracy:  0.67065\n",
      "Epoch:  216  Training Loss:  26.9113  Training Accuracy:  0.671991\n",
      "Epoch:  217  Training Loss:  26.6845  Training Accuracy:  0.673624\n",
      "Epoch:  218  Training Loss:  26.4805  Training Accuracy:  0.674848\n",
      "Epoch:  219  Training Loss:  26.2493  Training Accuracy:  0.675781\n",
      "Epoch:  220  Training Loss:  26.0442  Training Accuracy:  0.67718\n",
      "Epoch:  221  Training Loss:  25.8115  Training Accuracy:  0.678579\n",
      "Epoch:  222  Training Loss:  25.5879  Training Accuracy:  0.680037\n",
      "Epoch:  223  Training Loss:  25.3782  Training Accuracy:  0.681903\n",
      "Epoch:  224  Training Loss:  25.1629  Training Accuracy:  0.683302\n",
      "Epoch:  225  Training Loss:  24.9816  Training Accuracy:  0.684993\n",
      "Epoch:  226  Training Loss:  24.7707  Training Accuracy:  0.686334\n",
      "Epoch:  227  Training Loss:  24.5928  Training Accuracy:  0.687675\n",
      "Epoch:  228  Training Loss:  24.3756  Training Accuracy:  0.689424\n",
      "Epoch:  229  Training Loss:  24.1939  Training Accuracy:  0.690823\n",
      "Epoch:  230  Training Loss:  23.9793  Training Accuracy:  0.691931\n",
      "Epoch:  231  Training Loss:  23.7827  Training Accuracy:  0.693563\n",
      "Epoch:  232  Training Loss:  23.6138  Training Accuracy:  0.694496\n",
      "Epoch:  233  Training Loss:  23.4306  Training Accuracy:  0.695487\n",
      "Epoch:  234  Training Loss:  23.2528  Training Accuracy:  0.696595\n",
      "Epoch:  235  Training Loss:  23.0696  Training Accuracy:  0.697469\n",
      "Epoch:  236  Training Loss:  22.8943  Training Accuracy:  0.698752\n",
      "Epoch:  237  Training Loss:  22.7174  Training Accuracy:  0.700443\n",
      "Epoch:  238  Training Loss:  22.5569  Training Accuracy:  0.701959\n",
      "Epoch:  239  Training Loss:  22.3659  Training Accuracy:  0.703125\n",
      "Epoch:  240  Training Loss:  22.2051  Training Accuracy:  0.704349\n",
      "Epoch:  241  Training Loss:  22.025  Training Accuracy:  0.705224\n",
      "Epoch:  242  Training Loss:  21.849  Training Accuracy:  0.706564\n",
      "Epoch:  243  Training Loss:  21.6843  Training Accuracy:  0.707614\n",
      "Epoch:  244  Training Loss:  21.5217  Training Accuracy:  0.708897\n",
      "Epoch:  245  Training Loss:  21.3531  Training Accuracy:  0.710004\n",
      "Epoch:  246  Training Loss:  21.1979  Training Accuracy:  0.711112\n",
      "Epoch:  247  Training Loss:  21.0359  Training Accuracy:  0.712278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  20.8721  Training Accuracy:  0.713328\n",
      "Epoch:  249  Training Loss:  20.725  Training Accuracy:  0.714319\n",
      "Epoch:  250  Training Loss:  20.5541  Training Accuracy:  0.715718\n",
      "Epoch:  251  Training Loss:  20.411  Training Accuracy:  0.716767\n",
      "Epoch:  252  Training Loss:  20.2499  Training Accuracy:  0.718167\n",
      "Epoch:  253  Training Loss:  20.1009  Training Accuracy:  0.719449\n",
      "Epoch:  254  Training Loss:  19.9361  Training Accuracy:  0.72044\n",
      "Epoch:  255  Training Loss:  19.7835  Training Accuracy:  0.721606\n",
      "Epoch:  256  Training Loss:  19.6221  Training Accuracy:  0.723006\n",
      "Epoch:  257  Training Loss:  19.4711  Training Accuracy:  0.723939\n",
      "Epoch:  258  Training Loss:  19.3055  Training Accuracy:  0.725338\n",
      "Epoch:  259  Training Loss:  19.1536  Training Accuracy:  0.726387\n",
      "Epoch:  260  Training Loss:  18.9854  Training Accuracy:  0.727612\n",
      "Epoch:  261  Training Loss:  18.8446  Training Accuracy:  0.728719\n",
      "Epoch:  262  Training Loss:  18.6847  Training Accuracy:  0.729594\n",
      "Epoch:  263  Training Loss:  18.5349  Training Accuracy:  0.731285\n",
      "Epoch:  264  Training Loss:  18.3859  Training Accuracy:  0.732334\n",
      "Epoch:  265  Training Loss:  18.2143  Training Accuracy:  0.7335\n",
      "Epoch:  266  Training Loss:  18.092  Training Accuracy:  0.735016\n",
      "Epoch:  267  Training Loss:  17.953  Training Accuracy:  0.73624\n",
      "Epoch:  268  Training Loss:  17.7952  Training Accuracy:  0.737173\n",
      "Epoch:  269  Training Loss:  17.6802  Training Accuracy:  0.738572\n",
      "Epoch:  270  Training Loss:  17.5341  Training Accuracy:  0.739622\n",
      "Epoch:  271  Training Loss:  17.3782  Training Accuracy:  0.740963\n",
      "Epoch:  272  Training Loss:  17.2525  Training Accuracy:  0.742129\n",
      "Epoch:  273  Training Loss:  17.1293  Training Accuracy:  0.74277\n",
      "Epoch:  274  Training Loss:  16.9743  Training Accuracy:  0.744344\n",
      "Epoch:  275  Training Loss:  16.8626  Training Accuracy:  0.745452\n",
      "Epoch:  276  Training Loss:  16.7144  Training Accuracy:  0.746851\n",
      "Epoch:  277  Training Loss:  16.5905  Training Accuracy:  0.747434\n",
      "Epoch:  278  Training Loss:  16.476  Training Accuracy:  0.748542\n",
      "Epoch:  279  Training Loss:  16.3464  Training Accuracy:  0.749183\n",
      "Epoch:  280  Training Loss:  16.2247  Training Accuracy:  0.750116\n",
      "Epoch:  281  Training Loss:  16.1012  Training Accuracy:  0.751107\n",
      "Epoch:  282  Training Loss:  15.9829  Training Accuracy:  0.752856\n",
      "Epoch:  283  Training Loss:  15.8662  Training Accuracy:  0.753789\n",
      "Epoch:  284  Training Loss:  15.7346  Training Accuracy:  0.755014\n",
      "Epoch:  285  Training Loss:  15.6264  Training Accuracy:  0.756121\n",
      "Epoch:  286  Training Loss:  15.4974  Training Accuracy:  0.757054\n",
      "Epoch:  287  Training Loss:  15.3841  Training Accuracy:  0.758104\n",
      "Epoch:  288  Training Loss:  15.2517  Training Accuracy:  0.75927\n",
      "Epoch:  289  Training Loss:  15.1303  Training Accuracy:  0.760261\n",
      "Epoch:  290  Training Loss:  15.0215  Training Accuracy:  0.761194\n",
      "Epoch:  291  Training Loss:  14.8947  Training Accuracy:  0.761835\n",
      "Epoch:  292  Training Loss:  14.795  Training Accuracy:  0.762418\n",
      "Epoch:  293  Training Loss:  14.6722  Training Accuracy:  0.763059\n",
      "Epoch:  294  Training Loss:  14.5533  Training Accuracy:  0.764284\n",
      "Epoch:  295  Training Loss:  14.4529  Training Accuracy:  0.765566\n",
      "Epoch:  296  Training Loss:  14.3252  Training Accuracy:  0.766791\n",
      "Epoch:  297  Training Loss:  14.2293  Training Accuracy:  0.767432\n",
      "Epoch:  298  Training Loss:  14.1346  Training Accuracy:  0.768773\n",
      "Epoch:  299  Training Loss:  14.0067  Training Accuracy:  0.769881\n",
      "Testing Accuracy: 0.724728\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  438.936  Training Accuracy:  0.0411031\n",
      "Epoch:  1  Training Loss:  380.767  Training Accuracy:  0.0458256\n",
      "Epoch:  2  Training Loss:  359.341  Training Accuracy:  0.050548\n",
      "Epoch:  3  Training Loss:  343.18  Training Accuracy:  0.0541045\n",
      "Epoch:  4  Training Loss:  329.443  Training Accuracy:  0.0578941\n",
      "Epoch:  5  Training Loss:  318.669  Training Accuracy:  0.0611591\n",
      "Epoch:  6  Training Loss:  308.909  Training Accuracy:  0.0649487\n",
      "Epoch:  7  Training Loss:  300.199  Training Accuracy:  0.0693214\n",
      "Epoch:  8  Training Loss:  293.185  Training Accuracy:  0.0735774\n",
      "Epoch:  9  Training Loss:  287.75  Training Accuracy:  0.0805154\n",
      "Epoch:  10  Training Loss:  282.464  Training Accuracy:  0.0870452\n",
      "Epoch:  11  Training Loss:  277.017  Training Accuracy:  0.0942164\n",
      "Epoch:  12  Training Loss:  271.936  Training Accuracy:  0.102262\n",
      "Epoch:  13  Training Loss:  267.43  Training Accuracy:  0.109667\n",
      "Epoch:  14  Training Loss:  263.632  Training Accuracy:  0.117945\n",
      "Epoch:  15  Training Loss:  260.254  Training Accuracy:  0.125233\n",
      "Epoch:  16  Training Loss:  257.168  Training Accuracy:  0.134153\n",
      "Epoch:  17  Training Loss:  254.2  Training Accuracy:  0.142141\n",
      "Epoch:  18  Training Loss:  250.993  Training Accuracy:  0.148846\n",
      "Epoch:  19  Training Loss:  247.861  Training Accuracy:  0.156133\n",
      "Epoch:  20  Training Loss:  244.845  Training Accuracy:  0.162663\n",
      "Epoch:  21  Training Loss:  241.716  Training Accuracy:  0.16861\n",
      "Epoch:  22  Training Loss:  238.501  Training Accuracy:  0.175781\n",
      "Epoch:  23  Training Loss:  235.406  Training Accuracy:  0.183594\n",
      "Epoch:  24  Training Loss:  232.335  Training Accuracy:  0.191523\n",
      "Epoch:  25  Training Loss:  229.427  Training Accuracy:  0.197703\n",
      "Epoch:  26  Training Loss:  226.495  Training Accuracy:  0.205049\n",
      "Epoch:  27  Training Loss:  223.458  Training Accuracy:  0.211637\n",
      "Epoch:  28  Training Loss:  220.588  Training Accuracy:  0.218925\n",
      "Epoch:  29  Training Loss:  217.787  Training Accuracy:  0.225163\n",
      "Epoch:  30  Training Loss:  214.849  Training Accuracy:  0.232451\n",
      "Epoch:  31  Training Loss:  212.073  Training Accuracy:  0.238689\n",
      "Epoch:  32  Training Loss:  209.331  Training Accuracy:  0.245977\n",
      "Epoch:  33  Training Loss:  206.68  Training Accuracy:  0.252157\n",
      "Epoch:  34  Training Loss:  203.983  Training Accuracy:  0.258396\n",
      "Epoch:  35  Training Loss:  201.287  Training Accuracy:  0.264167\n",
      "Epoch:  36  Training Loss:  198.611  Training Accuracy:  0.268715\n",
      "Epoch:  37  Training Loss:  196.005  Training Accuracy:  0.273962\n",
      "Epoch:  38  Training Loss:  193.488  Training Accuracy:  0.278976\n",
      "Epoch:  39  Training Loss:  190.858  Training Accuracy:  0.282941\n",
      "Epoch:  40  Training Loss:  188.258  Training Accuracy:  0.287139\n",
      "Epoch:  41  Training Loss:  185.738  Training Accuracy:  0.291511\n",
      "Epoch:  42  Training Loss:  183.271  Training Accuracy:  0.295709\n",
      "Epoch:  43  Training Loss:  180.918  Training Accuracy:  0.300431\n",
      "Epoch:  44  Training Loss:  178.465  Training Accuracy:  0.304804\n",
      "Epoch:  45  Training Loss:  176.117  Training Accuracy:  0.308594\n",
      "Epoch:  46  Training Loss:  173.635  Training Accuracy:  0.31285\n",
      "Epoch:  47  Training Loss:  171.305  Training Accuracy:  0.316931\n",
      "Epoch:  48  Training Loss:  168.858  Training Accuracy:  0.321304\n",
      "Epoch:  49  Training Loss:  166.648  Training Accuracy:  0.326609\n",
      "Epoch:  50  Training Loss:  164.147  Training Accuracy:  0.331215\n",
      "Epoch:  51  Training Loss:  161.934  Training Accuracy:  0.335121\n",
      "Epoch:  52  Training Loss:  159.525  Training Accuracy:  0.339552\n",
      "Epoch:  53  Training Loss:  157.297  Training Accuracy:  0.343808\n",
      "Epoch:  54  Training Loss:  154.939  Training Accuracy:  0.347773\n",
      "Epoch:  55  Training Loss:  152.605  Training Accuracy:  0.352845\n",
      "Epoch:  56  Training Loss:  150.435  Training Accuracy:  0.35611\n",
      "Epoch:  57  Training Loss:  147.814  Training Accuracy:  0.359841\n",
      "Epoch:  58  Training Loss:  145.54  Training Accuracy:  0.364097\n",
      "Epoch:  59  Training Loss:  143.383  Training Accuracy:  0.369578\n",
      "Epoch:  60  Training Loss:  141.195  Training Accuracy:  0.374417\n",
      "Epoch:  61  Training Loss:  139.111  Training Accuracy:  0.377915\n",
      "Epoch:  62  Training Loss:  136.823  Training Accuracy:  0.381238\n",
      "Epoch:  63  Training Loss:  134.639  Training Accuracy:  0.385553\n",
      "Epoch:  64  Training Loss:  132.627  Training Accuracy:  0.389809\n",
      "Epoch:  65  Training Loss:  130.43  Training Accuracy:  0.392899\n",
      "Epoch:  66  Training Loss:  128.468  Training Accuracy:  0.396688\n",
      "Epoch:  67  Training Loss:  126.366  Training Accuracy:  0.39972\n",
      "Epoch:  68  Training Loss:  124.365  Training Accuracy:  0.403626\n",
      "Epoch:  69  Training Loss:  122.316  Training Accuracy:  0.407299\n",
      "Epoch:  70  Training Loss:  120.444  Training Accuracy:  0.410681\n",
      "Epoch:  71  Training Loss:  118.496  Training Accuracy:  0.414004\n",
      "Epoch:  72  Training Loss:  116.576  Training Accuracy:  0.416744\n",
      "Epoch:  73  Training Loss:  114.796  Training Accuracy:  0.419776\n",
      "Epoch:  74  Training Loss:  113.055  Training Accuracy:  0.423216\n",
      "Epoch:  75  Training Loss:  111.32  Training Accuracy:  0.426772\n",
      "Epoch:  76  Training Loss:  109.508  Training Accuracy:  0.429921\n",
      "Epoch:  77  Training Loss:  107.921  Training Accuracy:  0.433302\n",
      "Epoch:  78  Training Loss:  106.241  Training Accuracy:  0.4368\n",
      "Epoch:  79  Training Loss:  104.711  Training Accuracy:  0.439774\n",
      "Epoch:  80  Training Loss:  103.198  Training Accuracy:  0.442572\n",
      "Epoch:  81  Training Loss:  101.821  Training Accuracy:  0.445662\n",
      "Epoch:  82  Training Loss:  100.267  Training Accuracy:  0.449219\n",
      "Epoch:  83  Training Loss:  98.8555  Training Accuracy:  0.451492\n",
      "Epoch:  84  Training Loss:  97.4608  Training Accuracy:  0.454058\n",
      "Epoch:  85  Training Loss:  96.144  Training Accuracy:  0.45674\n",
      "Epoch:  86  Training Loss:  94.7307  Training Accuracy:  0.459771\n",
      "Epoch:  87  Training Loss:  93.4217  Training Accuracy:  0.462686\n",
      "Epoch:  88  Training Loss:  92.0923  Training Accuracy:  0.465893\n",
      "Epoch:  89  Training Loss:  90.8019  Training Accuracy:  0.468633\n",
      "Epoch:  90  Training Loss:  89.545  Training Accuracy:  0.47114\n",
      "Epoch:  91  Training Loss:  88.3406  Training Accuracy:  0.47388\n",
      "Epoch:  92  Training Loss:  87.0537  Training Accuracy:  0.476154\n",
      "Epoch:  93  Training Loss:  85.8895  Training Accuracy:  0.478428\n",
      "Epoch:  94  Training Loss:  84.7215  Training Accuracy:  0.480818\n",
      "Epoch:  95  Training Loss:  83.5345  Training Accuracy:  0.483384\n",
      "Epoch:  96  Training Loss:  82.4265  Training Accuracy:  0.48624\n",
      "Epoch:  97  Training Loss:  81.311  Training Accuracy:  0.488864\n",
      "Epoch:  98  Training Loss:  80.2155  Training Accuracy:  0.491546\n",
      "Epoch:  99  Training Loss:  79.1581  Training Accuracy:  0.49382\n",
      "Epoch:  100  Training Loss:  78.1276  Training Accuracy:  0.49621\n",
      "Epoch:  101  Training Loss:  77.1664  Training Accuracy:  0.498601\n",
      "Epoch:  102  Training Loss:  76.1542  Training Accuracy:  0.500641\n",
      "Epoch:  103  Training Loss:  75.2153  Training Accuracy:  0.503381\n",
      "Epoch:  104  Training Loss:  74.2353  Training Accuracy:  0.506063\n",
      "Epoch:  105  Training Loss:  73.3719  Training Accuracy:  0.508512\n",
      "Epoch:  106  Training Loss:  72.3148  Training Accuracy:  0.510727\n",
      "Epoch:  107  Training Loss:  71.5038  Training Accuracy:  0.513584\n",
      "Epoch:  108  Training Loss:  70.5714  Training Accuracy:  0.515741\n",
      "Epoch:  109  Training Loss:  69.6945  Training Accuracy:  0.517665\n",
      "Epoch:  110  Training Loss:  68.8068  Training Accuracy:  0.519473\n",
      "Epoch:  111  Training Loss:  68.0243  Training Accuracy:  0.522038\n",
      "Epoch:  112  Training Loss:  67.2314  Training Accuracy:  0.523554\n",
      "Epoch:  113  Training Loss:  66.4244  Training Accuracy:  0.525769\n",
      "Epoch:  114  Training Loss:  65.5954  Training Accuracy:  0.52746\n",
      "Epoch:  115  Training Loss:  64.9151  Training Accuracy:  0.529326\n",
      "Epoch:  116  Training Loss:  64.0591  Training Accuracy:  0.530725\n",
      "Epoch:  117  Training Loss:  63.3277  Training Accuracy:  0.532707\n",
      "Epoch:  118  Training Loss:  62.5806  Training Accuracy:  0.534748\n",
      "Epoch:  119  Training Loss:  61.8706  Training Accuracy:  0.53673\n",
      "Epoch:  120  Training Loss:  61.1216  Training Accuracy:  0.538829\n",
      "Epoch:  121  Training Loss:  60.4921  Training Accuracy:  0.540753\n",
      "Epoch:  122  Training Loss:  59.7576  Training Accuracy:  0.542036\n",
      "Epoch:  123  Training Loss:  59.1513  Training Accuracy:  0.544076\n",
      "Epoch:  124  Training Loss:  58.433  Training Accuracy:  0.545476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  57.8152  Training Accuracy:  0.546816\n",
      "Epoch:  126  Training Loss:  57.1008  Training Accuracy:  0.548216\n",
      "Epoch:  127  Training Loss:  56.5327  Training Accuracy:  0.549557\n",
      "Epoch:  128  Training Loss:  55.862  Training Accuracy:  0.551481\n",
      "Epoch:  129  Training Loss:  55.2446  Training Accuracy:  0.553113\n",
      "Epoch:  130  Training Loss:  54.6495  Training Accuracy:  0.55492\n",
      "Epoch:  131  Training Loss:  54.0582  Training Accuracy:  0.556495\n",
      "Epoch:  132  Training Loss:  53.5022  Training Accuracy:  0.558768\n",
      "Epoch:  133  Training Loss:  52.9387  Training Accuracy:  0.560051\n",
      "Epoch:  134  Training Loss:  52.3584  Training Accuracy:  0.5618\n",
      "Epoch:  135  Training Loss:  51.843  Training Accuracy:  0.563549\n",
      "Epoch:  136  Training Loss:  51.3006  Training Accuracy:  0.565007\n",
      "Epoch:  137  Training Loss:  50.7852  Training Accuracy:  0.567047\n",
      "Epoch:  138  Training Loss:  50.2993  Training Accuracy:  0.568913\n",
      "Epoch:  139  Training Loss:  49.7704  Training Accuracy:  0.570779\n",
      "Epoch:  140  Training Loss:  49.303  Training Accuracy:  0.572936\n",
      "Epoch:  141  Training Loss:  48.821  Training Accuracy:  0.574743\n",
      "Epoch:  142  Training Loss:  48.3436  Training Accuracy:  0.577309\n",
      "Epoch:  143  Training Loss:  47.8933  Training Accuracy:  0.579233\n",
      "Epoch:  144  Training Loss:  47.4262  Training Accuracy:  0.58069\n",
      "Epoch:  145  Training Loss:  46.9951  Training Accuracy:  0.582264\n",
      "Epoch:  146  Training Loss:  46.5532  Training Accuracy:  0.584305\n",
      "Epoch:  147  Training Loss:  46.1494  Training Accuracy:  0.586345\n",
      "Epoch:  148  Training Loss:  45.6729  Training Accuracy:  0.587745\n",
      "Epoch:  149  Training Loss:  45.2935  Training Accuracy:  0.589494\n",
      "Epoch:  150  Training Loss:  44.842  Training Accuracy:  0.591301\n",
      "Epoch:  151  Training Loss:  44.4677  Training Accuracy:  0.593633\n",
      "Epoch:  152  Training Loss:  44.0868  Training Accuracy:  0.595965\n",
      "Epoch:  153  Training Loss:  43.6525  Training Accuracy:  0.597889\n",
      "Epoch:  154  Training Loss:  43.2504  Training Accuracy:  0.600338\n",
      "Epoch:  155  Training Loss:  42.9091  Training Accuracy:  0.601329\n",
      "Epoch:  156  Training Loss:  42.4969  Training Accuracy:  0.60302\n",
      "Epoch:  157  Training Loss:  42.1463  Training Accuracy:  0.604244\n",
      "Epoch:  158  Training Loss:  41.7598  Training Accuracy:  0.605935\n",
      "Epoch:  159  Training Loss:  41.401  Training Accuracy:  0.60815\n",
      "Epoch:  160  Training Loss:  41.0356  Training Accuracy:  0.610657\n",
      "Epoch:  161  Training Loss:  40.7019  Training Accuracy:  0.612406\n",
      "Epoch:  162  Training Loss:  40.3273  Training Accuracy:  0.613747\n",
      "Epoch:  163  Training Loss:  39.9936  Training Accuracy:  0.61538\n",
      "Epoch:  164  Training Loss:  39.6526  Training Accuracy:  0.616662\n",
      "Epoch:  165  Training Loss:  39.3188  Training Accuracy:  0.617945\n",
      "Epoch:  166  Training Loss:  39.0096  Training Accuracy:  0.619111\n",
      "Epoch:  167  Training Loss:  38.6362  Training Accuracy:  0.620277\n",
      "Epoch:  168  Training Loss:  38.3648  Training Accuracy:  0.621327\n",
      "Epoch:  169  Training Loss:  37.9762  Training Accuracy:  0.622551\n",
      "Epoch:  170  Training Loss:  37.715  Training Accuracy:  0.624708\n",
      "Epoch:  171  Training Loss:  37.3617  Training Accuracy:  0.626049\n",
      "Epoch:  172  Training Loss:  37.1168  Training Accuracy:  0.62739\n",
      "Epoch:  173  Training Loss:  36.7441  Training Accuracy:  0.628323\n",
      "Epoch:  174  Training Loss:  36.4854  Training Accuracy:  0.629781\n",
      "Epoch:  175  Training Loss:  36.1922  Training Accuracy:  0.631646\n",
      "Epoch:  176  Training Loss:  35.9596  Training Accuracy:  0.632987\n",
      "Epoch:  177  Training Loss:  35.6145  Training Accuracy:  0.633628\n",
      "Epoch:  178  Training Loss:  35.3777  Training Accuracy:  0.635378\n",
      "Epoch:  179  Training Loss:  35.0365  Training Accuracy:  0.63631\n",
      "Epoch:  180  Training Loss:  34.8607  Training Accuracy:  0.638234\n",
      "Epoch:  181  Training Loss:  34.491  Training Accuracy:  0.639051\n",
      "Epoch:  182  Training Loss:  34.3768  Training Accuracy:  0.640741\n",
      "Epoch:  183  Training Loss:  34.0036  Training Accuracy:  0.641499\n",
      "Epoch:  184  Training Loss:  33.8693  Training Accuracy:  0.643715\n",
      "Epoch:  185  Training Loss:  33.5043  Training Accuracy:  0.645172\n",
      "Epoch:  186  Training Loss:  33.3774  Training Accuracy:  0.647213\n",
      "Epoch:  187  Training Loss:  33.0059  Training Accuracy:  0.648554\n",
      "Epoch:  188  Training Loss:  32.8626  Training Accuracy:  0.650186\n",
      "Epoch:  189  Training Loss:  32.5096  Training Accuracy:  0.651469\n",
      "Epoch:  190  Training Loss:  32.3903  Training Accuracy:  0.653393\n",
      "Epoch:  191  Training Loss:  32.0537  Training Accuracy:  0.654675\n",
      "Epoch:  192  Training Loss:  31.9231  Training Accuracy:  0.656191\n",
      "Epoch:  193  Training Loss:  31.5728  Training Accuracy:  0.657124\n",
      "Epoch:  194  Training Loss:  31.4625  Training Accuracy:  0.658757\n",
      "Epoch:  195  Training Loss:  31.1293  Training Accuracy:  0.660214\n",
      "Epoch:  196  Training Loss:  31.0047  Training Accuracy:  0.661672\n",
      "Epoch:  197  Training Loss:  30.6504  Training Accuracy:  0.662896\n",
      "Epoch:  198  Training Loss:  30.5283  Training Accuracy:  0.664995\n",
      "Epoch:  199  Training Loss:  30.1878  Training Accuracy:  0.665928\n",
      "Epoch:  200  Training Loss:  30.0711  Training Accuracy:  0.667444\n",
      "Epoch:  201  Training Loss:  29.734  Training Accuracy:  0.669076\n",
      "Epoch:  202  Training Loss:  29.6335  Training Accuracy:  0.670067\n",
      "Epoch:  203  Training Loss:  29.2846  Training Accuracy:  0.671991\n",
      "Epoch:  204  Training Loss:  29.1723  Training Accuracy:  0.673274\n",
      "Epoch:  205  Training Loss:  28.8313  Training Accuracy:  0.674265\n",
      "Epoch:  206  Training Loss:  28.747  Training Accuracy:  0.675256\n",
      "Epoch:  207  Training Loss:  28.4001  Training Accuracy:  0.676247\n",
      "Epoch:  208  Training Loss:  28.3074  Training Accuracy:  0.677472\n",
      "Epoch:  209  Training Loss:  27.9928  Training Accuracy:  0.678871\n",
      "Epoch:  210  Training Loss:  27.8915  Training Accuracy:  0.679745\n",
      "Epoch:  211  Training Loss:  27.5695  Training Accuracy:  0.681145\n",
      "Epoch:  212  Training Loss:  27.5023  Training Accuracy:  0.682369\n",
      "Epoch:  213  Training Loss:  27.1713  Training Accuracy:  0.68336\n",
      "Epoch:  214  Training Loss:  27.0938  Training Accuracy:  0.684235\n",
      "Epoch:  215  Training Loss:  26.7747  Training Accuracy:  0.685284\n",
      "Epoch:  216  Training Loss:  26.6711  Training Accuracy:  0.685984\n",
      "Epoch:  217  Training Loss:  26.3868  Training Accuracy:  0.687441\n",
      "Epoch:  218  Training Loss:  26.2927  Training Accuracy:  0.688607\n",
      "Epoch:  219  Training Loss:  25.9868  Training Accuracy:  0.690065\n",
      "Epoch:  220  Training Loss:  25.883  Training Accuracy:  0.691056\n",
      "Epoch:  221  Training Loss:  25.5905  Training Accuracy:  0.693272\n",
      "Epoch:  222  Training Loss:  25.5444  Training Accuracy:  0.694321\n",
      "Epoch:  223  Training Loss:  25.2319  Training Accuracy:  0.695545\n",
      "Epoch:  224  Training Loss:  25.1541  Training Accuracy:  0.696303\n",
      "Epoch:  225  Training Loss:  24.8429  Training Accuracy:  0.697994\n",
      "Epoch:  226  Training Loss:  24.7843  Training Accuracy:  0.699568\n",
      "Epoch:  227  Training Loss:  24.508  Training Accuracy:  0.700909\n",
      "Epoch:  228  Training Loss:  24.4388  Training Accuracy:  0.701959\n",
      "Epoch:  229  Training Loss:  24.1615  Training Accuracy:  0.703358\n",
      "Epoch:  230  Training Loss:  24.1026  Training Accuracy:  0.704641\n",
      "Epoch:  231  Training Loss:  23.8324  Training Accuracy:  0.706098\n",
      "Epoch:  232  Training Loss:  23.7756  Training Accuracy:  0.707089\n",
      "Epoch:  233  Training Loss:  23.5062  Training Accuracy:  0.707731\n",
      "Epoch:  234  Training Loss:  23.4638  Training Accuracy:  0.708663\n",
      "Epoch:  235  Training Loss:  23.1701  Training Accuracy:  0.709771\n",
      "Epoch:  236  Training Loss:  23.1059  Training Accuracy:  0.71117\n",
      "Epoch:  237  Training Loss:  22.8557  Training Accuracy:  0.712511\n",
      "Epoch:  238  Training Loss:  22.7952  Training Accuracy:  0.713328\n",
      "Epoch:  239  Training Loss:  22.5172  Training Accuracy:  0.714785\n",
      "Epoch:  240  Training Loss:  22.4715  Training Accuracy:  0.716184\n",
      "Epoch:  241  Training Loss:  22.2259  Training Accuracy:  0.717525\n",
      "Epoch:  242  Training Loss:  22.1331  Training Accuracy:  0.718925\n",
      "Epoch:  243  Training Loss:  21.9114  Training Accuracy:  0.720149\n",
      "Epoch:  244  Training Loss:  21.8532  Training Accuracy:  0.721431\n",
      "Epoch:  245  Training Loss:  21.6038  Training Accuracy:  0.722772\n",
      "Epoch:  246  Training Loss:  21.5639  Training Accuracy:  0.72423\n",
      "Epoch:  247  Training Loss:  21.289  Training Accuracy:  0.725105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  21.2356  Training Accuracy:  0.726154\n",
      "Epoch:  249  Training Loss:  20.9918  Training Accuracy:  0.72732\n",
      "Epoch:  250  Training Loss:  20.9672  Training Accuracy:  0.728486\n",
      "Epoch:  251  Training Loss:  20.6822  Training Accuracy:  0.72971\n",
      "Epoch:  252  Training Loss:  20.6143  Training Accuracy:  0.730352\n",
      "Epoch:  253  Training Loss:  20.4079  Training Accuracy:  0.731401\n",
      "Epoch:  254  Training Loss:  20.3354  Training Accuracy:  0.732159\n",
      "Epoch:  255  Training Loss:  20.0977  Training Accuracy:  0.732917\n",
      "Epoch:  256  Training Loss:  20.0912  Training Accuracy:  0.73385\n",
      "Epoch:  257  Training Loss:  19.8188  Training Accuracy:  0.735307\n",
      "Epoch:  258  Training Loss:  19.7299  Training Accuracy:  0.736007\n",
      "Epoch:  259  Training Loss:  19.5586  Training Accuracy:  0.73729\n",
      "Epoch:  260  Training Loss:  19.4751  Training Accuracy:  0.737931\n",
      "Epoch:  261  Training Loss:  19.2934  Training Accuracy:  0.739389\n",
      "Epoch:  262  Training Loss:  19.1781  Training Accuracy:  0.740846\n",
      "Epoch:  263  Training Loss:  19.0935  Training Accuracy:  0.741721\n",
      "Epoch:  264  Training Loss:  18.8839  Training Accuracy:  0.742187\n",
      "Epoch:  265  Training Loss:  18.8401  Training Accuracy:  0.743878\n",
      "Epoch:  266  Training Loss:  18.622  Training Accuracy:  0.745335\n",
      "Epoch:  267  Training Loss:  18.5009  Training Accuracy:  0.74621\n",
      "Epoch:  268  Training Loss:  18.3602  Training Accuracy:  0.747842\n",
      "Epoch:  269  Training Loss:  18.2471  Training Accuracy:  0.748309\n",
      "Epoch:  270  Training Loss:  18.1081  Training Accuracy:  0.749183\n",
      "Epoch:  271  Training Loss:  18.0101  Training Accuracy:  0.750524\n",
      "Epoch:  272  Training Loss:  17.8157  Training Accuracy:  0.751515\n",
      "Epoch:  273  Training Loss:  17.7982  Training Accuracy:  0.75274\n",
      "Epoch:  274  Training Loss:  17.534  Training Accuracy:  0.753731\n",
      "Epoch:  275  Training Loss:  17.444  Training Accuracy:  0.754722\n",
      "Epoch:  276  Training Loss:  17.3415  Training Accuracy:  0.755946\n",
      "Epoch:  277  Training Loss:  17.2054  Training Accuracy:  0.757229\n",
      "Epoch:  278  Training Loss:  17.1103  Training Accuracy:  0.757812\n",
      "Epoch:  279  Training Loss:  16.9834  Training Accuracy:  0.759503\n",
      "Epoch:  280  Training Loss:  16.8281  Training Accuracy:  0.760552\n",
      "Epoch:  281  Training Loss:  16.83  Training Accuracy:  0.76166\n",
      "Epoch:  282  Training Loss:  16.5696  Training Accuracy:  0.762593\n",
      "Epoch:  283  Training Loss:  16.4612  Training Accuracy:  0.763176\n",
      "Epoch:  284  Training Loss:  16.4446  Training Accuracy:  0.76475\n",
      "Epoch:  285  Training Loss:  16.2185  Training Accuracy:  0.765275\n",
      "Epoch:  286  Training Loss:  16.1508  Training Accuracy:  0.766266\n",
      "Epoch:  287  Training Loss:  15.9893  Training Accuracy:  0.767024\n",
      "Epoch:  288  Training Loss:  15.9171  Training Accuracy:  0.768015\n",
      "Epoch:  289  Training Loss:  15.7598  Training Accuracy:  0.769064\n",
      "Epoch:  290  Training Loss:  15.6469  Training Accuracy:  0.769647\n",
      "Epoch:  291  Training Loss:  15.6128  Training Accuracy:  0.770697\n",
      "Epoch:  292  Training Loss:  15.4138  Training Accuracy:  0.771455\n",
      "Epoch:  293  Training Loss:  15.32  Training Accuracy:  0.772854\n",
      "Epoch:  294  Training Loss:  15.2027  Training Accuracy:  0.773787\n",
      "Epoch:  295  Training Loss:  15.0935  Training Accuracy:  0.774953\n",
      "Epoch:  296  Training Loss:  14.9721  Training Accuracy:  0.775711\n",
      "Epoch:  297  Training Loss:  14.9487  Training Accuracy:  0.776819\n",
      "Epoch:  298  Training Loss:  14.7399  Training Accuracy:  0.77746\n",
      "Epoch:  299  Training Loss:  14.6635  Training Accuracy:  0.778393\n",
      "Epoch:  300  Training Loss:  14.5398  Training Accuracy:  0.779326\n",
      "Epoch:  301  Training Loss:  14.507  Training Accuracy:  0.77985\n",
      "Epoch:  302  Training Loss:  14.3131  Training Accuracy:  0.780433\n",
      "Epoch:  303  Training Loss:  14.2471  Training Accuracy:  0.781191\n",
      "Epoch:  304  Training Loss:  14.1404  Training Accuracy:  0.781949\n",
      "Epoch:  305  Training Loss:  14.0417  Training Accuracy:  0.78294\n",
      "Epoch:  306  Training Loss:  13.9437  Training Accuracy:  0.784106\n",
      "Epoch:  307  Training Loss:  13.8453  Training Accuracy:  0.785214\n",
      "Epoch:  308  Training Loss:  13.8101  Training Accuracy:  0.786322\n",
      "Epoch:  309  Training Loss:  13.6338  Training Accuracy:  0.786905\n",
      "Epoch:  310  Training Loss:  13.5859  Training Accuracy:  0.78743\n",
      "Epoch:  311  Training Loss:  13.4605  Training Accuracy:  0.788421\n",
      "Epoch:  312  Training Loss:  13.3681  Training Accuracy:  0.789062\n",
      "Epoch:  313  Training Loss:  13.3027  Training Accuracy:  0.790053\n",
      "Epoch:  314  Training Loss:  13.1878  Training Accuracy:  0.790403\n",
      "Epoch:  315  Training Loss:  13.0827  Training Accuracy:  0.790869\n",
      "Epoch:  316  Training Loss:  13.0234  Training Accuracy:  0.791686\n",
      "Epoch:  317  Training Loss:  12.9089  Training Accuracy:  0.792094\n",
      "Epoch:  318  Training Loss:  12.8519  Training Accuracy:  0.792852\n",
      "Epoch:  319  Training Loss:  12.737  Training Accuracy:  0.793784\n",
      "Epoch:  320  Training Loss:  12.6662  Training Accuracy:  0.794542\n",
      "Epoch:  321  Training Loss:  12.5526  Training Accuracy:  0.795534\n",
      "Epoch:  322  Training Loss:  12.5699  Training Accuracy:  0.796292\n",
      "Epoch:  323  Training Loss:  12.3717  Training Accuracy:  0.797049\n",
      "Epoch:  324  Training Loss:  12.2977  Training Accuracy:  0.797341\n",
      "Epoch:  325  Training Loss:  12.2353  Training Accuracy:  0.798157\n",
      "Epoch:  326  Training Loss:  12.1426  Training Accuracy:  0.798857\n",
      "Epoch:  327  Training Loss:  12.1154  Training Accuracy:  0.799673\n",
      "Epoch:  328  Training Loss:  11.9432  Training Accuracy:  0.800081\n",
      "Epoch:  329  Training Loss:  11.8992  Training Accuracy:  0.800606\n",
      "Epoch:  330  Training Loss:  11.7828  Training Accuracy:  0.801247\n",
      "Epoch:  331  Training Loss:  11.7838  Training Accuracy:  0.801889\n",
      "Epoch:  332  Training Loss:  11.618  Training Accuracy:  0.802588\n",
      "Epoch:  333  Training Loss:  11.5718  Training Accuracy:  0.803813\n",
      "Epoch:  334  Training Loss:  11.4698  Training Accuracy:  0.804104\n",
      "Epoch:  335  Training Loss:  11.4114  Training Accuracy:  0.805503\n",
      "Epoch:  336  Training Loss:  11.317  Training Accuracy:  0.805853\n",
      "Epoch:  337  Training Loss:  11.2684  Training Accuracy:  0.806669\n",
      "Epoch:  338  Training Loss:  11.1256  Training Accuracy:  0.807427\n",
      "Epoch:  339  Training Loss:  11.1587  Training Accuracy:  0.808302\n",
      "Epoch:  340  Training Loss:  10.9728  Training Accuracy:  0.808885\n",
      "Epoch:  341  Training Loss:  10.9067  Training Accuracy:  0.809701\n",
      "Epoch:  342  Training Loss:  10.8579  Training Accuracy:  0.810517\n",
      "Epoch:  343  Training Loss:  10.7596  Training Accuracy:  0.811275\n",
      "Epoch:  344  Training Loss:  10.6904  Training Accuracy:  0.812091\n",
      "Epoch:  345  Training Loss:  10.6493  Training Accuracy:  0.813024\n",
      "Epoch:  346  Training Loss:  10.5381  Training Accuracy:  0.813432\n",
      "Epoch:  347  Training Loss:  10.4673  Training Accuracy:  0.814132\n",
      "Epoch:  348  Training Loss:  10.4039  Training Accuracy:  0.814773\n",
      "Epoch:  349  Training Loss:  10.3241  Training Accuracy:  0.815298\n",
      "Epoch:  350  Training Loss:  10.2606  Training Accuracy:  0.815939\n",
      "Epoch:  351  Training Loss:  10.1977  Training Accuracy:  0.816872\n",
      "Epoch:  352  Training Loss:  10.1106  Training Accuracy:  0.817164\n",
      "Epoch:  353  Training Loss:  10.0758  Training Accuracy:  0.81833\n",
      "Epoch:  354  Training Loss:  9.97913  Training Accuracy:  0.818796\n",
      "Epoch:  355  Training Loss:  9.90613  Training Accuracy:  0.819554\n",
      "Epoch:  356  Training Loss:  9.84424  Training Accuracy:  0.820137\n",
      "Epoch:  357  Training Loss:  9.80837  Training Accuracy:  0.820953\n",
      "Epoch:  358  Training Loss:  9.70096  Training Accuracy:  0.821245\n",
      "Epoch:  359  Training Loss:  9.68604  Training Accuracy:  0.822353\n",
      "Epoch:  360  Training Loss:  9.55242  Training Accuracy:  0.822644\n",
      "Epoch:  361  Training Loss:  9.55236  Training Accuracy:  0.823577\n",
      "Epoch:  362  Training Loss:  9.43193  Training Accuracy:  0.824043\n",
      "Epoch:  363  Training Loss:  9.40602  Training Accuracy:  0.825093\n",
      "Epoch:  364  Training Loss:  9.30497  Training Accuracy:  0.826026\n",
      "Epoch:  365  Training Loss:  9.2975  Training Accuracy:  0.827133\n",
      "Epoch:  366  Training Loss:  9.17638  Training Accuracy:  0.827425\n",
      "Epoch:  367  Training Loss:  9.15571  Training Accuracy:  0.828066\n",
      "Epoch:  368  Training Loss:  9.06701  Training Accuracy:  0.829232\n",
      "Epoch:  369  Training Loss:  9.04805  Training Accuracy:  0.830282\n",
      "Epoch:  370  Training Loss:  8.93754  Training Accuracy:  0.830748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  8.91835  Training Accuracy:  0.831389\n",
      "Epoch:  372  Training Loss:  8.8213  Training Accuracy:  0.831739\n",
      "Epoch:  373  Training Loss:  8.84662  Training Accuracy:  0.832847\n",
      "Epoch:  374  Training Loss:  8.71482  Training Accuracy:  0.832964\n",
      "Epoch:  375  Training Loss:  8.65797  Training Accuracy:  0.834071\n",
      "Epoch:  376  Training Loss:  8.64192  Training Accuracy:  0.834654\n",
      "Epoch:  377  Training Loss:  8.56216  Training Accuracy:  0.835529\n",
      "Epoch:  378  Training Loss:  8.50612  Training Accuracy:  0.836112\n",
      "Epoch:  379  Training Loss:  8.50434  Training Accuracy:  0.837103\n",
      "Epoch:  380  Training Loss:  8.39182  Training Accuracy:  0.837628\n",
      "Epoch:  381  Training Loss:  8.39933  Training Accuracy:  0.838561\n",
      "Epoch:  382  Training Loss:  8.28692  Training Accuracy:  0.839202\n",
      "Epoch:  383  Training Loss:  8.29493  Training Accuracy:  0.840018\n",
      "Epoch:  384  Training Loss:  8.1932  Training Accuracy:  0.840601\n",
      "Epoch:  385  Training Loss:  8.19577  Training Accuracy:  0.841184\n",
      "Epoch:  386  Training Loss:  8.09351  Training Accuracy:  0.841709\n",
      "Epoch:  387  Training Loss:  8.09644  Training Accuracy:  0.842467\n",
      "Epoch:  388  Training Loss:  7.99983  Training Accuracy:  0.8427\n",
      "Epoch:  389  Training Loss:  8.02775  Training Accuracy:  0.843691\n",
      "Epoch:  390  Training Loss:  7.91504  Training Accuracy:  0.844041\n",
      "Epoch:  391  Training Loss:  7.87012  Training Accuracy:  0.844274\n",
      "Epoch:  392  Training Loss:  7.83985  Training Accuracy:  0.844741\n",
      "Epoch:  393  Training Loss:  7.80995  Training Accuracy:  0.845499\n",
      "Epoch:  394  Training Loss:  7.74507  Training Accuracy:  0.845732\n",
      "Epoch:  395  Training Loss:  7.70051  Training Accuracy:  0.846606\n",
      "Epoch:  396  Training Loss:  7.68507  Training Accuracy:  0.847423\n",
      "Epoch:  397  Training Loss:  7.61793  Training Accuracy:  0.847656\n",
      "Epoch:  398  Training Loss:  7.57545  Training Accuracy:  0.848239\n",
      "Epoch:  399  Training Loss:  7.54168  Training Accuracy:  0.848938\n",
      "Testing Accuracy: 0.777824\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 400\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  422.684  Training Accuracy:  0.0405201\n",
      "Epoch:  1  Training Loss:  335.716  Training Accuracy:  0.0416861\n",
      "Epoch:  2  Training Loss:  316.765  Training Accuracy:  0.048041\n",
      "Epoch:  3  Training Loss:  305.135  Training Accuracy:  0.0535215\n",
      "Epoch:  4  Training Loss:  295.663  Training Accuracy:  0.0592351\n",
      "Epoch:  5  Training Loss:  288.212  Training Accuracy:  0.0645406\n",
      "Epoch:  6  Training Loss:  282.65  Training Accuracy:  0.0685051\n",
      "Epoch:  7  Training Loss:  278.443  Training Accuracy:  0.0729361\n",
      "Epoch:  8  Training Loss:  274.774  Training Accuracy:  0.0781833\n",
      "Epoch:  9  Training Loss:  271.798  Training Accuracy:  0.083722\n",
      "Epoch:  10  Training Loss:  269.515  Training Accuracy:  0.0897854\n",
      "Epoch:  11  Training Loss:  266.724  Training Accuracy:  0.0957906\n",
      "Epoch:  12  Training Loss:  264.855  Training Accuracy:  0.101737\n",
      "Epoch:  13  Training Loss:  263.528  Training Accuracy:  0.108267\n",
      "Epoch:  14  Training Loss:  262.271  Training Accuracy:  0.11573\n",
      "Epoch:  15  Training Loss:  260.78  Training Accuracy:  0.122085\n",
      "Epoch:  16  Training Loss:  259.463  Training Accuracy:  0.127799\n",
      "Epoch:  17  Training Loss:  258.045  Training Accuracy:  0.135086\n",
      "Epoch:  18  Training Loss:  256.385  Training Accuracy:  0.141383\n",
      "Epoch:  19  Training Loss:  255.612  Training Accuracy:  0.147621\n",
      "Epoch:  20  Training Loss:  254.434  Training Accuracy:  0.155492\n",
      "Epoch:  21  Training Loss:  253.382  Training Accuracy:  0.162197\n",
      "Epoch:  22  Training Loss:  252.089  Training Accuracy:  0.168493\n",
      "Epoch:  23  Training Loss:  250.652  Training Accuracy:  0.17549\n",
      "Epoch:  24  Training Loss:  249.298  Training Accuracy:  0.183419\n",
      "Epoch:  25  Training Loss:  247.65  Training Accuracy:  0.19094\n",
      "Epoch:  26  Training Loss:  246.11  Training Accuracy:  0.197178\n",
      "Epoch:  27  Training Loss:  244.425  Training Accuracy:  0.203475\n",
      "Epoch:  28  Training Loss:  242.813  Training Accuracy:  0.20948\n",
      "Epoch:  29  Training Loss:  241.056  Training Accuracy:  0.216126\n",
      "Epoch:  30  Training Loss:  239.281  Training Accuracy:  0.221432\n",
      "Epoch:  31  Training Loss:  237.523  Training Accuracy:  0.228078\n",
      "Epoch:  32  Training Loss:  235.702  Training Accuracy:  0.233734\n",
      "Epoch:  33  Training Loss:  233.907  Training Accuracy:  0.240205\n",
      "Epoch:  34  Training Loss:  232.038  Training Accuracy:  0.245336\n",
      "Epoch:  35  Training Loss:  230.236  Training Accuracy:  0.250466\n",
      "Epoch:  36  Training Loss:  228.281  Training Accuracy:  0.255655\n",
      "Epoch:  37  Training Loss:  226.408  Training Accuracy:  0.260903\n",
      "Epoch:  38  Training Loss:  224.442  Training Accuracy:  0.265275\n",
      "Epoch:  39  Training Loss:  222.599  Training Accuracy:  0.269298\n",
      "Epoch:  40  Training Loss:  220.632  Training Accuracy:  0.27402\n",
      "Epoch:  41  Training Loss:  218.555  Training Accuracy:  0.27816\n",
      "Epoch:  42  Training Loss:  216.682  Training Accuracy:  0.281192\n",
      "Epoch:  43  Training Loss:  214.815  Training Accuracy:  0.285506\n",
      "Epoch:  44  Training Loss:  212.868  Training Accuracy:  0.289412\n",
      "Epoch:  45  Training Loss:  211.05  Training Accuracy:  0.293377\n",
      "Epoch:  46  Training Loss:  209.008  Training Accuracy:  0.298333\n",
      "Epoch:  47  Training Loss:  207.345  Training Accuracy:  0.301947\n",
      "Epoch:  48  Training Loss:  205.302  Training Accuracy:  0.305504\n",
      "Epoch:  49  Training Loss:  203.302  Training Accuracy:  0.308885\n",
      "Epoch:  50  Training Loss:  201.44  Training Accuracy:  0.310926\n",
      "Epoch:  51  Training Loss:  199.507  Training Accuracy:  0.315124\n",
      "Epoch:  52  Training Loss:  197.67  Training Accuracy:  0.318505\n",
      "Epoch:  53  Training Loss:  195.742  Training Accuracy:  0.321945\n",
      "Epoch:  54  Training Loss:  193.815  Training Accuracy:  0.325443\n",
      "Epoch:  55  Training Loss:  192.021  Training Accuracy:  0.328533\n",
      "Epoch:  56  Training Loss:  189.978  Training Accuracy:  0.331623\n",
      "Epoch:  57  Training Loss:  188.127  Training Accuracy:  0.334422\n",
      "Epoch:  58  Training Loss:  186.149  Training Accuracy:  0.33722\n",
      "Epoch:  59  Training Loss:  184.285  Training Accuracy:  0.33996\n",
      "Epoch:  60  Training Loss:  182.366  Training Accuracy:  0.343342\n",
      "Epoch:  61  Training Loss:  180.369  Training Accuracy:  0.34719\n",
      "Epoch:  62  Training Loss:  178.5  Training Accuracy:  0.350047\n",
      "Epoch:  63  Training Loss:  176.579  Training Accuracy:  0.352145\n",
      "Epoch:  64  Training Loss:  174.626  Training Accuracy:  0.355119\n",
      "Epoch:  65  Training Loss:  172.72  Training Accuracy:  0.358792\n",
      "Epoch:  66  Training Loss:  170.888  Training Accuracy:  0.361824\n",
      "Epoch:  67  Training Loss:  169.086  Training Accuracy:  0.364564\n",
      "Epoch:  68  Training Loss:  167.197  Training Accuracy:  0.368703\n",
      "Epoch:  69  Training Loss:  165.415  Training Accuracy:  0.371094\n",
      "Epoch:  70  Training Loss:  163.527  Training Accuracy:  0.373717\n",
      "Epoch:  71  Training Loss:  161.647  Training Accuracy:  0.376399\n",
      "Epoch:  72  Training Loss:  159.873  Training Accuracy:  0.380247\n",
      "Epoch:  73  Training Loss:  158.101  Training Accuracy:  0.383104\n",
      "Epoch:  74  Training Loss:  156.298  Training Accuracy:  0.386194\n",
      "Epoch:  75  Training Loss:  154.481  Training Accuracy:  0.389867\n",
      "Epoch:  76  Training Loss:  152.675  Training Accuracy:  0.392724\n",
      "Epoch:  77  Training Loss:  150.973  Training Accuracy:  0.394589\n",
      "Epoch:  78  Training Loss:  149.221  Training Accuracy:  0.397679\n",
      "Epoch:  79  Training Loss:  147.452  Training Accuracy:  0.400944\n",
      "Epoch:  80  Training Loss:  145.751  Training Accuracy:  0.404151\n",
      "Epoch:  81  Training Loss:  143.964  Training Accuracy:  0.406891\n",
      "Epoch:  82  Training Loss:  142.392  Training Accuracy:  0.409865\n",
      "Epoch:  83  Training Loss:  140.752  Training Accuracy:  0.412896\n",
      "Epoch:  84  Training Loss:  138.931  Training Accuracy:  0.415637\n",
      "Epoch:  85  Training Loss:  137.157  Training Accuracy:  0.417969\n",
      "Epoch:  86  Training Loss:  135.436  Training Accuracy:  0.421408\n",
      "Epoch:  87  Training Loss:  133.792  Training Accuracy:  0.424207\n",
      "Epoch:  88  Training Loss:  132.145  Training Accuracy:  0.427355\n",
      "Epoch:  89  Training Loss:  130.564  Training Accuracy:  0.429862\n",
      "Epoch:  90  Training Loss:  128.953  Training Accuracy:  0.432253\n",
      "Epoch:  91  Training Loss:  127.376  Training Accuracy:  0.435226\n",
      "Epoch:  92  Training Loss:  125.805  Training Accuracy:  0.438549\n",
      "Epoch:  93  Training Loss:  124.279  Training Accuracy:  0.442222\n",
      "Epoch:  94  Training Loss:  122.719  Training Accuracy:  0.444904\n",
      "Epoch:  95  Training Loss:  121.178  Training Accuracy:  0.447353\n",
      "Epoch:  96  Training Loss:  119.659  Training Accuracy:  0.450093\n",
      "Epoch:  97  Training Loss:  118.178  Training Accuracy:  0.452717\n",
      "Epoch:  98  Training Loss:  116.663  Training Accuracy:  0.455865\n",
      "Epoch:  99  Training Loss:  115.247  Training Accuracy:  0.458372\n",
      "Epoch:  100  Training Loss:  113.764  Training Accuracy:  0.460413\n",
      "Epoch:  101  Training Loss:  112.418  Training Accuracy:  0.462453\n",
      "Epoch:  102  Training Loss:  111.006  Training Accuracy:  0.464494\n",
      "Epoch:  103  Training Loss:  109.576  Training Accuracy:  0.466476\n",
      "Epoch:  104  Training Loss:  108.222  Training Accuracy:  0.467642\n",
      "Epoch:  105  Training Loss:  106.855  Training Accuracy:  0.469566\n",
      "Epoch:  106  Training Loss:  105.549  Training Accuracy:  0.471665\n",
      "Epoch:  107  Training Loss:  104.329  Training Accuracy:  0.47388\n",
      "Epoch:  108  Training Loss:  103.068  Training Accuracy:  0.476213\n",
      "Epoch:  109  Training Loss:  101.783  Training Accuracy:  0.479069\n",
      "Epoch:  110  Training Loss:  100.601  Training Accuracy:  0.481168\n",
      "Epoch:  111  Training Loss:  99.3219  Training Accuracy:  0.483151\n",
      "Epoch:  112  Training Loss:  98.2007  Training Accuracy:  0.484783\n",
      "Epoch:  113  Training Loss:  96.9439  Training Accuracy:  0.486241\n",
      "Epoch:  114  Training Loss:  95.8949  Training Accuracy:  0.488048\n",
      "Epoch:  115  Training Loss:  94.7647  Training Accuracy:  0.489972\n",
      "Epoch:  116  Training Loss:  93.6178  Training Accuracy:  0.492304\n",
      "Epoch:  117  Training Loss:  92.5213  Training Accuracy:  0.494403\n",
      "Epoch:  118  Training Loss:  91.3845  Training Accuracy:  0.49726\n",
      "Epoch:  119  Training Loss:  90.3264  Training Accuracy:  0.498775\n",
      "Epoch:  120  Training Loss:  89.2766  Training Accuracy:  0.501166\n",
      "Epoch:  121  Training Loss:  88.2591  Training Accuracy:  0.503206\n",
      "Epoch:  122  Training Loss:  87.2303  Training Accuracy:  0.50513\n",
      "Epoch:  123  Training Loss:  86.2536  Training Accuracy:  0.506763\n",
      "Epoch:  124  Training Loss:  85.2503  Training Accuracy:  0.507812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  84.3085  Training Accuracy:  0.509153\n",
      "Epoch:  126  Training Loss:  83.3965  Training Accuracy:  0.511194\n",
      "Epoch:  127  Training Loss:  82.4176  Training Accuracy:  0.514109\n",
      "Epoch:  128  Training Loss:  81.5466  Training Accuracy:  0.515858\n",
      "Epoch:  129  Training Loss:  80.6723  Training Accuracy:  0.517782\n",
      "Epoch:  130  Training Loss:  79.773  Training Accuracy:  0.519881\n",
      "Epoch:  131  Training Loss:  78.949  Training Accuracy:  0.521863\n",
      "Epoch:  132  Training Loss:  78.0403  Training Accuracy:  0.52437\n",
      "Epoch:  133  Training Loss:  77.2361  Training Accuracy:  0.526352\n",
      "Epoch:  134  Training Loss:  76.3925  Training Accuracy:  0.528626\n",
      "Epoch:  135  Training Loss:  75.6079  Training Accuracy:  0.530259\n",
      "Epoch:  136  Training Loss:  74.8124  Training Accuracy:  0.531949\n",
      "Epoch:  137  Training Loss:  74.1214  Training Accuracy:  0.533932\n",
      "Epoch:  138  Training Loss:  73.3326  Training Accuracy:  0.535273\n",
      "Epoch:  139  Training Loss:  72.5563  Training Accuracy:  0.537255\n",
      "Epoch:  140  Training Loss:  71.8344  Training Accuracy:  0.539295\n",
      "Epoch:  141  Training Loss:  71.1262  Training Accuracy:  0.541453\n",
      "Epoch:  142  Training Loss:  70.4048  Training Accuracy:  0.542969\n",
      "Epoch:  143  Training Loss:  69.673  Training Accuracy:  0.544776\n",
      "Epoch:  144  Training Loss:  68.9658  Training Accuracy:  0.546175\n",
      "Epoch:  145  Training Loss:  68.2722  Training Accuracy:  0.547983\n",
      "Epoch:  146  Training Loss:  67.6569  Training Accuracy:  0.549848\n",
      "Epoch:  147  Training Loss:  67.008  Training Accuracy:  0.552297\n",
      "Epoch:  148  Training Loss:  66.3556  Training Accuracy:  0.554746\n",
      "Epoch:  149  Training Loss:  65.7203  Training Accuracy:  0.556786\n",
      "Epoch:  150  Training Loss:  65.1158  Training Accuracy:  0.55836\n",
      "Epoch:  151  Training Loss:  64.4981  Training Accuracy:  0.560168\n",
      "Epoch:  152  Training Loss:  63.9513  Training Accuracy:  0.562033\n",
      "Epoch:  153  Training Loss:  63.3912  Training Accuracy:  0.563899\n",
      "Epoch:  154  Training Loss:  62.763  Training Accuracy:  0.565415\n",
      "Epoch:  155  Training Loss:  62.2503  Training Accuracy:  0.567281\n",
      "Epoch:  156  Training Loss:  61.6737  Training Accuracy:  0.568038\n",
      "Epoch:  157  Training Loss:  61.1129  Training Accuracy:  0.569554\n",
      "Epoch:  158  Training Loss:  60.5401  Training Accuracy:  0.570837\n",
      "Epoch:  159  Training Loss:  60.0102  Training Accuracy:  0.572178\n",
      "Epoch:  160  Training Loss:  59.5115  Training Accuracy:  0.573519\n",
      "Epoch:  161  Training Loss:  58.9856  Training Accuracy:  0.575326\n",
      "Epoch:  162  Training Loss:  58.5004  Training Accuracy:  0.576959\n",
      "Epoch:  163  Training Loss:  57.9861  Training Accuracy:  0.577892\n",
      "Epoch:  164  Training Loss:  57.5031  Training Accuracy:  0.579816\n",
      "Epoch:  165  Training Loss:  56.9961  Training Accuracy:  0.581098\n",
      "Epoch:  166  Training Loss:  56.5298  Training Accuracy:  0.582381\n",
      "Epoch:  167  Training Loss:  56.0909  Training Accuracy:  0.583955\n",
      "Epoch:  168  Training Loss:  55.5628  Training Accuracy:  0.585471\n",
      "Epoch:  169  Training Loss:  55.1103  Training Accuracy:  0.586404\n",
      "Epoch:  170  Training Loss:  54.7101  Training Accuracy:  0.588269\n",
      "Epoch:  171  Training Loss:  54.2156  Training Accuracy:  0.590018\n",
      "Epoch:  172  Training Loss:  53.8043  Training Accuracy:  0.591184\n",
      "Epoch:  173  Training Loss:  53.3715  Training Accuracy:  0.592817\n",
      "Epoch:  174  Training Loss:  52.9241  Training Accuracy:  0.593925\n",
      "Epoch:  175  Training Loss:  52.5016  Training Accuracy:  0.595382\n",
      "Epoch:  176  Training Loss:  52.0997  Training Accuracy:  0.596781\n",
      "Epoch:  177  Training Loss:  51.6958  Training Accuracy:  0.597889\n",
      "Epoch:  178  Training Loss:  51.2564  Training Accuracy:  0.598764\n",
      "Epoch:  179  Training Loss:  50.9093  Training Accuracy:  0.599988\n",
      "Epoch:  180  Training Loss:  50.462  Training Accuracy:  0.600863\n",
      "Epoch:  181  Training Loss:  50.0944  Training Accuracy:  0.60197\n",
      "Epoch:  182  Training Loss:  49.7159  Training Accuracy:  0.60337\n",
      "Epoch:  183  Training Loss:  49.3101  Training Accuracy:  0.604827\n",
      "Epoch:  184  Training Loss:  48.9544  Training Accuracy:  0.606226\n",
      "Epoch:  185  Training Loss:  48.5414  Training Accuracy:  0.607859\n",
      "Epoch:  186  Training Loss:  48.1779  Training Accuracy:  0.60885\n",
      "Epoch:  187  Training Loss:  47.798  Training Accuracy:  0.610016\n",
      "Epoch:  188  Training Loss:  47.4632  Training Accuracy:  0.61159\n",
      "Epoch:  189  Training Loss:  47.0675  Training Accuracy:  0.613106\n",
      "Epoch:  190  Training Loss:  46.7289  Training Accuracy:  0.614855\n",
      "Epoch:  191  Training Loss:  46.3142  Training Accuracy:  0.615963\n",
      "Epoch:  192  Training Loss:  45.9779  Training Accuracy:  0.617362\n",
      "Epoch:  193  Training Loss:  45.6237  Training Accuracy:  0.618761\n",
      "Epoch:  194  Training Loss:  45.2755  Training Accuracy:  0.619927\n",
      "Epoch:  195  Training Loss:  44.969  Training Accuracy:  0.621385\n",
      "Epoch:  196  Training Loss:  44.6104  Training Accuracy:  0.622726\n",
      "Epoch:  197  Training Loss:  44.2675  Training Accuracy:  0.623426\n",
      "Epoch:  198  Training Loss:  43.9442  Training Accuracy:  0.624825\n",
      "Epoch:  199  Training Loss:  43.6277  Training Accuracy:  0.626166\n",
      "Epoch:  200  Training Loss:  43.2906  Training Accuracy:  0.627448\n",
      "Epoch:  201  Training Loss:  42.9943  Training Accuracy:  0.628906\n",
      "Epoch:  202  Training Loss:  42.6798  Training Accuracy:  0.630189\n",
      "Epoch:  203  Training Loss:  42.3771  Training Accuracy:  0.631588\n",
      "Epoch:  204  Training Loss:  42.0633  Training Accuracy:  0.632579\n",
      "Epoch:  205  Training Loss:  41.7842  Training Accuracy:  0.634445\n",
      "Epoch:  206  Training Loss:  41.4887  Training Accuracy:  0.636485\n",
      "Epoch:  207  Training Loss:  41.2076  Training Accuracy:  0.637826\n",
      "Epoch:  208  Training Loss:  40.8942  Training Accuracy:  0.639459\n",
      "Epoch:  209  Training Loss:  40.6255  Training Accuracy:  0.641149\n",
      "Epoch:  210  Training Loss:  40.3281  Training Accuracy:  0.64249\n",
      "Epoch:  211  Training Loss:  40.0461  Training Accuracy:  0.643948\n",
      "Epoch:  212  Training Loss:  39.7275  Training Accuracy:  0.645755\n",
      "Epoch:  213  Training Loss:  39.4735  Training Accuracy:  0.647504\n",
      "Epoch:  214  Training Loss:  39.1513  Training Accuracy:  0.64902\n",
      "Epoch:  215  Training Loss:  38.8975  Training Accuracy:  0.650828\n",
      "Epoch:  216  Training Loss:  38.574  Training Accuracy:  0.652285\n",
      "Epoch:  217  Training Loss:  38.2972  Training Accuracy:  0.653743\n",
      "Epoch:  218  Training Loss:  38.0354  Training Accuracy:  0.655608\n",
      "Epoch:  219  Training Loss:  37.7484  Training Accuracy:  0.657591\n",
      "Epoch:  220  Training Loss:  37.4875  Training Accuracy:  0.659281\n",
      "Epoch:  221  Training Loss:  37.2457  Training Accuracy:  0.660856\n",
      "Epoch:  222  Training Loss:  36.9747  Training Accuracy:  0.662663\n",
      "Epoch:  223  Training Loss:  36.7173  Training Accuracy:  0.664295\n",
      "Epoch:  224  Training Loss:  36.418  Training Accuracy:  0.665928\n",
      "Epoch:  225  Training Loss:  36.1827  Training Accuracy:  0.667444\n",
      "Epoch:  226  Training Loss:  35.9034  Training Accuracy:  0.66861\n",
      "Epoch:  227  Training Loss:  35.6723  Training Accuracy:  0.669659\n",
      "Epoch:  228  Training Loss:  35.4206  Training Accuracy:  0.67065\n",
      "Epoch:  229  Training Loss:  35.198  Training Accuracy:  0.671816\n",
      "Epoch:  230  Training Loss:  34.9359  Training Accuracy:  0.673216\n",
      "Epoch:  231  Training Loss:  34.7011  Training Accuracy:  0.674673\n",
      "Epoch:  232  Training Loss:  34.4468  Training Accuracy:  0.676131\n",
      "Epoch:  233  Training Loss:  34.2136  Training Accuracy:  0.678113\n",
      "Epoch:  234  Training Loss:  33.9957  Training Accuracy:  0.679512\n",
      "Epoch:  235  Training Loss:  33.7491  Training Accuracy:  0.680503\n",
      "Epoch:  236  Training Loss:  33.5108  Training Accuracy:  0.681553\n",
      "Epoch:  237  Training Loss:  33.2655  Training Accuracy:  0.682661\n",
      "Epoch:  238  Training Loss:  33.0218  Training Accuracy:  0.683477\n",
      "Epoch:  239  Training Loss:  32.7899  Training Accuracy:  0.684585\n",
      "Epoch:  240  Training Loss:  32.5669  Training Accuracy:  0.685634\n",
      "Epoch:  241  Training Loss:  32.3397  Training Accuracy:  0.686392\n",
      "Epoch:  242  Training Loss:  32.1197  Training Accuracy:  0.687441\n",
      "Epoch:  243  Training Loss:  31.8941  Training Accuracy:  0.688841\n",
      "Epoch:  244  Training Loss:  31.6707  Training Accuracy:  0.690007\n",
      "Epoch:  245  Training Loss:  31.4392  Training Accuracy:  0.690531\n",
      "Epoch:  246  Training Loss:  31.2406  Training Accuracy:  0.691464\n",
      "Epoch:  247  Training Loss:  30.991  Training Accuracy:  0.692514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  30.7935  Training Accuracy:  0.693447\n",
      "Epoch:  249  Training Loss:  30.5471  Training Accuracy:  0.694263\n",
      "Epoch:  250  Training Loss:  30.3547  Training Accuracy:  0.69537\n",
      "Epoch:  251  Training Loss:  30.1175  Training Accuracy:  0.696945\n",
      "Epoch:  252  Training Loss:  29.9223  Training Accuracy:  0.698344\n",
      "Epoch:  253  Training Loss:  29.7062  Training Accuracy:  0.700326\n",
      "Epoch:  254  Training Loss:  29.4977  Training Accuracy:  0.701317\n",
      "Epoch:  255  Training Loss:  29.2865  Training Accuracy:  0.702367\n",
      "Epoch:  256  Training Loss:  29.09  Training Accuracy:  0.703708\n",
      "Epoch:  257  Training Loss:  28.8891  Training Accuracy:  0.704407\n",
      "Epoch:  258  Training Loss:  28.6926  Training Accuracy:  0.705748\n",
      "Epoch:  259  Training Loss:  28.484  Training Accuracy:  0.707089\n",
      "Epoch:  260  Training Loss:  28.2774  Training Accuracy:  0.70843\n",
      "Epoch:  261  Training Loss:  28.0927  Training Accuracy:  0.709829\n",
      "Epoch:  262  Training Loss:  27.8879  Training Accuracy:  0.711287\n",
      "Epoch:  263  Training Loss:  27.6819  Training Accuracy:  0.71257\n",
      "Epoch:  264  Training Loss:  27.4995  Training Accuracy:  0.713561\n",
      "Epoch:  265  Training Loss:  27.2968  Training Accuracy:  0.715018\n",
      "Epoch:  266  Training Loss:  27.1028  Training Accuracy:  0.716709\n",
      "Epoch:  267  Training Loss:  26.9332  Training Accuracy:  0.718225\n",
      "Epoch:  268  Training Loss:  26.7303  Training Accuracy:  0.719216\n",
      "Epoch:  269  Training Loss:  26.5426  Training Accuracy:  0.720266\n",
      "Epoch:  270  Training Loss:  26.3763  Training Accuracy:  0.721898\n",
      "Epoch:  271  Training Loss:  26.1691  Training Accuracy:  0.722656\n",
      "Epoch:  272  Training Loss:  25.9848  Training Accuracy:  0.723764\n",
      "Epoch:  273  Training Loss:  25.8016  Training Accuracy:  0.725454\n",
      "Epoch:  274  Training Loss:  25.634  Training Accuracy:  0.726795\n",
      "Epoch:  275  Training Loss:  25.4621  Training Accuracy:  0.727845\n",
      "Epoch:  276  Training Loss:  25.2561  Training Accuracy:  0.728778\n",
      "Epoch:  277  Training Loss:  25.0871  Training Accuracy:  0.73041\n",
      "Epoch:  278  Training Loss:  24.8988  Training Accuracy:  0.731343\n",
      "Epoch:  279  Training Loss:  24.7353  Training Accuracy:  0.732626\n",
      "Epoch:  280  Training Loss:  24.5421  Training Accuracy:  0.733792\n",
      "Epoch:  281  Training Loss:  24.383  Training Accuracy:  0.73455\n",
      "Epoch:  282  Training Loss:  24.1947  Training Accuracy:  0.735307\n",
      "Epoch:  283  Training Loss:  24.0227  Training Accuracy:  0.73624\n",
      "Epoch:  284  Training Loss:  23.8504  Training Accuracy:  0.737173\n",
      "Epoch:  285  Training Loss:  23.6828  Training Accuracy:  0.738397\n",
      "Epoch:  286  Training Loss:  23.5035  Training Accuracy:  0.739564\n",
      "Epoch:  287  Training Loss:  23.3442  Training Accuracy:  0.74073\n",
      "Epoch:  288  Training Loss:  23.1701  Training Accuracy:  0.741954\n",
      "Epoch:  289  Training Loss:  23.0209  Training Accuracy:  0.743295\n",
      "Epoch:  290  Training Loss:  22.8839  Training Accuracy:  0.744344\n",
      "Epoch:  291  Training Loss:  22.6947  Training Accuracy:  0.746152\n",
      "Epoch:  292  Training Loss:  22.5653  Training Accuracy:  0.746968\n",
      "Epoch:  293  Training Loss:  22.3859  Training Accuracy:  0.748076\n",
      "Epoch:  294  Training Loss:  22.2487  Training Accuracy:  0.749417\n",
      "Epoch:  295  Training Loss:  22.0789  Training Accuracy:  0.750583\n",
      "Epoch:  296  Training Loss:  21.9153  Training Accuracy:  0.75204\n",
      "Epoch:  297  Training Loss:  21.7443  Training Accuracy:  0.753673\n",
      "Epoch:  298  Training Loss:  21.6076  Training Accuracy:  0.754372\n",
      "Epoch:  299  Training Loss:  21.4435  Training Accuracy:  0.75548\n",
      "Epoch:  300  Training Loss:  21.2803  Training Accuracy:  0.756588\n",
      "Epoch:  301  Training Loss:  21.1195  Training Accuracy:  0.757637\n",
      "Epoch:  302  Training Loss:  20.9814  Training Accuracy:  0.759153\n",
      "Epoch:  303  Training Loss:  20.8206  Training Accuracy:  0.760203\n",
      "Epoch:  304  Training Loss:  20.6635  Training Accuracy:  0.76166\n",
      "Epoch:  305  Training Loss:  20.5293  Training Accuracy:  0.762535\n",
      "Epoch:  306  Training Loss:  20.3601  Training Accuracy:  0.763817\n",
      "Epoch:  307  Training Loss:  20.2317  Training Accuracy:  0.76545\n",
      "Epoch:  308  Training Loss:  20.0751  Training Accuracy:  0.766499\n",
      "Epoch:  309  Training Loss:  19.9372  Training Accuracy:  0.767549\n",
      "Epoch:  310  Training Loss:  19.7841  Training Accuracy:  0.768132\n",
      "Epoch:  311  Training Loss:  19.6661  Training Accuracy:  0.769181\n",
      "Epoch:  312  Training Loss:  19.5103  Training Accuracy:  0.770114\n",
      "Epoch:  313  Training Loss:  19.3814  Training Accuracy:  0.771047\n",
      "Epoch:  314  Training Loss:  19.2266  Training Accuracy:  0.772038\n",
      "Epoch:  315  Training Loss:  19.1035  Training Accuracy:  0.772854\n",
      "Epoch:  316  Training Loss:  18.9447  Training Accuracy:  0.77332\n",
      "Epoch:  317  Training Loss:  18.8211  Training Accuracy:  0.774778\n",
      "Epoch:  318  Training Loss:  18.6736  Training Accuracy:  0.775594\n",
      "Epoch:  319  Training Loss:  18.5535  Training Accuracy:  0.776644\n",
      "Epoch:  320  Training Loss:  18.412  Training Accuracy:  0.777402\n",
      "Epoch:  321  Training Loss:  18.2849  Training Accuracy:  0.778393\n",
      "Epoch:  322  Training Loss:  18.1351  Training Accuracy:  0.779501\n",
      "Epoch:  323  Training Loss:  18.0147  Training Accuracy:  0.78055\n",
      "Epoch:  324  Training Loss:  17.8883  Training Accuracy:  0.781599\n",
      "Epoch:  325  Training Loss:  17.7546  Training Accuracy:  0.782182\n",
      "Epoch:  326  Training Loss:  17.6171  Training Accuracy:  0.78294\n",
      "Epoch:  327  Training Loss:  17.4856  Training Accuracy:  0.783815\n",
      "Epoch:  328  Training Loss:  17.3609  Training Accuracy:  0.785272\n",
      "Epoch:  329  Training Loss:  17.2522  Training Accuracy:  0.786322\n",
      "Epoch:  330  Training Loss:  17.1056  Training Accuracy:  0.787779\n",
      "Epoch:  331  Training Loss:  17.0028  Training Accuracy:  0.78877\n",
      "Epoch:  332  Training Loss:  16.8804  Training Accuracy:  0.789995\n",
      "Epoch:  333  Training Loss:  16.7669  Training Accuracy:  0.791219\n",
      "Epoch:  334  Training Loss:  16.6276  Training Accuracy:  0.791744\n",
      "Epoch:  335  Training Loss:  16.5095  Training Accuracy:  0.792619\n",
      "Epoch:  336  Training Loss:  16.3954  Training Accuracy:  0.793901\n",
      "Epoch:  337  Training Loss:  16.2771  Training Accuracy:  0.794251\n",
      "Epoch:  338  Training Loss:  16.1549  Training Accuracy:  0.795242\n",
      "Epoch:  339  Training Loss:  16.0165  Training Accuracy:  0.796\n",
      "Epoch:  340  Training Loss:  15.9362  Training Accuracy:  0.796816\n",
      "Epoch:  341  Training Loss:  15.8136  Training Accuracy:  0.797924\n",
      "Epoch:  342  Training Loss:  15.7134  Training Accuracy:  0.798857\n",
      "Epoch:  343  Training Loss:  15.5808  Training Accuracy:  0.799556\n",
      "Epoch:  344  Training Loss:  15.503  Training Accuracy:  0.800606\n",
      "Epoch:  345  Training Loss:  15.3696  Training Accuracy:  0.801306\n",
      "Epoch:  346  Training Loss:  15.272  Training Accuracy:  0.80253\n",
      "Epoch:  347  Training Loss:  15.1584  Training Accuracy:  0.803404\n",
      "Epoch:  348  Training Loss:  15.0671  Training Accuracy:  0.803929\n",
      "Epoch:  349  Training Loss:  14.9392  Training Accuracy:  0.804512\n",
      "Epoch:  350  Training Loss:  14.8444  Training Accuracy:  0.805212\n",
      "Epoch:  351  Training Loss:  14.7403  Training Accuracy:  0.80632\n",
      "Epoch:  352  Training Loss:  14.6284  Training Accuracy:  0.807019\n",
      "Epoch:  353  Training Loss:  14.5292  Training Accuracy:  0.807602\n",
      "Epoch:  354  Training Loss:  14.4386  Training Accuracy:  0.808185\n",
      "Epoch:  355  Training Loss:  14.3169  Training Accuracy:  0.808768\n",
      "Epoch:  356  Training Loss:  14.226  Training Accuracy:  0.809468\n",
      "Epoch:  357  Training Loss:  14.1231  Training Accuracy:  0.809992\n",
      "Epoch:  358  Training Loss:  14.0132  Training Accuracy:  0.810984\n",
      "Epoch:  359  Training Loss:  13.9205  Training Accuracy:  0.811508\n",
      "Epoch:  360  Training Loss:  13.8177  Training Accuracy:  0.812208\n",
      "Epoch:  361  Training Loss:  13.7101  Training Accuracy:  0.812966\n",
      "Epoch:  362  Training Loss:  13.6167  Training Accuracy:  0.813432\n",
      "Epoch:  363  Training Loss:  13.5371  Training Accuracy:  0.814249\n",
      "Epoch:  364  Training Loss:  13.4236  Training Accuracy:  0.814715\n",
      "Epoch:  365  Training Loss:  13.329  Training Accuracy:  0.815415\n",
      "Epoch:  366  Training Loss:  13.2445  Training Accuracy:  0.815998\n",
      "Epoch:  367  Training Loss:  13.1528  Training Accuracy:  0.816581\n",
      "Epoch:  368  Training Loss:  13.0505  Training Accuracy:  0.817455\n",
      "Epoch:  369  Training Loss:  12.9647  Training Accuracy:  0.817747\n",
      "Epoch:  370  Training Loss:  12.8782  Training Accuracy:  0.818505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  12.7881  Training Accuracy:  0.819204\n",
      "Epoch:  372  Training Loss:  12.7032  Training Accuracy:  0.820021\n",
      "Epoch:  373  Training Loss:  12.609  Training Accuracy:  0.82037\n",
      "Epoch:  374  Training Loss:  12.5421  Training Accuracy:  0.820895\n",
      "Epoch:  375  Training Loss:  12.4415  Training Accuracy:  0.821595\n",
      "Epoch:  376  Training Loss:  12.3595  Training Accuracy:  0.822003\n",
      "Epoch:  377  Training Loss:  12.2801  Training Accuracy:  0.822819\n",
      "Epoch:  378  Training Loss:  12.1937  Training Accuracy:  0.823285\n",
      "Epoch:  379  Training Loss:  12.1091  Training Accuracy:  0.82416\n",
      "Epoch:  380  Training Loss:  12.021  Training Accuracy:  0.824568\n",
      "Epoch:  381  Training Loss:  11.9402  Training Accuracy:  0.825209\n",
      "Epoch:  382  Training Loss:  11.8576  Training Accuracy:  0.825618\n",
      "Epoch:  383  Training Loss:  11.7699  Training Accuracy:  0.826434\n",
      "Epoch:  384  Training Loss:  11.6955  Training Accuracy:  0.827192\n",
      "Epoch:  385  Training Loss:  11.6082  Training Accuracy:  0.827833\n",
      "Epoch:  386  Training Loss:  11.5302  Training Accuracy:  0.828241\n",
      "Epoch:  387  Training Loss:  11.4532  Training Accuracy:  0.829116\n",
      "Epoch:  388  Training Loss:  11.3759  Training Accuracy:  0.829466\n",
      "Epoch:  389  Training Loss:  11.2953  Training Accuracy:  0.830107\n",
      "Epoch:  390  Training Loss:  11.2222  Training Accuracy:  0.830515\n",
      "Epoch:  391  Training Loss:  11.1489  Training Accuracy:  0.831098\n",
      "Epoch:  392  Training Loss:  11.065  Training Accuracy:  0.831681\n",
      "Epoch:  393  Training Loss:  11.0025  Training Accuracy:  0.832789\n",
      "Epoch:  394  Training Loss:  10.91  Training Accuracy:  0.833372\n",
      "Epoch:  395  Training Loss:  10.8493  Training Accuracy:  0.83378\n",
      "Epoch:  396  Training Loss:  10.7667  Training Accuracy:  0.834538\n",
      "Epoch:  397  Training Loss:  10.7026  Training Accuracy:  0.835296\n",
      "Epoch:  398  Training Loss:  10.6353  Training Accuracy:  0.83582\n",
      "Epoch:  399  Training Loss:  10.5525  Training Accuracy:  0.836928\n",
      "Epoch:  400  Training Loss:  10.4972  Training Accuracy:  0.837453\n",
      "Epoch:  401  Training Loss:  10.4058  Training Accuracy:  0.838094\n",
      "Epoch:  402  Training Loss:  10.3661  Training Accuracy:  0.838852\n",
      "Epoch:  403  Training Loss:  10.2854  Training Accuracy:  0.839319\n",
      "Epoch:  404  Training Loss:  10.2145  Training Accuracy:  0.839727\n",
      "Epoch:  405  Training Loss:  10.1546  Training Accuracy:  0.839785\n",
      "Epoch:  406  Training Loss:  10.0917  Training Accuracy:  0.840485\n",
      "Epoch:  407  Training Loss:  10.0231  Training Accuracy:  0.840951\n",
      "Epoch:  408  Training Loss:  9.97367  Training Accuracy:  0.841592\n",
      "Epoch:  409  Training Loss:  9.89501  Training Accuracy:  0.842234\n",
      "Epoch:  410  Training Loss:  9.82917  Training Accuracy:  0.842875\n",
      "Epoch:  411  Training Loss:  9.7778  Training Accuracy:  0.8434\n",
      "Epoch:  412  Training Loss:  9.71474  Training Accuracy:  0.843866\n",
      "Epoch:  413  Training Loss:  9.63762  Training Accuracy:  0.844333\n",
      "Epoch:  414  Training Loss:  9.59427  Training Accuracy:  0.844799\n",
      "Epoch:  415  Training Loss:  9.52822  Training Accuracy:  0.845265\n",
      "Epoch:  416  Training Loss:  9.46761  Training Accuracy:  0.845907\n",
      "Epoch:  417  Training Loss:  9.41916  Training Accuracy:  0.846431\n",
      "Epoch:  418  Training Loss:  9.34455  Training Accuracy:  0.846781\n",
      "Epoch:  419  Training Loss:  9.29438  Training Accuracy:  0.847539\n",
      "Epoch:  420  Training Loss:  9.23802  Training Accuracy:  0.848239\n",
      "Epoch:  421  Training Loss:  9.16724  Training Accuracy:  0.848938\n",
      "Epoch:  422  Training Loss:  9.11822  Training Accuracy:  0.849521\n",
      "Epoch:  423  Training Loss:  9.06427  Training Accuracy:  0.849988\n",
      "Epoch:  424  Training Loss:  8.99305  Training Accuracy:  0.850338\n",
      "Epoch:  425  Training Loss:  8.94733  Training Accuracy:  0.851154\n",
      "Epoch:  426  Training Loss:  8.89463  Training Accuracy:  0.85162\n",
      "Epoch:  427  Training Loss:  8.83173  Training Accuracy:  0.852203\n",
      "Epoch:  428  Training Loss:  8.79115  Training Accuracy:  0.852961\n",
      "Epoch:  429  Training Loss:  8.73511  Training Accuracy:  0.853661\n",
      "Epoch:  430  Training Loss:  8.67387  Training Accuracy:  0.854186\n",
      "Epoch:  431  Training Loss:  8.64088  Training Accuracy:  0.854885\n",
      "Epoch:  432  Training Loss:  8.58037  Training Accuracy:  0.85541\n",
      "Epoch:  433  Training Loss:  8.51915  Training Accuracy:  0.85611\n",
      "Epoch:  434  Training Loss:  8.46682  Training Accuracy:  0.856401\n",
      "Epoch:  435  Training Loss:  8.43655  Training Accuracy:  0.856926\n",
      "Epoch:  436  Training Loss:  8.36715  Training Accuracy:  0.857159\n",
      "Epoch:  437  Training Loss:  8.32157  Training Accuracy:  0.857509\n",
      "Epoch:  438  Training Loss:  8.26606  Training Accuracy:  0.8578\n",
      "Epoch:  439  Training Loss:  8.21789  Training Accuracy:  0.858675\n",
      "Epoch:  440  Training Loss:  8.15671  Training Accuracy:  0.858966\n",
      "Epoch:  441  Training Loss:  8.11028  Training Accuracy:  0.859433\n",
      "Epoch:  442  Training Loss:  8.06881  Training Accuracy:  0.859841\n",
      "Epoch:  443  Training Loss:  8.02339  Training Accuracy:  0.860482\n",
      "Epoch:  444  Training Loss:  7.97492  Training Accuracy:  0.861065\n",
      "Epoch:  445  Training Loss:  7.91759  Training Accuracy:  0.861299\n",
      "Epoch:  446  Training Loss:  7.86996  Training Accuracy:  0.861532\n",
      "Epoch:  447  Training Loss:  7.84383  Training Accuracy:  0.86229\n",
      "Epoch:  448  Training Loss:  7.78233  Training Accuracy:  0.862756\n",
      "Epoch:  449  Training Loss:  7.7572  Training Accuracy:  0.863397\n",
      "Epoch:  450  Training Loss:  7.70315  Training Accuracy:  0.863572\n",
      "Epoch:  451  Training Loss:  7.65906  Training Accuracy:  0.863864\n",
      "Epoch:  452  Training Loss:  7.61088  Training Accuracy:  0.86433\n",
      "Epoch:  453  Training Loss:  7.57307  Training Accuracy:  0.864738\n",
      "Epoch:  454  Training Loss:  7.52756  Training Accuracy:  0.865263\n",
      "Epoch:  455  Training Loss:  7.4944  Training Accuracy:  0.865613\n",
      "Epoch:  456  Training Loss:  7.44809  Training Accuracy:  0.865788\n",
      "Epoch:  457  Training Loss:  7.40378  Training Accuracy:  0.866429\n",
      "Epoch:  458  Training Loss:  7.38326  Training Accuracy:  0.86707\n",
      "Epoch:  459  Training Loss:  7.32611  Training Accuracy:  0.867479\n",
      "Epoch:  460  Training Loss:  7.3167  Training Accuracy:  0.868062\n",
      "Epoch:  461  Training Loss:  7.27753  Training Accuracy:  0.868645\n",
      "Epoch:  462  Training Loss:  7.23827  Training Accuracy:  0.869111\n",
      "Epoch:  463  Training Loss:  7.20014  Training Accuracy:  0.869461\n",
      "Epoch:  464  Training Loss:  7.16904  Training Accuracy:  0.870102\n",
      "Epoch:  465  Training Loss:  7.12541  Training Accuracy:  0.870627\n",
      "Epoch:  466  Training Loss:  7.09351  Training Accuracy:  0.871385\n",
      "Epoch:  467  Training Loss:  7.05377  Training Accuracy:  0.87156\n",
      "Epoch:  468  Training Loss:  7.01773  Training Accuracy:  0.872259\n",
      "Epoch:  469  Training Loss:  6.97397  Training Accuracy:  0.872551\n",
      "Epoch:  470  Training Loss:  6.93333  Training Accuracy:  0.873075\n",
      "Epoch:  471  Training Loss:  6.90204  Training Accuracy:  0.873134\n",
      "Epoch:  472  Training Loss:  6.86709  Training Accuracy:  0.874125\n",
      "Epoch:  473  Training Loss:  6.82968  Training Accuracy:  0.874708\n",
      "Epoch:  474  Training Loss:  6.7952  Training Accuracy:  0.875058\n",
      "Epoch:  475  Training Loss:  6.75156  Training Accuracy:  0.875583\n",
      "Epoch:  476  Training Loss:  6.72136  Training Accuracy:  0.876457\n",
      "Epoch:  477  Training Loss:  6.68733  Training Accuracy:  0.876982\n",
      "Epoch:  478  Training Loss:  6.65145  Training Accuracy:  0.877623\n",
      "Epoch:  479  Training Loss:  6.60779  Training Accuracy:  0.878148\n",
      "Epoch:  480  Training Loss:  6.57277  Training Accuracy:  0.878672\n",
      "Epoch:  481  Training Loss:  6.545  Training Accuracy:  0.879139\n",
      "Epoch:  482  Training Loss:  6.51403  Training Accuracy:  0.879664\n",
      "Epoch:  483  Training Loss:  6.4735  Training Accuracy:  0.880072\n",
      "Epoch:  484  Training Loss:  6.44412  Training Accuracy:  0.881005\n",
      "Epoch:  485  Training Loss:  6.40964  Training Accuracy:  0.881588\n",
      "Epoch:  486  Training Loss:  6.37792  Training Accuracy:  0.882054\n",
      "Epoch:  487  Training Loss:  6.34817  Training Accuracy:  0.882404\n",
      "Epoch:  488  Training Loss:  6.31506  Training Accuracy:  0.883045\n",
      "Epoch:  489  Training Loss:  6.28603  Training Accuracy:  0.883453\n",
      "Epoch:  490  Training Loss:  6.25902  Training Accuracy:  0.883628\n",
      "Epoch:  491  Training Loss:  6.22265  Training Accuracy:  0.883978\n",
      "Epoch:  492  Training Loss:  6.19149  Training Accuracy:  0.884386\n",
      "Epoch:  493  Training Loss:  6.16443  Training Accuracy:  0.884794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  6.1348  Training Accuracy:  0.885319\n",
      "Epoch:  495  Training Loss:  6.1057  Training Accuracy:  0.885436\n",
      "Epoch:  496  Training Loss:  6.07767  Training Accuracy:  0.885785\n",
      "Epoch:  497  Training Loss:  6.04575  Training Accuracy:  0.886019\n",
      "Epoch:  498  Training Loss:  6.02342  Training Accuracy:  0.886427\n",
      "Epoch:  499  Training Loss:  5.99372  Training Accuracy:  0.886602\n",
      "Testing Accuracy: 0.804303\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64 #(128)\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  1060.45  Training Accuracy:  0.0394706\n",
      "Epoch:  1  Training Loss:  904.633  Training Accuracy:  0.044368\n",
      "Epoch:  2  Training Loss:  851.505  Training Accuracy:  0.0660564\n",
      "Epoch:  3  Training Loss:  809.492  Training Accuracy:  0.0745686\n",
      "Epoch:  4  Training Loss:  771.055  Training Accuracy:  0.0789995\n",
      "Epoch:  5  Training Loss:  741.02  Training Accuracy:  0.0812733\n",
      "Epoch:  6  Training Loss:  712.902  Training Accuracy:  0.0847715\n",
      "Epoch:  7  Training Loss:  688.299  Training Accuracy:  0.0891441\n",
      "Epoch:  8  Training Loss:  669.18  Training Accuracy:  0.0949743\n",
      "Epoch:  9  Training Loss:  650.825  Training Accuracy:  0.100805\n",
      "Epoch:  10  Training Loss:  633.515  Training Accuracy:  0.106926\n",
      "Epoch:  11  Training Loss:  619.908  Training Accuracy:  0.115497\n",
      "Epoch:  12  Training Loss:  606.665  Training Accuracy:  0.124242\n",
      "Epoch:  13  Training Loss:  594.166  Training Accuracy:  0.133279\n",
      "Epoch:  14  Training Loss:  583.069  Training Accuracy:  0.14045\n",
      "Epoch:  15  Training Loss:  573.343  Training Accuracy:  0.148146\n",
      "Epoch:  16  Training Loss:  563.979  Training Accuracy:  0.156367\n",
      "Epoch:  17  Training Loss:  554.476  Training Accuracy:  0.165112\n",
      "Epoch:  18  Training Loss:  546.145  Training Accuracy:  0.173799\n",
      "Epoch:  19  Training Loss:  538.524  Training Accuracy:  0.182778\n",
      "Epoch:  20  Training Loss:  530.127  Training Accuracy:  0.190065\n",
      "Epoch:  21  Training Loss:  522.573  Training Accuracy:  0.197703\n",
      "Epoch:  22  Training Loss:  514.846  Training Accuracy:  0.205807\n",
      "Epoch:  23  Training Loss:  507.018  Training Accuracy:  0.212337\n",
      "Epoch:  24  Training Loss:  499.73  Training Accuracy:  0.21875\n",
      "Epoch:  25  Training Loss:  492.444  Training Accuracy:  0.225805\n",
      "Epoch:  26  Training Loss:  485.084  Training Accuracy:  0.231052\n",
      "Epoch:  27  Training Loss:  478.196  Training Accuracy:  0.236707\n",
      "Epoch:  28  Training Loss:  471.12  Training Accuracy:  0.242479\n",
      "Epoch:  29  Training Loss:  463.719  Training Accuracy:  0.248193\n",
      "Epoch:  30  Training Loss:  457.334  Training Accuracy:  0.254314\n",
      "Epoch:  31  Training Loss:  450.586  Training Accuracy:  0.259737\n",
      "Epoch:  32  Training Loss:  444.146  Training Accuracy:  0.264634\n",
      "Epoch:  33  Training Loss:  437.562  Training Accuracy:  0.269764\n",
      "Epoch:  34  Training Loss:  430.788  Training Accuracy:  0.275478\n",
      "Epoch:  35  Training Loss:  424.913  Training Accuracy:  0.280725\n",
      "Epoch:  36  Training Loss:  418.252  Training Accuracy:  0.286147\n",
      "Epoch:  37  Training Loss:  411.762  Training Accuracy:  0.292444\n",
      "Epoch:  38  Training Loss:  405.403  Training Accuracy:  0.297866\n",
      "Epoch:  39  Training Loss:  399.108  Training Accuracy:  0.303988\n",
      "Epoch:  40  Training Loss:  393.583  Training Accuracy:  0.309352\n",
      "Epoch:  41  Training Loss:  387.462  Training Accuracy:  0.314541\n",
      "Epoch:  42  Training Loss:  381.961  Training Accuracy:  0.320546\n",
      "Epoch:  43  Training Loss:  376.298  Training Accuracy:  0.326143\n",
      "Epoch:  44  Training Loss:  370.699  Training Accuracy:  0.331273\n",
      "Epoch:  45  Training Loss:  365.424  Training Accuracy:  0.335821\n",
      "Epoch:  46  Training Loss:  360.18  Training Accuracy:  0.340602\n",
      "Epoch:  47  Training Loss:  354.777  Training Accuracy:  0.345791\n",
      "Epoch:  48  Training Loss:  349.717  Training Accuracy:  0.350863\n",
      "Epoch:  49  Training Loss:  344.307  Training Accuracy:  0.355935\n",
      "Epoch:  50  Training Loss:  339.476  Training Accuracy:  0.359958\n",
      "Epoch:  51  Training Loss:  334.401  Training Accuracy:  0.365147\n",
      "Epoch:  52  Training Loss:  329.822  Training Accuracy:  0.370044\n",
      "Epoch:  53  Training Loss:  324.612  Training Accuracy:  0.374242\n",
      "Epoch:  54  Training Loss:  320.24  Training Accuracy:  0.379839\n",
      "Epoch:  55  Training Loss:  315.27  Training Accuracy:  0.384853\n",
      "Epoch:  56  Training Loss:  310.837  Training Accuracy:  0.389692\n",
      "Epoch:  57  Training Loss:  306.079  Training Accuracy:  0.393832\n",
      "Epoch:  58  Training Loss:  301.892  Training Accuracy:  0.397796\n",
      "Epoch:  59  Training Loss:  297.425  Training Accuracy:  0.402227\n",
      "Epoch:  60  Training Loss:  292.996  Training Accuracy:  0.406366\n",
      "Epoch:  61  Training Loss:  288.787  Training Accuracy:  0.40934\n",
      "Epoch:  62  Training Loss:  284.444  Training Accuracy:  0.41313\n",
      "Epoch:  63  Training Loss:  280.316  Training Accuracy:  0.416861\n",
      "Epoch:  64  Training Loss:  275.994  Training Accuracy:  0.420767\n",
      "Epoch:  65  Training Loss:  272.182  Training Accuracy:  0.425023\n",
      "Epoch:  66  Training Loss:  268.052  Training Accuracy:  0.428929\n",
      "Epoch:  67  Training Loss:  263.998  Training Accuracy:  0.433069\n",
      "Epoch:  68  Training Loss:  260.365  Training Accuracy:  0.437325\n",
      "Epoch:  69  Training Loss:  256.343  Training Accuracy:  0.441639\n",
      "Epoch:  70  Training Loss:  252.466  Training Accuracy:  0.445546\n",
      "Epoch:  71  Training Loss:  248.56  Training Accuracy:  0.449743\n",
      "Epoch:  72  Training Loss:  245.216  Training Accuracy:  0.4533\n",
      "Epoch:  73  Training Loss:  241.008  Training Accuracy:  0.456331\n",
      "Epoch:  74  Training Loss:  237.395  Training Accuracy:  0.459072\n",
      "Epoch:  75  Training Loss:  233.836  Training Accuracy:  0.462628\n",
      "Epoch:  76  Training Loss:  230.122  Training Accuracy:  0.465019\n",
      "Epoch:  77  Training Loss:  226.495  Training Accuracy:  0.468866\n",
      "Epoch:  78  Training Loss:  223.25  Training Accuracy:  0.471956\n",
      "Epoch:  79  Training Loss:  219.87  Training Accuracy:  0.47493\n",
      "Epoch:  80  Training Loss:  216.432  Training Accuracy:  0.478195\n",
      "Epoch:  81  Training Loss:  213.12  Training Accuracy:  0.481868\n",
      "Epoch:  82  Training Loss:  209.968  Training Accuracy:  0.485716\n",
      "Epoch:  83  Training Loss:  206.657  Training Accuracy:  0.488806\n",
      "Epoch:  84  Training Loss:  203.279  Training Accuracy:  0.491546\n",
      "Epoch:  85  Training Loss:  200.201  Training Accuracy:  0.494053\n",
      "Epoch:  86  Training Loss:  197.073  Training Accuracy:  0.497376\n",
      "Epoch:  87  Training Loss:  194.055  Training Accuracy:  0.499767\n",
      "Epoch:  88  Training Loss:  190.973  Training Accuracy:  0.502507\n",
      "Epoch:  89  Training Loss:  187.651  Training Accuracy:  0.505014\n",
      "Epoch:  90  Training Loss:  184.746  Training Accuracy:  0.508045\n",
      "Epoch:  91  Training Loss:  181.729  Training Accuracy:  0.511427\n",
      "Epoch:  92  Training Loss:  178.865  Training Accuracy:  0.513876\n",
      "Epoch:  93  Training Loss:  176.341  Training Accuracy:  0.516907\n",
      "Epoch:  94  Training Loss:  173.425  Training Accuracy:  0.519473\n",
      "Epoch:  95  Training Loss:  170.84  Training Accuracy:  0.521805\n",
      "Epoch:  96  Training Loss:  168.248  Training Accuracy:  0.524662\n",
      "Epoch:  97  Training Loss:  165.669  Training Accuracy:  0.526994\n",
      "Epoch:  98  Training Loss:  163.252  Training Accuracy:  0.529384\n",
      "Epoch:  99  Training Loss:  160.774  Training Accuracy:  0.531891\n",
      "Epoch:  100  Training Loss:  158.451  Training Accuracy:  0.534456\n",
      "Epoch:  101  Training Loss:  155.96  Training Accuracy:  0.536439\n",
      "Epoch:  102  Training Loss:  153.635  Training Accuracy:  0.538246\n",
      "Epoch:  103  Training Loss:  151.175  Training Accuracy:  0.540345\n",
      "Epoch:  104  Training Loss:  149.107  Training Accuracy:  0.542269\n",
      "Epoch:  105  Training Loss:  146.769  Training Accuracy:  0.544893\n",
      "Epoch:  106  Training Loss:  144.679  Training Accuracy:  0.547341\n",
      "Epoch:  107  Training Loss:  142.539  Training Accuracy:  0.549557\n",
      "Epoch:  108  Training Loss:  140.435  Training Accuracy:  0.552064\n",
      "Epoch:  109  Training Loss:  138.574  Training Accuracy:  0.553929\n",
      "Epoch:  110  Training Loss:  136.343  Training Accuracy:  0.555678\n",
      "Epoch:  111  Training Loss:  134.415  Training Accuracy:  0.557719\n",
      "Epoch:  112  Training Loss:  132.449  Training Accuracy:  0.55976\n",
      "Epoch:  113  Training Loss:  130.53  Training Accuracy:  0.56145\n",
      "Epoch:  114  Training Loss:  128.634  Training Accuracy:  0.563374\n",
      "Epoch:  115  Training Loss:  126.685  Training Accuracy:  0.56524\n",
      "Epoch:  116  Training Loss:  124.959  Training Accuracy:  0.567747\n",
      "Epoch:  117  Training Loss:  123.176  Training Accuracy:  0.570779\n",
      "Epoch:  118  Training Loss:  121.364  Training Accuracy:  0.572936\n",
      "Epoch:  119  Training Loss:  119.6  Training Accuracy:  0.574627\n",
      "Epoch:  120  Training Loss:  117.943  Training Accuracy:  0.576492\n",
      "Epoch:  121  Training Loss:  116.284  Training Accuracy:  0.578999\n",
      "Epoch:  122  Training Loss:  114.692  Training Accuracy:  0.580748\n",
      "Epoch:  123  Training Loss:  113.058  Training Accuracy:  0.582789\n",
      "Epoch:  124  Training Loss:  111.52  Training Accuracy:  0.585004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  110.0  Training Accuracy:  0.58757\n",
      "Epoch:  126  Training Loss:  108.542  Training Accuracy:  0.589435\n",
      "Epoch:  127  Training Loss:  106.938  Training Accuracy:  0.591068\n",
      "Epoch:  128  Training Loss:  105.525  Training Accuracy:  0.592467\n",
      "Epoch:  129  Training Loss:  103.982  Training Accuracy:  0.594391\n",
      "Epoch:  130  Training Loss:  102.632  Training Accuracy:  0.596024\n",
      "Epoch:  131  Training Loss:  101.251  Training Accuracy:  0.598181\n",
      "Epoch:  132  Training Loss:  99.9132  Training Accuracy:  0.600046\n",
      "Epoch:  133  Training Loss:  98.5575  Training Accuracy:  0.601562\n",
      "Epoch:  134  Training Loss:  97.323  Training Accuracy:  0.603486\n",
      "Epoch:  135  Training Loss:  95.9231  Training Accuracy:  0.605585\n",
      "Epoch:  136  Training Loss:  94.8297  Training Accuracy:  0.607392\n",
      "Epoch:  137  Training Loss:  93.449  Training Accuracy:  0.609258\n",
      "Epoch:  138  Training Loss:  92.3779  Training Accuracy:  0.610774\n",
      "Epoch:  139  Training Loss:  91.0743  Training Accuracy:  0.61264\n",
      "Epoch:  140  Training Loss:  90.0044  Training Accuracy:  0.614039\n",
      "Epoch:  141  Training Loss:  88.8058  Training Accuracy:  0.615263\n",
      "Epoch:  142  Training Loss:  87.7562  Training Accuracy:  0.617362\n",
      "Epoch:  143  Training Loss:  86.4898  Training Accuracy:  0.618878\n",
      "Epoch:  144  Training Loss:  85.4685  Training Accuracy:  0.620336\n",
      "Epoch:  145  Training Loss:  84.3538  Training Accuracy:  0.622668\n",
      "Epoch:  146  Training Loss:  83.2847  Training Accuracy:  0.624533\n",
      "Epoch:  147  Training Loss:  82.0818  Training Accuracy:  0.626107\n",
      "Epoch:  148  Training Loss:  81.1494  Training Accuracy:  0.627682\n",
      "Epoch:  149  Training Loss:  80.0238  Training Accuracy:  0.629197\n",
      "Epoch:  150  Training Loss:  78.972  Training Accuracy:  0.63118\n",
      "Epoch:  151  Training Loss:  77.9558  Training Accuracy:  0.632987\n",
      "Epoch:  152  Training Loss:  77.0032  Training Accuracy:  0.634211\n",
      "Epoch:  153  Training Loss:  76.099  Training Accuracy:  0.635902\n",
      "Epoch:  154  Training Loss:  75.1452  Training Accuracy:  0.636777\n",
      "Epoch:  155  Training Loss:  74.2079  Training Accuracy:  0.638176\n",
      "Epoch:  156  Training Loss:  73.3638  Training Accuracy:  0.639692\n",
      "Epoch:  157  Training Loss:  72.4092  Training Accuracy:  0.640916\n",
      "Epoch:  158  Training Loss:  71.4936  Training Accuracy:  0.642315\n",
      "Epoch:  159  Training Loss:  70.5881  Training Accuracy:  0.64319\n",
      "Epoch:  160  Training Loss:  69.8114  Training Accuracy:  0.644881\n",
      "Epoch:  161  Training Loss:  68.8312  Training Accuracy:  0.646163\n",
      "Epoch:  162  Training Loss:  68.0616  Training Accuracy:  0.648495\n",
      "Epoch:  163  Training Loss:  67.2164  Training Accuracy:  0.650128\n",
      "Epoch:  164  Training Loss:  66.5  Training Accuracy:  0.651702\n",
      "Epoch:  165  Training Loss:  65.7124  Training Accuracy:  0.65316\n",
      "Epoch:  166  Training Loss:  64.8974  Training Accuracy:  0.654617\n",
      "Epoch:  167  Training Loss:  64.189  Training Accuracy:  0.655842\n",
      "Epoch:  168  Training Loss:  63.4067  Training Accuracy:  0.656774\n",
      "Epoch:  169  Training Loss:  62.6511  Training Accuracy:  0.65829\n",
      "Epoch:  170  Training Loss:  61.9085  Training Accuracy:  0.659981\n",
      "Epoch:  171  Training Loss:  61.22  Training Accuracy:  0.661905\n",
      "Epoch:  172  Training Loss:  60.3742  Training Accuracy:  0.663363\n",
      "Epoch:  173  Training Loss:  59.7309  Training Accuracy:  0.665053\n",
      "Epoch:  174  Training Loss:  59.0322  Training Accuracy:  0.667211\n",
      "Epoch:  175  Training Loss:  58.2524  Training Accuracy:  0.668726\n",
      "Epoch:  176  Training Loss:  57.5831  Training Accuracy:  0.670301\n",
      "Epoch:  177  Training Loss:  56.94  Training Accuracy:  0.671525\n",
      "Epoch:  178  Training Loss:  56.2506  Training Accuracy:  0.673041\n",
      "Epoch:  179  Training Loss:  55.6196  Training Accuracy:  0.674615\n",
      "Epoch:  180  Training Loss:  54.9939  Training Accuracy:  0.676014\n",
      "Epoch:  181  Training Loss:  54.4335  Training Accuracy:  0.677297\n",
      "Epoch:  182  Training Loss:  53.7341  Training Accuracy:  0.678813\n",
      "Epoch:  183  Training Loss:  53.2106  Training Accuracy:  0.680445\n",
      "Epoch:  184  Training Loss:  52.5642  Training Accuracy:  0.681378\n",
      "Epoch:  185  Training Loss:  52.0317  Training Accuracy:  0.682602\n",
      "Epoch:  186  Training Loss:  51.4214  Training Accuracy:  0.683535\n",
      "Epoch:  187  Training Loss:  50.7947  Training Accuracy:  0.685226\n",
      "Epoch:  188  Training Loss:  50.3132  Training Accuracy:  0.686509\n",
      "Epoch:  189  Training Loss:  49.7473  Training Accuracy:  0.688141\n",
      "Epoch:  190  Training Loss:  49.1704  Training Accuracy:  0.68919\n",
      "Epoch:  191  Training Loss:  48.6333  Training Accuracy:  0.690298\n",
      "Epoch:  192  Training Loss:  48.14  Training Accuracy:  0.691114\n",
      "Epoch:  193  Training Loss:  47.6434  Training Accuracy:  0.692863\n",
      "Epoch:  194  Training Loss:  47.1225  Training Accuracy:  0.694263\n",
      "Epoch:  195  Training Loss:  46.7034  Training Accuracy:  0.695662\n",
      "Epoch:  196  Training Loss:  46.2262  Training Accuracy:  0.697178\n",
      "Epoch:  197  Training Loss:  45.8693  Training Accuracy:  0.698577\n",
      "Epoch:  198  Training Loss:  45.3192  Training Accuracy:  0.699218\n",
      "Epoch:  199  Training Loss:  44.862  Training Accuracy:  0.700384\n",
      "Testing Accuracy: 0.655358\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 200\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  1473.84  Training Accuracy:  0.046875\n",
      "Epoch:  1  Training Loss:  927.084  Training Accuracy:  0.0577192\n",
      "Epoch:  2  Training Loss:  822.954  Training Accuracy:  0.0638993\n",
      "Epoch:  3  Training Loss:  768.102  Training Accuracy:  0.0739855\n",
      "Epoch:  4  Training Loss:  731.537  Training Accuracy:  0.0846548\n",
      "Epoch:  5  Training Loss:  704.461  Training Accuracy:  0.0961987\n",
      "Epoch:  6  Training Loss:  685.269  Training Accuracy:  0.107568\n",
      "Epoch:  7  Training Loss:  669.92  Training Accuracy:  0.118179\n",
      "Epoch:  8  Training Loss:  655.565  Training Accuracy:  0.128498\n",
      "Epoch:  9  Training Loss:  643.199  Training Accuracy:  0.137127\n",
      "Epoch:  10  Training Loss:  634.062  Training Accuracy:  0.14459\n",
      "Epoch:  11  Training Loss:  626.792  Training Accuracy:  0.151236\n",
      "Epoch:  12  Training Loss:  618.269  Training Accuracy:  0.157066\n",
      "Epoch:  13  Training Loss:  608.746  Training Accuracy:  0.163013\n",
      "Epoch:  14  Training Loss:  602.366  Training Accuracy:  0.169426\n",
      "Epoch:  15  Training Loss:  594.348  Training Accuracy:  0.176889\n",
      "Epoch:  16  Training Loss:  583.477  Training Accuracy:  0.182836\n",
      "Epoch:  17  Training Loss:  573.797  Training Accuracy:  0.189074\n",
      "Epoch:  18  Training Loss:  565.774  Training Accuracy:  0.194555\n",
      "Epoch:  19  Training Loss:  556.873  Training Accuracy:  0.200793\n",
      "Epoch:  20  Training Loss:  546.774  Training Accuracy:  0.207847\n",
      "Epoch:  21  Training Loss:  537.567  Training Accuracy:  0.214902\n",
      "Epoch:  22  Training Loss:  528.778  Training Accuracy:  0.221257\n",
      "Epoch:  23  Training Loss:  521.034  Training Accuracy:  0.227379\n",
      "Epoch:  24  Training Loss:  512.968  Training Accuracy:  0.232743\n",
      "Epoch:  25  Training Loss:  505.205  Training Accuracy:  0.238281\n",
      "Epoch:  26  Training Loss:  497.343  Training Accuracy:  0.242771\n",
      "Epoch:  27  Training Loss:  489.98  Training Accuracy:  0.248834\n",
      "Epoch:  28  Training Loss:  482.691  Training Accuracy:  0.255131\n",
      "Epoch:  29  Training Loss:  475.737  Training Accuracy:  0.26166\n",
      "Epoch:  30  Training Loss:  468.786  Training Accuracy:  0.267374\n",
      "Epoch:  31  Training Loss:  461.964  Training Accuracy:  0.273204\n",
      "Epoch:  32  Training Loss:  455.074  Training Accuracy:  0.278685\n",
      "Epoch:  33  Training Loss:  448.167  Training Accuracy:  0.284282\n",
      "Epoch:  34  Training Loss:  441.604  Training Accuracy:  0.290462\n",
      "Epoch:  35  Training Loss:  435.18  Training Accuracy:  0.295534\n",
      "Epoch:  36  Training Loss:  428.832  Training Accuracy:  0.301889\n",
      "Epoch:  37  Training Loss:  422.421  Training Accuracy:  0.306553\n",
      "Epoch:  38  Training Loss:  416.405  Training Accuracy:  0.312267\n",
      "Epoch:  39  Training Loss:  410.291  Training Accuracy:  0.317572\n",
      "Epoch:  40  Training Loss:  404.25  Training Accuracy:  0.322936\n",
      "Epoch:  41  Training Loss:  398.254  Training Accuracy:  0.328008\n",
      "Epoch:  42  Training Loss:  392.766  Training Accuracy:  0.333022\n",
      "Epoch:  43  Training Loss:  386.528  Training Accuracy:  0.338561\n",
      "Epoch:  44  Training Loss:  380.762  Training Accuracy:  0.343808\n",
      "Epoch:  45  Training Loss:  374.752  Training Accuracy:  0.348181\n",
      "Epoch:  46  Training Loss:  368.979  Training Accuracy:  0.35372\n",
      "Epoch:  47  Training Loss:  363.158  Training Accuracy:  0.359375\n",
      "Epoch:  48  Training Loss:  357.485  Training Accuracy:  0.36468\n",
      "Epoch:  49  Training Loss:  351.853  Training Accuracy:  0.370569\n",
      "Epoch:  50  Training Loss:  346.264  Training Accuracy:  0.375058\n",
      "Epoch:  51  Training Loss:  340.646  Training Accuracy:  0.379722\n",
      "Epoch:  52  Training Loss:  335.004  Training Accuracy:  0.385436\n",
      "Epoch:  53  Training Loss:  329.371  Training Accuracy:  0.390683\n",
      "Epoch:  54  Training Loss:  323.819  Training Accuracy:  0.395464\n",
      "Epoch:  55  Training Loss:  318.027  Training Accuracy:  0.402169\n",
      "Epoch:  56  Training Loss:  312.666  Training Accuracy:  0.407416\n",
      "Epoch:  57  Training Loss:  307.549  Training Accuracy:  0.411964\n",
      "Epoch:  58  Training Loss:  302.137  Training Accuracy:  0.417269\n",
      "Epoch:  59  Training Loss:  296.796  Training Accuracy:  0.422633\n",
      "Epoch:  60  Training Loss:  291.782  Training Accuracy:  0.427822\n",
      "Epoch:  61  Training Loss:  286.584  Training Accuracy:  0.432777\n",
      "Epoch:  62  Training Loss:  281.436  Training Accuracy:  0.436917\n",
      "Epoch:  63  Training Loss:  276.288  Training Accuracy:  0.44059\n",
      "Epoch:  64  Training Loss:  271.339  Training Accuracy:  0.445021\n",
      "Epoch:  65  Training Loss:  266.376  Training Accuracy:  0.448752\n",
      "Epoch:  66  Training Loss:  261.286  Training Accuracy:  0.453358\n",
      "Epoch:  67  Training Loss:  256.437  Training Accuracy:  0.457264\n",
      "Epoch:  68  Training Loss:  251.442  Training Accuracy:  0.461462\n",
      "Epoch:  69  Training Loss:  246.547  Training Accuracy:  0.465485\n",
      "Epoch:  70  Training Loss:  241.802  Training Accuracy:  0.469566\n",
      "Epoch:  71  Training Loss:  237.191  Training Accuracy:  0.473764\n",
      "Epoch:  72  Training Loss:  232.395  Training Accuracy:  0.477145\n",
      "Epoch:  73  Training Loss:  227.939  Training Accuracy:  0.48111\n",
      "Epoch:  74  Training Loss:  223.541  Training Accuracy:  0.48455\n",
      "Epoch:  75  Training Loss:  218.803  Training Accuracy:  0.48799\n",
      "Epoch:  76  Training Loss:  214.674  Training Accuracy:  0.490846\n",
      "Epoch:  77  Training Loss:  210.574  Training Accuracy:  0.493878\n",
      "Epoch:  78  Training Loss:  206.153  Training Accuracy:  0.497143\n",
      "Epoch:  79  Training Loss:  202.085  Training Accuracy:  0.499883\n",
      "Epoch:  80  Training Loss:  198.008  Training Accuracy:  0.503498\n",
      "Epoch:  81  Training Loss:  194.273  Training Accuracy:  0.506588\n",
      "Epoch:  82  Training Loss:  190.414  Training Accuracy:  0.509969\n",
      "Epoch:  83  Training Loss:  186.593  Training Accuracy:  0.513526\n",
      "Epoch:  84  Training Loss:  182.854  Training Accuracy:  0.516441\n",
      "Epoch:  85  Training Loss:  179.309  Training Accuracy:  0.519414\n",
      "Epoch:  86  Training Loss:  175.729  Training Accuracy:  0.522679\n",
      "Epoch:  87  Training Loss:  172.277  Training Accuracy:  0.526411\n",
      "Epoch:  88  Training Loss:  168.873  Training Accuracy:  0.529909\n",
      "Epoch:  89  Training Loss:  165.606  Training Accuracy:  0.533057\n",
      "Epoch:  90  Training Loss:  162.236  Training Accuracy:  0.536322\n",
      "Epoch:  91  Training Loss:  159.339  Training Accuracy:  0.539295\n",
      "Epoch:  92  Training Loss:  156.327  Training Accuracy:  0.542619\n",
      "Epoch:  93  Training Loss:  153.363  Training Accuracy:  0.545126\n",
      "Epoch:  94  Training Loss:  150.422  Training Accuracy:  0.547516\n",
      "Epoch:  95  Training Loss:  147.646  Training Accuracy:  0.550489\n",
      "Epoch:  96  Training Loss:  144.947  Training Accuracy:  0.552938\n",
      "Epoch:  97  Training Loss:  142.231  Training Accuracy:  0.55527\n",
      "Epoch:  98  Training Loss:  139.664  Training Accuracy:  0.558535\n",
      "Epoch:  99  Training Loss:  137.108  Training Accuracy:  0.561042\n",
      "Epoch:  100  Training Loss:  134.697  Training Accuracy:  0.563782\n",
      "Epoch:  101  Training Loss:  132.207  Training Accuracy:  0.566173\n",
      "Epoch:  102  Training Loss:  129.903  Training Accuracy:  0.568796\n",
      "Epoch:  103  Training Loss:  127.43  Training Accuracy:  0.571303\n",
      "Epoch:  104  Training Loss:  125.247  Training Accuracy:  0.573461\n",
      "Epoch:  105  Training Loss:  122.946  Training Accuracy:  0.575501\n",
      "Epoch:  106  Training Loss:  120.768  Training Accuracy:  0.578066\n",
      "Epoch:  107  Training Loss:  118.786  Training Accuracy:  0.580515\n",
      "Epoch:  108  Training Loss:  116.714  Training Accuracy:  0.583139\n",
      "Epoch:  109  Training Loss:  114.828  Training Accuracy:  0.585296\n",
      "Epoch:  110  Training Loss:  112.864  Training Accuracy:  0.588036\n",
      "Epoch:  111  Training Loss:  111.038  Training Accuracy:  0.589552\n",
      "Epoch:  112  Training Loss:  109.174  Training Accuracy:  0.591534\n",
      "Epoch:  113  Training Loss:  107.593  Training Accuracy:  0.593342\n",
      "Epoch:  114  Training Loss:  105.887  Training Accuracy:  0.594916\n",
      "Epoch:  115  Training Loss:  104.205  Training Accuracy:  0.596956\n",
      "Epoch:  116  Training Loss:  102.738  Training Accuracy:  0.599347\n",
      "Epoch:  117  Training Loss:  101.108  Training Accuracy:  0.601854\n",
      "Epoch:  118  Training Loss:  99.7348  Training Accuracy:  0.603195\n",
      "Epoch:  119  Training Loss:  98.2171  Training Accuracy:  0.605002\n",
      "Epoch:  120  Training Loss:  96.8084  Training Accuracy:  0.606809\n",
      "Epoch:  121  Training Loss:  95.556  Training Accuracy:  0.608617\n",
      "Epoch:  122  Training Loss:  94.0716  Training Accuracy:  0.610716\n",
      "Epoch:  123  Training Loss:  92.81  Training Accuracy:  0.613398\n",
      "Epoch:  124  Training Loss:  91.4912  Training Accuracy:  0.615613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  90.2571  Training Accuracy:  0.618237\n",
      "Epoch:  126  Training Loss:  89.0056  Training Accuracy:  0.620627\n",
      "Epoch:  127  Training Loss:  87.8746  Training Accuracy:  0.622668\n",
      "Epoch:  128  Training Loss:  86.6863  Training Accuracy:  0.624766\n",
      "Epoch:  129  Training Loss:  85.5018  Training Accuracy:  0.626457\n",
      "Epoch:  130  Training Loss:  84.4116  Training Accuracy:  0.628498\n",
      "Epoch:  131  Training Loss:  83.3428  Training Accuracy:  0.629606\n",
      "Epoch:  132  Training Loss:  82.1511  Training Accuracy:  0.632171\n",
      "Epoch:  133  Training Loss:  81.1873  Training Accuracy:  0.633512\n",
      "Epoch:  134  Training Loss:  80.0539  Training Accuracy:  0.635261\n",
      "Epoch:  135  Training Loss:  79.1385  Training Accuracy:  0.637243\n",
      "Epoch:  136  Training Loss:  78.0107  Training Accuracy:  0.639109\n",
      "Epoch:  137  Training Loss:  77.1621  Training Accuracy:  0.641441\n",
      "Epoch:  138  Training Loss:  76.0815  Training Accuracy:  0.643073\n",
      "Epoch:  139  Training Loss:  75.2452  Training Accuracy:  0.645056\n",
      "Epoch:  140  Training Loss:  74.2766  Training Accuracy:  0.64698\n",
      "Epoch:  141  Training Loss:  73.4476  Training Accuracy:  0.648612\n",
      "Epoch:  142  Training Loss:  72.5296  Training Accuracy:  0.650653\n",
      "Epoch:  143  Training Loss:  71.7459  Training Accuracy:  0.652926\n",
      "Epoch:  144  Training Loss:  70.8393  Training Accuracy:  0.654326\n",
      "Epoch:  145  Training Loss:  70.1174  Training Accuracy:  0.6559\n",
      "Epoch:  146  Training Loss:  69.287  Training Accuracy:  0.657474\n",
      "Epoch:  147  Training Loss:  68.5826  Training Accuracy:  0.659106\n",
      "Epoch:  148  Training Loss:  67.8101  Training Accuracy:  0.660273\n",
      "Epoch:  149  Training Loss:  67.1293  Training Accuracy:  0.661555\n",
      "Epoch:  150  Training Loss:  66.4958  Training Accuracy:  0.663654\n",
      "Epoch:  151  Training Loss:  65.7493  Training Accuracy:  0.664878\n",
      "Epoch:  152  Training Loss:  65.1353  Training Accuracy:  0.667036\n",
      "Epoch:  153  Training Loss:  64.4365  Training Accuracy:  0.668143\n",
      "Epoch:  154  Training Loss:  63.8571  Training Accuracy:  0.669892\n",
      "Epoch:  155  Training Loss:  63.3826  Training Accuracy:  0.671641\n",
      "Epoch:  156  Training Loss:  62.6502  Training Accuracy:  0.673157\n",
      "Epoch:  157  Training Loss:  62.1543  Training Accuracy:  0.674673\n",
      "Epoch:  158  Training Loss:  61.504  Training Accuracy:  0.676655\n",
      "Epoch:  159  Training Loss:  60.9392  Training Accuracy:  0.677705\n",
      "Epoch:  160  Training Loss:  60.3953  Training Accuracy:  0.678929\n",
      "Epoch:  161  Training Loss:  59.8751  Training Accuracy:  0.680328\n",
      "Epoch:  162  Training Loss:  59.2951  Training Accuracy:  0.681611\n",
      "Epoch:  163  Training Loss:  58.7897  Training Accuracy:  0.68301\n",
      "Epoch:  164  Training Loss:  58.3024  Training Accuracy:  0.68371\n",
      "Epoch:  165  Training Loss:  57.7105  Training Accuracy:  0.684818\n",
      "Epoch:  166  Training Loss:  57.3413  Training Accuracy:  0.686275\n",
      "Epoch:  167  Training Loss:  56.7804  Training Accuracy:  0.687791\n",
      "Epoch:  168  Training Loss:  56.3045  Training Accuracy:  0.689482\n",
      "Epoch:  169  Training Loss:  55.8466  Training Accuracy:  0.690531\n",
      "Epoch:  170  Training Loss:  55.2833  Training Accuracy:  0.691406\n",
      "Epoch:  171  Training Loss:  54.8735  Training Accuracy:  0.692747\n",
      "Epoch:  172  Training Loss:  54.3888  Training Accuracy:  0.69403\n",
      "Epoch:  173  Training Loss:  53.9961  Training Accuracy:  0.695254\n",
      "Epoch:  174  Training Loss:  53.5491  Training Accuracy:  0.69677\n",
      "Epoch:  175  Training Loss:  53.1286  Training Accuracy:  0.698402\n",
      "Epoch:  176  Training Loss:  52.5991  Training Accuracy:  0.699976\n",
      "Epoch:  177  Training Loss:  52.2647  Training Accuracy:  0.701667\n",
      "Epoch:  178  Training Loss:  51.814  Training Accuracy:  0.703591\n",
      "Epoch:  179  Training Loss:  51.3621  Training Accuracy:  0.705398\n",
      "Epoch:  180  Training Loss:  50.9962  Training Accuracy:  0.706681\n",
      "Epoch:  181  Training Loss:  50.588  Training Accuracy:  0.708139\n",
      "Epoch:  182  Training Loss:  50.2028  Training Accuracy:  0.708955\n",
      "Epoch:  183  Training Loss:  49.8306  Training Accuracy:  0.709713\n",
      "Epoch:  184  Training Loss:  49.4524  Training Accuracy:  0.710879\n",
      "Epoch:  185  Training Loss:  49.0734  Training Accuracy:  0.712045\n",
      "Epoch:  186  Training Loss:  48.6973  Training Accuracy:  0.713094\n",
      "Epoch:  187  Training Loss:  48.3382  Training Accuracy:  0.71426\n",
      "Epoch:  188  Training Loss:  47.9758  Training Accuracy:  0.716068\n",
      "Epoch:  189  Training Loss:  47.6322  Training Accuracy:  0.717584\n",
      "Epoch:  190  Training Loss:  47.3218  Training Accuracy:  0.718516\n",
      "Epoch:  191  Training Loss:  46.9425  Training Accuracy:  0.719508\n",
      "Epoch:  192  Training Loss:  46.6138  Training Accuracy:  0.720674\n",
      "Epoch:  193  Training Loss:  46.2998  Training Accuracy:  0.72149\n",
      "Epoch:  194  Training Loss:  45.98  Training Accuracy:  0.722189\n",
      "Epoch:  195  Training Loss:  45.6389  Training Accuracy:  0.723297\n",
      "Epoch:  196  Training Loss:  45.3357  Training Accuracy:  0.724522\n",
      "Epoch:  197  Training Loss:  45.0239  Training Accuracy:  0.725746\n",
      "Epoch:  198  Training Loss:  44.7051  Training Accuracy:  0.726387\n",
      "Epoch:  199  Training Loss:  44.3904  Training Accuracy:  0.727378\n",
      "Epoch:  200  Training Loss:  44.1596  Training Accuracy:  0.729011\n",
      "Epoch:  201  Training Loss:  43.8832  Training Accuracy:  0.730177\n",
      "Epoch:  202  Training Loss:  43.5553  Training Accuracy:  0.731809\n",
      "Epoch:  203  Training Loss:  43.2746  Training Accuracy:  0.732975\n",
      "Epoch:  204  Training Loss:  42.9575  Training Accuracy:  0.733908\n",
      "Epoch:  205  Training Loss:  42.7583  Training Accuracy:  0.734316\n",
      "Epoch:  206  Training Loss:  42.4577  Training Accuracy:  0.735599\n",
      "Epoch:  207  Training Loss:  42.2335  Training Accuracy:  0.736707\n",
      "Epoch:  208  Training Loss:  41.9236  Training Accuracy:  0.737698\n",
      "Epoch:  209  Training Loss:  41.5835  Training Accuracy:  0.738806\n",
      "Epoch:  210  Training Loss:  41.418  Training Accuracy:  0.739972\n",
      "Epoch:  211  Training Loss:  41.1507  Training Accuracy:  0.740613\n",
      "Epoch:  212  Training Loss:  40.9178  Training Accuracy:  0.741604\n",
      "Epoch:  213  Training Loss:  40.6383  Training Accuracy:  0.742828\n",
      "Epoch:  214  Training Loss:  40.4377  Training Accuracy:  0.743703\n",
      "Epoch:  215  Training Loss:  40.1027  Training Accuracy:  0.744578\n",
      "Epoch:  216  Training Loss:  39.9107  Training Accuracy:  0.745452\n",
      "Epoch:  217  Training Loss:  39.6314  Training Accuracy:  0.746502\n",
      "Epoch:  218  Training Loss:  39.4127  Training Accuracy:  0.747609\n",
      "Epoch:  219  Training Loss:  39.1494  Training Accuracy:  0.749125\n",
      "Epoch:  220  Training Loss:  38.9494  Training Accuracy:  0.750291\n",
      "Epoch:  221  Training Loss:  38.682  Training Accuracy:  0.751341\n",
      "Epoch:  222  Training Loss:  38.3766  Training Accuracy:  0.752681\n",
      "Epoch:  223  Training Loss:  38.1599  Training Accuracy:  0.753498\n",
      "Epoch:  224  Training Loss:  37.9292  Training Accuracy:  0.754197\n",
      "Epoch:  225  Training Loss:  37.6708  Training Accuracy:  0.754606\n",
      "Epoch:  226  Training Loss:  37.4128  Training Accuracy:  0.755538\n",
      "Epoch:  227  Training Loss:  37.191  Training Accuracy:  0.756296\n",
      "Epoch:  228  Training Loss:  36.9515  Training Accuracy:  0.757171\n",
      "Epoch:  229  Training Loss:  36.7262  Training Accuracy:  0.758337\n",
      "Epoch:  230  Training Loss:  36.4877  Training Accuracy:  0.759211\n",
      "Epoch:  231  Training Loss:  36.2657  Training Accuracy:  0.760028\n",
      "Epoch:  232  Training Loss:  36.0574  Training Accuracy:  0.761077\n",
      "Epoch:  233  Training Loss:  35.8227  Training Accuracy:  0.761835\n",
      "Epoch:  234  Training Loss:  35.6168  Training Accuracy:  0.762768\n",
      "Epoch:  235  Training Loss:  35.4204  Training Accuracy:  0.763526\n",
      "Epoch:  236  Training Loss:  35.2138  Training Accuracy:  0.764225\n",
      "Epoch:  237  Training Loss:  34.9784  Training Accuracy:  0.765042\n",
      "Epoch:  238  Training Loss:  34.7854  Training Accuracy:  0.766033\n",
      "Epoch:  239  Training Loss:  34.5937  Training Accuracy:  0.766441\n",
      "Epoch:  240  Training Loss:  34.444  Training Accuracy:  0.767607\n",
      "Epoch:  241  Training Loss:  34.191  Training Accuracy:  0.768481\n",
      "Epoch:  242  Training Loss:  34.009  Training Accuracy:  0.769181\n",
      "Epoch:  243  Training Loss:  33.8181  Training Accuracy:  0.770056\n",
      "Epoch:  244  Training Loss:  33.5951  Training Accuracy:  0.771397\n",
      "Epoch:  245  Training Loss:  33.4003  Training Accuracy:  0.772154\n",
      "Epoch:  246  Training Loss:  33.1678  Training Accuracy:  0.772971\n",
      "Epoch:  247  Training Loss:  33.0167  Training Accuracy:  0.773495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  32.8085  Training Accuracy:  0.774545\n",
      "Epoch:  249  Training Loss:  32.6124  Training Accuracy:  0.775361\n",
      "Epoch:  250  Training Loss:  32.4074  Training Accuracy:  0.776294\n",
      "Epoch:  251  Training Loss:  32.2217  Training Accuracy:  0.777635\n",
      "Epoch:  252  Training Loss:  32.0106  Training Accuracy:  0.777751\n",
      "Epoch:  253  Training Loss:  31.8201  Training Accuracy:  0.778509\n",
      "Epoch:  254  Training Loss:  31.7051  Training Accuracy:  0.779209\n",
      "Epoch:  255  Training Loss:  31.487  Training Accuracy:  0.780025\n",
      "Epoch:  256  Training Loss:  31.2764  Training Accuracy:  0.780783\n",
      "Epoch:  257  Training Loss:  31.0984  Training Accuracy:  0.781483\n",
      "Epoch:  258  Training Loss:  30.8807  Training Accuracy:  0.782474\n",
      "Epoch:  259  Training Loss:  30.6819  Training Accuracy:  0.783174\n",
      "Epoch:  260  Training Loss:  30.515  Training Accuracy:  0.784223\n",
      "Epoch:  261  Training Loss:  30.3311  Training Accuracy:  0.785039\n",
      "Epoch:  262  Training Loss:  30.1428  Training Accuracy:  0.785564\n",
      "Epoch:  263  Training Loss:  29.9342  Training Accuracy:  0.78638\n",
      "Epoch:  264  Training Loss:  29.786  Training Accuracy:  0.78708\n",
      "Epoch:  265  Training Loss:  29.6034  Training Accuracy:  0.787721\n",
      "Epoch:  266  Training Loss:  29.4119  Training Accuracy:  0.788596\n",
      "Epoch:  267  Training Loss:  29.2351  Training Accuracy:  0.789645\n",
      "Epoch:  268  Training Loss:  29.0723  Training Accuracy:  0.790053\n",
      "Epoch:  269  Training Loss:  28.8738  Training Accuracy:  0.790869\n",
      "Epoch:  270  Training Loss:  28.7308  Training Accuracy:  0.792152\n",
      "Epoch:  271  Training Loss:  28.5369  Training Accuracy:  0.79326\n",
      "Epoch:  272  Training Loss:  28.3507  Training Accuracy:  0.793901\n",
      "Epoch:  273  Training Loss:  28.2037  Training Accuracy:  0.794892\n",
      "Epoch:  274  Training Loss:  28.0216  Training Accuracy:  0.79565\n",
      "Epoch:  275  Training Loss:  27.8993  Training Accuracy:  0.796466\n",
      "Epoch:  276  Training Loss:  27.6945  Training Accuracy:  0.797458\n",
      "Epoch:  277  Training Loss:  27.5746  Training Accuracy:  0.797866\n",
      "Epoch:  278  Training Loss:  27.3614  Training Accuracy:  0.798565\n",
      "Epoch:  279  Training Loss:  27.2688  Training Accuracy:  0.79909\n",
      "Epoch:  280  Training Loss:  27.0531  Training Accuracy:  0.799848\n",
      "Epoch:  281  Training Loss:  26.9173  Training Accuracy:  0.800431\n",
      "Epoch:  282  Training Loss:  26.766  Training Accuracy:  0.801014\n",
      "Epoch:  283  Training Loss:  26.626  Training Accuracy:  0.801539\n",
      "Epoch:  284  Training Loss:  26.432  Training Accuracy:  0.801947\n",
      "Epoch:  285  Training Loss:  26.3063  Training Accuracy:  0.802588\n",
      "Epoch:  286  Training Loss:  26.0981  Training Accuracy:  0.803229\n",
      "Epoch:  287  Training Loss:  25.9988  Training Accuracy:  0.804104\n",
      "Epoch:  288  Training Loss:  25.7835  Training Accuracy:  0.804512\n",
      "Epoch:  289  Training Loss:  25.6934  Training Accuracy:  0.805911\n",
      "Epoch:  290  Training Loss:  25.4893  Training Accuracy:  0.806611\n",
      "Epoch:  291  Training Loss:  25.3325  Training Accuracy:  0.807194\n",
      "Epoch:  292  Training Loss:  25.2216  Training Accuracy:  0.807427\n",
      "Epoch:  293  Training Loss:  25.0599  Training Accuracy:  0.80836\n",
      "Epoch:  294  Training Loss:  24.9043  Training Accuracy:  0.809118\n",
      "Epoch:  295  Training Loss:  24.772  Training Accuracy:  0.809701\n",
      "Epoch:  296  Training Loss:  24.6091  Training Accuracy:  0.810342\n",
      "Epoch:  297  Training Loss:  24.475  Training Accuracy:  0.810809\n",
      "Epoch:  298  Training Loss:  24.3339  Training Accuracy:  0.811334\n",
      "Epoch:  299  Training Loss:  24.1695  Training Accuracy:  0.811975\n",
      "Testing Accuracy: 0.758516\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 300\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  736.479  Training Accuracy:  0.0504314\n",
      "Epoch:  1  Training Loss:  751.757  Training Accuracy:  0.0562617\n",
      "Epoch:  2  Training Loss:  722.908  Training Accuracy:  0.0606343\n",
      "Epoch:  3  Training Loss:  690.925  Training Accuracy:  0.0652985\n",
      "Epoch:  4  Training Loss:  660.04  Training Accuracy:  0.0717701\n",
      "Epoch:  5  Training Loss:  643.927  Training Accuracy:  0.0795242\n",
      "Epoch:  6  Training Loss:  628.171  Training Accuracy:  0.0886194\n",
      "Epoch:  7  Training Loss:  612.925  Training Accuracy:  0.0957323\n",
      "Epoch:  8  Training Loss:  603.697  Training Accuracy:  0.104011\n",
      "Epoch:  9  Training Loss:  592.597  Training Accuracy:  0.114389\n",
      "Epoch:  10  Training Loss:  581.341  Training Accuracy:  0.124067\n",
      "Epoch:  11  Training Loss:  571.553  Training Accuracy:  0.133162\n",
      "Epoch:  12  Training Loss:  562.884  Training Accuracy:  0.140217\n",
      "Epoch:  13  Training Loss:  556.527  Training Accuracy:  0.148088\n",
      "Epoch:  14  Training Loss:  549.618  Training Accuracy:  0.155492\n",
      "Epoch:  15  Training Loss:  542.724  Training Accuracy:  0.163363\n",
      "Epoch:  16  Training Loss:  536.569  Training Accuracy:  0.170534\n",
      "Epoch:  17  Training Loss:  532.003  Training Accuracy:  0.177181\n",
      "Epoch:  18  Training Loss:  527.13  Training Accuracy:  0.183652\n",
      "Epoch:  19  Training Loss:  521.765  Training Accuracy:  0.189132\n",
      "Epoch:  20  Training Loss:  516.672  Training Accuracy:  0.195371\n",
      "Epoch:  21  Training Loss:  511.135  Training Accuracy:  0.200385\n",
      "Epoch:  22  Training Loss:  505.441  Training Accuracy:  0.207498\n",
      "Epoch:  23  Training Loss:  498.724  Training Accuracy:  0.213619\n",
      "Epoch:  24  Training Loss:  492.259  Training Accuracy:  0.218575\n",
      "Epoch:  25  Training Loss:  484.729  Training Accuracy:  0.223006\n",
      "Epoch:  26  Training Loss:  478.328  Training Accuracy:  0.22872\n",
      "Epoch:  27  Training Loss:  470.962  Training Accuracy:  0.233442\n",
      "Epoch:  28  Training Loss:  463.181  Training Accuracy:  0.238923\n",
      "Epoch:  29  Training Loss:  455.457  Training Accuracy:  0.245394\n",
      "Epoch:  30  Training Loss:  448.316  Training Accuracy:  0.251049\n",
      "Epoch:  31  Training Loss:  441.729  Training Accuracy:  0.25653\n",
      "Epoch:  32  Training Loss:  435.414  Training Accuracy:  0.261894\n",
      "Epoch:  33  Training Loss:  428.719  Training Accuracy:  0.266733\n",
      "Epoch:  34  Training Loss:  422.236  Training Accuracy:  0.272621\n",
      "Epoch:  35  Training Loss:  415.718  Training Accuracy:  0.278102\n",
      "Epoch:  36  Training Loss:  408.979  Training Accuracy:  0.28364\n",
      "Epoch:  37  Training Loss:  402.936  Training Accuracy:  0.288946\n",
      "Epoch:  38  Training Loss:  398.055  Training Accuracy:  0.294368\n",
      "Epoch:  39  Training Loss:  393.558  Training Accuracy:  0.299499\n",
      "Epoch:  40  Training Loss:  386.645  Training Accuracy:  0.305737\n",
      "Epoch:  41  Training Loss:  379.995  Training Accuracy:  0.311334\n",
      "Epoch:  42  Training Loss:  373.677  Training Accuracy:  0.316465\n",
      "Epoch:  43  Training Loss:  367.579  Training Accuracy:  0.322236\n",
      "Epoch:  44  Training Loss:  361.389  Training Accuracy:  0.328067\n",
      "Epoch:  45  Training Loss:  355.283  Training Accuracy:  0.334888\n",
      "Epoch:  46  Training Loss:  348.875  Training Accuracy:  0.340077\n",
      "Epoch:  47  Training Loss:  343.294  Training Accuracy:  0.345791\n",
      "Epoch:  48  Training Loss:  337.406  Training Accuracy:  0.350863\n",
      "Epoch:  49  Training Loss:  331.925  Training Accuracy:  0.356693\n",
      "Epoch:  50  Training Loss:  326.23  Training Accuracy:  0.362115\n",
      "Epoch:  51  Training Loss:  320.605  Training Accuracy:  0.367712\n",
      "Epoch:  52  Training Loss:  314.94  Training Accuracy:  0.373951\n",
      "Epoch:  53  Training Loss:  309.658  Training Accuracy:  0.379198\n",
      "Epoch:  54  Training Loss:  304.07  Training Accuracy:  0.384678\n",
      "Epoch:  55  Training Loss:  298.714  Training Accuracy:  0.389226\n",
      "Epoch:  56  Training Loss:  293.514  Training Accuracy:  0.393657\n",
      "Epoch:  57  Training Loss:  288.129  Training Accuracy:  0.396455\n",
      "Epoch:  58  Training Loss:  282.858  Training Accuracy:  0.400769\n",
      "Epoch:  59  Training Loss:  277.486  Training Accuracy:  0.404384\n",
      "Epoch:  60  Training Loss:  272.268  Training Accuracy:  0.407474\n",
      "Epoch:  61  Training Loss:  267.043  Training Accuracy:  0.410856\n",
      "Epoch:  62  Training Loss:  262.05  Training Accuracy:  0.415054\n",
      "Epoch:  63  Training Loss:  257.164  Training Accuracy:  0.41826\n",
      "Epoch:  64  Training Loss:  252.374  Training Accuracy:  0.421642\n",
      "Epoch:  65  Training Loss:  247.599  Training Accuracy:  0.42718\n",
      "Epoch:  66  Training Loss:  242.917  Training Accuracy:  0.430154\n",
      "Epoch:  67  Training Loss:  238.286  Training Accuracy:  0.43371\n",
      "Epoch:  68  Training Loss:  233.604  Training Accuracy:  0.437558\n",
      "Epoch:  69  Training Loss:  229.129  Training Accuracy:  0.441814\n",
      "Epoch:  70  Training Loss:  224.524  Training Accuracy:  0.445196\n",
      "Epoch:  71  Training Loss:  220.202  Training Accuracy:  0.449102\n",
      "Epoch:  72  Training Loss:  216.152  Training Accuracy:  0.4526\n",
      "Epoch:  73  Training Loss:  212.085  Training Accuracy:  0.456157\n",
      "Epoch:  74  Training Loss:  208.152  Training Accuracy:  0.45983\n",
      "Epoch:  75  Training Loss:  204.287  Training Accuracy:  0.464844\n",
      "Epoch:  76  Training Loss:  200.434  Training Accuracy:  0.468633\n",
      "Epoch:  77  Training Loss:  196.973  Training Accuracy:  0.472073\n",
      "Epoch:  78  Training Loss:  193.363  Training Accuracy:  0.475338\n",
      "Epoch:  79  Training Loss:  189.761  Training Accuracy:  0.479303\n",
      "Epoch:  80  Training Loss:  186.231  Training Accuracy:  0.483325\n",
      "Epoch:  81  Training Loss:  182.901  Training Accuracy:  0.486415\n",
      "Epoch:  82  Training Loss:  179.437  Training Accuracy:  0.48968\n",
      "Epoch:  83  Training Loss:  176.349  Training Accuracy:  0.494111\n",
      "Epoch:  84  Training Loss:  173.155  Training Accuracy:  0.497609\n",
      "Epoch:  85  Training Loss:  170.121  Training Accuracy:  0.501691\n",
      "Epoch:  86  Training Loss:  167.015  Training Accuracy:  0.505189\n",
      "Epoch:  87  Training Loss:  164.074  Training Accuracy:  0.508045\n",
      "Epoch:  88  Training Loss:  161.261  Training Accuracy:  0.510611\n",
      "Epoch:  89  Training Loss:  158.516  Training Accuracy:  0.514167\n",
      "Epoch:  90  Training Loss:  155.731  Training Accuracy:  0.518423\n",
      "Epoch:  91  Training Loss:  153.124  Training Accuracy:  0.522213\n",
      "Epoch:  92  Training Loss:  150.494  Training Accuracy:  0.525011\n",
      "Epoch:  93  Training Loss:  147.97  Training Accuracy:  0.526994\n",
      "Epoch:  94  Training Loss:  145.266  Training Accuracy:  0.529501\n",
      "Epoch:  95  Training Loss:  142.93  Training Accuracy:  0.533407\n",
      "Epoch:  96  Training Loss:  140.461  Training Accuracy:  0.536439\n",
      "Epoch:  97  Training Loss:  138.073  Training Accuracy:  0.539296\n",
      "Epoch:  98  Training Loss:  135.668  Training Accuracy:  0.54256\n",
      "Epoch:  99  Training Loss:  133.481  Training Accuracy:  0.545067\n",
      "Epoch:  100  Training Loss:  131.321  Training Accuracy:  0.547574\n",
      "Epoch:  101  Training Loss:  129.054  Training Accuracy:  0.550781\n",
      "Epoch:  102  Training Loss:  126.862  Training Accuracy:  0.55358\n",
      "Epoch:  103  Training Loss:  124.8  Training Accuracy:  0.556028\n",
      "Epoch:  104  Training Loss:  122.791  Training Accuracy:  0.558768\n",
      "Epoch:  105  Training Loss:  120.878  Training Accuracy:  0.562033\n",
      "Epoch:  106  Training Loss:  118.852  Training Accuracy:  0.56454\n",
      "Epoch:  107  Training Loss:  116.921  Training Accuracy:  0.568038\n",
      "Epoch:  108  Training Loss:  115.086  Training Accuracy:  0.570196\n",
      "Epoch:  109  Training Loss:  113.202  Training Accuracy:  0.572469\n",
      "Epoch:  110  Training Loss:  111.533  Training Accuracy:  0.574802\n",
      "Epoch:  111  Training Loss:  109.607  Training Accuracy:  0.576725\n",
      "Epoch:  112  Training Loss:  108.057  Training Accuracy:  0.578708\n",
      "Epoch:  113  Training Loss:  106.366  Training Accuracy:  0.581098\n",
      "Epoch:  114  Training Loss:  104.726  Training Accuracy:  0.583722\n",
      "Epoch:  115  Training Loss:  103.101  Training Accuracy:  0.586345\n",
      "Epoch:  116  Training Loss:  101.557  Training Accuracy:  0.589144\n",
      "Epoch:  117  Training Loss:  100.013  Training Accuracy:  0.591418\n",
      "Epoch:  118  Training Loss:  98.5439  Training Accuracy:  0.593283\n",
      "Epoch:  119  Training Loss:  97.0034  Training Accuracy:  0.595441\n",
      "Epoch:  120  Training Loss:  95.5312  Training Accuracy:  0.597423\n",
      "Epoch:  121  Training Loss:  94.2331  Training Accuracy:  0.600046\n",
      "Epoch:  122  Training Loss:  92.7879  Training Accuracy:  0.602087\n",
      "Epoch:  123  Training Loss:  91.3376  Training Accuracy:  0.604477\n",
      "Epoch:  124  Training Loss:  90.0192  Training Accuracy:  0.60646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  88.6392  Training Accuracy:  0.608675\n",
      "Epoch:  126  Training Loss:  87.414  Training Accuracy:  0.610541\n",
      "Epoch:  127  Training Loss:  86.1898  Training Accuracy:  0.612173\n",
      "Epoch:  128  Training Loss:  84.9296  Training Accuracy:  0.614097\n",
      "Epoch:  129  Training Loss:  83.7392  Training Accuracy:  0.615671\n",
      "Epoch:  130  Training Loss:  82.7213  Training Accuracy:  0.61812\n",
      "Epoch:  131  Training Loss:  81.4955  Training Accuracy:  0.619986\n",
      "Epoch:  132  Training Loss:  80.3266  Training Accuracy:  0.621443\n",
      "Epoch:  133  Training Loss:  79.2622  Training Accuracy:  0.623542\n",
      "Epoch:  134  Training Loss:  78.2297  Training Accuracy:  0.624767\n",
      "Epoch:  135  Training Loss:  77.1068  Training Accuracy:  0.626865\n",
      "Epoch:  136  Training Loss:  76.1786  Training Accuracy:  0.628731\n",
      "Epoch:  137  Training Loss:  75.0837  Training Accuracy:  0.630597\n",
      "Epoch:  138  Training Loss:  74.1627  Training Accuracy:  0.632637\n",
      "Epoch:  139  Training Loss:  73.2109  Training Accuracy:  0.634853\n",
      "Epoch:  140  Training Loss:  72.226  Training Accuracy:  0.63631\n",
      "Epoch:  141  Training Loss:  71.3419  Training Accuracy:  0.638176\n",
      "Epoch:  142  Training Loss:  70.4904  Training Accuracy:  0.639809\n",
      "Epoch:  143  Training Loss:  69.5833  Training Accuracy:  0.641149\n",
      "Epoch:  144  Training Loss:  68.7679  Training Accuracy:  0.64284\n",
      "Epoch:  145  Training Loss:  67.9518  Training Accuracy:  0.644356\n",
      "Epoch:  146  Training Loss:  67.1018  Training Accuracy:  0.645522\n",
      "Epoch:  147  Training Loss:  66.3725  Training Accuracy:  0.647329\n",
      "Epoch:  148  Training Loss:  65.5285  Training Accuracy:  0.649312\n",
      "Epoch:  149  Training Loss:  64.7381  Training Accuracy:  0.650478\n",
      "Epoch:  150  Training Loss:  64.0004  Training Accuracy:  0.651994\n",
      "Epoch:  151  Training Loss:  63.1578  Training Accuracy:  0.654151\n",
      "Epoch:  152  Training Loss:  62.4559  Training Accuracy:  0.655492\n",
      "Epoch:  153  Training Loss:  61.6769  Training Accuracy:  0.656716\n",
      "Epoch:  154  Training Loss:  61.0347  Training Accuracy:  0.658465\n",
      "Epoch:  155  Training Loss:  60.2975  Training Accuracy:  0.660331\n",
      "Epoch:  156  Training Loss:  59.6701  Training Accuracy:  0.662371\n",
      "Epoch:  157  Training Loss:  58.9739  Training Accuracy:  0.664062\n",
      "Epoch:  158  Training Loss:  58.3055  Training Accuracy:  0.665403\n",
      "Epoch:  159  Training Loss:  57.6179  Training Accuracy:  0.666802\n",
      "Epoch:  160  Training Loss:  57.0047  Training Accuracy:  0.667968\n",
      "Epoch:  161  Training Loss:  56.3755  Training Accuracy:  0.669309\n",
      "Epoch:  162  Training Loss:  55.744  Training Accuracy:  0.670126\n",
      "Epoch:  163  Training Loss:  55.0723  Training Accuracy:  0.671233\n",
      "Epoch:  164  Training Loss:  54.543  Training Accuracy:  0.672574\n",
      "Epoch:  165  Training Loss:  53.9561  Training Accuracy:  0.673507\n",
      "Epoch:  166  Training Loss:  53.3619  Training Accuracy:  0.674731\n",
      "Epoch:  167  Training Loss:  52.772  Training Accuracy:  0.675723\n",
      "Epoch:  168  Training Loss:  52.2277  Training Accuracy:  0.677122\n",
      "Epoch:  169  Training Loss:  51.6921  Training Accuracy:  0.678346\n",
      "Epoch:  170  Training Loss:  51.1412  Training Accuracy:  0.679162\n",
      "Epoch:  171  Training Loss:  50.6295  Training Accuracy:  0.680387\n",
      "Epoch:  172  Training Loss:  50.1322  Training Accuracy:  0.682311\n",
      "Epoch:  173  Training Loss:  49.6231  Training Accuracy:  0.683302\n",
      "Epoch:  174  Training Loss:  49.0837  Training Accuracy:  0.684876\n",
      "Epoch:  175  Training Loss:  48.6429  Training Accuracy:  0.685925\n",
      "Epoch:  176  Training Loss:  48.1493  Training Accuracy:  0.687266\n",
      "Epoch:  177  Training Loss:  47.7379  Training Accuracy:  0.688374\n",
      "Epoch:  178  Training Loss:  47.2798  Training Accuracy:  0.689482\n",
      "Epoch:  179  Training Loss:  46.8346  Training Accuracy:  0.690415\n",
      "Epoch:  180  Training Loss:  46.3358  Training Accuracy:  0.691756\n",
      "Epoch:  181  Training Loss:  45.9502  Training Accuracy:  0.693563\n",
      "Epoch:  182  Training Loss:  45.5488  Training Accuracy:  0.694846\n",
      "Epoch:  183  Training Loss:  45.1333  Training Accuracy:  0.696187\n",
      "Epoch:  184  Training Loss:  44.6753  Training Accuracy:  0.69712\n",
      "Epoch:  185  Training Loss:  44.285  Training Accuracy:  0.698111\n",
      "Epoch:  186  Training Loss:  43.8911  Training Accuracy:  0.699277\n",
      "Epoch:  187  Training Loss:  43.4683  Training Accuracy:  0.700501\n",
      "Epoch:  188  Training Loss:  43.1335  Training Accuracy:  0.70225\n",
      "Epoch:  189  Training Loss:  42.7937  Training Accuracy:  0.703416\n",
      "Epoch:  190  Training Loss:  42.3864  Training Accuracy:  0.704582\n",
      "Epoch:  191  Training Loss:  42.0276  Training Accuracy:  0.705515\n",
      "Epoch:  192  Training Loss:  41.6537  Training Accuracy:  0.706564\n",
      "Epoch:  193  Training Loss:  41.27  Training Accuracy:  0.707964\n",
      "Epoch:  194  Training Loss:  40.9442  Training Accuracy:  0.709013\n",
      "Epoch:  195  Training Loss:  40.6203  Training Accuracy:  0.709771\n",
      "Epoch:  196  Training Loss:  40.2444  Training Accuracy:  0.710879\n",
      "Epoch:  197  Training Loss:  39.9284  Training Accuracy:  0.712103\n",
      "Epoch:  198  Training Loss:  39.6168  Training Accuracy:  0.712978\n",
      "Epoch:  199  Training Loss:  39.2414  Training Accuracy:  0.71461\n",
      "Epoch:  200  Training Loss:  38.9221  Training Accuracy:  0.71566\n",
      "Epoch:  201  Training Loss:  38.6225  Training Accuracy:  0.716942\n",
      "Epoch:  202  Training Loss:  38.27  Training Accuracy:  0.717875\n",
      "Epoch:  203  Training Loss:  37.9687  Training Accuracy:  0.719333\n",
      "Epoch:  204  Training Loss:  37.676  Training Accuracy:  0.721431\n",
      "Epoch:  205  Training Loss:  37.3644  Training Accuracy:  0.722539\n",
      "Epoch:  206  Training Loss:  37.0315  Training Accuracy:  0.723822\n",
      "Epoch:  207  Training Loss:  36.8063  Training Accuracy:  0.725046\n",
      "Epoch:  208  Training Loss:  36.4723  Training Accuracy:  0.726154\n",
      "Epoch:  209  Training Loss:  36.1968  Training Accuracy:  0.727553\n",
      "Epoch:  210  Training Loss:  35.8985  Training Accuracy:  0.728661\n",
      "Epoch:  211  Training Loss:  35.6434  Training Accuracy:  0.729944\n",
      "Epoch:  212  Training Loss:  35.3417  Training Accuracy:  0.731168\n",
      "Epoch:  213  Training Loss:  35.0996  Training Accuracy:  0.732159\n",
      "Epoch:  214  Training Loss:  34.7816  Training Accuracy:  0.733034\n",
      "Epoch:  215  Training Loss:  34.5294  Training Accuracy:  0.7342\n",
      "Epoch:  216  Training Loss:  34.2273  Training Accuracy:  0.734899\n",
      "Epoch:  217  Training Loss:  33.9959  Training Accuracy:  0.735832\n",
      "Epoch:  218  Training Loss:  33.7421  Training Accuracy:  0.736765\n",
      "Epoch:  219  Training Loss:  33.4685  Training Accuracy:  0.737931\n",
      "Epoch:  220  Training Loss:  33.2346  Training Accuracy:  0.738631\n",
      "Epoch:  221  Training Loss:  33.0126  Training Accuracy:  0.739505\n",
      "Epoch:  222  Training Loss:  32.7212  Training Accuracy:  0.741021\n",
      "Epoch:  223  Training Loss:  32.5569  Training Accuracy:  0.742595\n",
      "Epoch:  224  Training Loss:  32.2722  Training Accuracy:  0.743761\n",
      "Epoch:  225  Training Loss:  32.0667  Training Accuracy:  0.746035\n",
      "Epoch:  226  Training Loss:  31.7831  Training Accuracy:  0.746793\n",
      "Epoch:  227  Training Loss:  31.6116  Training Accuracy:  0.747842\n",
      "Epoch:  228  Training Loss:  31.3054  Training Accuracy:  0.749183\n",
      "Epoch:  229  Training Loss:  31.1079  Training Accuracy:  0.750058\n",
      "Epoch:  230  Training Loss:  30.8566  Training Accuracy:  0.751632\n",
      "Epoch:  231  Training Loss:  30.6289  Training Accuracy:  0.753323\n",
      "Epoch:  232  Training Loss:  30.436  Training Accuracy:  0.754722\n",
      "Epoch:  233  Training Loss:  30.1498  Training Accuracy:  0.756121\n",
      "Epoch:  234  Training Loss:  29.9454  Training Accuracy:  0.756938\n",
      "Epoch:  235  Training Loss:  29.7126  Training Accuracy:  0.75822\n",
      "Epoch:  236  Training Loss:  29.5413  Training Accuracy:  0.759328\n",
      "Epoch:  237  Training Loss:  29.292  Training Accuracy:  0.760436\n",
      "Epoch:  238  Training Loss:  29.1075  Training Accuracy:  0.761485\n",
      "Epoch:  239  Training Loss:  28.9093  Training Accuracy:  0.763176\n",
      "Epoch:  240  Training Loss:  28.7325  Training Accuracy:  0.763992\n",
      "Epoch:  241  Training Loss:  28.4733  Training Accuracy:  0.765333\n",
      "Epoch:  242  Training Loss:  28.4505  Training Accuracy:  0.766266\n",
      "Epoch:  243  Training Loss:  28.2089  Training Accuracy:  0.76749\n",
      "Epoch:  244  Training Loss:  27.9786  Training Accuracy:  0.768248\n",
      "Epoch:  245  Training Loss:  27.6713  Training Accuracy:  0.768948\n",
      "Epoch:  246  Training Loss:  27.5444  Training Accuracy:  0.769822\n",
      "Epoch:  247  Training Loss:  27.4634  Training Accuracy:  0.770755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  27.2196  Training Accuracy:  0.771397\n",
      "Epoch:  249  Training Loss:  27.0932  Training Accuracy:  0.772446\n",
      "Epoch:  250  Training Loss:  26.8858  Training Accuracy:  0.773262\n",
      "Epoch:  251  Training Loss:  26.7046  Training Accuracy:  0.773962\n",
      "Epoch:  252  Training Loss:  26.5294  Training Accuracy:  0.774836\n",
      "Epoch:  253  Training Loss:  26.3686  Training Accuracy:  0.776177\n",
      "Epoch:  254  Training Loss:  26.2173  Training Accuracy:  0.776935\n",
      "Epoch:  255  Training Loss:  26.0449  Training Accuracy:  0.77816\n",
      "Epoch:  256  Training Loss:  25.8954  Training Accuracy:  0.778859\n",
      "Epoch:  257  Training Loss:  25.7493  Training Accuracy:  0.779559\n",
      "Epoch:  258  Training Loss:  25.5792  Training Accuracy:  0.780433\n",
      "Epoch:  259  Training Loss:  25.3914  Training Accuracy:  0.780958\n",
      "Epoch:  260  Training Loss:  25.2668  Training Accuracy:  0.781366\n",
      "Epoch:  261  Training Loss:  25.0649  Training Accuracy:  0.782066\n",
      "Epoch:  262  Training Loss:  24.9602  Training Accuracy:  0.783174\n",
      "Epoch:  263  Training Loss:  24.77  Training Accuracy:  0.783873\n",
      "Epoch:  264  Training Loss:  24.5985  Training Accuracy:  0.784456\n",
      "Epoch:  265  Training Loss:  24.4901  Training Accuracy:  0.785564\n",
      "Epoch:  266  Training Loss:  24.3304  Training Accuracy:  0.786322\n",
      "Epoch:  267  Training Loss:  24.1489  Training Accuracy:  0.787138\n",
      "Epoch:  268  Training Loss:  24.0232  Training Accuracy:  0.787838\n",
      "Epoch:  269  Training Loss:  23.8772  Training Accuracy:  0.788654\n",
      "Epoch:  270  Training Loss:  23.7022  Training Accuracy:  0.789645\n",
      "Epoch:  271  Training Loss:  23.579  Training Accuracy:  0.790228\n",
      "Epoch:  272  Training Loss:  23.4172  Training Accuracy:  0.790928\n",
      "Epoch:  273  Training Loss:  23.2511  Training Accuracy:  0.791744\n",
      "Epoch:  274  Training Loss:  23.132  Training Accuracy:  0.792793\n",
      "Epoch:  275  Training Loss:  22.9747  Training Accuracy:  0.793959\n",
      "Epoch:  276  Training Loss:  22.816  Training Accuracy:  0.794543\n",
      "Epoch:  277  Training Loss:  22.722  Training Accuracy:  0.7953\n",
      "Epoch:  278  Training Loss:  22.5552  Training Accuracy:  0.796058\n",
      "Epoch:  279  Training Loss:  22.4265  Training Accuracy:  0.796583\n",
      "Epoch:  280  Training Loss:  22.3325  Training Accuracy:  0.797633\n",
      "Epoch:  281  Training Loss:  22.1583  Training Accuracy:  0.798274\n",
      "Epoch:  282  Training Loss:  22.0122  Training Accuracy:  0.79909\n",
      "Epoch:  283  Training Loss:  21.8869  Training Accuracy:  0.800081\n",
      "Epoch:  284  Training Loss:  21.7395  Training Accuracy:  0.800723\n",
      "Epoch:  285  Training Loss:  21.6196  Training Accuracy:  0.801655\n",
      "Epoch:  286  Training Loss:  21.4619  Training Accuracy:  0.802472\n",
      "Epoch:  287  Training Loss:  21.3358  Training Accuracy:  0.803055\n",
      "Epoch:  288  Training Loss:  21.1933  Training Accuracy:  0.803813\n",
      "Epoch:  289  Training Loss:  21.0605  Training Accuracy:  0.804395\n",
      "Epoch:  290  Training Loss:  20.9183  Training Accuracy:  0.805037\n",
      "Epoch:  291  Training Loss:  20.8011  Training Accuracy:  0.805911\n",
      "Epoch:  292  Training Loss:  20.6323  Training Accuracy:  0.806553\n",
      "Epoch:  293  Training Loss:  20.509  Training Accuracy:  0.807252\n",
      "Epoch:  294  Training Loss:  20.4095  Training Accuracy:  0.807894\n",
      "Epoch:  295  Training Loss:  20.249  Training Accuracy:  0.808477\n",
      "Epoch:  296  Training Loss:  20.0912  Training Accuracy:  0.809118\n",
      "Epoch:  297  Training Loss:  20.0079  Training Accuracy:  0.809643\n",
      "Epoch:  298  Training Loss:  19.8606  Training Accuracy:  0.810342\n",
      "Epoch:  299  Training Loss:  19.7286  Training Accuracy:  0.810925\n",
      "Epoch:  300  Training Loss:  19.6313  Training Accuracy:  0.811392\n",
      "Epoch:  301  Training Loss:  19.5111  Training Accuracy:  0.812033\n",
      "Epoch:  302  Training Loss:  19.3575  Training Accuracy:  0.812674\n",
      "Epoch:  303  Training Loss:  19.279  Training Accuracy:  0.813432\n",
      "Epoch:  304  Training Loss:  19.1508  Training Accuracy:  0.813899\n",
      "Epoch:  305  Training Loss:  19.0356  Training Accuracy:  0.814832\n",
      "Epoch:  306  Training Loss:  18.9082  Training Accuracy:  0.815356\n",
      "Epoch:  307  Training Loss:  18.7995  Training Accuracy:  0.815765\n",
      "Epoch:  308  Training Loss:  18.6789  Training Accuracy:  0.816639\n",
      "Epoch:  309  Training Loss:  18.5833  Training Accuracy:  0.81728\n",
      "Epoch:  310  Training Loss:  18.4291  Training Accuracy:  0.818155\n",
      "Epoch:  311  Training Loss:  18.3703  Training Accuracy:  0.819029\n",
      "Epoch:  312  Training Loss:  18.2564  Training Accuracy:  0.819846\n",
      "Epoch:  313  Training Loss:  18.15  Training Accuracy:  0.820545\n",
      "Epoch:  314  Training Loss:  18.0133  Training Accuracy:  0.821536\n",
      "Epoch:  315  Training Loss:  17.907  Training Accuracy:  0.822061\n",
      "Epoch:  316  Training Loss:  17.7893  Training Accuracy:  0.822469\n",
      "Epoch:  317  Training Loss:  17.6958  Training Accuracy:  0.822761\n",
      "Epoch:  318  Training Loss:  17.563  Training Accuracy:  0.823227\n",
      "Epoch:  319  Training Loss:  17.45  Training Accuracy:  0.824102\n",
      "Epoch:  320  Training Loss:  17.3456  Training Accuracy:  0.824801\n",
      "Epoch:  321  Training Loss:  17.225  Training Accuracy:  0.825326\n",
      "Epoch:  322  Training Loss:  17.0995  Training Accuracy:  0.826026\n",
      "Epoch:  323  Training Loss:  17.0039  Training Accuracy:  0.82655\n",
      "Epoch:  324  Training Loss:  16.9001  Training Accuracy:  0.827192\n",
      "Epoch:  325  Training Loss:  16.786  Training Accuracy:  0.827775\n",
      "Epoch:  326  Training Loss:  16.6679  Training Accuracy:  0.828358\n",
      "Epoch:  327  Training Loss:  16.5783  Training Accuracy:  0.829232\n",
      "Epoch:  328  Training Loss:  16.4682  Training Accuracy:  0.829932\n",
      "Epoch:  329  Training Loss:  16.4015  Training Accuracy:  0.830806\n",
      "Epoch:  330  Training Loss:  16.2559  Training Accuracy:  0.831331\n",
      "Epoch:  331  Training Loss:  16.1916  Training Accuracy:  0.832089\n",
      "Epoch:  332  Training Loss:  16.0595  Training Accuracy:  0.832556\n",
      "Epoch:  333  Training Loss:  16.0105  Training Accuracy:  0.83308\n",
      "Epoch:  334  Training Loss:  15.8944  Training Accuracy:  0.833663\n",
      "Epoch:  335  Training Loss:  15.7919  Training Accuracy:  0.834363\n",
      "Epoch:  336  Training Loss:  15.7039  Training Accuracy:  0.834888\n",
      "Epoch:  337  Training Loss:  15.6341  Training Accuracy:  0.835529\n",
      "Epoch:  338  Training Loss:  15.4985  Training Accuracy:  0.835937\n",
      "Epoch:  339  Training Loss:  15.4462  Training Accuracy:  0.836986\n",
      "Epoch:  340  Training Loss:  15.3233  Training Accuracy:  0.837744\n",
      "Epoch:  341  Training Loss:  15.2348  Training Accuracy:  0.838036\n",
      "Epoch:  342  Training Loss:  15.1543  Training Accuracy:  0.838619\n",
      "Epoch:  343  Training Loss:  15.0519  Training Accuracy:  0.839202\n",
      "Epoch:  344  Training Loss:  14.9173  Training Accuracy:  0.839727\n",
      "Epoch:  345  Training Loss:  14.8613  Training Accuracy:  0.840077\n",
      "Epoch:  346  Training Loss:  14.7553  Training Accuracy:  0.840426\n",
      "Epoch:  347  Training Loss:  14.692  Training Accuracy:  0.840893\n",
      "Epoch:  348  Training Loss:  14.567  Training Accuracy:  0.841942\n",
      "Epoch:  349  Training Loss:  14.4595  Training Accuracy:  0.84235\n",
      "Epoch:  350  Training Loss:  14.386  Training Accuracy:  0.842933\n",
      "Epoch:  351  Training Loss:  14.2959  Training Accuracy:  0.843167\n",
      "Epoch:  352  Training Loss:  14.1713  Training Accuracy:  0.8434\n",
      "Epoch:  353  Training Loss:  14.1198  Training Accuracy:  0.844333\n",
      "Epoch:  354  Training Loss:  14.0192  Training Accuracy:  0.845032\n",
      "Epoch:  355  Training Loss:  13.9134  Training Accuracy:  0.845615\n",
      "Epoch:  356  Training Loss:  13.8159  Training Accuracy:  0.846023\n",
      "Epoch:  357  Training Loss:  13.7236  Training Accuracy:  0.846781\n",
      "Epoch:  358  Training Loss:  13.6579  Training Accuracy:  0.847131\n",
      "Epoch:  359  Training Loss:  13.5633  Training Accuracy:  0.847714\n",
      "Epoch:  360  Training Loss:  13.4946  Training Accuracy:  0.848122\n",
      "Epoch:  361  Training Loss:  13.3847  Training Accuracy:  0.848472\n",
      "Epoch:  362  Training Loss:  13.3266  Training Accuracy:  0.848939\n",
      "Epoch:  363  Training Loss:  13.2437  Training Accuracy:  0.849347\n",
      "Epoch:  364  Training Loss:  13.1544  Training Accuracy:  0.849755\n",
      "Epoch:  365  Training Loss:  13.0687  Training Accuracy:  0.850338\n",
      "Epoch:  366  Training Loss:  12.9744  Training Accuracy:  0.850629\n",
      "Epoch:  367  Training Loss:  12.9115  Training Accuracy:  0.851329\n",
      "Epoch:  368  Training Loss:  12.8485  Training Accuracy:  0.851912\n",
      "Epoch:  369  Training Loss:  12.7501  Training Accuracy:  0.85197\n",
      "Epoch:  370  Training Loss:  12.6596  Training Accuracy:  0.852145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  12.6394  Training Accuracy:  0.853078\n",
      "Epoch:  372  Training Loss:  12.5174  Training Accuracy:  0.853369\n",
      "Epoch:  373  Training Loss:  12.4423  Training Accuracy:  0.853777\n",
      "Epoch:  374  Training Loss:  12.3966  Training Accuracy:  0.854361\n",
      "Epoch:  375  Training Loss:  12.2871  Training Accuracy:  0.855002\n",
      "Epoch:  376  Training Loss:  12.2461  Training Accuracy:  0.855585\n",
      "Epoch:  377  Training Loss:  12.1405  Training Accuracy:  0.855818\n",
      "Epoch:  378  Training Loss:  12.1005  Training Accuracy:  0.856226\n",
      "Epoch:  379  Training Loss:  11.9924  Training Accuracy:  0.856693\n",
      "Epoch:  380  Training Loss:  11.9417  Training Accuracy:  0.857217\n",
      "Epoch:  381  Training Loss:  11.8629  Training Accuracy:  0.857742\n",
      "Epoch:  382  Training Loss:  11.792  Training Accuracy:  0.8585\n",
      "Epoch:  383  Training Loss:  11.7246  Training Accuracy:  0.858792\n",
      "Epoch:  384  Training Loss:  11.6053  Training Accuracy:  0.859433\n",
      "Epoch:  385  Training Loss:  11.5691  Training Accuracy:  0.860016\n",
      "Epoch:  386  Training Loss:  11.4892  Training Accuracy:  0.860424\n",
      "Epoch:  387  Training Loss:  11.438  Training Accuracy:  0.860832\n",
      "Epoch:  388  Training Loss:  11.3309  Training Accuracy:  0.861182\n",
      "Epoch:  389  Training Loss:  11.3196  Training Accuracy:  0.861707\n",
      "Epoch:  390  Training Loss:  11.2086  Training Accuracy:  0.862698\n",
      "Epoch:  391  Training Loss:  11.1816  Training Accuracy:  0.862756\n",
      "Epoch:  392  Training Loss:  11.0479  Training Accuracy:  0.863397\n",
      "Epoch:  393  Training Loss:  11.0497  Training Accuracy:  0.863514\n",
      "Epoch:  394  Training Loss:  10.9431  Training Accuracy:  0.864447\n",
      "Epoch:  395  Training Loss:  10.9217  Training Accuracy:  0.864388\n",
      "Epoch:  396  Training Loss:  10.8234  Training Accuracy:  0.865147\n",
      "Epoch:  397  Training Loss:  10.7958  Training Accuracy:  0.865555\n",
      "Epoch:  398  Training Loss:  10.7007  Training Accuracy:  0.866079\n",
      "Epoch:  399  Training Loss:  10.6738  Training Accuracy:  0.866138\n",
      "Testing Accuracy: 0.794649\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 400\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  978.022  Training Accuracy:  0.0415112\n",
      "Epoch:  1  Training Loss:  863.345  Training Accuracy:  0.0532882\n",
      "Epoch:  2  Training Loss:  825.14  Training Accuracy:  0.0595266\n",
      "Epoch:  3  Training Loss:  792.727  Training Accuracy:  0.0706623\n",
      "Epoch:  4  Training Loss:  767.979  Training Accuracy:  0.079291\n",
      "Epoch:  5  Training Loss:  742.533  Training Accuracy:  0.0861707\n",
      "Epoch:  6  Training Loss:  722.482  Training Accuracy:  0.0922341\n",
      "Epoch:  7  Training Loss:  703.429  Training Accuracy:  0.100688\n",
      "Epoch:  8  Training Loss:  686.753  Training Accuracy:  0.108326\n",
      "Epoch:  9  Training Loss:  673.105  Training Accuracy:  0.115905\n",
      "Epoch:  10  Training Loss:  659.723  Training Accuracy:  0.125292\n",
      "Epoch:  11  Training Loss:  648.27  Training Accuracy:  0.134153\n",
      "Epoch:  12  Training Loss:  638.029  Training Accuracy:  0.14319\n",
      "Epoch:  13  Training Loss:  629.302  Training Accuracy:  0.150128\n",
      "Epoch:  14  Training Loss:  618.384  Training Accuracy:  0.157183\n",
      "Epoch:  15  Training Loss:  611.258  Training Accuracy:  0.164063\n",
      "Epoch:  16  Training Loss:  603.607  Training Accuracy:  0.169485\n",
      "Epoch:  17  Training Loss:  594.997  Training Accuracy:  0.17409\n",
      "Epoch:  18  Training Loss:  587.742  Training Accuracy:  0.179163\n",
      "Epoch:  19  Training Loss:  580.28  Training Accuracy:  0.182952\n",
      "Epoch:  20  Training Loss:  573.891  Training Accuracy:  0.187792\n",
      "Epoch:  21  Training Loss:  568.047  Training Accuracy:  0.192281\n",
      "Epoch:  22  Training Loss:  562.02  Training Accuracy:  0.196712\n",
      "Epoch:  23  Training Loss:  555.712  Training Accuracy:  0.20091\n",
      "Epoch:  24  Training Loss:  549.161  Training Accuracy:  0.205049\n",
      "Epoch:  25  Training Loss:  542.943  Training Accuracy:  0.20983\n",
      "Epoch:  26  Training Loss:  537.627  Training Accuracy:  0.213853\n",
      "Epoch:  27  Training Loss:  531.95  Training Accuracy:  0.2184\n",
      "Epoch:  28  Training Loss:  526.55  Training Accuracy:  0.22423\n",
      "Epoch:  29  Training Loss:  521.378  Training Accuracy:  0.229769\n",
      "Epoch:  30  Training Loss:  515.707  Training Accuracy:  0.236066\n",
      "Epoch:  31  Training Loss:  510.146  Training Accuracy:  0.241196\n",
      "Epoch:  32  Training Loss:  504.461  Training Accuracy:  0.246152\n",
      "Epoch:  33  Training Loss:  499.496  Training Accuracy:  0.251574\n",
      "Epoch:  34  Training Loss:  494.538  Training Accuracy:  0.256588\n",
      "Epoch:  35  Training Loss:  489.392  Training Accuracy:  0.261777\n",
      "Epoch:  36  Training Loss:  484.18  Training Accuracy:  0.267199\n",
      "Epoch:  37  Training Loss:  478.802  Training Accuracy:  0.273263\n",
      "Epoch:  38  Training Loss:  473.532  Training Accuracy:  0.279734\n",
      "Epoch:  39  Training Loss:  468.352  Training Accuracy:  0.286905\n",
      "Epoch:  40  Training Loss:  463.473  Training Accuracy:  0.294135\n",
      "Epoch:  41  Training Loss:  459.116  Training Accuracy:  0.300606\n",
      "Epoch:  42  Training Loss:  454.6  Training Accuracy:  0.305562\n",
      "Epoch:  43  Training Loss:  449.638  Training Accuracy:  0.310168\n",
      "Epoch:  44  Training Loss:  444.588  Training Accuracy:  0.314774\n",
      "Epoch:  45  Training Loss:  439.386  Training Accuracy:  0.320895\n",
      "Epoch:  46  Training Loss:  434.452  Training Accuracy:  0.324977\n",
      "Epoch:  47  Training Loss:  429.902  Training Accuracy:  0.329641\n",
      "Epoch:  48  Training Loss:  425.054  Training Accuracy:  0.334713\n",
      "Epoch:  49  Training Loss:  420.352  Training Accuracy:  0.339494\n",
      "Epoch:  50  Training Loss:  415.962  Training Accuracy:  0.343867\n",
      "Epoch:  51  Training Loss:  411.281  Training Accuracy:  0.347831\n",
      "Epoch:  52  Training Loss:  407.193  Training Accuracy:  0.35302\n",
      "Epoch:  53  Training Loss:  402.516  Training Accuracy:  0.35885\n",
      "Epoch:  54  Training Loss:  397.911  Training Accuracy:  0.363864\n",
      "Epoch:  55  Training Loss:  393.338  Training Accuracy:  0.368645\n",
      "Epoch:  56  Training Loss:  389.142  Training Accuracy:  0.372959\n",
      "Epoch:  57  Training Loss:  384.244  Training Accuracy:  0.37844\n",
      "Epoch:  58  Training Loss:  379.595  Training Accuracy:  0.382638\n",
      "Epoch:  59  Training Loss:  375.234  Training Accuracy:  0.386602\n",
      "Epoch:  60  Training Loss:  370.526  Training Accuracy:  0.390975\n",
      "Epoch:  61  Training Loss:  366.012  Training Accuracy:  0.395814\n",
      "Epoch:  62  Training Loss:  362.03  Training Accuracy:  0.400711\n",
      "Epoch:  63  Training Loss:  357.228  Training Accuracy:  0.405725\n",
      "Epoch:  64  Training Loss:  352.445  Training Accuracy:  0.410389\n",
      "Epoch:  65  Training Loss:  347.993  Training Accuracy:  0.415112\n",
      "Epoch:  66  Training Loss:  343.671  Training Accuracy:  0.419018\n",
      "Epoch:  67  Training Loss:  339.241  Training Accuracy:  0.422749\n",
      "Epoch:  68  Training Loss:  334.903  Training Accuracy:  0.42718\n",
      "Epoch:  69  Training Loss:  330.277  Training Accuracy:  0.43132\n",
      "Epoch:  70  Training Loss:  325.959  Training Accuracy:  0.435343\n",
      "Epoch:  71  Training Loss:  321.737  Training Accuracy:  0.439482\n",
      "Epoch:  72  Training Loss:  317.318  Training Accuracy:  0.443913\n",
      "Epoch:  73  Training Loss:  313.415  Training Accuracy:  0.447878\n",
      "Epoch:  74  Training Loss:  309.419  Training Accuracy:  0.451551\n",
      "Epoch:  75  Training Loss:  305.527  Training Accuracy:  0.45534\n",
      "Epoch:  76  Training Loss:  301.625  Training Accuracy:  0.458605\n",
      "Epoch:  77  Training Loss:  297.718  Training Accuracy:  0.462162\n",
      "Epoch:  78  Training Loss:  293.343  Training Accuracy:  0.466476\n",
      "Epoch:  79  Training Loss:  289.135  Training Accuracy:  0.470324\n",
      "Epoch:  80  Training Loss:  285.098  Training Accuracy:  0.474347\n",
      "Epoch:  81  Training Loss:  281.062  Training Accuracy:  0.477845\n",
      "Epoch:  82  Training Loss:  277.332  Training Accuracy:  0.481052\n",
      "Epoch:  83  Training Loss:  273.265  Training Accuracy:  0.484025\n",
      "Epoch:  84  Training Loss:  269.972  Training Accuracy:  0.487465\n",
      "Epoch:  85  Training Loss:  265.763  Training Accuracy:  0.49038\n",
      "Epoch:  86  Training Loss:  261.889  Training Accuracy:  0.493761\n",
      "Epoch:  87  Training Loss:  257.953  Training Accuracy:  0.496735\n",
      "Epoch:  88  Training Loss:  254.475  Training Accuracy:  0.499825\n",
      "Epoch:  89  Training Loss:  250.86  Training Accuracy:  0.502915\n",
      "Epoch:  90  Training Loss:  247.048  Training Accuracy:  0.505364\n",
      "Epoch:  91  Training Loss:  243.773  Training Accuracy:  0.508629\n",
      "Epoch:  92  Training Loss:  240.518  Training Accuracy:  0.512127\n",
      "Epoch:  93  Training Loss:  236.857  Training Accuracy:  0.514226\n",
      "Epoch:  94  Training Loss:  233.574  Training Accuracy:  0.517782\n",
      "Epoch:  95  Training Loss:  230.365  Training Accuracy:  0.520989\n",
      "Epoch:  96  Training Loss:  226.977  Training Accuracy:  0.523087\n",
      "Epoch:  97  Training Loss:  223.712  Training Accuracy:  0.525653\n",
      "Epoch:  98  Training Loss:  220.378  Training Accuracy:  0.528101\n",
      "Epoch:  99  Training Loss:  217.071  Training Accuracy:  0.5302\n",
      "Epoch:  100  Training Loss:  213.595  Training Accuracy:  0.533116\n",
      "Epoch:  101  Training Loss:  210.521  Training Accuracy:  0.535681\n",
      "Epoch:  102  Training Loss:  207.385  Training Accuracy:  0.538654\n",
      "Epoch:  103  Training Loss:  204.638  Training Accuracy:  0.541861\n",
      "Epoch:  104  Training Loss:  201.616  Training Accuracy:  0.543726\n",
      "Epoch:  105  Training Loss:  198.894  Training Accuracy:  0.546292\n",
      "Epoch:  106  Training Loss:  195.974  Training Accuracy:  0.54874\n",
      "Epoch:  107  Training Loss:  193.416  Training Accuracy:  0.551539\n",
      "Epoch:  108  Training Loss:  190.538  Training Accuracy:  0.554337\n",
      "Epoch:  109  Training Loss:  187.926  Training Accuracy:  0.556436\n",
      "Epoch:  110  Training Loss:  185.288  Training Accuracy:  0.55906\n",
      "Epoch:  111  Training Loss:  182.746  Training Accuracy:  0.561159\n",
      "Epoch:  112  Training Loss:  180.045  Training Accuracy:  0.562208\n",
      "Epoch:  113  Training Loss:  177.71  Training Accuracy:  0.565123\n",
      "Epoch:  114  Training Loss:  174.941  Training Accuracy:  0.567222\n",
      "Epoch:  115  Training Loss:  172.688  Training Accuracy:  0.568796\n",
      "Epoch:  116  Training Loss:  170.049  Training Accuracy:  0.570954\n",
      "Epoch:  117  Training Loss:  167.886  Training Accuracy:  0.572644\n",
      "Epoch:  118  Training Loss:  165.303  Training Accuracy:  0.57416\n",
      "Epoch:  119  Training Loss:  163.211  Training Accuracy:  0.576667\n",
      "Epoch:  120  Training Loss:  160.887  Training Accuracy:  0.578883\n",
      "Epoch:  121  Training Loss:  158.651  Training Accuracy:  0.581215\n",
      "Epoch:  122  Training Loss:  156.369  Training Accuracy:  0.583255\n",
      "Epoch:  123  Training Loss:  154.438  Training Accuracy:  0.585121\n",
      "Epoch:  124  Training Loss:  152.046  Training Accuracy:  0.587045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  150.215  Training Accuracy:  0.589319\n",
      "Epoch:  126  Training Loss:  147.947  Training Accuracy:  0.591942\n",
      "Epoch:  127  Training Loss:  145.988  Training Accuracy:  0.593575\n",
      "Epoch:  128  Training Loss:  143.851  Training Accuracy:  0.595499\n",
      "Epoch:  129  Training Loss:  141.91  Training Accuracy:  0.597889\n",
      "Epoch:  130  Training Loss:  139.926  Training Accuracy:  0.59923\n",
      "Epoch:  131  Training Loss:  138.009  Training Accuracy:  0.601212\n",
      "Epoch:  132  Training Loss:  136.102  Training Accuracy:  0.602553\n",
      "Epoch:  133  Training Loss:  134.312  Training Accuracy:  0.604361\n",
      "Epoch:  134  Training Loss:  132.525  Training Accuracy:  0.605935\n",
      "Epoch:  135  Training Loss:  130.666  Training Accuracy:  0.607684\n",
      "Epoch:  136  Training Loss:  128.988  Training Accuracy:  0.609142\n",
      "Epoch:  137  Training Loss:  127.151  Training Accuracy:  0.611066\n",
      "Epoch:  138  Training Loss:  125.41  Training Accuracy:  0.613106\n",
      "Epoch:  139  Training Loss:  123.721  Training Accuracy:  0.61503\n",
      "Epoch:  140  Training Loss:  121.883  Training Accuracy:  0.616546\n",
      "Epoch:  141  Training Loss:  120.339  Training Accuracy:  0.61882\n",
      "Epoch:  142  Training Loss:  118.619  Training Accuracy:  0.620044\n",
      "Epoch:  143  Training Loss:  116.959  Training Accuracy:  0.620744\n",
      "Epoch:  144  Training Loss:  115.386  Training Accuracy:  0.622434\n",
      "Epoch:  145  Training Loss:  113.925  Training Accuracy:  0.624242\n",
      "Epoch:  146  Training Loss:  112.271  Training Accuracy:  0.625408\n",
      "Epoch:  147  Training Loss:  110.873  Training Accuracy:  0.626399\n",
      "Epoch:  148  Training Loss:  109.259  Training Accuracy:  0.627857\n",
      "Epoch:  149  Training Loss:  107.843  Training Accuracy:  0.629256\n",
      "Epoch:  150  Training Loss:  106.266  Training Accuracy:  0.631005\n",
      "Epoch:  151  Training Loss:  104.957  Training Accuracy:  0.632462\n",
      "Epoch:  152  Training Loss:  103.435  Training Accuracy:  0.634153\n",
      "Epoch:  153  Training Loss:  102.287  Training Accuracy:  0.636485\n",
      "Epoch:  154  Training Loss:  100.873  Training Accuracy:  0.637593\n",
      "Epoch:  155  Training Loss:  99.6474  Training Accuracy:  0.639109\n",
      "Epoch:  156  Training Loss:  98.3009  Training Accuracy:  0.640508\n",
      "Epoch:  157  Training Loss:  97.0622  Training Accuracy:  0.64249\n",
      "Epoch:  158  Training Loss:  95.827  Training Accuracy:  0.644239\n",
      "Epoch:  159  Training Loss:  94.7122  Training Accuracy:  0.645988\n",
      "Epoch:  160  Training Loss:  93.5012  Training Accuracy:  0.647854\n",
      "Epoch:  161  Training Loss:  92.33  Training Accuracy:  0.649137\n",
      "Epoch:  162  Training Loss:  91.0959  Training Accuracy:  0.650303\n",
      "Epoch:  163  Training Loss:  90.0726  Training Accuracy:  0.652227\n",
      "Epoch:  164  Training Loss:  88.8487  Training Accuracy:  0.653743\n",
      "Epoch:  165  Training Loss:  87.7773  Training Accuracy:  0.655492\n",
      "Epoch:  166  Training Loss:  86.711  Training Accuracy:  0.657766\n",
      "Epoch:  167  Training Loss:  85.5725  Training Accuracy:  0.65934\n",
      "Epoch:  168  Training Loss:  84.494  Training Accuracy:  0.660389\n",
      "Epoch:  169  Training Loss:  83.4931  Training Accuracy:  0.66173\n",
      "Epoch:  170  Training Loss:  82.4435  Training Accuracy:  0.663188\n",
      "Epoch:  171  Training Loss:  81.4318  Training Accuracy:  0.665228\n",
      "Epoch:  172  Training Loss:  80.4868  Training Accuracy:  0.666453\n",
      "Epoch:  173  Training Loss:  79.5538  Training Accuracy:  0.668318\n",
      "Epoch:  174  Training Loss:  78.5948  Training Accuracy:  0.669426\n",
      "Epoch:  175  Training Loss:  77.624  Training Accuracy:  0.670767\n",
      "Epoch:  176  Training Loss:  76.7202  Training Accuracy:  0.671991\n",
      "Epoch:  177  Training Loss:  75.7615  Training Accuracy:  0.674032\n",
      "Epoch:  178  Training Loss:  74.8601  Training Accuracy:  0.675256\n",
      "Epoch:  179  Training Loss:  73.9663  Training Accuracy:  0.676772\n",
      "Epoch:  180  Training Loss:  73.1135  Training Accuracy:  0.678055\n",
      "Epoch:  181  Training Loss:  72.1674  Training Accuracy:  0.679046\n",
      "Epoch:  182  Training Loss:  71.3111  Training Accuracy:  0.68062\n",
      "Epoch:  183  Training Loss:  70.4559  Training Accuracy:  0.681728\n",
      "Epoch:  184  Training Loss:  69.5928  Training Accuracy:  0.68301\n",
      "Epoch:  185  Training Loss:  68.7774  Training Accuracy:  0.68441\n",
      "Epoch:  186  Training Loss:  68.0069  Training Accuracy:  0.685401\n",
      "Epoch:  187  Training Loss:  67.2073  Training Accuracy:  0.687208\n",
      "Epoch:  188  Training Loss:  66.3587  Training Accuracy:  0.688024\n",
      "Epoch:  189  Training Loss:  65.6779  Training Accuracy:  0.689016\n",
      "Epoch:  190  Training Loss:  64.9325  Training Accuracy:  0.689948\n",
      "Epoch:  191  Training Loss:  64.1552  Training Accuracy:  0.691756\n",
      "Epoch:  192  Training Loss:  63.3863  Training Accuracy:  0.692689\n",
      "Epoch:  193  Training Loss:  62.7107  Training Accuracy:  0.694496\n",
      "Epoch:  194  Training Loss:  61.9846  Training Accuracy:  0.696012\n",
      "Epoch:  195  Training Loss:  61.2858  Training Accuracy:  0.697528\n",
      "Epoch:  196  Training Loss:  60.5911  Training Accuracy:  0.69881\n",
      "Epoch:  197  Training Loss:  59.9167  Training Accuracy:  0.700384\n",
      "Epoch:  198  Training Loss:  59.2508  Training Accuracy:  0.701492\n",
      "Epoch:  199  Training Loss:  58.5757  Training Accuracy:  0.702658\n",
      "Epoch:  200  Training Loss:  57.8874  Training Accuracy:  0.703591\n",
      "Epoch:  201  Training Loss:  57.2701  Training Accuracy:  0.704641\n",
      "Epoch:  202  Training Loss:  56.5797  Training Accuracy:  0.705748\n",
      "Epoch:  203  Training Loss:  56.023  Training Accuracy:  0.706856\n",
      "Epoch:  204  Training Loss:  55.3231  Training Accuracy:  0.708255\n",
      "Epoch:  205  Training Loss:  54.6846  Training Accuracy:  0.709013\n",
      "Epoch:  206  Training Loss:  54.0677  Training Accuracy:  0.710587\n",
      "Epoch:  207  Training Loss:  53.4246  Training Accuracy:  0.711578\n",
      "Epoch:  208  Training Loss:  52.8112  Training Accuracy:  0.712803\n",
      "Epoch:  209  Training Loss:  52.1961  Training Accuracy:  0.714202\n",
      "Epoch:  210  Training Loss:  51.7097  Training Accuracy:  0.715193\n",
      "Epoch:  211  Training Loss:  51.1358  Training Accuracy:  0.716184\n",
      "Epoch:  212  Training Loss:  50.593  Training Accuracy:  0.717642\n",
      "Epoch:  213  Training Loss:  50.0336  Training Accuracy:  0.718575\n",
      "Epoch:  214  Training Loss:  49.5253  Training Accuracy:  0.720091\n",
      "Epoch:  215  Training Loss:  48.9661  Training Accuracy:  0.721257\n",
      "Epoch:  216  Training Loss:  48.4719  Training Accuracy:  0.722714\n",
      "Epoch:  217  Training Loss:  47.9387  Training Accuracy:  0.723764\n",
      "Epoch:  218  Training Loss:  47.4574  Training Accuracy:  0.725221\n",
      "Epoch:  219  Training Loss:  46.9284  Training Accuracy:  0.726329\n",
      "Epoch:  220  Training Loss:  46.452  Training Accuracy:  0.727903\n",
      "Epoch:  221  Training Loss:  45.928  Training Accuracy:  0.728428\n",
      "Epoch:  222  Training Loss:  45.4893  Training Accuracy:  0.729594\n",
      "Epoch:  223  Training Loss:  45.0209  Training Accuracy:  0.730702\n",
      "Epoch:  224  Training Loss:  44.5816  Training Accuracy:  0.732509\n",
      "Epoch:  225  Training Loss:  44.1342  Training Accuracy:  0.73385\n",
      "Epoch:  226  Training Loss:  43.6923  Training Accuracy:  0.734491\n",
      "Epoch:  227  Training Loss:  43.2366  Training Accuracy:  0.735716\n",
      "Epoch:  228  Training Loss:  42.7812  Training Accuracy:  0.736823\n",
      "Epoch:  229  Training Loss:  42.4156  Training Accuracy:  0.737756\n",
      "Epoch:  230  Training Loss:  41.9194  Training Accuracy:  0.739097\n",
      "Epoch:  231  Training Loss:  41.5554  Training Accuracy:  0.739855\n",
      "Epoch:  232  Training Loss:  41.1523  Training Accuracy:  0.741429\n",
      "Epoch:  233  Training Loss:  40.7301  Training Accuracy:  0.742595\n",
      "Epoch:  234  Training Loss:  40.37  Training Accuracy:  0.743703\n",
      "Epoch:  235  Training Loss:  39.909  Training Accuracy:  0.745102\n",
      "Epoch:  236  Training Loss:  39.5887  Training Accuracy:  0.746385\n",
      "Epoch:  237  Training Loss:  39.2073  Training Accuracy:  0.747551\n",
      "Epoch:  238  Training Loss:  38.8699  Training Accuracy:  0.748659\n",
      "Epoch:  239  Training Loss:  38.4966  Training Accuracy:  0.749358\n",
      "Epoch:  240  Training Loss:  38.1496  Training Accuracy:  0.750291\n",
      "Epoch:  241  Training Loss:  37.8139  Training Accuracy:  0.75169\n",
      "Epoch:  242  Training Loss:  37.5103  Training Accuracy:  0.752507\n",
      "Epoch:  243  Training Loss:  37.1532  Training Accuracy:  0.753265\n",
      "Epoch:  244  Training Loss:  36.8422  Training Accuracy:  0.754431\n",
      "Epoch:  245  Training Loss:  36.5086  Training Accuracy:  0.755422\n",
      "Epoch:  246  Training Loss:  36.1844  Training Accuracy:  0.756355\n",
      "Epoch:  247  Training Loss:  35.9021  Training Accuracy:  0.757637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  35.6168  Training Accuracy:  0.758395\n",
      "Epoch:  249  Training Loss:  35.2832  Training Accuracy:  0.759095\n",
      "Epoch:  250  Training Loss:  34.9805  Training Accuracy:  0.760319\n",
      "Epoch:  251  Training Loss:  34.6236  Training Accuracy:  0.761252\n",
      "Epoch:  252  Training Loss:  34.379  Training Accuracy:  0.762185\n",
      "Epoch:  253  Training Loss:  34.0925  Training Accuracy:  0.763176\n",
      "Epoch:  254  Training Loss:  33.8145  Training Accuracy:  0.763701\n",
      "Epoch:  255  Training Loss:  33.5256  Training Accuracy:  0.764692\n",
      "Epoch:  256  Training Loss:  33.2529  Training Accuracy:  0.765566\n",
      "Epoch:  257  Training Loss:  32.9601  Training Accuracy:  0.766324\n",
      "Epoch:  258  Training Loss:  32.6998  Training Accuracy:  0.767082\n",
      "Epoch:  259  Training Loss:  32.4422  Training Accuracy:  0.767957\n",
      "Epoch:  260  Training Loss:  32.2066  Training Accuracy:  0.768948\n",
      "Epoch:  261  Training Loss:  31.9254  Training Accuracy:  0.769881\n",
      "Epoch:  262  Training Loss:  31.6642  Training Accuracy:  0.771047\n",
      "Epoch:  263  Training Loss:  31.4039  Training Accuracy:  0.771571\n",
      "Epoch:  264  Training Loss:  31.1424  Training Accuracy:  0.772388\n",
      "Epoch:  265  Training Loss:  30.8639  Training Accuracy:  0.773437\n",
      "Epoch:  266  Training Loss:  30.6199  Training Accuracy:  0.774195\n",
      "Epoch:  267  Training Loss:  30.3467  Training Accuracy:  0.775419\n",
      "Epoch:  268  Training Loss:  30.092  Training Accuracy:  0.776061\n",
      "Epoch:  269  Training Loss:  29.8427  Training Accuracy:  0.776644\n",
      "Epoch:  270  Training Loss:  29.6063  Training Accuracy:  0.777751\n",
      "Epoch:  271  Training Loss:  29.3725  Training Accuracy:  0.778801\n",
      "Epoch:  272  Training Loss:  29.1684  Training Accuracy:  0.779734\n",
      "Epoch:  273  Training Loss:  28.8953  Training Accuracy:  0.780375\n",
      "Epoch:  274  Training Loss:  28.6904  Training Accuracy:  0.781424\n",
      "Epoch:  275  Training Loss:  28.4386  Training Accuracy:  0.782241\n",
      "Epoch:  276  Training Loss:  28.2351  Training Accuracy:  0.782824\n",
      "Epoch:  277  Training Loss:  27.9854  Training Accuracy:  0.783582\n",
      "Epoch:  278  Training Loss:  27.809  Training Accuracy:  0.784281\n",
      "Epoch:  279  Training Loss:  27.5629  Training Accuracy:  0.784981\n",
      "Epoch:  280  Training Loss:  27.3686  Training Accuracy:  0.786264\n",
      "Epoch:  281  Training Loss:  27.1792  Training Accuracy:  0.787022\n",
      "Epoch:  282  Training Loss:  26.9634  Training Accuracy:  0.788013\n",
      "Epoch:  283  Training Loss:  26.7734  Training Accuracy:  0.788829\n",
      "Epoch:  284  Training Loss:  26.5756  Training Accuracy:  0.789762\n",
      "Epoch:  285  Training Loss:  26.3669  Training Accuracy:  0.79017\n",
      "Epoch:  286  Training Loss:  26.171  Training Accuracy:  0.791336\n",
      "Epoch:  287  Training Loss:  25.9978  Training Accuracy:  0.79221\n",
      "Epoch:  288  Training Loss:  25.7899  Training Accuracy:  0.793318\n",
      "Epoch:  289  Training Loss:  25.6159  Training Accuracy:  0.794193\n",
      "Epoch:  290  Training Loss:  25.4124  Training Accuracy:  0.795067\n",
      "Epoch:  291  Training Loss:  25.265  Training Accuracy:  0.795883\n",
      "Epoch:  292  Training Loss:  25.0797  Training Accuracy:  0.796816\n",
      "Epoch:  293  Training Loss:  24.9083  Training Accuracy:  0.797691\n",
      "Epoch:  294  Training Loss:  24.713  Training Accuracy:  0.79909\n",
      "Epoch:  295  Training Loss:  24.5333  Training Accuracy:  0.799848\n",
      "Epoch:  296  Training Loss:  24.3829  Training Accuracy:  0.800606\n",
      "Epoch:  297  Training Loss:  24.2269  Training Accuracy:  0.801422\n",
      "Epoch:  298  Training Loss:  24.0307  Training Accuracy:  0.802297\n",
      "Epoch:  299  Training Loss:  23.8807  Training Accuracy:  0.803113\n",
      "Epoch:  300  Training Loss:  23.6824  Training Accuracy:  0.804337\n",
      "Epoch:  301  Training Loss:  23.5819  Training Accuracy:  0.805212\n",
      "Epoch:  302  Training Loss:  23.3575  Training Accuracy:  0.805911\n",
      "Epoch:  303  Training Loss:  23.2668  Training Accuracy:  0.806961\n",
      "Epoch:  304  Training Loss:  23.044  Training Accuracy:  0.807777\n",
      "Epoch:  305  Training Loss:  22.9352  Training Accuracy:  0.808244\n",
      "Epoch:  306  Training Loss:  22.7365  Training Accuracy:  0.809235\n",
      "Epoch:  307  Training Loss:  22.6427  Training Accuracy:  0.809584\n",
      "Epoch:  308  Training Loss:  22.4167  Training Accuracy:  0.810226\n",
      "Epoch:  309  Training Loss:  22.3408  Training Accuracy:  0.810634\n",
      "Epoch:  310  Training Loss:  22.1461  Training Accuracy:  0.811858\n",
      "Epoch:  311  Training Loss:  22.0524  Training Accuracy:  0.812675\n",
      "Epoch:  312  Training Loss:  21.8556  Training Accuracy:  0.813666\n",
      "Epoch:  313  Training Loss:  21.7798  Training Accuracy:  0.814132\n",
      "Epoch:  314  Training Loss:  21.6028  Training Accuracy:  0.81454\n",
      "Epoch:  315  Training Loss:  21.5012  Training Accuracy:  0.81524\n",
      "Epoch:  316  Training Loss:  21.3379  Training Accuracy:  0.816464\n",
      "Epoch:  317  Training Loss:  21.2152  Training Accuracy:  0.816989\n",
      "Epoch:  318  Training Loss:  21.0449  Training Accuracy:  0.817747\n",
      "Epoch:  319  Training Loss:  20.9589  Training Accuracy:  0.818563\n",
      "Epoch:  320  Training Loss:  20.7583  Training Accuracy:  0.819204\n",
      "Epoch:  321  Training Loss:  20.6711  Training Accuracy:  0.819671\n",
      "Epoch:  322  Training Loss:  20.5105  Training Accuracy:  0.820254\n",
      "Epoch:  323  Training Loss:  20.4167  Training Accuracy:  0.820487\n",
      "Epoch:  324  Training Loss:  20.2283  Training Accuracy:  0.821128\n",
      "Epoch:  325  Training Loss:  20.1561  Training Accuracy:  0.821711\n",
      "Epoch:  326  Training Loss:  19.9852  Training Accuracy:  0.822061\n",
      "Epoch:  327  Training Loss:  19.9029  Training Accuracy:  0.822994\n",
      "Epoch:  328  Training Loss:  19.7496  Training Accuracy:  0.823694\n",
      "Epoch:  329  Training Loss:  19.6461  Training Accuracy:  0.824393\n",
      "Epoch:  330  Training Loss:  19.4703  Training Accuracy:  0.825209\n",
      "Epoch:  331  Training Loss:  19.4175  Training Accuracy:  0.825909\n",
      "Epoch:  332  Training Loss:  19.2113  Training Accuracy:  0.826725\n",
      "Epoch:  333  Training Loss:  19.1502  Training Accuracy:  0.827425\n",
      "Epoch:  334  Training Loss:  18.9924  Training Accuracy:  0.828183\n",
      "Epoch:  335  Training Loss:  18.8891  Training Accuracy:  0.829174\n",
      "Epoch:  336  Training Loss:  18.7227  Training Accuracy:  0.829815\n",
      "Epoch:  337  Training Loss:  18.6819  Training Accuracy:  0.830632\n",
      "Epoch:  338  Training Loss:  18.5017  Training Accuracy:  0.831215\n",
      "Epoch:  339  Training Loss:  18.4358  Training Accuracy:  0.831972\n",
      "Epoch:  340  Training Loss:  18.2742  Training Accuracy:  0.83273\n",
      "Epoch:  341  Training Loss:  18.1803  Training Accuracy:  0.83308\n",
      "Epoch:  342  Training Loss:  18.0261  Training Accuracy:  0.83413\n",
      "Epoch:  343  Training Loss:  17.9748  Training Accuracy:  0.834713\n",
      "Epoch:  344  Training Loss:  17.798  Training Accuracy:  0.835354\n",
      "Epoch:  345  Training Loss:  17.7488  Training Accuracy:  0.835879\n",
      "Epoch:  346  Training Loss:  17.5987  Training Accuracy:  0.836637\n",
      "Epoch:  347  Training Loss:  17.4768  Training Accuracy:  0.837161\n",
      "Epoch:  348  Training Loss:  17.4014  Training Accuracy:  0.837278\n",
      "Epoch:  349  Training Loss:  17.2776  Training Accuracy:  0.837861\n",
      "Epoch:  350  Training Loss:  17.1726  Training Accuracy:  0.838269\n",
      "Epoch:  351  Training Loss:  17.0487  Training Accuracy:  0.838794\n",
      "Epoch:  352  Training Loss:  16.9628  Training Accuracy:  0.83926\n",
      "Epoch:  353  Training Loss:  16.8449  Training Accuracy:  0.83996\n",
      "Epoch:  354  Training Loss:  16.7447  Training Accuracy:  0.840718\n",
      "Epoch:  355  Training Loss:  16.62  Training Accuracy:  0.841359\n",
      "Epoch:  356  Training Loss:  16.5587  Training Accuracy:  0.842292\n",
      "Epoch:  357  Training Loss:  16.4115  Training Accuracy:  0.842642\n",
      "Epoch:  358  Training Loss:  16.3487  Training Accuracy:  0.842875\n",
      "Epoch:  359  Training Loss:  16.2171  Training Accuracy:  0.843225\n",
      "Epoch:  360  Training Loss:  16.1503  Training Accuracy:  0.844041\n",
      "Epoch:  361  Training Loss:  16.0504  Training Accuracy:  0.844449\n",
      "Epoch:  362  Training Loss:  15.9726  Training Accuracy:  0.844682\n",
      "Epoch:  363  Training Loss:  15.8561  Training Accuracy:  0.845265\n",
      "Epoch:  364  Training Loss:  15.8152  Training Accuracy:  0.845324\n",
      "Epoch:  365  Training Loss:  15.6837  Training Accuracy:  0.845848\n",
      "Epoch:  366  Training Loss:  15.6449  Training Accuracy:  0.846257\n",
      "Epoch:  367  Training Loss:  15.5261  Training Accuracy:  0.846898\n",
      "Epoch:  368  Training Loss:  15.4749  Training Accuracy:  0.847248\n",
      "Epoch:  369  Training Loss:  15.3562  Training Accuracy:  0.848181\n",
      "Epoch:  370  Training Loss:  15.3237  Training Accuracy:  0.84853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  15.195  Training Accuracy:  0.849055\n",
      "Epoch:  372  Training Loss:  15.137  Training Accuracy:  0.849463\n",
      "Epoch:  373  Training Loss:  15.0828  Training Accuracy:  0.849638\n",
      "Epoch:  374  Training Loss:  14.974  Training Accuracy:  0.850046\n",
      "Epoch:  375  Training Loss:  14.9144  Training Accuracy:  0.850279\n",
      "Epoch:  376  Training Loss:  14.8206  Training Accuracy:  0.851037\n",
      "Epoch:  377  Training Loss:  14.7289  Training Accuracy:  0.851445\n",
      "Epoch:  378  Training Loss:  14.6708  Training Accuracy:  0.851679\n",
      "Epoch:  379  Training Loss:  14.5895  Training Accuracy:  0.852436\n",
      "Epoch:  380  Training Loss:  14.4969  Training Accuracy:  0.852961\n",
      "Epoch:  381  Training Loss:  14.4638  Training Accuracy:  0.853486\n",
      "Epoch:  382  Training Loss:  14.3378  Training Accuracy:  0.853719\n",
      "Epoch:  383  Training Loss:  14.2779  Training Accuracy:  0.854127\n",
      "Epoch:  384  Training Loss:  14.2275  Training Accuracy:  0.854361\n",
      "Epoch:  385  Training Loss:  14.1134  Training Accuracy:  0.854652\n",
      "Epoch:  386  Training Loss:  14.0669  Training Accuracy:  0.855119\n",
      "Epoch:  387  Training Loss:  13.9626  Training Accuracy:  0.855527\n",
      "Epoch:  388  Training Loss:  13.9172  Training Accuracy:  0.855935\n",
      "Epoch:  389  Training Loss:  13.8348  Training Accuracy:  0.856693\n",
      "Epoch:  390  Training Loss:  13.74  Training Accuracy:  0.857217\n",
      "Epoch:  391  Training Loss:  13.6733  Training Accuracy:  0.857625\n",
      "Epoch:  392  Training Loss:  13.6279  Training Accuracy:  0.857975\n",
      "Epoch:  393  Training Loss:  13.5175  Training Accuracy:  0.8585\n",
      "Epoch:  394  Training Loss:  13.4944  Training Accuracy:  0.858792\n",
      "Epoch:  395  Training Loss:  13.3892  Training Accuracy:  0.859375\n",
      "Epoch:  396  Training Loss:  13.3217  Training Accuracy:  0.859608\n",
      "Epoch:  397  Training Loss:  13.2203  Training Accuracy:  0.860366\n",
      "Epoch:  398  Training Loss:  13.2045  Training Accuracy:  0.860949\n",
      "Epoch:  399  Training Loss:  13.0849  Training Accuracy:  0.861473\n",
      "Epoch:  400  Training Loss:  13.0491  Training Accuracy:  0.861532\n",
      "Epoch:  401  Training Loss:  12.9467  Training Accuracy:  0.86194\n",
      "Epoch:  402  Training Loss:  12.8588  Training Accuracy:  0.86229\n",
      "Epoch:  403  Training Loss:  12.8388  Training Accuracy:  0.862989\n",
      "Epoch:  404  Training Loss:  12.7252  Training Accuracy:  0.863048\n",
      "Epoch:  405  Training Loss:  12.6593  Training Accuracy:  0.863631\n",
      "Epoch:  406  Training Loss:  12.6119  Training Accuracy:  0.863805\n",
      "Epoch:  407  Training Loss:  12.5109  Training Accuracy:  0.86433\n",
      "Epoch:  408  Training Loss:  12.4892  Training Accuracy:  0.864738\n",
      "Epoch:  409  Training Loss:  12.3672  Training Accuracy:  0.865321\n",
      "Epoch:  410  Training Loss:  12.3632  Training Accuracy:  0.865729\n",
      "Epoch:  411  Training Loss:  12.2373  Training Accuracy:  0.866313\n",
      "Epoch:  412  Training Loss:  12.2161  Training Accuracy:  0.866371\n",
      "Epoch:  413  Training Loss:  12.1082  Training Accuracy:  0.866837\n",
      "Epoch:  414  Training Loss:  12.0788  Training Accuracy:  0.866779\n",
      "Epoch:  415  Training Loss:  11.9978  Training Accuracy:  0.867479\n",
      "Epoch:  416  Training Loss:  11.9449  Training Accuracy:  0.867537\n",
      "Epoch:  417  Training Loss:  11.8562  Training Accuracy:  0.868353\n",
      "Epoch:  418  Training Loss:  11.8306  Training Accuracy:  0.868528\n",
      "Epoch:  419  Training Loss:  11.7333  Training Accuracy:  0.869111\n",
      "Epoch:  420  Training Loss:  11.6706  Training Accuracy:  0.869403\n",
      "Epoch:  421  Training Loss:  11.6302  Training Accuracy:  0.869811\n",
      "Epoch:  422  Training Loss:  11.5448  Training Accuracy:  0.87016\n",
      "Epoch:  423  Training Loss:  11.5245  Training Accuracy:  0.870569\n",
      "Epoch:  424  Training Loss:  11.4307  Training Accuracy:  0.870685\n",
      "Epoch:  425  Training Loss:  11.3641  Training Accuracy:  0.871385\n",
      "Epoch:  426  Training Loss:  11.349  Training Accuracy:  0.87121\n",
      "Epoch:  427  Training Loss:  11.2524  Training Accuracy:  0.871909\n",
      "Epoch:  428  Training Loss:  11.2078  Training Accuracy:  0.872318\n",
      "Epoch:  429  Training Loss:  11.1377  Training Accuracy:  0.872901\n",
      "Epoch:  430  Training Loss:  11.0974  Training Accuracy:  0.872959\n",
      "Epoch:  431  Training Loss:  11.0072  Training Accuracy:  0.8736\n",
      "Epoch:  432  Training Loss:  10.9882  Training Accuracy:  0.873833\n",
      "Epoch:  433  Training Loss:  10.8931  Training Accuracy:  0.874125\n",
      "Epoch:  434  Training Loss:  10.8773  Training Accuracy:  0.874416\n",
      "Epoch:  435  Training Loss:  10.7733  Training Accuracy:  0.87465\n",
      "Epoch:  436  Training Loss:  10.7737  Training Accuracy:  0.874825\n",
      "Epoch:  437  Training Loss:  10.665  Training Accuracy:  0.875116\n",
      "Epoch:  438  Training Loss:  10.6645  Training Accuracy:  0.875641\n",
      "Epoch:  439  Training Loss:  10.5571  Training Accuracy:  0.875874\n",
      "Epoch:  440  Training Loss:  10.5435  Training Accuracy:  0.876282\n",
      "Epoch:  441  Training Loss:  10.448  Training Accuracy:  0.876749\n",
      "Epoch:  442  Training Loss:  10.437  Training Accuracy:  0.876865\n",
      "Epoch:  443  Training Loss:  10.342  Training Accuracy:  0.877332\n",
      "Epoch:  444  Training Loss:  10.3356  Training Accuracy:  0.877506\n",
      "Epoch:  445  Training Loss:  10.2306  Training Accuracy:  0.877973\n",
      "Epoch:  446  Training Loss:  10.2122  Training Accuracy:  0.878148\n",
      "Epoch:  447  Training Loss:  10.1377  Training Accuracy:  0.878614\n",
      "Epoch:  448  Training Loss:  10.1141  Training Accuracy:  0.878964\n",
      "Epoch:  449  Training Loss:  10.0209  Training Accuracy:  0.879256\n",
      "Epoch:  450  Training Loss:  10.0157  Training Accuracy:  0.879314\n",
      "Epoch:  451  Training Loss:  9.92935  Training Accuracy:  0.879664\n",
      "Epoch:  452  Training Loss:  9.87409  Training Accuracy:  0.880014\n",
      "Epoch:  453  Training Loss:  9.84353  Training Accuracy:  0.88013\n",
      "Epoch:  454  Training Loss:  9.75878  Training Accuracy:  0.880422\n",
      "Epoch:  455  Training Loss:  9.74468  Training Accuracy:  0.881005\n",
      "Epoch:  456  Training Loss:  9.66986  Training Accuracy:  0.881296\n",
      "Epoch:  457  Training Loss:  9.61719  Training Accuracy:  0.881471\n",
      "Epoch:  458  Training Loss:  9.60136  Training Accuracy:  0.881996\n",
      "Epoch:  459  Training Loss:  9.51261  Training Accuracy:  0.882695\n",
      "Epoch:  460  Training Loss:  9.5099  Training Accuracy:  0.883162\n",
      "Epoch:  461  Training Loss:  9.41354  Training Accuracy:  0.883745\n",
      "Epoch:  462  Training Loss:  9.41124  Training Accuracy:  0.883861\n",
      "Epoch:  463  Training Loss:  9.32977  Training Accuracy:  0.884211\n",
      "Epoch:  464  Training Loss:  9.31498  Training Accuracy:  0.884503\n",
      "Epoch:  465  Training Loss:  9.24656  Training Accuracy:  0.885027\n",
      "Epoch:  466  Training Loss:  9.20468  Training Accuracy:  0.885319\n",
      "Epoch:  467  Training Loss:  9.17988  Training Accuracy:  0.886077\n",
      "Epoch:  468  Training Loss:  9.0999  Training Accuracy:  0.886368\n",
      "Epoch:  469  Training Loss:  9.09969  Training Accuracy:  0.886602\n",
      "Epoch:  470  Training Loss:  9.02397  Training Accuracy:  0.887068\n",
      "Epoch:  471  Training Loss:  9.0256  Training Accuracy:  0.887185\n",
      "Epoch:  472  Training Loss:  8.92785  Training Accuracy:  0.887826\n",
      "Epoch:  473  Training Loss:  8.91179  Training Accuracy:  0.888292\n",
      "Epoch:  474  Training Loss:  8.88288  Training Accuracy:  0.888584\n",
      "Epoch:  475  Training Loss:  8.82185  Training Accuracy:  0.888759\n",
      "Epoch:  476  Training Loss:  8.78837  Training Accuracy:  0.889284\n",
      "Epoch:  477  Training Loss:  8.73281  Training Accuracy:  0.8894\n",
      "Epoch:  478  Training Loss:  8.71778  Training Accuracy:  0.889808\n",
      "Epoch:  479  Training Loss:  8.64899  Training Accuracy:  0.890333\n",
      "Epoch:  480  Training Loss:  8.6347  Training Accuracy:  0.890858\n",
      "Epoch:  481  Training Loss:  8.57851  Training Accuracy:  0.891266\n",
      "Epoch:  482  Training Loss:  8.56236  Training Accuracy:  0.891616\n",
      "Epoch:  483  Training Loss:  8.4973  Training Accuracy:  0.891791\n",
      "Epoch:  484  Training Loss:  8.48005  Training Accuracy:  0.89214\n",
      "Epoch:  485  Training Loss:  8.4273  Training Accuracy:  0.892315\n",
      "Epoch:  486  Training Loss:  8.41099  Training Accuracy:  0.892665\n",
      "Epoch:  487  Training Loss:  8.34129  Training Accuracy:  0.892898\n",
      "Epoch:  488  Training Loss:  8.30722  Training Accuracy:  0.893248\n",
      "Epoch:  489  Training Loss:  8.30226  Training Accuracy:  0.893656\n",
      "Epoch:  490  Training Loss:  8.24689  Training Accuracy:  0.893831\n",
      "Epoch:  491  Training Loss:  8.22158  Training Accuracy:  0.894472\n",
      "Epoch:  492  Training Loss:  8.17046  Training Accuracy:  0.894822\n",
      "Epoch:  493  Training Loss:  8.16026  Training Accuracy:  0.894939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  8.1023  Training Accuracy:  0.895347\n",
      "Epoch:  495  Training Loss:  8.05872  Training Accuracy:  0.895638\n",
      "Epoch:  496  Training Loss:  8.05541  Training Accuracy:  0.895872\n",
      "Epoch:  497  Training Loss:  7.98783  Training Accuracy:  0.896338\n",
      "Epoch:  498  Training Loss:  7.99114  Training Accuracy:  0.896688\n",
      "Epoch:  499  Training Loss:  7.90448  Training Accuracy:  0.897096\n",
      "Testing Accuracy: 0.821128\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  1001.5  Training Accuracy:  0.0407533\n",
      "Epoch:  1  Training Loss:  845.854  Training Accuracy:  0.0493237\n",
      "Epoch:  2  Training Loss:  783.433  Training Accuracy:  0.0610424\n",
      "Epoch:  3  Training Loss:  738.805  Training Accuracy:  0.0757346\n",
      "Epoch:  4  Training Loss:  705.491  Training Accuracy:  0.0900187\n",
      "Epoch:  5  Training Loss:  676.033  Training Accuracy:  0.102146\n",
      "Epoch:  6  Training Loss:  652.869  Training Accuracy:  0.115497\n",
      "Epoch:  7  Training Loss:  632.897  Training Accuracy:  0.127449\n",
      "Epoch:  8  Training Loss:  614.153  Training Accuracy:  0.138701\n",
      "Epoch:  9  Training Loss:  597.98  Training Accuracy:  0.148088\n",
      "Epoch:  10  Training Loss:  585.743  Training Accuracy:  0.155667\n",
      "Epoch:  11  Training Loss:  573.884  Training Accuracy:  0.163538\n",
      "Epoch:  12  Training Loss:  562.375  Training Accuracy:  0.172225\n",
      "Epoch:  13  Training Loss:  548.984  Training Accuracy:  0.181262\n",
      "Epoch:  14  Training Loss:  537.846  Training Accuracy:  0.189832\n",
      "Epoch:  15  Training Loss:  525.912  Training Accuracy:  0.196945\n",
      "Epoch:  16  Training Loss:  515.076  Training Accuracy:  0.205107\n",
      "Epoch:  17  Training Loss:  505.04  Training Accuracy:  0.212337\n",
      "Epoch:  18  Training Loss:  495.543  Training Accuracy:  0.21945\n",
      "Epoch:  19  Training Loss:  486.491  Training Accuracy:  0.226154\n",
      "Epoch:  20  Training Loss:  477.427  Training Accuracy:  0.2335\n",
      "Epoch:  21  Training Loss:  469.405  Training Accuracy:  0.238048\n",
      "Epoch:  22  Training Loss:  461.983  Training Accuracy:  0.243995\n",
      "Epoch:  23  Training Loss:  452.743  Training Accuracy:  0.24965\n",
      "Epoch:  24  Training Loss:  446.455  Training Accuracy:  0.254839\n",
      "Epoch:  25  Training Loss:  440.505  Training Accuracy:  0.260203\n",
      "Epoch:  26  Training Loss:  434.502  Training Accuracy:  0.26615\n",
      "Epoch:  27  Training Loss:  428.383  Training Accuracy:  0.271805\n",
      "Epoch:  28  Training Loss:  422.814  Training Accuracy:  0.276702\n",
      "Epoch:  29  Training Loss:  417.655  Training Accuracy:  0.280201\n",
      "Epoch:  30  Training Loss:  412.59  Training Accuracy:  0.285273\n",
      "Epoch:  31  Training Loss:  407.171  Training Accuracy:  0.289995\n",
      "Epoch:  32  Training Loss:  402.404  Training Accuracy:  0.294485\n",
      "Epoch:  33  Training Loss:  397.453  Training Accuracy:  0.298566\n",
      "Epoch:  34  Training Loss:  392.39  Training Accuracy:  0.302122\n",
      "Epoch:  35  Training Loss:  387.236  Training Accuracy:  0.30667\n",
      "Epoch:  36  Training Loss:  382.466  Training Accuracy:  0.310868\n",
      "Epoch:  37  Training Loss:  377.815  Training Accuracy:  0.315473\n",
      "Epoch:  38  Training Loss:  372.753  Training Accuracy:  0.318855\n",
      "Epoch:  39  Training Loss:  368.002  Training Accuracy:  0.322994\n",
      "Epoch:  40  Training Loss:  363.765  Training Accuracy:  0.327076\n",
      "Epoch:  41  Training Loss:  358.951  Training Accuracy:  0.330049\n",
      "Epoch:  42  Training Loss:  354.115  Training Accuracy:  0.334188\n",
      "Epoch:  43  Training Loss:  349.627  Training Accuracy:  0.33827\n",
      "Epoch:  44  Training Loss:  344.91  Training Accuracy:  0.342409\n",
      "Epoch:  45  Training Loss:  340.173  Training Accuracy:  0.346898\n",
      "Epoch:  46  Training Loss:  336.088  Training Accuracy:  0.351213\n",
      "Epoch:  47  Training Loss:  331.544  Training Accuracy:  0.357626\n",
      "Epoch:  48  Training Loss:  327.062  Training Accuracy:  0.363048\n",
      "Epoch:  49  Training Loss:  323.092  Training Accuracy:  0.367829\n",
      "Epoch:  50  Training Loss:  318.571  Training Accuracy:  0.371968\n",
      "Epoch:  51  Training Loss:  314.819  Training Accuracy:  0.376574\n",
      "Epoch:  52  Training Loss:  310.511  Training Accuracy:  0.380539\n",
      "Epoch:  53  Training Loss:  306.501  Training Accuracy:  0.384037\n",
      "Epoch:  54  Training Loss:  302.206  Training Accuracy:  0.388934\n",
      "Epoch:  55  Training Loss:  298.188  Training Accuracy:  0.391849\n",
      "Epoch:  56  Training Loss:  293.847  Training Accuracy:  0.395406\n",
      "Epoch:  57  Training Loss:  289.988  Training Accuracy:  0.399137\n",
      "Epoch:  58  Training Loss:  286.161  Training Accuracy:  0.402285\n",
      "Epoch:  59  Training Loss:  282.413  Training Accuracy:  0.405842\n",
      "Epoch:  60  Training Loss:  278.067  Training Accuracy:  0.408873\n",
      "Epoch:  61  Training Loss:  274.646  Training Accuracy:  0.411206\n",
      "Epoch:  62  Training Loss:  270.306  Training Accuracy:  0.415637\n",
      "Epoch:  63  Training Loss:  266.437  Training Accuracy:  0.41931\n",
      "Epoch:  64  Training Loss:  262.638  Training Accuracy:  0.423682\n",
      "Epoch:  65  Training Loss:  258.925  Training Accuracy:  0.427414\n",
      "Epoch:  66  Training Loss:  254.938  Training Accuracy:  0.432369\n",
      "Epoch:  67  Training Loss:  251.066  Training Accuracy:  0.436276\n",
      "Epoch:  68  Training Loss:  246.732  Training Accuracy:  0.439657\n",
      "Epoch:  69  Training Loss:  243.348  Training Accuracy:  0.443913\n",
      "Epoch:  70  Training Loss:  239.629  Training Accuracy:  0.446595\n",
      "Epoch:  71  Training Loss:  235.894  Training Accuracy:  0.450151\n",
      "Epoch:  72  Training Loss:  232.022  Training Accuracy:  0.454349\n",
      "Epoch:  73  Training Loss:  228.537  Training Accuracy:  0.458372\n",
      "Epoch:  74  Training Loss:  225.159  Training Accuracy:  0.460762\n",
      "Epoch:  75  Training Loss:  221.37  Training Accuracy:  0.464086\n",
      "Epoch:  76  Training Loss:  217.922  Training Accuracy:  0.467525\n",
      "Epoch:  77  Training Loss:  214.423  Training Accuracy:  0.470441\n",
      "Epoch:  78  Training Loss:  211.151  Training Accuracy:  0.473764\n",
      "Epoch:  79  Training Loss:  207.452  Training Accuracy:  0.476737\n",
      "Epoch:  80  Training Loss:  204.244  Training Accuracy:  0.479419\n",
      "Epoch:  81  Training Loss:  200.974  Training Accuracy:  0.482334\n",
      "Epoch:  82  Training Loss:  197.721  Training Accuracy:  0.486474\n",
      "Epoch:  83  Training Loss:  194.123  Training Accuracy:  0.489914\n",
      "Epoch:  84  Training Loss:  190.789  Training Accuracy:  0.493237\n",
      "Epoch:  85  Training Loss:  187.493  Training Accuracy:  0.496851\n",
      "Epoch:  86  Training Loss:  184.127  Training Accuracy:  0.500058\n",
      "Epoch:  87  Training Loss:  180.926  Training Accuracy:  0.502915\n",
      "Epoch:  88  Training Loss:  177.846  Training Accuracy:  0.50548\n",
      "Epoch:  89  Training Loss:  174.877  Training Accuracy:  0.508454\n",
      "Epoch:  90  Training Loss:  171.927  Training Accuracy:  0.51236\n",
      "Epoch:  91  Training Loss:  168.972  Training Accuracy:  0.514867\n",
      "Epoch:  92  Training Loss:  166.15  Training Accuracy:  0.517374\n",
      "Epoch:  93  Training Loss:  163.409  Training Accuracy:  0.519823\n",
      "Epoch:  94  Training Loss:  160.563  Training Accuracy:  0.522913\n",
      "Epoch:  95  Training Loss:  157.747  Training Accuracy:  0.524953\n",
      "Epoch:  96  Training Loss:  155.068  Training Accuracy:  0.52711\n",
      "Epoch:  97  Training Loss:  152.344  Training Accuracy:  0.529559\n",
      "Epoch:  98  Training Loss:  149.835  Training Accuracy:  0.532591\n",
      "Epoch:  99  Training Loss:  146.995  Training Accuracy:  0.535448\n",
      "Epoch:  100  Training Loss:  144.458  Training Accuracy:  0.538188\n",
      "Epoch:  101  Training Loss:  141.652  Training Accuracy:  0.54087\n",
      "Epoch:  102  Training Loss:  139.452  Training Accuracy:  0.543143\n",
      "Epoch:  103  Training Loss:  137.04  Training Accuracy:  0.545184\n",
      "Epoch:  104  Training Loss:  134.797  Training Accuracy:  0.547516\n",
      "Epoch:  105  Training Loss:  132.576  Training Accuracy:  0.549615\n",
      "Epoch:  106  Training Loss:  130.42  Training Accuracy:  0.55218\n",
      "Epoch:  107  Training Loss:  128.278  Training Accuracy:  0.554512\n",
      "Epoch:  108  Training Loss:  126.177  Training Accuracy:  0.555678\n",
      "Epoch:  109  Training Loss:  124.086  Training Accuracy:  0.558069\n",
      "Epoch:  110  Training Loss:  122.116  Training Accuracy:  0.559934\n",
      "Epoch:  111  Training Loss:  120.145  Training Accuracy:  0.56215\n",
      "Epoch:  112  Training Loss:  118.142  Training Accuracy:  0.565123\n",
      "Epoch:  113  Training Loss:  116.387  Training Accuracy:  0.567922\n",
      "Epoch:  114  Training Loss:  114.549  Training Accuracy:  0.570254\n",
      "Epoch:  115  Training Loss:  112.619  Training Accuracy:  0.572411\n",
      "Epoch:  116  Training Loss:  110.778  Training Accuracy:  0.574627\n",
      "Epoch:  117  Training Loss:  109.156  Training Accuracy:  0.576609\n",
      "Epoch:  118  Training Loss:  107.381  Training Accuracy:  0.578941\n",
      "Epoch:  119  Training Loss:  105.755  Training Accuracy:  0.581506\n",
      "Epoch:  120  Training Loss:  104.128  Training Accuracy:  0.583897\n",
      "Epoch:  121  Training Loss:  102.592  Training Accuracy:  0.586987\n",
      "Epoch:  122  Training Loss:  100.984  Training Accuracy:  0.589086\n",
      "Epoch:  123  Training Loss:  99.5071  Training Accuracy:  0.590951\n",
      "Epoch:  124  Training Loss:  97.8692  Training Accuracy:  0.593108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  96.3983  Training Accuracy:  0.594624\n",
      "Epoch:  126  Training Loss:  95.011  Training Accuracy:  0.596024\n",
      "Epoch:  127  Training Loss:  93.5461  Training Accuracy:  0.598064\n",
      "Epoch:  128  Training Loss:  92.1109  Training Accuracy:  0.599755\n",
      "Epoch:  129  Training Loss:  90.7894  Training Accuracy:  0.601096\n",
      "Epoch:  130  Training Loss:  89.3865  Training Accuracy:  0.603253\n",
      "Epoch:  131  Training Loss:  88.032  Training Accuracy:  0.604536\n",
      "Epoch:  132  Training Loss:  86.7453  Training Accuracy:  0.60611\n",
      "Epoch:  133  Training Loss:  85.4386  Training Accuracy:  0.607975\n",
      "Epoch:  134  Training Loss:  84.1248  Training Accuracy:  0.610308\n",
      "Epoch:  135  Training Loss:  82.9125  Training Accuracy:  0.612931\n",
      "Epoch:  136  Training Loss:  81.6274  Training Accuracy:  0.615263\n",
      "Epoch:  137  Training Loss:  80.4539  Training Accuracy:  0.617071\n",
      "Epoch:  138  Training Loss:  79.2198  Training Accuracy:  0.61812\n",
      "Epoch:  139  Training Loss:  78.1319  Training Accuracy:  0.619694\n",
      "Epoch:  140  Training Loss:  76.9681  Training Accuracy:  0.621268\n",
      "Epoch:  141  Training Loss:  75.7973  Training Accuracy:  0.6236\n",
      "Epoch:  142  Training Loss:  74.6901  Training Accuracy:  0.625291\n",
      "Epoch:  143  Training Loss:  73.5483  Training Accuracy:  0.626865\n",
      "Epoch:  144  Training Loss:  72.4905  Training Accuracy:  0.628206\n",
      "Epoch:  145  Training Loss:  71.426  Training Accuracy:  0.630364\n",
      "Epoch:  146  Training Loss:  70.3266  Training Accuracy:  0.632637\n",
      "Epoch:  147  Training Loss:  69.315  Training Accuracy:  0.634911\n",
      "Epoch:  148  Training Loss:  68.3345  Training Accuracy:  0.63631\n",
      "Epoch:  149  Training Loss:  67.4107  Training Accuracy:  0.637768\n",
      "Epoch:  150  Training Loss:  66.4357  Training Accuracy:  0.639284\n",
      "Epoch:  151  Training Loss:  65.5198  Training Accuracy:  0.64045\n",
      "Epoch:  152  Training Loss:  64.6565  Training Accuracy:  0.641907\n",
      "Epoch:  153  Training Loss:  63.7513  Training Accuracy:  0.643015\n",
      "Epoch:  154  Training Loss:  62.908  Training Accuracy:  0.645172\n",
      "Epoch:  155  Training Loss:  62.1844  Training Accuracy:  0.646572\n",
      "Epoch:  156  Training Loss:  61.3792  Training Accuracy:  0.647621\n",
      "Epoch:  157  Training Loss:  60.6215  Training Accuracy:  0.648962\n",
      "Epoch:  158  Training Loss:  59.8141  Training Accuracy:  0.650594\n",
      "Epoch:  159  Training Loss:  59.1232  Training Accuracy:  0.651994\n",
      "Epoch:  160  Training Loss:  58.3304  Training Accuracy:  0.653918\n",
      "Epoch:  161  Training Loss:  57.6784  Training Accuracy:  0.655317\n",
      "Epoch:  162  Training Loss:  56.8987  Training Accuracy:  0.656891\n",
      "Epoch:  163  Training Loss:  56.2189  Training Accuracy:  0.657882\n",
      "Epoch:  164  Training Loss:  55.5168  Training Accuracy:  0.658932\n",
      "Epoch:  165  Training Loss:  54.8388  Training Accuracy:  0.659631\n",
      "Epoch:  166  Training Loss:  54.1866  Training Accuracy:  0.661089\n",
      "Epoch:  167  Training Loss:  53.53  Training Accuracy:  0.662546\n",
      "Epoch:  168  Training Loss:  52.9251  Training Accuracy:  0.664179\n",
      "Epoch:  169  Training Loss:  52.3681  Training Accuracy:  0.665287\n",
      "Epoch:  170  Training Loss:  51.7482  Training Accuracy:  0.666802\n",
      "Epoch:  171  Training Loss:  51.1585  Training Accuracy:  0.667852\n",
      "Epoch:  172  Training Loss:  50.5401  Training Accuracy:  0.669193\n",
      "Epoch:  173  Training Loss:  49.9706  Training Accuracy:  0.670009\n",
      "Epoch:  174  Training Loss:  49.413  Training Accuracy:  0.671058\n",
      "Epoch:  175  Training Loss:  48.768  Training Accuracy:  0.671991\n",
      "Epoch:  176  Training Loss:  48.2644  Training Accuracy:  0.673682\n",
      "Epoch:  177  Training Loss:  47.7164  Training Accuracy:  0.674906\n",
      "Epoch:  178  Training Loss:  47.158  Training Accuracy:  0.676189\n",
      "Epoch:  179  Training Loss:  46.6988  Training Accuracy:  0.67753\n",
      "Epoch:  180  Training Loss:  46.0998  Training Accuracy:  0.679629\n",
      "Epoch:  181  Training Loss:  45.6054  Training Accuracy:  0.680853\n",
      "Epoch:  182  Training Loss:  45.0809  Training Accuracy:  0.682427\n",
      "Epoch:  183  Training Loss:  44.575  Training Accuracy:  0.683943\n",
      "Epoch:  184  Training Loss:  44.0688  Training Accuracy:  0.684934\n",
      "Epoch:  185  Training Loss:  43.5936  Training Accuracy:  0.686567\n",
      "Epoch:  186  Training Loss:  43.0962  Training Accuracy:  0.688083\n",
      "Epoch:  187  Training Loss:  42.687  Training Accuracy:  0.689132\n",
      "Epoch:  188  Training Loss:  42.1846  Training Accuracy:  0.690298\n",
      "Epoch:  189  Training Loss:  41.7999  Training Accuracy:  0.691697\n",
      "Epoch:  190  Training Loss:  41.3329  Training Accuracy:  0.693155\n",
      "Epoch:  191  Training Loss:  40.9714  Training Accuracy:  0.694379\n",
      "Epoch:  192  Training Loss:  40.5751  Training Accuracy:  0.695779\n",
      "Epoch:  193  Training Loss:  40.2147  Training Accuracy:  0.697411\n",
      "Epoch:  194  Training Loss:  39.8132  Training Accuracy:  0.698227\n",
      "Epoch:  195  Training Loss:  39.5302  Training Accuracy:  0.69951\n",
      "Epoch:  196  Training Loss:  39.0495  Training Accuracy:  0.701142\n",
      "Epoch:  197  Training Loss:  38.7214  Training Accuracy:  0.70225\n",
      "Epoch:  198  Training Loss:  38.3813  Training Accuracy:  0.703475\n",
      "Epoch:  199  Training Loss:  38.0182  Training Accuracy:  0.704232\n",
      "Epoch:  200  Training Loss:  37.6802  Training Accuracy:  0.705981\n",
      "Epoch:  201  Training Loss:  37.3695  Training Accuracy:  0.707147\n",
      "Epoch:  202  Training Loss:  37.0435  Training Accuracy:  0.709071\n",
      "Epoch:  203  Training Loss:  36.7264  Training Accuracy:  0.710704\n",
      "Epoch:  204  Training Loss:  36.3929  Training Accuracy:  0.711637\n",
      "Epoch:  205  Training Loss:  36.1745  Training Accuracy:  0.712336\n",
      "Epoch:  206  Training Loss:  35.8672  Training Accuracy:  0.713444\n",
      "Epoch:  207  Training Loss:  35.5738  Training Accuracy:  0.715135\n",
      "Epoch:  208  Training Loss:  35.3116  Training Accuracy:  0.715834\n",
      "Epoch:  209  Training Loss:  35.0466  Training Accuracy:  0.716418\n",
      "Epoch:  210  Training Loss:  34.7261  Training Accuracy:  0.717001\n",
      "Epoch:  211  Training Loss:  34.4467  Training Accuracy:  0.717992\n",
      "Epoch:  212  Training Loss:  34.197  Training Accuracy:  0.719099\n",
      "Epoch:  213  Training Loss:  33.9398  Training Accuracy:  0.720732\n",
      "Epoch:  214  Training Loss:  33.615  Training Accuracy:  0.72149\n",
      "Epoch:  215  Training Loss:  33.3737  Training Accuracy:  0.722656\n",
      "Epoch:  216  Training Loss:  33.0572  Training Accuracy:  0.723297\n",
      "Epoch:  217  Training Loss:  32.8609  Training Accuracy:  0.724696\n",
      "Epoch:  218  Training Loss:  32.5529  Training Accuracy:  0.726271\n",
      "Epoch:  219  Training Loss:  32.3519  Training Accuracy:  0.727437\n",
      "Epoch:  220  Training Loss:  32.1177  Training Accuracy:  0.728661\n",
      "Epoch:  221  Training Loss:  31.9113  Training Accuracy:  0.729769\n",
      "Epoch:  222  Training Loss:  31.6633  Training Accuracy:  0.730643\n",
      "Epoch:  223  Training Loss:  31.4588  Training Accuracy:  0.731401\n",
      "Epoch:  224  Training Loss:  31.2235  Training Accuracy:  0.732043\n",
      "Epoch:  225  Training Loss:  31.0215  Training Accuracy:  0.732742\n",
      "Epoch:  226  Training Loss:  30.7982  Training Accuracy:  0.733792\n",
      "Epoch:  227  Training Loss:  30.5668  Training Accuracy:  0.735016\n",
      "Epoch:  228  Training Loss:  30.3028  Training Accuracy:  0.736124\n",
      "Epoch:  229  Training Loss:  30.1436  Training Accuracy:  0.737231\n",
      "Epoch:  230  Training Loss:  29.9318  Training Accuracy:  0.738281\n",
      "Epoch:  231  Training Loss:  29.7209  Training Accuracy:  0.739155\n",
      "Epoch:  232  Training Loss:  29.5326  Training Accuracy:  0.740088\n",
      "Epoch:  233  Training Loss:  29.3389  Training Accuracy:  0.741021\n",
      "Epoch:  234  Training Loss:  29.1115  Training Accuracy:  0.741604\n",
      "Epoch:  235  Training Loss:  28.9837  Training Accuracy:  0.742479\n",
      "Epoch:  236  Training Loss:  28.7739  Training Accuracy:  0.743412\n",
      "Epoch:  237  Training Loss:  28.5913  Training Accuracy:  0.744344\n",
      "Epoch:  238  Training Loss:  28.4539  Training Accuracy:  0.745277\n",
      "Epoch:  239  Training Loss:  28.2375  Training Accuracy:  0.74586\n",
      "Epoch:  240  Training Loss:  28.084  Training Accuracy:  0.746968\n",
      "Epoch:  241  Training Loss:  27.8903  Training Accuracy:  0.748017\n",
      "Epoch:  242  Training Loss:  27.732  Training Accuracy:  0.748425\n",
      "Epoch:  243  Training Loss:  27.578  Training Accuracy:  0.7493\n",
      "Epoch:  244  Training Loss:  27.4106  Training Accuracy:  0.749825\n",
      "Epoch:  245  Training Loss:  27.2367  Training Accuracy:  0.750932\n",
      "Epoch:  246  Training Loss:  27.0508  Training Accuracy:  0.751749\n",
      "Epoch:  247  Training Loss:  26.9436  Training Accuracy:  0.752565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  26.7685  Training Accuracy:  0.753439\n",
      "Epoch:  249  Training Loss:  26.5618  Training Accuracy:  0.754197\n",
      "Epoch:  250  Training Loss:  26.4406  Training Accuracy:  0.75478\n",
      "Epoch:  251  Training Loss:  26.2609  Training Accuracy:  0.755655\n",
      "Epoch:  252  Training Loss:  26.0793  Training Accuracy:  0.757054\n",
      "Epoch:  253  Training Loss:  25.9291  Training Accuracy:  0.757812\n",
      "Epoch:  254  Training Loss:  25.7624  Training Accuracy:  0.758337\n",
      "Epoch:  255  Training Loss:  25.584  Training Accuracy:  0.759153\n",
      "Epoch:  256  Training Loss:  25.4623  Training Accuracy:  0.760203\n",
      "Epoch:  257  Training Loss:  25.3218  Training Accuracy:  0.761427\n",
      "Epoch:  258  Training Loss:  25.137  Training Accuracy:  0.762127\n",
      "Epoch:  259  Training Loss:  25.0008  Training Accuracy:  0.763001\n",
      "Epoch:  260  Training Loss:  24.8799  Training Accuracy:  0.763876\n",
      "Epoch:  261  Training Loss:  24.7583  Training Accuracy:  0.76475\n",
      "Epoch:  262  Training Loss:  24.6431  Training Accuracy:  0.765974\n",
      "Epoch:  263  Training Loss:  24.5136  Training Accuracy:  0.766557\n",
      "Epoch:  264  Training Loss:  24.3452  Training Accuracy:  0.767607\n",
      "Epoch:  265  Training Loss:  24.2267  Training Accuracy:  0.768656\n",
      "Epoch:  266  Training Loss:  24.0556  Training Accuracy:  0.768948\n",
      "Epoch:  267  Training Loss:  23.9203  Training Accuracy:  0.769939\n",
      "Epoch:  268  Training Loss:  23.7829  Training Accuracy:  0.770522\n",
      "Epoch:  269  Training Loss:  23.6887  Training Accuracy:  0.771338\n",
      "Epoch:  270  Training Loss:  23.5228  Training Accuracy:  0.771863\n",
      "Epoch:  271  Training Loss:  23.4439  Training Accuracy:  0.773029\n",
      "Epoch:  272  Training Loss:  23.2688  Training Accuracy:  0.774428\n",
      "Epoch:  273  Training Loss:  23.1759  Training Accuracy:  0.775827\n",
      "Epoch:  274  Training Loss:  23.0007  Training Accuracy:  0.776935\n",
      "Epoch:  275  Training Loss:  22.8965  Training Accuracy:  0.777751\n",
      "Epoch:  276  Training Loss:  22.7514  Training Accuracy:  0.778451\n",
      "Epoch:  277  Training Loss:  22.6501  Training Accuracy:  0.779326\n",
      "Epoch:  278  Training Loss:  22.4816  Training Accuracy:  0.780433\n",
      "Epoch:  279  Training Loss:  22.3991  Training Accuracy:  0.781541\n",
      "Epoch:  280  Training Loss:  22.2519  Training Accuracy:  0.782357\n",
      "Epoch:  281  Training Loss:  22.1338  Training Accuracy:  0.782882\n",
      "Epoch:  282  Training Loss:  22.0041  Training Accuracy:  0.784048\n",
      "Epoch:  283  Training Loss:  21.9091  Training Accuracy:  0.784806\n",
      "Epoch:  284  Training Loss:  21.7646  Training Accuracy:  0.785681\n",
      "Epoch:  285  Training Loss:  21.651  Training Accuracy:  0.78673\n",
      "Epoch:  286  Training Loss:  21.5666  Training Accuracy:  0.787896\n",
      "Epoch:  287  Training Loss:  21.4577  Training Accuracy:  0.788712\n",
      "Epoch:  288  Training Loss:  21.3333  Training Accuracy:  0.790053\n",
      "Epoch:  289  Training Loss:  21.2283  Training Accuracy:  0.790869\n",
      "Epoch:  290  Training Loss:  21.115  Training Accuracy:  0.791569\n",
      "Epoch:  291  Training Loss:  20.9909  Training Accuracy:  0.792677\n",
      "Epoch:  292  Training Loss:  20.8885  Training Accuracy:  0.79361\n",
      "Epoch:  293  Training Loss:  20.771  Training Accuracy:  0.794659\n",
      "Epoch:  294  Training Loss:  20.6715  Training Accuracy:  0.7953\n",
      "Epoch:  295  Training Loss:  20.5812  Training Accuracy:  0.796117\n",
      "Epoch:  296  Training Loss:  20.4398  Training Accuracy:  0.796758\n",
      "Epoch:  297  Training Loss:  20.3453  Training Accuracy:  0.797283\n",
      "Epoch:  298  Training Loss:  20.2293  Training Accuracy:  0.797924\n",
      "Epoch:  299  Training Loss:  20.1399  Training Accuracy:  0.79909\n",
      "Epoch:  300  Training Loss:  20.0318  Training Accuracy:  0.800664\n",
      "Epoch:  301  Training Loss:  19.9261  Training Accuracy:  0.801714\n",
      "Epoch:  302  Training Loss:  19.8197  Training Accuracy:  0.802763\n",
      "Epoch:  303  Training Loss:  19.7308  Training Accuracy:  0.803696\n",
      "Epoch:  304  Training Loss:  19.6326  Training Accuracy:  0.804629\n",
      "Epoch:  305  Training Loss:  19.534  Training Accuracy:  0.805445\n",
      "Epoch:  306  Training Loss:  19.4318  Training Accuracy:  0.806203\n",
      "Epoch:  307  Training Loss:  19.3414  Training Accuracy:  0.807194\n",
      "Epoch:  308  Training Loss:  19.2409  Training Accuracy:  0.807952\n",
      "Epoch:  309  Training Loss:  19.1383  Training Accuracy:  0.808652\n",
      "Epoch:  310  Training Loss:  19.029  Training Accuracy:  0.809293\n",
      "Epoch:  311  Training Loss:  18.9552  Training Accuracy:  0.809934\n",
      "Epoch:  312  Training Loss:  18.8473  Training Accuracy:  0.810692\n",
      "Epoch:  313  Training Loss:  18.7534  Training Accuracy:  0.8111\n",
      "Epoch:  314  Training Loss:  18.67  Training Accuracy:  0.811567\n",
      "Epoch:  315  Training Loss:  18.5623  Training Accuracy:  0.811858\n",
      "Epoch:  316  Training Loss:  18.4699  Training Accuracy:  0.812441\n",
      "Epoch:  317  Training Loss:  18.3729  Training Accuracy:  0.813199\n",
      "Epoch:  318  Training Loss:  18.2816  Training Accuracy:  0.813957\n",
      "Epoch:  319  Training Loss:  18.1666  Training Accuracy:  0.814948\n",
      "Epoch:  320  Training Loss:  18.1134  Training Accuracy:  0.815415\n",
      "Epoch:  321  Training Loss:  17.9779  Training Accuracy:  0.816114\n",
      "Epoch:  322  Training Loss:  17.9284  Training Accuracy:  0.816872\n",
      "Epoch:  323  Training Loss:  17.8451  Training Accuracy:  0.81763\n",
      "Epoch:  324  Training Loss:  17.7437  Training Accuracy:  0.81833\n",
      "Epoch:  325  Training Loss:  17.691  Training Accuracy:  0.818971\n",
      "Epoch:  326  Training Loss:  17.5716  Training Accuracy:  0.819554\n",
      "Epoch:  327  Training Loss:  17.4927  Training Accuracy:  0.820195\n",
      "Epoch:  328  Training Loss:  17.4099  Training Accuracy:  0.820545\n",
      "Epoch:  329  Training Loss:  17.3173  Training Accuracy:  0.820953\n",
      "Epoch:  330  Training Loss:  17.2069  Training Accuracy:  0.822003\n",
      "Epoch:  331  Training Loss:  17.15  Training Accuracy:  0.822469\n",
      "Epoch:  332  Training Loss:  17.0344  Training Accuracy:  0.823169\n",
      "Epoch:  333  Training Loss:  16.9471  Training Accuracy:  0.82381\n",
      "Epoch:  334  Training Loss:  16.8563  Training Accuracy:  0.824218\n",
      "Epoch:  335  Training Loss:  16.7848  Training Accuracy:  0.824335\n",
      "Epoch:  336  Training Loss:  16.6661  Training Accuracy:  0.825151\n",
      "Epoch:  337  Training Loss:  16.6139  Training Accuracy:  0.825501\n",
      "Epoch:  338  Training Loss:  16.5256  Training Accuracy:  0.825967\n",
      "Epoch:  339  Training Loss:  16.3836  Training Accuracy:  0.826492\n",
      "Epoch:  340  Training Loss:  16.3655  Training Accuracy:  0.826609\n",
      "Epoch:  341  Training Loss:  16.2324  Training Accuracy:  0.827133\n",
      "Epoch:  342  Training Loss:  16.157  Training Accuracy:  0.827542\n",
      "Epoch:  343  Training Loss:  16.0383  Training Accuracy:  0.828358\n",
      "Epoch:  344  Training Loss:  15.99  Training Accuracy:  0.828416\n",
      "Epoch:  345  Training Loss:  15.847  Training Accuracy:  0.829057\n",
      "Epoch:  346  Training Loss:  15.818  Training Accuracy:  0.829932\n",
      "Epoch:  347  Training Loss:  15.6625  Training Accuracy:  0.830573\n",
      "Epoch:  348  Training Loss:  15.6293  Training Accuracy:  0.831156\n",
      "Epoch:  349  Training Loss:  15.5025  Training Accuracy:  0.831506\n",
      "Epoch:  350  Training Loss:  15.4524  Training Accuracy:  0.832089\n",
      "Epoch:  351  Training Loss:  15.3101  Training Accuracy:  0.832322\n",
      "Epoch:  352  Training Loss:  15.2508  Training Accuracy:  0.832847\n",
      "Epoch:  353  Training Loss:  15.1635  Training Accuracy:  0.833138\n",
      "Epoch:  354  Training Loss:  15.0602  Training Accuracy:  0.833955\n",
      "Epoch:  355  Training Loss:  14.9741  Training Accuracy:  0.834479\n",
      "Epoch:  356  Training Loss:  14.9188  Training Accuracy:  0.834946\n",
      "Epoch:  357  Training Loss:  14.8183  Training Accuracy:  0.835646\n",
      "Epoch:  358  Training Loss:  14.7463  Training Accuracy:  0.836054\n",
      "Epoch:  359  Training Loss:  14.6565  Training Accuracy:  0.836403\n",
      "Epoch:  360  Training Loss:  14.5838  Training Accuracy:  0.836753\n",
      "Epoch:  361  Training Loss:  14.4814  Training Accuracy:  0.837395\n",
      "Epoch:  362  Training Loss:  14.42  Training Accuracy:  0.837978\n",
      "Epoch:  363  Training Loss:  14.3184  Training Accuracy:  0.838969\n",
      "Epoch:  364  Training Loss:  14.2678  Training Accuracy:  0.839319\n",
      "Epoch:  365  Training Loss:  14.143  Training Accuracy:  0.839902\n",
      "Epoch:  366  Training Loss:  14.0826  Training Accuracy:  0.840193\n",
      "Epoch:  367  Training Loss:  14.0207  Training Accuracy:  0.840776\n",
      "Epoch:  368  Training Loss:  13.9668  Training Accuracy:  0.841184\n",
      "Epoch:  369  Training Loss:  13.8699  Training Accuracy:  0.841592\n",
      "Epoch:  370  Training Loss:  13.7958  Training Accuracy:  0.842175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  13.7193  Training Accuracy:  0.842642\n",
      "Epoch:  372  Training Loss:  13.6789  Training Accuracy:  0.843458\n",
      "Epoch:  373  Training Loss:  13.6076  Training Accuracy:  0.843866\n",
      "Epoch:  374  Training Loss:  13.5355  Training Accuracy:  0.844974\n",
      "Epoch:  375  Training Loss:  13.479  Training Accuracy:  0.845615\n",
      "Epoch:  376  Training Loss:  13.4183  Training Accuracy:  0.846548\n",
      "Epoch:  377  Training Loss:  13.3343  Training Accuracy:  0.847014\n",
      "Epoch:  378  Training Loss:  13.3065  Training Accuracy:  0.847248\n",
      "Epoch:  379  Training Loss:  13.1789  Training Accuracy:  0.847423\n",
      "Epoch:  380  Training Loss:  13.1513  Training Accuracy:  0.848122\n",
      "Epoch:  381  Training Loss:  13.0761  Training Accuracy:  0.848822\n",
      "Epoch:  382  Training Loss:  13.0166  Training Accuracy:  0.849055\n",
      "Epoch:  383  Training Loss:  12.9464  Training Accuracy:  0.849463\n",
      "Epoch:  384  Training Loss:  12.8911  Training Accuracy:  0.849813\n",
      "Epoch:  385  Training Loss:  12.8073  Training Accuracy:  0.850396\n",
      "Epoch:  386  Training Loss:  12.7491  Training Accuracy:  0.850746\n",
      "Epoch:  387  Training Loss:  12.6841  Training Accuracy:  0.851329\n",
      "Epoch:  388  Training Loss:  12.6372  Training Accuracy:  0.852028\n",
      "Epoch:  389  Training Loss:  12.5662  Training Accuracy:  0.852495\n",
      "Epoch:  390  Training Loss:  12.5176  Training Accuracy:  0.852728\n",
      "Epoch:  391  Training Loss:  12.4345  Training Accuracy:  0.853253\n",
      "Epoch:  392  Training Loss:  12.3872  Training Accuracy:  0.853719\n",
      "Epoch:  393  Training Loss:  12.2955  Training Accuracy:  0.853894\n",
      "Epoch:  394  Training Loss:  12.2673  Training Accuracy:  0.854419\n",
      "Epoch:  395  Training Loss:  12.1859  Training Accuracy:  0.854944\n",
      "Epoch:  396  Training Loss:  12.1437  Training Accuracy:  0.855235\n",
      "Epoch:  397  Training Loss:  12.0588  Training Accuracy:  0.855293\n",
      "Epoch:  398  Training Loss:  12.0034  Training Accuracy:  0.855701\n",
      "Epoch:  399  Training Loss:  11.9367  Training Accuracy:  0.856343\n",
      "Epoch:  400  Training Loss:  11.8941  Training Accuracy:  0.856693\n",
      "Epoch:  401  Training Loss:  11.8081  Training Accuracy:  0.857276\n",
      "Epoch:  402  Training Loss:  11.7804  Training Accuracy:  0.857625\n",
      "Epoch:  403  Training Loss:  11.687  Training Accuracy:  0.85815\n",
      "Epoch:  404  Training Loss:  11.657  Training Accuracy:  0.85885\n",
      "Epoch:  405  Training Loss:  11.5757  Training Accuracy:  0.8592\n",
      "Epoch:  406  Training Loss:  11.5453  Training Accuracy:  0.859841\n",
      "Epoch:  407  Training Loss:  11.4845  Training Accuracy:  0.860132\n",
      "Epoch:  408  Training Loss:  11.4106  Training Accuracy:  0.860599\n",
      "Epoch:  409  Training Loss:  11.3864  Training Accuracy:  0.860949\n",
      "Epoch:  410  Training Loss:  11.3059  Training Accuracy:  0.860949\n",
      "Epoch:  411  Training Loss:  11.296  Training Accuracy:  0.861357\n",
      "Epoch:  412  Training Loss:  11.186  Training Accuracy:  0.861706\n",
      "Epoch:  413  Training Loss:  11.2151  Training Accuracy:  0.861998\n",
      "Epoch:  414  Training Loss:  11.1033  Training Accuracy:  0.862348\n",
      "Epoch:  415  Training Loss:  11.0969  Training Accuracy:  0.862814\n",
      "Epoch:  416  Training Loss:  10.9922  Training Accuracy:  0.863222\n",
      "Epoch:  417  Training Loss:  11.0175  Training Accuracy:  0.863397\n",
      "Epoch:  418  Training Loss:  10.9039  Training Accuracy:  0.863805\n",
      "Epoch:  419  Training Loss:  10.9189  Training Accuracy:  0.86398\n",
      "Epoch:  420  Training Loss:  10.8094  Training Accuracy:  0.864563\n",
      "Epoch:  421  Training Loss:  10.8118  Training Accuracy:  0.86468\n",
      "Epoch:  422  Training Loss:  10.7429  Training Accuracy:  0.865146\n",
      "Epoch:  423  Training Loss:  10.7242  Training Accuracy:  0.865788\n",
      "Epoch:  424  Training Loss:  10.6589  Training Accuracy:  0.866371\n",
      "Epoch:  425  Training Loss:  10.6128  Training Accuracy:  0.866837\n",
      "Epoch:  426  Training Loss:  10.5779  Training Accuracy:  0.867187\n",
      "Epoch:  427  Training Loss:  10.5036  Training Accuracy:  0.867653\n",
      "Epoch:  428  Training Loss:  10.4927  Training Accuracy:  0.867887\n",
      "Epoch:  429  Training Loss:  10.4148  Training Accuracy:  0.86812\n",
      "Epoch:  430  Training Loss:  10.4161  Training Accuracy:  0.868586\n",
      "Epoch:  431  Training Loss:  10.3387  Training Accuracy:  0.868994\n",
      "Epoch:  432  Training Loss:  10.3066  Training Accuracy:  0.869694\n",
      "Epoch:  433  Training Loss:  10.2505  Training Accuracy:  0.870219\n",
      "Epoch:  434  Training Loss:  10.2102  Training Accuracy:  0.870569\n",
      "Epoch:  435  Training Loss:  10.1537  Training Accuracy:  0.870744\n",
      "Epoch:  436  Training Loss:  10.1312  Training Accuracy:  0.871093\n",
      "Epoch:  437  Training Loss:  10.0805  Training Accuracy:  0.871268\n",
      "Epoch:  438  Training Loss:  10.0507  Training Accuracy:  0.871501\n",
      "Epoch:  439  Training Loss:  10.0029  Training Accuracy:  0.871501\n",
      "Epoch:  440  Training Loss:  9.9582  Training Accuracy:  0.871735\n",
      "Epoch:  441  Training Loss:  9.91706  Training Accuracy:  0.871968\n",
      "Epoch:  442  Training Loss:  9.87201  Training Accuracy:  0.872434\n",
      "Epoch:  443  Training Loss:  9.83144  Training Accuracy:  0.872959\n",
      "Epoch:  444  Training Loss:  9.80147  Training Accuracy:  0.8736\n",
      "Epoch:  445  Training Loss:  9.75663  Training Accuracy:  0.873717\n",
      "Epoch:  446  Training Loss:  9.72818  Training Accuracy:  0.874242\n",
      "Epoch:  447  Training Loss:  9.68873  Training Accuracy:  0.87465\n",
      "Epoch:  448  Training Loss:  9.64383  Training Accuracy:  0.875349\n",
      "Epoch:  449  Training Loss:  9.61944  Training Accuracy:  0.875466\n",
      "Epoch:  450  Training Loss:  9.56867  Training Accuracy:  0.875932\n",
      "Epoch:  451  Training Loss:  9.54648  Training Accuracy:  0.876282\n",
      "Epoch:  452  Training Loss:  9.48413  Training Accuracy:  0.87669\n",
      "Epoch:  453  Training Loss:  9.48174  Training Accuracy:  0.876923\n",
      "Epoch:  454  Training Loss:  9.40701  Training Accuracy:  0.877157\n",
      "Epoch:  455  Training Loss:  9.40722  Training Accuracy:  0.877681\n",
      "Epoch:  456  Training Loss:  9.32297  Training Accuracy:  0.877973\n",
      "Epoch:  457  Training Loss:  9.34484  Training Accuracy:  0.878206\n",
      "Epoch:  458  Training Loss:  9.25419  Training Accuracy:  0.878614\n",
      "Epoch:  459  Training Loss:  9.27211  Training Accuracy:  0.878964\n",
      "Epoch:  460  Training Loss:  9.18011  Training Accuracy:  0.879489\n",
      "Epoch:  461  Training Loss:  9.18946  Training Accuracy:  0.879664\n",
      "Epoch:  462  Training Loss:  9.11154  Training Accuracy:  0.880305\n",
      "Epoch:  463  Training Loss:  9.09913  Training Accuracy:  0.880655\n",
      "Epoch:  464  Training Loss:  9.046  Training Accuracy:  0.881121\n",
      "Epoch:  465  Training Loss:  9.03683  Training Accuracy:  0.881413\n",
      "Epoch:  466  Training Loss:  8.958  Training Accuracy:  0.881763\n",
      "Epoch:  467  Training Loss:  8.99109  Training Accuracy:  0.881821\n",
      "Epoch:  468  Training Loss:  8.89913  Training Accuracy:  0.882287\n",
      "Epoch:  469  Training Loss:  8.89776  Training Accuracy:  0.882695\n",
      "Epoch:  470  Training Loss:  8.8251  Training Accuracy:  0.883103\n",
      "Epoch:  471  Training Loss:  8.82691  Training Accuracy:  0.883337\n",
      "Epoch:  472  Training Loss:  8.76718  Training Accuracy:  0.883687\n",
      "Epoch:  473  Training Loss:  8.77248  Training Accuracy:  0.884095\n",
      "Epoch:  474  Training Loss:  8.72194  Training Accuracy:  0.884328\n",
      "Epoch:  475  Training Loss:  8.70647  Training Accuracy:  0.884736\n",
      "Epoch:  476  Training Loss:  8.65688  Training Accuracy:  0.885027\n",
      "Epoch:  477  Training Loss:  8.62659  Training Accuracy:  0.885727\n",
      "Epoch:  478  Training Loss:  8.58461  Training Accuracy:  0.886019\n",
      "Epoch:  479  Training Loss:  8.56915  Training Accuracy:  0.886194\n",
      "Epoch:  480  Training Loss:  8.50816  Training Accuracy:  0.886427\n",
      "Epoch:  481  Training Loss:  8.50861  Training Accuracy:  0.886718\n",
      "Epoch:  482  Training Loss:  8.45174  Training Accuracy:  0.886893\n",
      "Epoch:  483  Training Loss:  8.42968  Training Accuracy:  0.88736\n",
      "Epoch:  484  Training Loss:  8.39024  Training Accuracy:  0.887593\n",
      "Epoch:  485  Training Loss:  8.37629  Training Accuracy:  0.887943\n",
      "Epoch:  486  Training Loss:  8.32052  Training Accuracy:  0.887943\n",
      "Epoch:  487  Training Loss:  8.26417  Training Accuracy:  0.888409\n",
      "Epoch:  488  Training Loss:  8.26834  Training Accuracy:  0.888409\n",
      "Epoch:  489  Training Loss:  8.24764  Training Accuracy:  0.888292\n",
      "Epoch:  490  Training Loss:  8.17997  Training Accuracy:  0.888759\n",
      "Epoch:  491  Training Loss:  8.13371  Training Accuracy:  0.88905\n",
      "Epoch:  492  Training Loss:  8.13095  Training Accuracy:  0.889342\n",
      "Epoch:  493  Training Loss:  8.10452  Training Accuracy:  0.88975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  8.07328  Training Accuracy:  0.890041\n",
      "Epoch:  495  Training Loss:  8.04691  Training Accuracy:  0.89045\n",
      "Epoch:  496  Training Loss:  7.99835  Training Accuracy:  0.890625\n",
      "Epoch:  497  Training Loss:  7.98153  Training Accuracy:  0.891033\n",
      "Epoch:  498  Training Loss:  7.94197  Training Accuracy:  0.891266\n",
      "Epoch:  499  Training Loss:  7.89997  Training Accuracy:  0.891441\n",
      "Epoch:  500  Training Loss:  7.86962  Training Accuracy:  0.891674\n",
      "Epoch:  501  Training Loss:  7.8395  Training Accuracy:  0.891965\n",
      "Epoch:  502  Training Loss:  7.8131  Training Accuracy:  0.892315\n",
      "Epoch:  503  Training Loss:  7.78531  Training Accuracy:  0.892607\n",
      "Epoch:  504  Training Loss:  7.74958  Training Accuracy:  0.893015\n",
      "Epoch:  505  Training Loss:  7.73275  Training Accuracy:  0.893365\n",
      "Epoch:  506  Training Loss:  7.70393  Training Accuracy:  0.893598\n",
      "Epoch:  507  Training Loss:  7.68625  Training Accuracy:  0.893598\n",
      "Epoch:  508  Training Loss:  7.646  Training Accuracy:  0.894123\n",
      "Epoch:  509  Training Loss:  7.62835  Training Accuracy:  0.894414\n",
      "Epoch:  510  Training Loss:  7.59161  Training Accuracy:  0.894881\n",
      "Epoch:  511  Training Loss:  7.57178  Training Accuracy:  0.895172\n",
      "Epoch:  512  Training Loss:  7.53993  Training Accuracy:  0.895464\n",
      "Epoch:  513  Training Loss:  7.50691  Training Accuracy:  0.89558\n",
      "Epoch:  514  Training Loss:  7.47651  Training Accuracy:  0.895755\n",
      "Epoch:  515  Training Loss:  7.45409  Training Accuracy:  0.896047\n",
      "Epoch:  516  Training Loss:  7.405  Training Accuracy:  0.89628\n",
      "Epoch:  517  Training Loss:  7.38354  Training Accuracy:  0.896513\n",
      "Epoch:  518  Training Loss:  7.34428  Training Accuracy:  0.89663\n",
      "Epoch:  519  Training Loss:  7.33135  Training Accuracy:  0.896979\n",
      "Epoch:  520  Training Loss:  7.27465  Training Accuracy:  0.897388\n",
      "Epoch:  521  Training Loss:  7.26324  Training Accuracy:  0.897563\n",
      "Epoch:  522  Training Loss:  7.18197  Training Accuracy:  0.897971\n",
      "Epoch:  523  Training Loss:  7.23382  Training Accuracy:  0.898262\n",
      "Epoch:  524  Training Loss:  7.11663  Training Accuracy:  0.898845\n",
      "Epoch:  525  Training Loss:  7.17287  Training Accuracy:  0.898962\n",
      "Epoch:  526  Training Loss:  7.04774  Training Accuracy:  0.899545\n",
      "Epoch:  527  Training Loss:  7.10568  Training Accuracy:  0.900069\n",
      "Epoch:  528  Training Loss:  6.99864  Training Accuracy:  0.900419\n",
      "Epoch:  529  Training Loss:  7.04313  Training Accuracy:  0.900769\n",
      "Epoch:  530  Training Loss:  6.93659  Training Accuracy:  0.901177\n",
      "Epoch:  531  Training Loss:  6.99752  Training Accuracy:  0.901352\n",
      "Epoch:  532  Training Loss:  6.86945  Training Accuracy:  0.901585\n",
      "Epoch:  533  Training Loss:  6.93141  Training Accuracy:  0.902052\n",
      "Epoch:  534  Training Loss:  6.82814  Training Accuracy:  0.902402\n",
      "Epoch:  535  Training Loss:  6.8848  Training Accuracy:  0.902693\n",
      "Epoch:  536  Training Loss:  6.78762  Training Accuracy:  0.903218\n",
      "Epoch:  537  Training Loss:  6.84726  Training Accuracy:  0.903393\n",
      "Epoch:  538  Training Loss:  6.72466  Training Accuracy:  0.903568\n",
      "Epoch:  539  Training Loss:  6.78169  Training Accuracy:  0.903742\n",
      "Epoch:  540  Training Loss:  6.6918  Training Accuracy:  0.903976\n",
      "Epoch:  541  Training Loss:  6.72812  Training Accuracy:  0.904092\n",
      "Epoch:  542  Training Loss:  6.67108  Training Accuracy:  0.9045\n",
      "Epoch:  543  Training Loss:  6.6653  Training Accuracy:  0.904792\n",
      "Epoch:  544  Training Loss:  6.63848  Training Accuracy:  0.90485\n",
      "Epoch:  545  Training Loss:  6.61958  Training Accuracy:  0.905433\n",
      "Epoch:  546  Training Loss:  6.55218  Training Accuracy:  0.905725\n",
      "Epoch:  547  Training Loss:  6.57684  Training Accuracy:  0.906016\n",
      "Epoch:  548  Training Loss:  6.51828  Training Accuracy:  0.906249\n",
      "Epoch:  549  Training Loss:  6.53197  Training Accuracy:  0.906366\n",
      "Epoch:  550  Training Loss:  6.47144  Training Accuracy:  0.906832\n",
      "Epoch:  551  Training Loss:  6.45941  Training Accuracy:  0.906833\n",
      "Epoch:  552  Training Loss:  6.40527  Training Accuracy:  0.907182\n",
      "Epoch:  553  Training Loss:  6.44088  Training Accuracy:  0.907707\n",
      "Epoch:  554  Training Loss:  6.3545  Training Accuracy:  0.907765\n",
      "Epoch:  555  Training Loss:  6.36105  Training Accuracy:  0.90829\n",
      "Epoch:  556  Training Loss:  6.32445  Training Accuracy:  0.908523\n",
      "Epoch:  557  Training Loss:  6.31914  Training Accuracy:  0.908756\n",
      "Epoch:  558  Training Loss:  6.2879  Training Accuracy:  0.909106\n",
      "Epoch:  559  Training Loss:  6.25182  Training Accuracy:  0.909339\n",
      "Epoch:  560  Training Loss:  6.19239  Training Accuracy:  0.909631\n",
      "Epoch:  561  Training Loss:  6.23475  Training Accuracy:  0.910097\n",
      "Epoch:  562  Training Loss:  6.14332  Training Accuracy:  0.910039\n",
      "Epoch:  563  Training Loss:  6.17641  Training Accuracy:  0.910447\n",
      "Epoch:  564  Training Loss:  6.11616  Training Accuracy:  0.910331\n",
      "Epoch:  565  Training Loss:  6.12552  Training Accuracy:  0.910739\n",
      "Epoch:  566  Training Loss:  6.05226  Training Accuracy:  0.911263\n",
      "Epoch:  567  Training Loss:  6.08944  Training Accuracy:  0.911497\n",
      "Epoch:  568  Training Loss:  6.01525  Training Accuracy:  0.911788\n",
      "Epoch:  569  Training Loss:  6.04901  Training Accuracy:  0.91208\n",
      "Epoch:  570  Training Loss:  5.96105  Training Accuracy:  0.912255\n",
      "Epoch:  571  Training Loss:  6.00504  Training Accuracy:  0.912779\n",
      "Epoch:  572  Training Loss:  5.90262  Training Accuracy:  0.912954\n",
      "Epoch:  573  Training Loss:  5.95698  Training Accuracy:  0.913362\n",
      "Epoch:  574  Training Loss:  5.8712  Training Accuracy:  0.913654\n",
      "Epoch:  575  Training Loss:  5.90561  Training Accuracy:  0.913945\n",
      "Epoch:  576  Training Loss:  5.8375  Training Accuracy:  0.91412\n",
      "Epoch:  577  Training Loss:  5.86351  Training Accuracy:  0.914179\n",
      "Epoch:  578  Training Loss:  5.77736  Training Accuracy:  0.914353\n",
      "Epoch:  579  Training Loss:  5.81915  Training Accuracy:  0.914645\n",
      "Epoch:  580  Training Loss:  5.72984  Training Accuracy:  0.914645\n",
      "Epoch:  581  Training Loss:  5.7794  Training Accuracy:  0.915053\n",
      "Epoch:  582  Training Loss:  5.69484  Training Accuracy:  0.915053\n",
      "Epoch:  583  Training Loss:  5.73079  Training Accuracy:  0.915286\n",
      "Epoch:  584  Training Loss:  5.64467  Training Accuracy:  0.91517\n",
      "Epoch:  585  Training Loss:  5.68563  Training Accuracy:  0.915753\n",
      "Epoch:  586  Training Loss:  5.60954  Training Accuracy:  0.915811\n",
      "Epoch:  587  Training Loss:  5.63777  Training Accuracy:  0.916452\n",
      "Epoch:  588  Training Loss:  5.56736  Training Accuracy:  0.916452\n",
      "Epoch:  589  Training Loss:  5.5924  Training Accuracy:  0.917269\n",
      "Epoch:  590  Training Loss:  5.52589  Training Accuracy:  0.916919\n",
      "Epoch:  591  Training Loss:  5.55716  Training Accuracy:  0.917735\n",
      "Epoch:  592  Training Loss:  5.48761  Training Accuracy:  0.91756\n",
      "Epoch:  593  Training Loss:  5.50526  Training Accuracy:  0.918376\n",
      "Epoch:  594  Training Loss:  5.44443  Training Accuracy:  0.91826\n",
      "Epoch:  595  Training Loss:  5.47698  Training Accuracy:  0.919134\n",
      "Epoch:  596  Training Loss:  5.39931  Training Accuracy:  0.918784\n",
      "Epoch:  597  Training Loss:  5.42397  Training Accuracy:  0.919659\n",
      "Epoch:  598  Training Loss:  5.37024  Training Accuracy:  0.919367\n",
      "Epoch:  599  Training Loss:  5.38465  Training Accuracy:  0.920417\n",
      "Testing Accuracy: 0.821404\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 128\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 600\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  406.32  Training Accuracy:  0.0409865\n",
      "Epoch:  1  Training Loss:  351.361  Training Accuracy:  0.0421525\n",
      "Epoch:  2  Training Loss:  324.969  Training Accuracy:  0.0430854\n",
      "Epoch:  3  Training Loss:  308.678  Training Accuracy:  0.0486241\n",
      "Epoch:  4  Training Loss:  296.975  Training Accuracy:  0.0571945\n",
      "Epoch:  5  Training Loss:  288.892  Training Accuracy:  0.0624417\n",
      "Epoch:  6  Training Loss:  282.659  Training Accuracy:  0.0680387\n",
      "Epoch:  7  Training Loss:  277.205  Training Accuracy:  0.0735774\n",
      "Epoch:  8  Training Loss:  272.355  Training Accuracy:  0.0780084\n",
      "Epoch:  9  Training Loss:  268.178  Training Accuracy:  0.0811567\n",
      "Epoch:  10  Training Loss:  264.245  Training Accuracy:  0.0858792\n",
      "Epoch:  11  Training Loss:  260.81  Training Accuracy:  0.0915345\n",
      "Epoch:  12  Training Loss:  257.501  Training Accuracy:  0.0979478\n",
      "Epoch:  13  Training Loss:  254.102  Training Accuracy:  0.104594\n",
      "Epoch:  14  Training Loss:  250.879  Training Accuracy:  0.111765\n",
      "Epoch:  15  Training Loss:  248.761  Training Accuracy:  0.117071\n",
      "Epoch:  16  Training Loss:  246.727  Training Accuracy:  0.123368\n",
      "Epoch:  17  Training Loss:  244.337  Training Accuracy:  0.130539\n",
      "Epoch:  18  Training Loss:  242.018  Training Accuracy:  0.138118\n",
      "Epoch:  19  Training Loss:  239.455  Training Accuracy:  0.144881\n",
      "Epoch:  20  Training Loss:  237.394  Training Accuracy:  0.150886\n",
      "Epoch:  21  Training Loss:  235.067  Training Accuracy:  0.157125\n",
      "Epoch:  22  Training Loss:  232.724  Training Accuracy:  0.164121\n",
      "Epoch:  23  Training Loss:  230.436  Training Accuracy:  0.171933\n",
      "Epoch:  24  Training Loss:  228.126  Training Accuracy:  0.177764\n",
      "Epoch:  25  Training Loss:  225.865  Training Accuracy:  0.185226\n",
      "Epoch:  26  Training Loss:  223.762  Training Accuracy:  0.192164\n",
      "Epoch:  27  Training Loss:  221.873  Training Accuracy:  0.200152\n",
      "Epoch:  28  Training Loss:  219.924  Training Accuracy:  0.207381\n",
      "Epoch:  29  Training Loss:  217.761  Training Accuracy:  0.214202\n",
      "Epoch:  30  Training Loss:  215.173  Training Accuracy:  0.220674\n",
      "Epoch:  31  Training Loss:  212.536  Training Accuracy:  0.228603\n",
      "Epoch:  32  Training Loss:  209.859  Training Accuracy:  0.234783\n",
      "Epoch:  33  Training Loss:  207.13  Training Accuracy:  0.240555\n",
      "Epoch:  34  Training Loss:  204.515  Training Accuracy:  0.246968\n",
      "Epoch:  35  Training Loss:  201.725  Training Accuracy:  0.253207\n",
      "Epoch:  36  Training Loss:  199.032  Training Accuracy:  0.25857\n",
      "Epoch:  37  Training Loss:  196.3  Training Accuracy:  0.2651\n",
      "Epoch:  38  Training Loss:  193.522  Training Accuracy:  0.269298\n",
      "Epoch:  39  Training Loss:  190.768  Training Accuracy:  0.274662\n",
      "Epoch:  40  Training Loss:  188.075  Training Accuracy:  0.280434\n",
      "Epoch:  41  Training Loss:  185.279  Training Accuracy:  0.284981\n",
      "Epoch:  42  Training Loss:  182.623  Training Accuracy:  0.290229\n",
      "Epoch:  43  Training Loss:  179.993  Training Accuracy:  0.294426\n",
      "Epoch:  44  Training Loss:  177.433  Training Accuracy:  0.29909\n",
      "Epoch:  45  Training Loss:  174.85  Training Accuracy:  0.304163\n",
      "Epoch:  46  Training Loss:  172.247  Training Accuracy:  0.308535\n",
      "Epoch:  47  Training Loss:  169.664  Training Accuracy:  0.31215\n",
      "Epoch:  48  Training Loss:  167.194  Training Accuracy:  0.316348\n",
      "Epoch:  49  Training Loss:  164.606  Training Accuracy:  0.320312\n",
      "Epoch:  50  Training Loss:  162.134  Training Accuracy:  0.324277\n",
      "Epoch:  51  Training Loss:  159.779  Training Accuracy:  0.328183\n",
      "Epoch:  52  Training Loss:  157.408  Training Accuracy:  0.332381\n",
      "Epoch:  53  Training Loss:  155.129  Training Accuracy:  0.336462\n",
      "Epoch:  54  Training Loss:  152.851  Training Accuracy:  0.340019\n",
      "Epoch:  55  Training Loss:  150.545  Training Accuracy:  0.343109\n",
      "Epoch:  56  Training Loss:  148.347  Training Accuracy:  0.347423\n",
      "Epoch:  57  Training Loss:  146.139  Training Accuracy:  0.350804\n",
      "Epoch:  58  Training Loss:  144.038  Training Accuracy:  0.354478\n",
      "Epoch:  59  Training Loss:  141.967  Training Accuracy:  0.358151\n",
      "Epoch:  60  Training Loss:  139.722  Training Accuracy:  0.361532\n",
      "Epoch:  61  Training Loss:  137.72  Training Accuracy:  0.366429\n",
      "Epoch:  62  Training Loss:  135.685  Training Accuracy:  0.370103\n",
      "Epoch:  63  Training Loss:  133.623  Training Accuracy:  0.372843\n",
      "Epoch:  64  Training Loss:  131.578  Training Accuracy:  0.376457\n",
      "Epoch:  65  Training Loss:  129.75  Training Accuracy:  0.379956\n",
      "Epoch:  66  Training Loss:  127.816  Training Accuracy:  0.383279\n",
      "Epoch:  67  Training Loss:  126.003  Training Accuracy:  0.386719\n",
      "Epoch:  68  Training Loss:  124.107  Training Accuracy:  0.389925\n",
      "Epoch:  69  Training Loss:  122.283  Training Accuracy:  0.392549\n",
      "Epoch:  70  Training Loss:  120.458  Training Accuracy:  0.395114\n",
      "Epoch:  71  Training Loss:  118.591  Training Accuracy:  0.398612\n",
      "Epoch:  72  Training Loss:  116.745  Training Accuracy:  0.401644\n",
      "Epoch:  73  Training Loss:  114.982  Training Accuracy:  0.404676\n",
      "Epoch:  74  Training Loss:  113.244  Training Accuracy:  0.407999\n",
      "Epoch:  75  Training Loss:  111.45  Training Accuracy:  0.411555\n",
      "Epoch:  76  Training Loss:  109.76  Training Accuracy:  0.414062\n",
      "Epoch:  77  Training Loss:  108.035  Training Accuracy:  0.416453\n",
      "Epoch:  78  Training Loss:  106.396  Training Accuracy:  0.418435\n",
      "Epoch:  79  Training Loss:  104.774  Training Accuracy:  0.421758\n",
      "Epoch:  80  Training Loss:  103.211  Training Accuracy:  0.42444\n",
      "Epoch:  81  Training Loss:  101.625  Training Accuracy:  0.427005\n",
      "Epoch:  82  Training Loss:  100.123  Training Accuracy:  0.429746\n",
      "Epoch:  83  Training Loss:  98.571  Training Accuracy:  0.432661\n",
      "Epoch:  84  Training Loss:  97.1194  Training Accuracy:  0.434818\n",
      "Epoch:  85  Training Loss:  95.6286  Training Accuracy:  0.437908\n",
      "Epoch:  86  Training Loss:  94.331  Training Accuracy:  0.44129\n",
      "Epoch:  87  Training Loss:  93.1693  Training Accuracy:  0.444846\n",
      "Epoch:  88  Training Loss:  91.8589  Training Accuracy:  0.448169\n",
      "Epoch:  89  Training Loss:  90.4111  Training Accuracy:  0.450851\n",
      "Epoch:  90  Training Loss:  89.1378  Training Accuracy:  0.453416\n",
      "Epoch:  91  Training Loss:  87.8581  Training Accuracy:  0.455923\n",
      "Epoch:  92  Training Loss:  86.6708  Training Accuracy:  0.458664\n",
      "Epoch:  93  Training Loss:  85.3517  Training Accuracy:  0.460762\n",
      "Epoch:  94  Training Loss:  84.2454  Training Accuracy:  0.464319\n",
      "Epoch:  95  Training Loss:  82.8845  Training Accuracy:  0.467059\n",
      "Epoch:  96  Training Loss:  81.852  Training Accuracy:  0.470557\n",
      "Epoch:  97  Training Loss:  80.4879  Training Accuracy:  0.472889\n",
      "Epoch:  98  Training Loss:  79.4393  Training Accuracy:  0.474289\n",
      "Epoch:  99  Training Loss:  78.254  Training Accuracy:  0.476271\n",
      "Epoch:  100  Training Loss:  77.254  Training Accuracy:  0.477903\n",
      "Epoch:  101  Training Loss:  76.0562  Training Accuracy:  0.480352\n",
      "Epoch:  102  Training Loss:  75.086  Training Accuracy:  0.483617\n",
      "Epoch:  103  Training Loss:  73.9483  Training Accuracy:  0.486474\n",
      "Epoch:  104  Training Loss:  73.0347  Training Accuracy:  0.487698\n",
      "Epoch:  105  Training Loss:  71.9338  Training Accuracy:  0.488922\n",
      "Epoch:  106  Training Loss:  71.0052  Training Accuracy:  0.492129\n",
      "Epoch:  107  Training Loss:  69.9632  Training Accuracy:  0.495394\n",
      "Epoch:  108  Training Loss:  69.0704  Training Accuracy:  0.497376\n",
      "Epoch:  109  Training Loss:  68.0665  Training Accuracy:  0.500116\n",
      "Epoch:  110  Training Loss:  67.1917  Training Accuracy:  0.502215\n",
      "Epoch:  111  Training Loss:  66.2977  Training Accuracy:  0.503498\n",
      "Epoch:  112  Training Loss:  65.3805  Training Accuracy:  0.50583\n",
      "Epoch:  113  Training Loss:  64.4889  Training Accuracy:  0.507929\n",
      "Epoch:  114  Training Loss:  63.6114  Training Accuracy:  0.510203\n",
      "Epoch:  115  Training Loss:  62.6744  Training Accuracy:  0.511894\n",
      "Epoch:  116  Training Loss:  61.9098  Training Accuracy:  0.513992\n",
      "Epoch:  117  Training Loss:  61.0509  Training Accuracy:  0.5158\n",
      "Epoch:  118  Training Loss:  60.2536  Training Accuracy:  0.517432\n",
      "Epoch:  119  Training Loss:  59.5714  Training Accuracy:  0.519589\n",
      "Epoch:  120  Training Loss:  58.7914  Training Accuracy:  0.521572\n",
      "Epoch:  121  Training Loss:  58.0174  Training Accuracy:  0.523204\n",
      "Epoch:  122  Training Loss:  57.2716  Training Accuracy:  0.526119\n",
      "Epoch:  123  Training Loss:  56.5096  Training Accuracy:  0.527402\n",
      "Epoch:  124  Training Loss:  55.7831  Training Accuracy:  0.528976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  125  Training Loss:  55.086  Training Accuracy:  0.530783\n",
      "Epoch:  126  Training Loss:  54.4521  Training Accuracy:  0.532124\n",
      "Epoch:  127  Training Loss:  53.7637  Training Accuracy:  0.533873\n",
      "Epoch:  128  Training Loss:  53.1214  Training Accuracy:  0.535448\n",
      "Epoch:  129  Training Loss:  52.4772  Training Accuracy:  0.536847\n",
      "Epoch:  130  Training Loss:  51.8728  Training Accuracy:  0.538363\n",
      "Epoch:  131  Training Loss:  51.2478  Training Accuracy:  0.540228\n",
      "Epoch:  132  Training Loss:  50.6164  Training Accuracy:  0.541394\n",
      "Epoch:  133  Training Loss:  50.0509  Training Accuracy:  0.542794\n",
      "Epoch:  134  Training Loss:  49.431  Training Accuracy:  0.544601\n",
      "Epoch:  135  Training Loss:  48.903  Training Accuracy:  0.546233\n",
      "Epoch:  136  Training Loss:  48.3299  Training Accuracy:  0.548041\n",
      "Epoch:  137  Training Loss:  47.7674  Training Accuracy:  0.550373\n",
      "Epoch:  138  Training Loss:  47.2029  Training Accuracy:  0.552472\n",
      "Epoch:  139  Training Loss:  46.7154  Training Accuracy:  0.553988\n",
      "Epoch:  140  Training Loss:  46.1507  Training Accuracy:  0.555853\n",
      "Epoch:  141  Training Loss:  45.667  Training Accuracy:  0.557602\n",
      "Epoch:  142  Training Loss:  45.1788  Training Accuracy:  0.558885\n",
      "Epoch:  143  Training Loss:  44.6746  Training Accuracy:  0.560751\n",
      "Epoch:  144  Training Loss:  44.2388  Training Accuracy:  0.562733\n",
      "Epoch:  145  Training Loss:  43.779  Training Accuracy:  0.564424\n",
      "Epoch:  146  Training Loss:  43.3386  Training Accuracy:  0.566464\n",
      "Epoch:  147  Training Loss:  42.8832  Training Accuracy:  0.567689\n",
      "Epoch:  148  Training Loss:  42.4432  Training Accuracy:  0.568971\n",
      "Epoch:  149  Training Loss:  42.0252  Training Accuracy:  0.570662\n",
      "Epoch:  150  Training Loss:  41.5958  Training Accuracy:  0.571886\n",
      "Epoch:  151  Training Loss:  41.2032  Training Accuracy:  0.573052\n",
      "Epoch:  152  Training Loss:  40.7665  Training Accuracy:  0.573752\n",
      "Epoch:  153  Training Loss:  40.3774  Training Accuracy:  0.575093\n",
      "Epoch:  154  Training Loss:  39.9855  Training Accuracy:  0.576551\n",
      "Epoch:  155  Training Loss:  39.6339  Training Accuracy:  0.578008\n",
      "Epoch:  156  Training Loss:  39.2557  Training Accuracy:  0.57999\n",
      "Epoch:  157  Training Loss:  38.8897  Training Accuracy:  0.582148\n",
      "Epoch:  158  Training Loss:  38.5564  Training Accuracy:  0.583722\n",
      "Epoch:  159  Training Loss:  38.1796  Training Accuracy:  0.585354\n",
      "Epoch:  160  Training Loss:  37.8428  Training Accuracy:  0.587453\n",
      "Epoch:  161  Training Loss:  37.5072  Training Accuracy:  0.588911\n",
      "Epoch:  162  Training Loss:  37.0939  Training Accuracy:  0.590718\n",
      "Epoch:  163  Training Loss:  36.7968  Training Accuracy:  0.592001\n",
      "Epoch:  164  Training Loss:  36.4781  Training Accuracy:  0.593575\n",
      "Epoch:  165  Training Loss:  36.167  Training Accuracy:  0.595091\n",
      "Epoch:  166  Training Loss:  35.8748  Training Accuracy:  0.596781\n",
      "Epoch:  167  Training Loss:  35.5658  Training Accuracy:  0.598647\n",
      "Epoch:  168  Training Loss:  35.2181  Training Accuracy:  0.600163\n",
      "Epoch:  169  Training Loss:  34.9491  Training Accuracy:  0.601212\n",
      "Epoch:  170  Training Loss:  34.649  Training Accuracy:  0.60267\n",
      "Epoch:  171  Training Loss:  34.3326  Training Accuracy:  0.604477\n",
      "Epoch:  172  Training Loss:  34.0493  Training Accuracy:  0.605993\n",
      "Epoch:  173  Training Loss:  33.7529  Training Accuracy:  0.607567\n",
      "Epoch:  174  Training Loss:  33.4712  Training Accuracy:  0.609142\n",
      "Epoch:  175  Training Loss:  33.2134  Training Accuracy:  0.610249\n",
      "Epoch:  176  Training Loss:  32.9096  Training Accuracy:  0.611882\n",
      "Epoch:  177  Training Loss:  32.6404  Training Accuracy:  0.613339\n",
      "Epoch:  178  Training Loss:  32.387  Training Accuracy:  0.61538\n",
      "Epoch:  179  Training Loss:  32.1531  Training Accuracy:  0.616896\n",
      "Epoch:  180  Training Loss:  31.8696  Training Accuracy:  0.618237\n",
      "Epoch:  181  Training Loss:  31.6222  Training Accuracy:  0.619869\n",
      "Epoch:  182  Training Loss:  31.4015  Training Accuracy:  0.621035\n",
      "Epoch:  183  Training Loss:  31.1377  Training Accuracy:  0.622609\n",
      "Epoch:  184  Training Loss:  30.8877  Training Accuracy:  0.623542\n",
      "Epoch:  185  Training Loss:  30.6363  Training Accuracy:  0.624883\n",
      "Epoch:  186  Training Loss:  30.4281  Training Accuracy:  0.625991\n",
      "Epoch:  187  Training Loss:  30.1591  Training Accuracy:  0.627157\n",
      "Epoch:  188  Training Loss:  29.9644  Training Accuracy:  0.628206\n",
      "Epoch:  189  Training Loss:  29.7201  Training Accuracy:  0.629955\n",
      "Epoch:  190  Training Loss:  29.5228  Training Accuracy:  0.631763\n",
      "Epoch:  191  Training Loss:  29.3051  Training Accuracy:  0.633803\n",
      "Epoch:  192  Training Loss:  29.0758  Training Accuracy:  0.635552\n",
      "Epoch:  193  Training Loss:  28.8502  Training Accuracy:  0.636952\n",
      "Epoch:  194  Training Loss:  28.6615  Training Accuracy:  0.638118\n",
      "Epoch:  195  Training Loss:  28.4517  Training Accuracy:  0.639692\n",
      "Epoch:  196  Training Loss:  28.2718  Training Accuracy:  0.641033\n",
      "Epoch:  197  Training Loss:  28.0357  Training Accuracy:  0.642199\n",
      "Epoch:  198  Training Loss:  27.8348  Training Accuracy:  0.64389\n",
      "Epoch:  199  Training Loss:  27.6227  Training Accuracy:  0.644881\n",
      "Epoch:  200  Training Loss:  27.4387  Training Accuracy:  0.646105\n",
      "Epoch:  201  Training Loss:  27.2509  Training Accuracy:  0.648204\n",
      "Epoch:  202  Training Loss:  27.0816  Training Accuracy:  0.649545\n",
      "Epoch:  203  Training Loss:  26.888  Training Accuracy:  0.650886\n",
      "Epoch:  204  Training Loss:  26.6916  Training Accuracy:  0.652169\n",
      "Epoch:  205  Training Loss:  26.5282  Training Accuracy:  0.653335\n",
      "Epoch:  206  Training Loss:  26.3364  Training Accuracy:  0.654676\n",
      "Epoch:  207  Training Loss:  26.1511  Training Accuracy:  0.656716\n",
      "Epoch:  208  Training Loss:  25.9938  Training Accuracy:  0.658115\n",
      "Epoch:  209  Training Loss:  25.8113  Training Accuracy:  0.659398\n",
      "Epoch:  210  Training Loss:  25.652  Training Accuracy:  0.660797\n",
      "Epoch:  211  Training Loss:  25.4721  Training Accuracy:  0.662838\n",
      "Epoch:  212  Training Loss:  25.2906  Training Accuracy:  0.664062\n",
      "Epoch:  213  Training Loss:  25.1339  Training Accuracy:  0.665986\n",
      "Epoch:  214  Training Loss:  24.9568  Training Accuracy:  0.667269\n",
      "Epoch:  215  Training Loss:  24.8095  Training Accuracy:  0.668668\n",
      "Epoch:  216  Training Loss:  24.6235  Training Accuracy:  0.670126\n",
      "Epoch:  217  Training Loss:  24.4845  Training Accuracy:  0.672108\n",
      "Epoch:  218  Training Loss:  24.333  Training Accuracy:  0.672924\n",
      "Epoch:  219  Training Loss:  24.2056  Training Accuracy:  0.674557\n",
      "Epoch:  220  Training Loss:  24.06  Training Accuracy:  0.676014\n",
      "Epoch:  221  Training Loss:  23.8966  Training Accuracy:  0.677472\n",
      "Epoch:  222  Training Loss:  23.7709  Training Accuracy:  0.678696\n",
      "Epoch:  223  Training Loss:  23.6438  Training Accuracy:  0.679862\n",
      "Epoch:  224  Training Loss:  23.4742  Training Accuracy:  0.681028\n",
      "Epoch:  225  Training Loss:  23.3453  Training Accuracy:  0.682078\n",
      "Epoch:  226  Training Loss:  23.2074  Training Accuracy:  0.68371\n",
      "Epoch:  227  Training Loss:  23.0596  Training Accuracy:  0.685109\n",
      "Epoch:  228  Training Loss:  22.9347  Training Accuracy:  0.686917\n",
      "Epoch:  229  Training Loss:  22.7842  Training Accuracy:  0.688024\n",
      "Epoch:  230  Training Loss:  22.6581  Training Accuracy:  0.689599\n",
      "Epoch:  231  Training Loss:  22.5385  Training Accuracy:  0.690473\n",
      "Epoch:  232  Training Loss:  22.4103  Training Accuracy:  0.691348\n",
      "Epoch:  233  Training Loss:  22.2869  Training Accuracy:  0.692747\n",
      "Epoch:  234  Training Loss:  22.1576  Training Accuracy:  0.694438\n",
      "Epoch:  235  Training Loss:  22.0624  Training Accuracy:  0.696187\n",
      "Epoch:  236  Training Loss:  21.9249  Training Accuracy:  0.697586\n",
      "Epoch:  237  Training Loss:  21.8048  Training Accuracy:  0.698752\n",
      "Epoch:  238  Training Loss:  21.6796  Training Accuracy:  0.700268\n",
      "Epoch:  239  Training Loss:  21.5731  Training Accuracy:  0.702017\n",
      "Epoch:  240  Training Loss:  21.4214  Training Accuracy:  0.703183\n",
      "Epoch:  241  Training Loss:  21.3164  Training Accuracy:  0.704291\n",
      "Epoch:  242  Training Loss:  21.1983  Training Accuracy:  0.705457\n",
      "Epoch:  243  Training Loss:  21.0789  Training Accuracy:  0.706856\n",
      "Epoch:  244  Training Loss:  20.9455  Training Accuracy:  0.708547\n",
      "Epoch:  245  Training Loss:  20.8683  Training Accuracy:  0.709713\n",
      "Epoch:  246  Training Loss:  20.7223  Training Accuracy:  0.711229\n",
      "Epoch:  247  Training Loss:  20.6187  Training Accuracy:  0.712803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  248  Training Loss:  20.5028  Training Accuracy:  0.714085\n",
      "Epoch:  249  Training Loss:  20.3803  Training Accuracy:  0.714785\n",
      "Epoch:  250  Training Loss:  20.2621  Training Accuracy:  0.715893\n",
      "Epoch:  251  Training Loss:  20.1589  Training Accuracy:  0.716884\n",
      "Epoch:  252  Training Loss:  20.0295  Training Accuracy:  0.718342\n",
      "Epoch:  253  Training Loss:  19.9296  Training Accuracy:  0.719916\n",
      "Epoch:  254  Training Loss:  19.7958  Training Accuracy:  0.721315\n",
      "Epoch:  255  Training Loss:  19.7052  Training Accuracy:  0.722772\n",
      "Epoch:  256  Training Loss:  19.6015  Training Accuracy:  0.724347\n",
      "Epoch:  257  Training Loss:  19.4857  Training Accuracy:  0.725221\n",
      "Epoch:  258  Training Loss:  19.3742  Training Accuracy:  0.726737\n",
      "Epoch:  259  Training Loss:  19.2583  Training Accuracy:  0.727786\n",
      "Epoch:  260  Training Loss:  19.1557  Training Accuracy:  0.729186\n",
      "Epoch:  261  Training Loss:  19.0511  Training Accuracy:  0.730527\n",
      "Epoch:  262  Training Loss:  18.9283  Training Accuracy:  0.731984\n",
      "Epoch:  263  Training Loss:  18.8547  Training Accuracy:  0.733092\n",
      "Epoch:  264  Training Loss:  18.6973  Training Accuracy:  0.734375\n",
      "Epoch:  265  Training Loss:  18.6125  Training Accuracy:  0.735541\n",
      "Epoch:  266  Training Loss:  18.5211  Training Accuracy:  0.736765\n",
      "Epoch:  267  Training Loss:  18.4072  Training Accuracy:  0.737406\n",
      "Epoch:  268  Training Loss:  18.3129  Training Accuracy:  0.738514\n",
      "Epoch:  269  Training Loss:  18.1883  Training Accuracy:  0.739797\n",
      "Epoch:  270  Training Loss:  18.1054  Training Accuracy:  0.740846\n",
      "Epoch:  271  Training Loss:  17.9771  Training Accuracy:  0.74242\n",
      "Epoch:  272  Training Loss:  17.8994  Training Accuracy:  0.743703\n",
      "Epoch:  273  Training Loss:  17.8028  Training Accuracy:  0.744869\n",
      "Epoch:  274  Training Loss:  17.7133  Training Accuracy:  0.745685\n",
      "Epoch:  275  Training Loss:  17.5953  Training Accuracy:  0.746851\n",
      "Epoch:  276  Training Loss:  17.514  Training Accuracy:  0.747784\n",
      "Epoch:  277  Training Loss:  17.4053  Training Accuracy:  0.748892\n",
      "Epoch:  278  Training Loss:  17.3162  Training Accuracy:  0.750233\n",
      "Epoch:  279  Training Loss:  17.1935  Training Accuracy:  0.751166\n",
      "Epoch:  280  Training Loss:  17.1076  Training Accuracy:  0.752099\n",
      "Epoch:  281  Training Loss:  17.0105  Training Accuracy:  0.753206\n",
      "Epoch:  282  Training Loss:  16.9043  Training Accuracy:  0.753964\n",
      "Epoch:  283  Training Loss:  16.7955  Training Accuracy:  0.754664\n",
      "Epoch:  284  Training Loss:  16.7211  Training Accuracy:  0.75583\n",
      "Epoch:  285  Training Loss:  16.6021  Training Accuracy:  0.756704\n",
      "Epoch:  286  Training Loss:  16.5153  Training Accuracy:  0.757637\n",
      "Epoch:  287  Training Loss:  16.4206  Training Accuracy:  0.758628\n",
      "Epoch:  288  Training Loss:  16.3267  Training Accuracy:  0.760144\n",
      "Epoch:  289  Training Loss:  16.2469  Training Accuracy:  0.761777\n",
      "Epoch:  290  Training Loss:  16.1453  Training Accuracy:  0.762476\n",
      "Epoch:  291  Training Loss:  16.0611  Training Accuracy:  0.763876\n",
      "Epoch:  292  Training Loss:  15.9654  Training Accuracy:  0.765158\n",
      "Epoch:  293  Training Loss:  15.8914  Training Accuracy:  0.766499\n",
      "Epoch:  294  Training Loss:  15.786  Training Accuracy:  0.767549\n",
      "Epoch:  295  Training Loss:  15.7064  Training Accuracy:  0.768481\n",
      "Epoch:  296  Training Loss:  15.6121  Training Accuracy:  0.769473\n",
      "Epoch:  297  Training Loss:  15.5182  Training Accuracy:  0.770464\n",
      "Epoch:  298  Training Loss:  15.429  Training Accuracy:  0.77163\n",
      "Epoch:  299  Training Loss:  15.3434  Training Accuracy:  0.772621\n",
      "Epoch:  300  Training Loss:  15.2471  Training Accuracy:  0.773845\n",
      "Epoch:  301  Training Loss:  15.1557  Training Accuracy:  0.77472\n",
      "Epoch:  302  Training Loss:  15.0566  Training Accuracy:  0.775886\n",
      "Epoch:  303  Training Loss:  14.9812  Training Accuracy:  0.77676\n",
      "Epoch:  304  Training Loss:  14.8756  Training Accuracy:  0.777635\n",
      "Epoch:  305  Training Loss:  14.7927  Training Accuracy:  0.779034\n",
      "Epoch:  306  Training Loss:  14.7007  Training Accuracy:  0.780025\n",
      "Epoch:  307  Training Loss:  14.6082  Training Accuracy:  0.781016\n",
      "Epoch:  308  Training Loss:  14.528  Training Accuracy:  0.782416\n",
      "Epoch:  309  Training Loss:  14.4325  Training Accuracy:  0.783407\n",
      "Epoch:  310  Training Loss:  14.3396  Training Accuracy:  0.784456\n",
      "Epoch:  311  Training Loss:  14.2664  Training Accuracy:  0.785447\n",
      "Epoch:  312  Training Loss:  14.1788  Training Accuracy:  0.78638\n",
      "Epoch:  313  Training Loss:  14.0928  Training Accuracy:  0.787313\n",
      "Epoch:  314  Training Loss:  14.0142  Training Accuracy:  0.788129\n",
      "Epoch:  315  Training Loss:  13.9379  Training Accuracy:  0.788712\n",
      "Epoch:  316  Training Loss:  13.8441  Training Accuracy:  0.789587\n",
      "Epoch:  317  Training Loss:  13.7613  Training Accuracy:  0.790811\n",
      "Epoch:  318  Training Loss:  13.6744  Training Accuracy:  0.79221\n",
      "Epoch:  319  Training Loss:  13.5928  Training Accuracy:  0.793435\n",
      "Epoch:  320  Training Loss:  13.5106  Training Accuracy:  0.794134\n",
      "Epoch:  321  Training Loss:  13.4371  Training Accuracy:  0.795009\n",
      "Epoch:  322  Training Loss:  13.3399  Training Accuracy:  0.795767\n",
      "Epoch:  323  Training Loss:  13.2687  Training Accuracy:  0.796758\n",
      "Epoch:  324  Training Loss:  13.1839  Training Accuracy:  0.797516\n",
      "Epoch:  325  Training Loss:  13.1046  Training Accuracy:  0.797982\n",
      "Epoch:  326  Training Loss:  13.011  Training Accuracy:  0.798973\n",
      "Epoch:  327  Training Loss:  12.9444  Training Accuracy:  0.799615\n",
      "Epoch:  328  Training Loss:  12.8568  Training Accuracy:  0.800781\n",
      "Epoch:  329  Training Loss:  12.776  Training Accuracy:  0.801889\n",
      "Epoch:  330  Training Loss:  12.6914  Training Accuracy:  0.802763\n",
      "Epoch:  331  Training Loss:  12.6144  Training Accuracy:  0.803463\n",
      "Epoch:  332  Training Loss:  12.5415  Training Accuracy:  0.804104\n",
      "Epoch:  333  Training Loss:  12.4622  Training Accuracy:  0.80492\n",
      "Epoch:  334  Training Loss:  12.3799  Training Accuracy:  0.806028\n",
      "Epoch:  335  Training Loss:  12.3032  Training Accuracy:  0.806728\n",
      "Epoch:  336  Training Loss:  12.2255  Training Accuracy:  0.807369\n",
      "Epoch:  337  Training Loss:  12.1498  Training Accuracy:  0.808244\n",
      "Epoch:  338  Training Loss:  12.0585  Training Accuracy:  0.808885\n",
      "Epoch:  339  Training Loss:  11.9889  Training Accuracy:  0.809526\n",
      "Epoch:  340  Training Loss:  11.8961  Training Accuracy:  0.810051\n",
      "Epoch:  341  Training Loss:  11.8313  Training Accuracy:  0.810984\n",
      "Epoch:  342  Training Loss:  11.7431  Training Accuracy:  0.8118\n",
      "Epoch:  343  Training Loss:  11.6821  Training Accuracy:  0.812733\n",
      "Epoch:  344  Training Loss:  11.5906  Training Accuracy:  0.813432\n",
      "Epoch:  345  Training Loss:  11.5247  Training Accuracy:  0.814482\n",
      "Epoch:  346  Training Loss:  11.4422  Training Accuracy:  0.815473\n",
      "Epoch:  347  Training Loss:  11.3649  Training Accuracy:  0.816114\n",
      "Epoch:  348  Training Loss:  11.2842  Training Accuracy:  0.81693\n",
      "Epoch:  349  Training Loss:  11.2148  Training Accuracy:  0.818155\n",
      "Epoch:  350  Training Loss:  11.1341  Training Accuracy:  0.818913\n",
      "Epoch:  351  Training Loss:  11.0538  Training Accuracy:  0.819263\n",
      "Epoch:  352  Training Loss:  10.9896  Training Accuracy:  0.819729\n",
      "Epoch:  353  Training Loss:  10.9046  Training Accuracy:  0.820312\n",
      "Epoch:  354  Training Loss:  10.8332  Training Accuracy:  0.820953\n",
      "Epoch:  355  Training Loss:  10.7609  Training Accuracy:  0.822178\n",
      "Epoch:  356  Training Loss:  10.6867  Training Accuracy:  0.822702\n",
      "Epoch:  357  Training Loss:  10.618  Training Accuracy:  0.823169\n",
      "Epoch:  358  Training Loss:  10.545  Training Accuracy:  0.823694\n",
      "Epoch:  359  Training Loss:  10.4712  Training Accuracy:  0.824626\n",
      "Epoch:  360  Training Loss:  10.3885  Training Accuracy:  0.825034\n",
      "Epoch:  361  Training Loss:  10.3249  Training Accuracy:  0.825326\n",
      "Epoch:  362  Training Loss:  10.2568  Training Accuracy:  0.825792\n",
      "Epoch:  363  Training Loss:  10.1834  Training Accuracy:  0.826375\n",
      "Epoch:  364  Training Loss:  10.1135  Training Accuracy:  0.82725\n",
      "Epoch:  365  Training Loss:  10.0458  Training Accuracy:  0.82795\n",
      "Epoch:  366  Training Loss:  9.98064  Training Accuracy:  0.828591\n",
      "Epoch:  367  Training Loss:  9.91912  Training Accuracy:  0.829291\n",
      "Epoch:  368  Training Loss:  9.8621  Training Accuracy:  0.829932\n",
      "Epoch:  369  Training Loss:  9.78817  Training Accuracy:  0.830515\n",
      "Epoch:  370  Training Loss:  9.74458  Training Accuracy:  0.831156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  371  Training Loss:  9.66998  Training Accuracy:  0.831798\n",
      "Epoch:  372  Training Loss:  9.60784  Training Accuracy:  0.832322\n",
      "Epoch:  373  Training Loss:  9.5591  Training Accuracy:  0.833022\n",
      "Epoch:  374  Training Loss:  9.48601  Training Accuracy:  0.833547\n",
      "Epoch:  375  Training Loss:  9.42569  Training Accuracy:  0.834363\n",
      "Epoch:  376  Training Loss:  9.3679  Training Accuracy:  0.834888\n",
      "Epoch:  377  Training Loss:  9.3073  Training Accuracy:  0.835471\n",
      "Epoch:  378  Training Loss:  9.24348  Training Accuracy:  0.836054\n",
      "Epoch:  379  Training Loss:  9.18737  Training Accuracy:  0.836287\n",
      "Epoch:  380  Training Loss:  9.1352  Training Accuracy:  0.836987\n",
      "Epoch:  381  Training Loss:  9.0707  Training Accuracy:  0.837803\n",
      "Epoch:  382  Training Loss:  9.02525  Training Accuracy:  0.838502\n",
      "Epoch:  383  Training Loss:  8.96442  Training Accuracy:  0.83926\n",
      "Epoch:  384  Training Loss:  8.90829  Training Accuracy:  0.840251\n",
      "Epoch:  385  Training Loss:  8.85886  Training Accuracy:  0.841068\n",
      "Epoch:  386  Training Loss:  8.79752  Training Accuracy:  0.841651\n",
      "Epoch:  387  Training Loss:  8.74615  Training Accuracy:  0.842525\n",
      "Epoch:  388  Training Loss:  8.70319  Training Accuracy:  0.842933\n",
      "Epoch:  389  Training Loss:  8.6417  Training Accuracy:  0.843458\n",
      "Epoch:  390  Training Loss:  8.59531  Training Accuracy:  0.844158\n",
      "Epoch:  391  Training Loss:  8.54447  Training Accuracy:  0.844624\n",
      "Epoch:  392  Training Loss:  8.49768  Training Accuracy:  0.845032\n",
      "Epoch:  393  Training Loss:  8.44242  Training Accuracy:  0.845557\n",
      "Epoch:  394  Training Loss:  8.39441  Training Accuracy:  0.84614\n",
      "Epoch:  395  Training Loss:  8.34815  Training Accuracy:  0.846898\n",
      "Epoch:  396  Training Loss:  8.2953  Training Accuracy:  0.847656\n",
      "Epoch:  397  Training Loss:  8.24784  Training Accuracy:  0.848355\n",
      "Epoch:  398  Training Loss:  8.19874  Training Accuracy:  0.849347\n",
      "Epoch:  399  Training Loss:  8.1457  Training Accuracy:  0.849813\n",
      "Epoch:  400  Training Loss:  8.10604  Training Accuracy:  0.850396\n",
      "Epoch:  401  Training Loss:  8.05681  Training Accuracy:  0.851037\n",
      "Epoch:  402  Training Loss:  8.00716  Training Accuracy:  0.851562\n",
      "Epoch:  403  Training Loss:  7.96856  Training Accuracy:  0.852145\n",
      "Epoch:  404  Training Loss:  7.9236  Training Accuracy:  0.852786\n",
      "Epoch:  405  Training Loss:  7.87529  Training Accuracy:  0.853194\n",
      "Epoch:  406  Training Loss:  7.83307  Training Accuracy:  0.853719\n",
      "Epoch:  407  Training Loss:  7.79194  Training Accuracy:  0.854011\n",
      "Epoch:  408  Training Loss:  7.74696  Training Accuracy:  0.854769\n",
      "Epoch:  409  Training Loss:  7.70979  Training Accuracy:  0.855293\n",
      "Epoch:  410  Training Loss:  7.66795  Training Accuracy:  0.855585\n",
      "Epoch:  411  Training Loss:  7.62267  Training Accuracy:  0.856343\n",
      "Epoch:  412  Training Loss:  7.58194  Training Accuracy:  0.856926\n",
      "Epoch:  413  Training Loss:  7.54748  Training Accuracy:  0.85745\n",
      "Epoch:  414  Training Loss:  7.50648  Training Accuracy:  0.858034\n",
      "Epoch:  415  Training Loss:  7.46455  Training Accuracy:  0.858617\n",
      "Epoch:  416  Training Loss:  7.43857  Training Accuracy:  0.859083\n",
      "Epoch:  417  Training Loss:  7.39433  Training Accuracy:  0.859491\n",
      "Epoch:  418  Training Loss:  7.36227  Training Accuracy:  0.859783\n",
      "Epoch:  419  Training Loss:  7.32035  Training Accuracy:  0.860657\n",
      "Epoch:  420  Training Loss:  7.28532  Training Accuracy:  0.861124\n",
      "Epoch:  421  Training Loss:  7.25189  Training Accuracy:  0.861648\n",
      "Epoch:  422  Training Loss:  7.21556  Training Accuracy:  0.862523\n",
      "Epoch:  423  Training Loss:  7.17148  Training Accuracy:  0.862756\n",
      "Epoch:  424  Training Loss:  7.13936  Training Accuracy:  0.863164\n",
      "Epoch:  425  Training Loss:  7.09963  Training Accuracy:  0.864155\n",
      "Epoch:  426  Training Loss:  7.05579  Training Accuracy:  0.864622\n",
      "Epoch:  427  Training Loss:  7.01971  Training Accuracy:  0.864913\n",
      "Epoch:  428  Training Loss:  6.98479  Training Accuracy:  0.865671\n",
      "Epoch:  429  Training Loss:  6.95261  Training Accuracy:  0.866079\n",
      "Epoch:  430  Training Loss:  6.90997  Training Accuracy:  0.866721\n",
      "Epoch:  431  Training Loss:  6.88551  Training Accuracy:  0.867245\n",
      "Epoch:  432  Training Loss:  6.84513  Training Accuracy:  0.867828\n",
      "Epoch:  433  Training Loss:  6.81175  Training Accuracy:  0.868295\n",
      "Epoch:  434  Training Loss:  6.77257  Training Accuracy:  0.868761\n",
      "Epoch:  435  Training Loss:  6.74026  Training Accuracy:  0.869286\n",
      "Epoch:  436  Training Loss:  6.70609  Training Accuracy:  0.869985\n",
      "Epoch:  437  Training Loss:  6.68005  Training Accuracy:  0.870335\n",
      "Epoch:  438  Training Loss:  6.6402  Training Accuracy:  0.870743\n",
      "Epoch:  439  Training Loss:  6.60999  Training Accuracy:  0.871385\n",
      "Epoch:  440  Training Loss:  6.57502  Training Accuracy:  0.871909\n",
      "Epoch:  441  Training Loss:  6.54387  Training Accuracy:  0.872259\n",
      "Epoch:  442  Training Loss:  6.50695  Training Accuracy:  0.872901\n",
      "Epoch:  443  Training Loss:  6.48463  Training Accuracy:  0.873425\n",
      "Epoch:  444  Training Loss:  6.44414  Training Accuracy:  0.873659\n",
      "Epoch:  445  Training Loss:  6.41735  Training Accuracy:  0.874242\n",
      "Epoch:  446  Training Loss:  6.38577  Training Accuracy:  0.874883\n",
      "Epoch:  447  Training Loss:  6.35161  Training Accuracy:  0.875058\n",
      "Epoch:  448  Training Loss:  6.31709  Training Accuracy:  0.875349\n",
      "Epoch:  449  Training Loss:  6.28715  Training Accuracy:  0.876166\n",
      "Epoch:  450  Training Loss:  6.25089  Training Accuracy:  0.876749\n",
      "Epoch:  451  Training Loss:  6.22591  Training Accuracy:  0.87704\n",
      "Epoch:  452  Training Loss:  6.19853  Training Accuracy:  0.877273\n",
      "Epoch:  453  Training Loss:  6.16208  Training Accuracy:  0.877973\n",
      "Epoch:  454  Training Loss:  6.13675  Training Accuracy:  0.878381\n",
      "Epoch:  455  Training Loss:  6.11174  Training Accuracy:  0.879022\n",
      "Epoch:  456  Training Loss:  6.07249  Training Accuracy:  0.879664\n",
      "Epoch:  457  Training Loss:  6.04917  Training Accuracy:  0.880072\n",
      "Epoch:  458  Training Loss:  6.01924  Training Accuracy:  0.880422\n",
      "Epoch:  459  Training Loss:  5.98569  Training Accuracy:  0.880771\n",
      "Epoch:  460  Training Loss:  5.95604  Training Accuracy:  0.881529\n",
      "Epoch:  461  Training Loss:  5.93418  Training Accuracy:  0.881996\n",
      "Epoch:  462  Training Loss:  5.89304  Training Accuracy:  0.88252\n",
      "Epoch:  463  Training Loss:  5.86964  Training Accuracy:  0.883162\n",
      "Epoch:  464  Training Loss:  5.84498  Training Accuracy:  0.88357\n",
      "Epoch:  465  Training Loss:  5.80986  Training Accuracy:  0.883861\n",
      "Epoch:  466  Training Loss:  5.78343  Training Accuracy:  0.884328\n",
      "Epoch:  467  Training Loss:  5.75439  Training Accuracy:  0.884736\n",
      "Epoch:  468  Training Loss:  5.72545  Training Accuracy:  0.885377\n",
      "Epoch:  469  Training Loss:  5.69127  Training Accuracy:  0.88631\n",
      "Epoch:  470  Training Loss:  5.67222  Training Accuracy:  0.886602\n",
      "Epoch:  471  Training Loss:  5.63655  Training Accuracy:  0.886893\n",
      "Epoch:  472  Training Loss:  5.60907  Training Accuracy:  0.887301\n",
      "Epoch:  473  Training Loss:  5.58445  Training Accuracy:  0.887768\n",
      "Epoch:  474  Training Loss:  5.55544  Training Accuracy:  0.888351\n",
      "Epoch:  475  Training Loss:  5.52734  Training Accuracy:  0.888992\n",
      "Epoch:  476  Training Loss:  5.50248  Training Accuracy:  0.889633\n",
      "Epoch:  477  Training Loss:  5.47595  Training Accuracy:  0.889925\n",
      "Epoch:  478  Training Loss:  5.4502  Training Accuracy:  0.89045\n",
      "Epoch:  479  Training Loss:  5.41841  Training Accuracy:  0.890916\n",
      "Epoch:  480  Training Loss:  5.39285  Training Accuracy:  0.891324\n",
      "Epoch:  481  Training Loss:  5.36757  Training Accuracy:  0.891732\n",
      "Epoch:  482  Training Loss:  5.34153  Training Accuracy:  0.891907\n",
      "Epoch:  483  Training Loss:  5.3185  Training Accuracy:  0.892257\n",
      "Epoch:  484  Training Loss:  5.29023  Training Accuracy:  0.893073\n",
      "Epoch:  485  Training Loss:  5.27038  Training Accuracy:  0.893365\n",
      "Epoch:  486  Training Loss:  5.24526  Training Accuracy:  0.893831\n",
      "Epoch:  487  Training Loss:  5.21697  Training Accuracy:  0.894298\n",
      "Epoch:  488  Training Loss:  5.19158  Training Accuracy:  0.894531\n",
      "Epoch:  489  Training Loss:  5.17034  Training Accuracy:  0.895055\n",
      "Epoch:  490  Training Loss:  5.14246  Training Accuracy:  0.89523\n",
      "Epoch:  491  Training Loss:  5.11931  Training Accuracy:  0.895813\n",
      "Epoch:  492  Training Loss:  5.08932  Training Accuracy:  0.896047\n",
      "Epoch:  493  Training Loss:  5.073  Training Accuracy:  0.896338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  494  Training Loss:  5.0434  Training Accuracy:  0.896921\n",
      "Epoch:  495  Training Loss:  5.0257  Training Accuracy:  0.897329\n",
      "Epoch:  496  Training Loss:  4.99344  Training Accuracy:  0.897796\n",
      "Epoch:  497  Training Loss:  4.97396  Training Accuracy:  0.898029\n",
      "Epoch:  498  Training Loss:  4.95386  Training Accuracy:  0.89832\n",
      "Epoch:  499  Training Loss:  4.92946  Training Accuracy:  0.89867\n",
      "Epoch:  500  Training Loss:  4.90425  Training Accuracy:  0.898903\n",
      "Epoch:  501  Training Loss:  4.8896  Training Accuracy:  0.899312\n",
      "Epoch:  502  Training Loss:  4.86286  Training Accuracy:  0.899661\n",
      "Epoch:  503  Training Loss:  4.84523  Training Accuracy:  0.900186\n",
      "Epoch:  504  Training Loss:  4.82032  Training Accuracy:  0.900361\n",
      "Epoch:  505  Training Loss:  4.79984  Training Accuracy:  0.900769\n",
      "Epoch:  506  Training Loss:  4.77455  Training Accuracy:  0.901002\n",
      "Epoch:  507  Training Loss:  4.75505  Training Accuracy:  0.901177\n",
      "Epoch:  508  Training Loss:  4.73514  Training Accuracy:  0.901702\n",
      "Epoch:  509  Training Loss:  4.71378  Training Accuracy:  0.901819\n",
      "Epoch:  510  Training Loss:  4.69247  Training Accuracy:  0.902052\n",
      "Epoch:  511  Training Loss:  4.67312  Training Accuracy:  0.902343\n",
      "Epoch:  512  Training Loss:  4.65332  Training Accuracy:  0.902518\n",
      "Epoch:  513  Training Loss:  4.63512  Training Accuracy:  0.90281\n",
      "Epoch:  514  Training Loss:  4.61155  Training Accuracy:  0.903276\n",
      "Epoch:  515  Training Loss:  4.5976  Training Accuracy:  0.903393\n",
      "Epoch:  516  Training Loss:  4.57603  Training Accuracy:  0.903743\n",
      "Epoch:  517  Training Loss:  4.55746  Training Accuracy:  0.904151\n",
      "Epoch:  518  Training Loss:  4.536  Training Accuracy:  0.904617\n",
      "Epoch:  519  Training Loss:  4.51859  Training Accuracy:  0.904792\n",
      "Epoch:  520  Training Loss:  4.49868  Training Accuracy:  0.905258\n",
      "Epoch:  521  Training Loss:  4.48158  Training Accuracy:  0.905666\n",
      "Epoch:  522  Training Loss:  4.45839  Training Accuracy:  0.906133\n",
      "Epoch:  523  Training Loss:  4.44251  Training Accuracy:  0.906541\n",
      "Epoch:  524  Training Loss:  4.42317  Training Accuracy:  0.906891\n",
      "Epoch:  525  Training Loss:  4.40273  Training Accuracy:  0.90759\n",
      "Epoch:  526  Training Loss:  4.38754  Training Accuracy:  0.907824\n",
      "Epoch:  527  Training Loss:  4.36756  Training Accuracy:  0.907765\n",
      "Epoch:  528  Training Loss:  4.34707  Training Accuracy:  0.908232\n",
      "Epoch:  529  Training Loss:  4.33068  Training Accuracy:  0.908465\n",
      "Epoch:  530  Training Loss:  4.31087  Training Accuracy:  0.90864\n",
      "Epoch:  531  Training Loss:  4.29429  Training Accuracy:  0.909048\n",
      "Epoch:  532  Training Loss:  4.27772  Training Accuracy:  0.90899\n",
      "Epoch:  533  Training Loss:  4.26205  Training Accuracy:  0.909048\n",
      "Epoch:  534  Training Loss:  4.23917  Training Accuracy:  0.909223\n",
      "Epoch:  535  Training Loss:  4.22589  Training Accuracy:  0.909456\n",
      "Epoch:  536  Training Loss:  4.21109  Training Accuracy:  0.909456\n",
      "Epoch:  537  Training Loss:  4.19203  Training Accuracy:  0.909398\n",
      "Epoch:  538  Training Loss:  4.17158  Training Accuracy:  0.909514\n",
      "Epoch:  539  Training Loss:  4.15857  Training Accuracy:  0.909631\n",
      "Epoch:  540  Training Loss:  4.13673  Training Accuracy:  0.909864\n",
      "Epoch:  541  Training Loss:  4.12595  Training Accuracy:  0.910039\n",
      "Epoch:  542  Training Loss:  4.10742  Training Accuracy:  0.910039\n",
      "Epoch:  543  Training Loss:  4.0952  Training Accuracy:  0.910097\n",
      "Epoch:  544  Training Loss:  4.07338  Training Accuracy:  0.910564\n",
      "Epoch:  545  Training Loss:  4.06608  Training Accuracy:  0.910622\n",
      "Epoch:  546  Training Loss:  4.04239  Training Accuracy:  0.910622\n",
      "Epoch:  547  Training Loss:  4.03226  Training Accuracy:  0.910914\n",
      "Epoch:  548  Training Loss:  4.01187  Training Accuracy:  0.910739\n",
      "Epoch:  549  Training Loss:  4.00027  Training Accuracy:  0.911147\n",
      "Epoch:  550  Training Loss:  3.97986  Training Accuracy:  0.91138\n",
      "Epoch:  551  Training Loss:  3.97128  Training Accuracy:  0.911497\n",
      "Epoch:  552  Training Loss:  3.94782  Training Accuracy:  0.911905\n",
      "Epoch:  553  Training Loss:  3.9385  Training Accuracy:  0.912138\n",
      "Epoch:  554  Training Loss:  3.92054  Training Accuracy:  0.912371\n",
      "Epoch:  555  Training Loss:  3.90396  Training Accuracy:  0.912546\n",
      "Epoch:  556  Training Loss:  3.88939  Training Accuracy:  0.912546\n",
      "Epoch:  557  Training Loss:  3.88088  Training Accuracy:  0.912604\n",
      "Epoch:  558  Training Loss:  3.85929  Training Accuracy:  0.912896\n",
      "Epoch:  559  Training Loss:  3.84787  Training Accuracy:  0.913013\n",
      "Epoch:  560  Training Loss:  3.83517  Training Accuracy:  0.913246\n",
      "Epoch:  561  Training Loss:  3.82237  Training Accuracy:  0.913421\n",
      "Epoch:  562  Training Loss:  3.80445  Training Accuracy:  0.914062\n",
      "Epoch:  563  Training Loss:  3.79726  Training Accuracy:  0.914179\n",
      "Epoch:  564  Training Loss:  3.77689  Training Accuracy:  0.914412\n",
      "Epoch:  565  Training Loss:  3.76683  Training Accuracy:  0.914645\n",
      "Epoch:  566  Training Loss:  3.75461  Training Accuracy:  0.915111\n",
      "Epoch:  567  Training Loss:  3.73949  Training Accuracy:  0.915228\n",
      "Epoch:  568  Training Loss:  3.72676  Training Accuracy:  0.915345\n",
      "Epoch:  569  Training Loss:  3.71582  Training Accuracy:  0.915519\n",
      "Epoch:  570  Training Loss:  3.70206  Training Accuracy:  0.915636\n",
      "Epoch:  571  Training Loss:  3.68859  Training Accuracy:  0.916103\n",
      "Epoch:  572  Training Loss:  3.67563  Training Accuracy:  0.916277\n",
      "Epoch:  573  Training Loss:  3.66406  Training Accuracy:  0.916744\n",
      "Epoch:  574  Training Loss:  3.65185  Training Accuracy:  0.916977\n",
      "Epoch:  575  Training Loss:  3.64438  Training Accuracy:  0.916977\n",
      "Epoch:  576  Training Loss:  3.62727  Training Accuracy:  0.91721\n",
      "Epoch:  577  Training Loss:  3.6182  Training Accuracy:  0.917677\n",
      "Epoch:  578  Training Loss:  3.60404  Training Accuracy:  0.91791\n",
      "Epoch:  579  Training Loss:  3.59433  Training Accuracy:  0.918085\n",
      "Epoch:  580  Training Loss:  3.57978  Training Accuracy:  0.918493\n",
      "Epoch:  581  Training Loss:  3.57357  Training Accuracy:  0.918668\n",
      "Epoch:  582  Training Loss:  3.56092  Training Accuracy:  0.918901\n",
      "Epoch:  583  Training Loss:  3.5495  Training Accuracy:  0.918959\n",
      "Epoch:  584  Training Loss:  3.54021  Training Accuracy:  0.919018\n",
      "Epoch:  585  Training Loss:  3.52851  Training Accuracy:  0.919309\n",
      "Epoch:  586  Training Loss:  3.516  Training Accuracy:  0.919426\n",
      "Epoch:  587  Training Loss:  3.50482  Training Accuracy:  0.919717\n",
      "Epoch:  588  Training Loss:  3.49221  Training Accuracy:  0.9203\n",
      "Epoch:  589  Training Loss:  3.48356  Training Accuracy:  0.920475\n",
      "Epoch:  590  Training Loss:  3.46949  Training Accuracy:  0.92065\n",
      "Epoch:  591  Training Loss:  3.4585  Training Accuracy:  0.920883\n",
      "Epoch:  592  Training Loss:  3.44461  Training Accuracy:  0.921058\n",
      "Epoch:  593  Training Loss:  3.43641  Training Accuracy:  0.921233\n",
      "Epoch:  594  Training Loss:  3.42302  Training Accuracy:  0.921641\n",
      "Epoch:  595  Training Loss:  3.41121  Training Accuracy:  0.9217\n",
      "Epoch:  596  Training Loss:  3.39736  Training Accuracy:  0.921991\n",
      "Epoch:  597  Training Loss:  3.3915  Training Accuracy:  0.922341\n",
      "Epoch:  598  Training Loss:  3.37248  Training Accuracy:  0.922458\n",
      "Epoch:  599  Training Loss:  3.37152  Training Accuracy:  0.922982\n",
      "Epoch:  600  Training Loss:  3.34858  Training Accuracy:  0.923041\n",
      "Epoch:  601  Training Loss:  3.34562  Training Accuracy:  0.92339\n",
      "Epoch:  602  Training Loss:  3.32707  Training Accuracy:  0.92339\n",
      "Epoch:  603  Training Loss:  3.319  Training Accuracy:  0.92374\n",
      "Epoch:  604  Training Loss:  3.30346  Training Accuracy:  0.923682\n",
      "Epoch:  605  Training Loss:  3.29837  Training Accuracy:  0.924207\n",
      "Epoch:  606  Training Loss:  3.27811  Training Accuracy:  0.924615\n",
      "Epoch:  607  Training Loss:  3.27347  Training Accuracy:  0.924964\n",
      "Epoch:  608  Training Loss:  3.2552  Training Accuracy:  0.925023\n",
      "Epoch:  609  Training Loss:  3.24865  Training Accuracy:  0.925314\n",
      "Epoch:  610  Training Loss:  3.23016  Training Accuracy:  0.925548\n",
      "Epoch:  611  Training Loss:  3.2255  Training Accuracy:  0.925548\n",
      "Epoch:  612  Training Loss:  3.2072  Training Accuracy:  0.925781\n",
      "Epoch:  613  Training Loss:  3.20249  Training Accuracy:  0.926131\n",
      "Epoch:  614  Training Loss:  3.18623  Training Accuracy:  0.926364\n",
      "Epoch:  615  Training Loss:  3.17572  Training Accuracy:  0.926539\n",
      "Epoch:  616  Training Loss:  3.16126  Training Accuracy:  0.926655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  617  Training Loss:  3.15542  Training Accuracy:  0.926772\n",
      "Epoch:  618  Training Loss:  3.13823  Training Accuracy:  0.92683\n",
      "Epoch:  619  Training Loss:  3.12983  Training Accuracy:  0.92718\n",
      "Epoch:  620  Training Loss:  3.11521  Training Accuracy:  0.927472\n",
      "Epoch:  621  Training Loss:  3.10427  Training Accuracy:  0.927646\n",
      "Epoch:  622  Training Loss:  3.09477  Training Accuracy:  0.928055\n",
      "Epoch:  623  Training Loss:  3.08533  Training Accuracy:  0.928463\n",
      "Epoch:  624  Training Loss:  3.07071  Training Accuracy:  0.928579\n",
      "Epoch:  625  Training Loss:  3.06038  Training Accuracy:  0.928754\n",
      "Epoch:  626  Training Loss:  3.05014  Training Accuracy:  0.928871\n",
      "Epoch:  627  Training Loss:  3.03835  Training Accuracy:  0.928987\n",
      "Epoch:  628  Training Loss:  3.02842  Training Accuracy:  0.929279\n",
      "Epoch:  629  Training Loss:  3.01863  Training Accuracy:  0.92957\n",
      "Epoch:  630  Training Loss:  3.00448  Training Accuracy:  0.929687\n",
      "Epoch:  631  Training Loss:  2.99845  Training Accuracy:  0.929862\n",
      "Epoch:  632  Training Loss:  2.98818  Training Accuracy:  0.930037\n",
      "Epoch:  633  Training Loss:  2.97819  Training Accuracy:  0.93027\n",
      "Epoch:  634  Training Loss:  2.9631  Training Accuracy:  0.930561\n",
      "Epoch:  635  Training Loss:  2.95938  Training Accuracy:  0.930561\n",
      "Epoch:  636  Training Loss:  2.94499  Training Accuracy:  0.93062\n",
      "Epoch:  637  Training Loss:  2.93778  Training Accuracy:  0.930853\n",
      "Epoch:  638  Training Loss:  2.92464  Training Accuracy:  0.931028\n",
      "Epoch:  639  Training Loss:  2.91808  Training Accuracy:  0.931378\n",
      "Epoch:  640  Training Loss:  2.9071  Training Accuracy:  0.931553\n",
      "Epoch:  641  Training Loss:  2.89571  Training Accuracy:  0.931844\n",
      "Epoch:  642  Training Loss:  2.88803  Training Accuracy:  0.932311\n",
      "Epoch:  643  Training Loss:  2.87293  Training Accuracy:  0.932719\n",
      "Epoch:  644  Training Loss:  2.86896  Training Accuracy:  0.932894\n",
      "Epoch:  645  Training Loss:  2.85554  Training Accuracy:  0.932952\n",
      "Epoch:  646  Training Loss:  2.85286  Training Accuracy:  0.933127\n",
      "Epoch:  647  Training Loss:  2.83853  Training Accuracy:  0.933302\n",
      "Epoch:  648  Training Loss:  2.83097  Training Accuracy:  0.93336\n",
      "Epoch:  649  Training Loss:  2.82119  Training Accuracy:  0.933593\n",
      "Epoch:  650  Training Loss:  2.81263  Training Accuracy:  0.93371\n",
      "Epoch:  651  Training Loss:  2.80557  Training Accuracy:  0.934001\n",
      "Epoch:  652  Training Loss:  2.79481  Training Accuracy:  0.93406\n",
      "Epoch:  653  Training Loss:  2.78628  Training Accuracy:  0.934351\n",
      "Epoch:  654  Training Loss:  2.7786  Training Accuracy:  0.934526\n",
      "Epoch:  655  Training Loss:  2.77095  Training Accuracy:  0.934643\n",
      "Epoch:  656  Training Loss:  2.76513  Training Accuracy:  0.934876\n",
      "Epoch:  657  Training Loss:  2.75266  Training Accuracy:  0.935051\n",
      "Epoch:  658  Training Loss:  2.747  Training Accuracy:  0.935342\n",
      "Epoch:  659  Training Loss:  2.73955  Training Accuracy:  0.935575\n",
      "Epoch:  660  Training Loss:  2.72774  Training Accuracy:  0.935692\n",
      "Epoch:  661  Training Loss:  2.72166  Training Accuracy:  0.935809\n",
      "Epoch:  662  Training Loss:  2.71316  Training Accuracy:  0.935867\n",
      "Epoch:  663  Training Loss:  2.69807  Training Accuracy:  0.9361\n",
      "Epoch:  664  Training Loss:  2.69506  Training Accuracy:  0.936217\n",
      "Epoch:  665  Training Loss:  2.68521  Training Accuracy:  0.936392\n",
      "Epoch:  666  Training Loss:  2.67545  Training Accuracy:  0.936508\n",
      "Epoch:  667  Training Loss:  2.6692  Training Accuracy:  0.936683\n",
      "Epoch:  668  Training Loss:  2.6597  Training Accuracy:  0.936858\n",
      "Epoch:  669  Training Loss:  2.65001  Training Accuracy:  0.937033\n",
      "Epoch:  670  Training Loss:  2.64427  Training Accuracy:  0.937266\n",
      "Epoch:  671  Training Loss:  2.63721  Training Accuracy:  0.937558\n",
      "Epoch:  672  Training Loss:  2.62347  Training Accuracy:  0.937616\n",
      "Epoch:  673  Training Loss:  2.61782  Training Accuracy:  0.937674\n",
      "Epoch:  674  Training Loss:  2.61334  Training Accuracy:  0.937674\n",
      "Epoch:  675  Training Loss:  2.59808  Training Accuracy:  0.937849\n",
      "Epoch:  676  Training Loss:  2.59457  Training Accuracy:  0.937966\n",
      "Epoch:  677  Training Loss:  2.58705  Training Accuracy:  0.938316\n",
      "Epoch:  678  Training Loss:  2.576  Training Accuracy:  0.938491\n",
      "Epoch:  679  Training Loss:  2.56858  Training Accuracy:  0.938666\n",
      "Epoch:  680  Training Loss:  2.56477  Training Accuracy:  0.938957\n",
      "Epoch:  681  Training Loss:  2.55182  Training Accuracy:  0.938957\n",
      "Epoch:  682  Training Loss:  2.54704  Training Accuracy:  0.939074\n",
      "Epoch:  683  Training Loss:  2.53659  Training Accuracy:  0.939307\n",
      "Epoch:  684  Training Loss:  2.53057  Training Accuracy:  0.93954\n",
      "Epoch:  685  Training Loss:  2.52215  Training Accuracy:  0.939773\n",
      "Epoch:  686  Training Loss:  2.51851  Training Accuracy:  0.939831\n",
      "Epoch:  687  Training Loss:  2.50849  Training Accuracy:  0.939948\n",
      "Epoch:  688  Training Loss:  2.50395  Training Accuracy:  0.940123\n",
      "Epoch:  689  Training Loss:  2.49455  Training Accuracy:  0.94024\n",
      "Epoch:  690  Training Loss:  2.48987  Training Accuracy:  0.940298\n",
      "Epoch:  691  Training Loss:  2.47686  Training Accuracy:  0.940415\n",
      "Epoch:  692  Training Loss:  2.47687  Training Accuracy:  0.940473\n",
      "Epoch:  693  Training Loss:  2.46575  Training Accuracy:  0.940473\n",
      "Epoch:  694  Training Loss:  2.46046  Training Accuracy:  0.940706\n",
      "Epoch:  695  Training Loss:  2.45409  Training Accuracy:  0.940881\n",
      "Epoch:  696  Training Loss:  2.44398  Training Accuracy:  0.941114\n",
      "Epoch:  697  Training Loss:  2.43957  Training Accuracy:  0.941347\n",
      "Epoch:  698  Training Loss:  2.43121  Training Accuracy:  0.941464\n",
      "Epoch:  699  Training Loss:  2.4243  Training Accuracy:  0.941697\n",
      "Epoch:  700  Training Loss:  2.41607  Training Accuracy:  0.941872\n",
      "Epoch:  701  Training Loss:  2.41098  Training Accuracy:  0.942047\n",
      "Epoch:  702  Training Loss:  2.40071  Training Accuracy:  0.942222\n",
      "Epoch:  703  Training Loss:  2.39325  Training Accuracy:  0.942222\n",
      "Epoch:  704  Training Loss:  2.38639  Training Accuracy:  0.942513\n",
      "Epoch:  705  Training Loss:  2.38197  Training Accuracy:  0.942572\n",
      "Epoch:  706  Training Loss:  2.37201  Training Accuracy:  0.942688\n",
      "Epoch:  707  Training Loss:  2.36611  Training Accuracy:  0.942922\n",
      "Epoch:  708  Training Loss:  2.35705  Training Accuracy:  0.943096\n",
      "Epoch:  709  Training Loss:  2.34715  Training Accuracy:  0.943213\n",
      "Epoch:  710  Training Loss:  2.34442  Training Accuracy:  0.943271\n",
      "Epoch:  711  Training Loss:  2.33499  Training Accuracy:  0.94333\n",
      "Epoch:  712  Training Loss:  2.33087  Training Accuracy:  0.943388\n",
      "Epoch:  713  Training Loss:  2.32147  Training Accuracy:  0.943505\n",
      "Epoch:  714  Training Loss:  2.31589  Training Accuracy:  0.943738\n",
      "Epoch:  715  Training Loss:  2.30806  Training Accuracy:  0.943796\n",
      "Epoch:  716  Training Loss:  2.29995  Training Accuracy:  0.943913\n",
      "Epoch:  717  Training Loss:  2.29326  Training Accuracy:  0.944029\n",
      "Epoch:  718  Training Loss:  2.28816  Training Accuracy:  0.944029\n",
      "Epoch:  719  Training Loss:  2.28073  Training Accuracy:  0.944321\n",
      "Epoch:  720  Training Loss:  2.27234  Training Accuracy:  0.944671\n",
      "Epoch:  721  Training Loss:  2.26695  Training Accuracy:  0.944787\n",
      "Epoch:  722  Training Loss:  2.25994  Training Accuracy:  0.944962\n",
      "Epoch:  723  Training Loss:  2.25356  Training Accuracy:  0.945079\n",
      "Epoch:  724  Training Loss:  2.24267  Training Accuracy:  0.945079\n",
      "Epoch:  725  Training Loss:  2.24102  Training Accuracy:  0.945195\n",
      "Epoch:  726  Training Loss:  2.23326  Training Accuracy:  0.945254\n",
      "Epoch:  727  Training Loss:  2.22559  Training Accuracy:  0.945429\n",
      "Epoch:  728  Training Loss:  2.21817  Training Accuracy:  0.945487\n",
      "Epoch:  729  Training Loss:  2.21405  Training Accuracy:  0.945545\n",
      "Epoch:  730  Training Loss:  2.20676  Training Accuracy:  0.945603\n",
      "Epoch:  731  Training Loss:  2.19981  Training Accuracy:  0.945837\n",
      "Epoch:  732  Training Loss:  2.19167  Training Accuracy:  0.945953\n",
      "Epoch:  733  Training Loss:  2.18791  Training Accuracy:  0.946186\n",
      "Epoch:  734  Training Loss:  2.18167  Training Accuracy:  0.946361\n",
      "Epoch:  735  Training Loss:  2.17291  Training Accuracy:  0.946536\n",
      "Epoch:  736  Training Loss:  2.16681  Training Accuracy:  0.946711\n",
      "Epoch:  737  Training Loss:  2.15916  Training Accuracy:  0.946944\n",
      "Epoch:  738  Training Loss:  2.15349  Training Accuracy:  0.947294\n",
      "Epoch:  739  Training Loss:  2.14565  Training Accuracy:  0.947469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  740  Training Loss:  2.1412  Training Accuracy:  0.947586\n",
      "Epoch:  741  Training Loss:  2.13538  Training Accuracy:  0.947702\n",
      "Epoch:  742  Training Loss:  2.12549  Training Accuracy:  0.947877\n",
      "Epoch:  743  Training Loss:  2.1196  Training Accuracy:  0.948169\n",
      "Epoch:  744  Training Loss:  2.11484  Training Accuracy:  0.948285\n",
      "Epoch:  745  Training Loss:  2.10828  Training Accuracy:  0.948577\n",
      "Epoch:  746  Training Loss:  2.09926  Training Accuracy:  0.948577\n",
      "Epoch:  747  Training Loss:  2.09443  Training Accuracy:  0.948693\n",
      "Epoch:  748  Training Loss:  2.08827  Training Accuracy:  0.948752\n",
      "Epoch:  749  Training Loss:  2.08216  Training Accuracy:  0.948985\n",
      "Epoch:  750  Training Loss:  2.07586  Training Accuracy:  0.949218\n",
      "Epoch:  751  Training Loss:  2.07039  Training Accuracy:  0.949276\n",
      "Epoch:  752  Training Loss:  2.06438  Training Accuracy:  0.949335\n",
      "Epoch:  753  Training Loss:  2.05656  Training Accuracy:  0.949451\n",
      "Epoch:  754  Training Loss:  2.05131  Training Accuracy:  0.949568\n",
      "Epoch:  755  Training Loss:  2.04695  Training Accuracy:  0.949743\n",
      "Epoch:  756  Training Loss:  2.03696  Training Accuracy:  0.949918\n",
      "Epoch:  757  Training Loss:  2.03378  Training Accuracy:  0.950268\n",
      "Epoch:  758  Training Loss:  2.02817  Training Accuracy:  0.950384\n",
      "Epoch:  759  Training Loss:  2.02298  Training Accuracy:  0.950559\n",
      "Epoch:  760  Training Loss:  2.0154  Training Accuracy:  0.950617\n",
      "Epoch:  761  Training Loss:  2.00929  Training Accuracy:  0.950676\n",
      "Epoch:  762  Training Loss:  2.00395  Training Accuracy:  0.950792\n",
      "Epoch:  763  Training Loss:  1.9967  Training Accuracy:  0.950909\n",
      "Epoch:  764  Training Loss:  1.99169  Training Accuracy:  0.950909\n",
      "Epoch:  765  Training Loss:  1.98748  Training Accuracy:  0.951142\n",
      "Epoch:  766  Training Loss:  1.9785  Training Accuracy:  0.9512\n",
      "Epoch:  767  Training Loss:  1.97382  Training Accuracy:  0.951317\n",
      "Epoch:  768  Training Loss:  1.97154  Training Accuracy:  0.951317\n",
      "Epoch:  769  Training Loss:  1.96258  Training Accuracy:  0.95155\n",
      "Epoch:  770  Training Loss:  1.95967  Training Accuracy:  0.951667\n",
      "Epoch:  771  Training Loss:  1.9515  Training Accuracy:  0.951842\n",
      "Epoch:  772  Training Loss:  1.94852  Training Accuracy:  0.951842\n",
      "Epoch:  773  Training Loss:  1.94148  Training Accuracy:  0.952017\n",
      "Epoch:  774  Training Loss:  1.93581  Training Accuracy:  0.952017\n",
      "Epoch:  775  Training Loss:  1.93291  Training Accuracy:  0.952075\n",
      "Epoch:  776  Training Loss:  1.92499  Training Accuracy:  0.952366\n",
      "Epoch:  777  Training Loss:  1.92078  Training Accuracy:  0.952483\n",
      "Epoch:  778  Training Loss:  1.91562  Training Accuracy:  0.952658\n",
      "Epoch:  779  Training Loss:  1.90752  Training Accuracy:  0.952775\n",
      "Epoch:  780  Training Loss:  1.90423  Training Accuracy:  0.952775\n",
      "Epoch:  781  Training Loss:  1.90022  Training Accuracy:  0.952775\n",
      "Epoch:  782  Training Loss:  1.89175  Training Accuracy:  0.952833\n",
      "Epoch:  783  Training Loss:  1.888  Training Accuracy:  0.952833\n",
      "Epoch:  784  Training Loss:  1.88375  Training Accuracy:  0.952891\n",
      "Epoch:  785  Training Loss:  1.87505  Training Accuracy:  0.95295\n",
      "Epoch:  786  Training Loss:  1.87416  Training Accuracy:  0.953299\n",
      "Epoch:  787  Training Loss:  1.86865  Training Accuracy:  0.953416\n",
      "Epoch:  788  Training Loss:  1.85968  Training Accuracy:  0.953649\n",
      "Epoch:  789  Training Loss:  1.85785  Training Accuracy:  0.953941\n",
      "Epoch:  790  Training Loss:  1.85243  Training Accuracy:  0.954057\n",
      "Epoch:  791  Training Loss:  1.84509  Training Accuracy:  0.954174\n",
      "Epoch:  792  Training Loss:  1.84225  Training Accuracy:  0.954232\n",
      "Epoch:  793  Training Loss:  1.83891  Training Accuracy:  0.954291\n",
      "Epoch:  794  Training Loss:  1.82991  Training Accuracy:  0.954524\n",
      "Epoch:  795  Training Loss:  1.82818  Training Accuracy:  0.95464\n",
      "Epoch:  796  Training Loss:  1.8241  Training Accuracy:  0.954757\n",
      "Epoch:  797  Training Loss:  1.81623  Training Accuracy:  0.95499\n",
      "Epoch:  798  Training Loss:  1.81394  Training Accuracy:  0.955048\n",
      "Epoch:  799  Training Loss:  1.80979  Training Accuracy:  0.955107\n",
      "Epoch:  800  Training Loss:  1.8035  Training Accuracy:  0.955457\n",
      "Epoch:  801  Training Loss:  1.80074  Training Accuracy:  0.955515\n",
      "Epoch:  802  Training Loss:  1.79545  Training Accuracy:  0.95569\n",
      "Epoch:  803  Training Loss:  1.78852  Training Accuracy:  0.955806\n",
      "Epoch:  804  Training Loss:  1.78617  Training Accuracy:  0.955923\n",
      "Epoch:  805  Training Loss:  1.77976  Training Accuracy:  0.955981\n",
      "Epoch:  806  Training Loss:  1.77635  Training Accuracy:  0.956098\n",
      "Epoch:  807  Training Loss:  1.76991  Training Accuracy:  0.956214\n",
      "Epoch:  808  Training Loss:  1.76772  Training Accuracy:  0.956156\n",
      "Epoch:  809  Training Loss:  1.76206  Training Accuracy:  0.956331\n",
      "Epoch:  810  Training Loss:  1.75594  Training Accuracy:  0.956448\n",
      "Epoch:  811  Training Loss:  1.75376  Training Accuracy:  0.956564\n",
      "Epoch:  812  Training Loss:  1.74987  Training Accuracy:  0.956564\n",
      "Epoch:  813  Training Loss:  1.74308  Training Accuracy:  0.956564\n",
      "Epoch:  814  Training Loss:  1.7398  Training Accuracy:  0.956681\n",
      "Epoch:  815  Training Loss:  1.73881  Training Accuracy:  0.956739\n",
      "Epoch:  816  Training Loss:  1.73001  Training Accuracy:  0.956856\n",
      "Epoch:  817  Training Loss:  1.7255  Training Accuracy:  0.957031\n",
      "Epoch:  818  Training Loss:  1.7236  Training Accuracy:  0.957147\n",
      "Epoch:  819  Training Loss:  1.7179  Training Accuracy:  0.957147\n",
      "Epoch:  820  Training Loss:  1.71331  Training Accuracy:  0.957264\n",
      "Epoch:  821  Training Loss:  1.70964  Training Accuracy:  0.95738\n",
      "Epoch:  822  Training Loss:  1.70508  Training Accuracy:  0.957497\n",
      "Epoch:  823  Training Loss:  1.69986  Training Accuracy:  0.957614\n",
      "Epoch:  824  Training Loss:  1.69725  Training Accuracy:  0.957672\n",
      "Epoch:  825  Training Loss:  1.69281  Training Accuracy:  0.957789\n",
      "Epoch:  826  Training Loss:  1.68628  Training Accuracy:  0.957789\n",
      "Epoch:  827  Training Loss:  1.68478  Training Accuracy:  0.957847\n",
      "Epoch:  828  Training Loss:  1.67987  Training Accuracy:  0.957964\n",
      "Epoch:  829  Training Loss:  1.67565  Training Accuracy:  0.958138\n",
      "Epoch:  830  Training Loss:  1.66973  Training Accuracy:  0.958197\n",
      "Epoch:  831  Training Loss:  1.66714  Training Accuracy:  0.958372\n",
      "Epoch:  832  Training Loss:  1.66348  Training Accuracy:  0.95843\n",
      "Epoch:  833  Training Loss:  1.6574  Training Accuracy:  0.958547\n",
      "Epoch:  834  Training Loss:  1.65449  Training Accuracy:  0.958605\n",
      "Epoch:  835  Training Loss:  1.64849  Training Accuracy:  0.958721\n",
      "Epoch:  836  Training Loss:  1.64328  Training Accuracy:  0.958838\n",
      "Epoch:  837  Training Loss:  1.64165  Training Accuracy:  0.958955\n",
      "Epoch:  838  Training Loss:  1.63533  Training Accuracy:  0.959013\n",
      "Epoch:  839  Training Loss:  1.63153  Training Accuracy:  0.959188\n",
      "Epoch:  840  Training Loss:  1.62833  Training Accuracy:  0.959188\n",
      "Epoch:  841  Training Loss:  1.62245  Training Accuracy:  0.959304\n",
      "Epoch:  842  Training Loss:  1.6193  Training Accuracy:  0.959479\n",
      "Epoch:  843  Training Loss:  1.61561  Training Accuracy:  0.959479\n",
      "Epoch:  844  Training Loss:  1.61044  Training Accuracy:  0.959596\n",
      "Epoch:  845  Training Loss:  1.60582  Training Accuracy:  0.959713\n",
      "Epoch:  846  Training Loss:  1.6019  Training Accuracy:  0.959771\n",
      "Epoch:  847  Training Loss:  1.59996  Training Accuracy:  0.959771\n",
      "Epoch:  848  Training Loss:  1.59353  Training Accuracy:  0.959946\n",
      "Epoch:  849  Training Loss:  1.58772  Training Accuracy:  0.960004\n",
      "Epoch:  850  Training Loss:  1.58564  Training Accuracy:  0.960062\n",
      "Epoch:  851  Training Loss:  1.58206  Training Accuracy:  0.960237\n",
      "Epoch:  852  Training Loss:  1.57683  Training Accuracy:  0.960471\n",
      "Epoch:  853  Training Loss:  1.5708  Training Accuracy:  0.960587\n",
      "Epoch:  854  Training Loss:  1.56927  Training Accuracy:  0.960645\n",
      "Epoch:  855  Training Loss:  1.56403  Training Accuracy:  0.960762\n",
      "Epoch:  856  Training Loss:  1.56196  Training Accuracy:  0.960879\n",
      "Epoch:  857  Training Loss:  1.55754  Training Accuracy:  0.960937\n",
      "Epoch:  858  Training Loss:  1.5515  Training Accuracy:  0.960937\n",
      "Epoch:  859  Training Loss:  1.55029  Training Accuracy:  0.961054\n",
      "Epoch:  860  Training Loss:  1.54433  Training Accuracy:  0.961112\n",
      "Epoch:  861  Training Loss:  1.54086  Training Accuracy:  0.96117\n",
      "Epoch:  862  Training Loss:  1.53664  Training Accuracy:  0.96117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  863  Training Loss:  1.53264  Training Accuracy:  0.96117\n",
      "Epoch:  864  Training Loss:  1.52875  Training Accuracy:  0.961228\n",
      "Epoch:  865  Training Loss:  1.52607  Training Accuracy:  0.961403\n",
      "Epoch:  866  Training Loss:  1.51995  Training Accuracy:  0.961578\n",
      "Epoch:  867  Training Loss:  1.51638  Training Accuracy:  0.961637\n",
      "Epoch:  868  Training Loss:  1.51351  Training Accuracy:  0.961695\n",
      "Epoch:  869  Training Loss:  1.50967  Training Accuracy:  0.961928\n",
      "Epoch:  870  Training Loss:  1.50479  Training Accuracy:  0.961928\n",
      "Epoch:  871  Training Loss:  1.50127  Training Accuracy:  0.962161\n",
      "Epoch:  872  Training Loss:  1.49856  Training Accuracy:  0.962161\n",
      "Epoch:  873  Training Loss:  1.49451  Training Accuracy:  0.962278\n",
      "Epoch:  874  Training Loss:  1.49031  Training Accuracy:  0.962336\n",
      "Epoch:  875  Training Loss:  1.48547  Training Accuracy:  0.962394\n",
      "Epoch:  876  Training Loss:  1.4808  Training Accuracy:  0.962394\n",
      "Epoch:  877  Training Loss:  1.47761  Training Accuracy:  0.962394\n",
      "Epoch:  878  Training Loss:  1.47559  Training Accuracy:  0.962511\n",
      "Epoch:  879  Training Loss:  1.47025  Training Accuracy:  0.962628\n",
      "Epoch:  880  Training Loss:  1.46655  Training Accuracy:  0.962628\n",
      "Epoch:  881  Training Loss:  1.46113  Training Accuracy:  0.962686\n",
      "Epoch:  882  Training Loss:  1.45833  Training Accuracy:  0.962686\n",
      "Epoch:  883  Training Loss:  1.45652  Training Accuracy:  0.962919\n",
      "Epoch:  884  Training Loss:  1.45106  Training Accuracy:  0.963094\n",
      "Epoch:  885  Training Loss:  1.44668  Training Accuracy:  0.963094\n",
      "Epoch:  886  Training Loss:  1.44374  Training Accuracy:  0.963269\n",
      "Epoch:  887  Training Loss:  1.44188  Training Accuracy:  0.963386\n",
      "Epoch:  888  Training Loss:  1.43529  Training Accuracy:  0.963444\n",
      "Epoch:  889  Training Loss:  1.43243  Training Accuracy:  0.963677\n",
      "Epoch:  890  Training Loss:  1.43016  Training Accuracy:  0.963852\n",
      "Epoch:  891  Training Loss:  1.42458  Training Accuracy:  0.96391\n",
      "Epoch:  892  Training Loss:  1.42445  Training Accuracy:  0.963969\n",
      "Epoch:  893  Training Loss:  1.41763  Training Accuracy:  0.964202\n",
      "Epoch:  894  Training Loss:  1.41505  Training Accuracy:  0.96426\n",
      "Epoch:  895  Training Loss:  1.41074  Training Accuracy:  0.964435\n",
      "Epoch:  896  Training Loss:  1.40799  Training Accuracy:  0.964493\n",
      "Epoch:  897  Training Loss:  1.40636  Training Accuracy:  0.964493\n",
      "Epoch:  898  Training Loss:  1.40057  Training Accuracy:  0.964552\n",
      "Epoch:  899  Training Loss:  1.3966  Training Accuracy:  0.964727\n",
      "Epoch:  900  Training Loss:  1.394  Training Accuracy:  0.96496\n",
      "Epoch:  901  Training Loss:  1.38974  Training Accuracy:  0.965251\n",
      "Epoch:  902  Training Loss:  1.38646  Training Accuracy:  0.96531\n",
      "Epoch:  903  Training Loss:  1.38229  Training Accuracy:  0.965368\n",
      "Epoch:  904  Training Loss:  1.37912  Training Accuracy:  0.965368\n",
      "Epoch:  905  Training Loss:  1.37605  Training Accuracy:  0.965426\n",
      "Epoch:  906  Training Loss:  1.37389  Training Accuracy:  0.965543\n",
      "Epoch:  907  Training Loss:  1.36847  Training Accuracy:  0.965659\n",
      "Epoch:  908  Training Loss:  1.36469  Training Accuracy:  0.965718\n",
      "Epoch:  909  Training Loss:  1.36156  Training Accuracy:  0.965893\n",
      "Epoch:  910  Training Loss:  1.35963  Training Accuracy:  0.965893\n",
      "Epoch:  911  Training Loss:  1.35448  Training Accuracy:  0.965951\n",
      "Epoch:  912  Training Loss:  1.35272  Training Accuracy:  0.965951\n",
      "Epoch:  913  Training Loss:  1.34765  Training Accuracy:  0.966009\n",
      "Epoch:  914  Training Loss:  1.34683  Training Accuracy:  0.965951\n",
      "Epoch:  915  Training Loss:  1.34235  Training Accuracy:  0.966126\n",
      "Epoch:  916  Training Loss:  1.33843  Training Accuracy:  0.966126\n",
      "Epoch:  917  Training Loss:  1.3359  Training Accuracy:  0.966184\n",
      "Epoch:  918  Training Loss:  1.33394  Training Accuracy:  0.966301\n",
      "Epoch:  919  Training Loss:  1.33005  Training Accuracy:  0.966301\n",
      "Epoch:  920  Training Loss:  1.32687  Training Accuracy:  0.966301\n",
      "Epoch:  921  Training Loss:  1.32301  Training Accuracy:  0.966417\n",
      "Epoch:  922  Training Loss:  1.31984  Training Accuracy:  0.966592\n",
      "Epoch:  923  Training Loss:  1.317  Training Accuracy:  0.966592\n",
      "Epoch:  924  Training Loss:  1.31519  Training Accuracy:  0.966651\n",
      "Epoch:  925  Training Loss:  1.31036  Training Accuracy:  0.966825\n",
      "Epoch:  926  Training Loss:  1.30584  Training Accuracy:  0.966884\n",
      "Epoch:  927  Training Loss:  1.30429  Training Accuracy:  0.967\n",
      "Epoch:  928  Training Loss:  1.30276  Training Accuracy:  0.967175\n",
      "Epoch:  929  Training Loss:  1.29759  Training Accuracy:  0.967234\n",
      "Epoch:  930  Training Loss:  1.29489  Training Accuracy:  0.967408\n",
      "Epoch:  931  Training Loss:  1.2921  Training Accuracy:  0.967525\n",
      "Epoch:  932  Training Loss:  1.28872  Training Accuracy:  0.9677\n",
      "Epoch:  933  Training Loss:  1.28478  Training Accuracy:  0.9677\n",
      "Epoch:  934  Training Loss:  1.28238  Training Accuracy:  0.9677\n",
      "Epoch:  935  Training Loss:  1.27888  Training Accuracy:  0.967758\n",
      "Epoch:  936  Training Loss:  1.27612  Training Accuracy:  0.967875\n",
      "Epoch:  937  Training Loss:  1.27362  Training Accuracy:  0.967991\n",
      "Epoch:  938  Training Loss:  1.26934  Training Accuracy:  0.968108\n",
      "Epoch:  939  Training Loss:  1.26638  Training Accuracy:  0.968166\n",
      "Epoch:  940  Training Loss:  1.26369  Training Accuracy:  0.968225\n",
      "Epoch:  941  Training Loss:  1.25878  Training Accuracy:  0.9684\n",
      "Epoch:  942  Training Loss:  1.25776  Training Accuracy:  0.968458\n",
      "Epoch:  943  Training Loss:  1.25572  Training Accuracy:  0.968516\n",
      "Epoch:  944  Training Loss:  1.24972  Training Accuracy:  0.968516\n",
      "Epoch:  945  Training Loss:  1.24688  Training Accuracy:  0.968575\n",
      "Epoch:  946  Training Loss:  1.24515  Training Accuracy:  0.968633\n",
      "Epoch:  947  Training Loss:  1.2415  Training Accuracy:  0.968749\n",
      "Epoch:  948  Training Loss:  1.24041  Training Accuracy:  0.968749\n",
      "Epoch:  949  Training Loss:  1.23671  Training Accuracy:  0.968808\n",
      "Epoch:  950  Training Loss:  1.23303  Training Accuracy:  0.968866\n",
      "Epoch:  951  Training Loss:  1.22922  Training Accuracy:  0.968983\n",
      "Epoch:  952  Training Loss:  1.22809  Training Accuracy:  0.969041\n",
      "Epoch:  953  Training Loss:  1.22267  Training Accuracy:  0.969041\n",
      "Epoch:  954  Training Loss:  1.22118  Training Accuracy:  0.969099\n",
      "Epoch:  955  Training Loss:  1.21723  Training Accuracy:  0.969099\n",
      "Epoch:  956  Training Loss:  1.21584  Training Accuracy:  0.969158\n",
      "Epoch:  957  Training Loss:  1.21148  Training Accuracy:  0.969158\n",
      "Epoch:  958  Training Loss:  1.20911  Training Accuracy:  0.969157\n",
      "Epoch:  959  Training Loss:  1.20494  Training Accuracy:  0.969216\n",
      "Epoch:  960  Training Loss:  1.2024  Training Accuracy:  0.969274\n",
      "Epoch:  961  Training Loss:  1.19994  Training Accuracy:  0.969332\n",
      "Epoch:  962  Training Loss:  1.19775  Training Accuracy:  0.969332\n",
      "Epoch:  963  Training Loss:  1.19231  Training Accuracy:  0.969449\n",
      "Epoch:  964  Training Loss:  1.19047  Training Accuracy:  0.969507\n",
      "Epoch:  965  Training Loss:  1.18751  Training Accuracy:  0.969507\n",
      "Epoch:  966  Training Loss:  1.18513  Training Accuracy:  0.969507\n",
      "Epoch:  967  Training Loss:  1.18027  Training Accuracy:  0.969624\n",
      "Epoch:  968  Training Loss:  1.1789  Training Accuracy:  0.969624\n",
      "Epoch:  969  Training Loss:  1.1749  Training Accuracy:  0.969624\n",
      "Epoch:  970  Training Loss:  1.17201  Training Accuracy:  0.969741\n",
      "Epoch:  971  Training Loss:  1.16988  Training Accuracy:  0.969741\n",
      "Epoch:  972  Training Loss:  1.1659  Training Accuracy:  0.969799\n",
      "Epoch:  973  Training Loss:  1.16266  Training Accuracy:  0.969799\n",
      "Epoch:  974  Training Loss:  1.16148  Training Accuracy:  0.969915\n",
      "Epoch:  975  Training Loss:  1.15794  Training Accuracy:  0.970032\n",
      "Epoch:  976  Training Loss:  1.15532  Training Accuracy:  0.97009\n",
      "Epoch:  977  Training Loss:  1.15199  Training Accuracy:  0.970382\n",
      "Epoch:  978  Training Loss:  1.15016  Training Accuracy:  0.970499\n",
      "Epoch:  979  Training Loss:  1.14616  Training Accuracy:  0.970498\n",
      "Epoch:  980  Training Loss:  1.14522  Training Accuracy:  0.970499\n",
      "Epoch:  981  Training Loss:  1.14108  Training Accuracy:  0.970615\n",
      "Epoch:  982  Training Loss:  1.13731  Training Accuracy:  0.970615\n",
      "Epoch:  983  Training Loss:  1.13698  Training Accuracy:  0.970673\n",
      "Epoch:  984  Training Loss:  1.13292  Training Accuracy:  0.970673\n",
      "Epoch:  985  Training Loss:  1.1314  Training Accuracy:  0.970732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  986  Training Loss:  1.12806  Training Accuracy:  0.970732\n",
      "Epoch:  987  Training Loss:  1.12433  Training Accuracy:  0.970848\n",
      "Epoch:  988  Training Loss:  1.12293  Training Accuracy:  0.970907\n",
      "Epoch:  989  Training Loss:  1.12028  Training Accuracy:  0.970907\n",
      "Epoch:  990  Training Loss:  1.11675  Training Accuracy:  0.970965\n",
      "Epoch:  991  Training Loss:  1.11516  Training Accuracy:  0.970907\n",
      "Epoch:  992  Training Loss:  1.11202  Training Accuracy:  0.970965\n",
      "Epoch:  993  Training Loss:  1.10915  Training Accuracy:  0.971023\n",
      "Epoch:  994  Training Loss:  1.10629  Training Accuracy:  0.97114\n",
      "Epoch:  995  Training Loss:  1.1042  Training Accuracy:  0.971256\n",
      "Epoch:  996  Training Loss:  1.10095  Training Accuracy:  0.971256\n",
      "Epoch:  997  Training Loss:  1.09878  Training Accuracy:  0.971256\n",
      "Epoch:  998  Training Loss:  1.09539  Training Accuracy:  0.971373\n",
      "Epoch:  999  Training Loss:  1.09224  Training Accuracy:  0.971373\n",
      "Testing Accuracy: 0.860847\n"
     ]
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 36 #number of users\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 64\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 10000\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
    "\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
